---

title: Reducing virtual machine pre-emption in virtualized environment
abstract: A method for reducing virtual machine preemption in a virtualized environment is provided. The method includes dispatching a virtual central processing unit (CPU) to run in an emulation mode on a real CPU until the real CPU exits the emulation mode, determining whether the virtual CPU has loaded a wait state, determining whether a remaining time slice of the virtual CPU as a result of the dispatching is below a predefined threshold in an event that the virtual CPU has loaded the wait state and rescheduling the virtual CPU with a full time slice in an event the remaining time slice of the virtual CPU is below the predefined threshold.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09411630&OS=09411630&RS=09411630
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09411630
owner_city: Armonk
owner_country: US
publication_date: 20150914
---
This application is a continuation of U.S. application Ser. No. 14 642 947 which was filed Mar. 10 2015. The entire disclosures of U.S. application Ser. No. 14 642 947 are incorporated herein by reference.

The present invention relates to reducing virtual machine pre emption and more specifically to a method for reducing virtual machine pre emption in a virtualized environment.

Multiprocessing computing systems are often built as a set of similar processors all using the same physical memory. This arrangement is generally referred to as symmetric multiprocessing SMP . An operating system running on such hardware system uses the processors in tandem to run the aggregate work of the system. For example when the operating system is a Linux system the Linux kernel dispatches user execution contexts called processes on the real processors of the system. Similarly when the operating system is a z VM system the z VM kernel dispatches its execution contexts called virtual processors on the real processors of the system. Windows another multiprocessor capable processor operating system operates similarly with its processes and processors. z VM is a registered trademark of International Business Machines Corporation of Armonk N.Y. USA and Windows is a registered trademark of Microsoft Corporation of Redmond Wash. USA. 

In operating systems such as Linux z VM or Windows processor time is consumed either by user application execution contexts or by the operating system kernel. Such consumption is interleaved. The typical behavior is that a user process will first run for a period of time thus accruing processor time. Then the user process will perform an operation or invoke an application programming interface API that will require the operating system kernel to intervene. The kernel intervenes to handle the condition and it runs for a period of time thus accruing processor time. When the kernel finishes handling the condition the kernel returns control to the user application process.

Generally the operating system kernel having to intervene requires the kernel to obtain serialization or mutual exclusion before handling the request. Such protection is required to assure that only one thread or processor at a time reads or updates the accessed kernel data structure s . While lightweight serialization techniques are available in the literature ultimately all such techniques accomplish the same thing namely one at a time access to these key data structure s . The serial access is required to assure system integrity.

According to one embodiment of the present invention a method for reducing virtual machine preemption in a virtualized environment is provided. The method includes dispatching a virtual central processing unit CPU to run in an emulation mode on a real CPU until the real CPU exits the emulation mode determining whether the virtual CPU has loaded a wait state determining whether a remaining time slice of the virtual CPU as a result of the dispatching is below a predefined threshold in an event that the virtual CPU has loaded the wait state and rescheduling the virtual CPU with a full time slice in an event the remaining time slice of the virtual CPU is below the predefined threshold.

According to another embodiment of the present invention a computer program product for reducing virtual machine preemption in a virtualized environment is provided. The computer program product includes a computer readable storage medium having program instructions embodied therewith wherein the computer readable storage medium is not a signal the program instructions readable by a processor to cause the processor to perform a method. The method includes dispatching a virtual central processing unit CPU to run in an emulation mode on a real CPU until the real CPU exits the emulation mode determining whether the virtual CPU has loaded a wait state determining whether a remaining time slice of the virtual CPU as a result of the dispatching is below a predefined threshold in an event that the virtual CPU has loaded the wait state and rescheduling the virtual CPU with a full time slice in an event the remaining time slice of the virtual CPU is below the predefined threshold.

According to yet another embodiment of the present invention a computing system configured to execute a reduction of virtual machine preemption in a virtualized environment is provided. The computing system includes a processor and memory and the processor is configured to dispatch a virtual central processing unit CPU to run in an emulation mode on a real CPU until the real CPU exits the emulation mode to determine whether the virtual CPU has loaded a wait state to determine whether a remaining time slice of the virtual CPU as a result of the dispatching is below a predefined threshold in an event that the virtual CPU has loaded the wait state and to reschedule the virtual CPU with a full time slice in an event the remaining time slice of the virtual CPU is below the predefined threshold.

Additional features and advantages are realized through the techniques of the present invention. Other embodiments and aspects of the invention are described in detail herein and are considered a part of the claimed invention. For a better understanding of the invention with the advantages and the features refer to the description and to the drawings.

A method for reducing virtual machine preemption in a virtualized environment is provided. The method includes dispatching a virtual central processing unit CPU to run in an emulation mode on a real CPU until the real CPU exits the emulation mode determining whether the virtual CPU has loaded a wait state determining whether a remaining time slice of the virtual CPU as a result of the dispatching is below a predefined threshold in an event that the virtual CPU has loaded the wait state and rescheduling the virtual CPU with a full time slice in an event the remaining time slice of the virtual CPU is below the predefined threshold.

An operating system kernel intervening in response to a user process performing an operation or invoking an API typically requires the kernel to obtain serialization or mutual exclusion before handling an application process request. Such protection is required to ensure that only one thread or processor at a time reads or updates an accessed kernel data structure s . Ultimately such techniques accomplish the same thing that is one at a time access to key kernel data structures to ensure system integrity.

Factors in determining the amount of serialization or mutual exclusion a system experiences include the number of processors competing for serialization the tendency of user application processes to require service from the operating system kernel and the frequency at which user applications require service from the operating system kernel. It follows then that if the kernel can reduce the rate of user processes requiring service from the kernel then it can reduce serialization overhead in the kernel.

Before describing various automated processes for facilitating such system efficiency in accordance with one or more aspects of the present invention examples of data processing systems which may utilize the facilities disclosed herein are discussed below with reference to .

SMP server computer system includes a physical SMP server . Physical SMP server includes physical hardware devices such as processors memory and I O adapters . These physical devices are managed by hypervisor . Processors are shared processors and each may be a simultaneous multithreading SMT capable processor that is capable of concurrently executing multiple different threads on the processor.

A virtual server is a proxy for a physical server that has the same capabilities interfaces and state. Virtual servers are created and managed by the hypervisor that resides on physical SMP server computer system . A virtual server appears to be a physical SMP server to its user the operating system middleware and application software that run upon it. SMP server computer system includes one or more virtual servers such as virtual server and virtual server

Each virtual server appears to its software to include its own processor s memory and I O adapter s that are available for the exclusive use of that virtual server. For example virtual server includes a virtual processor virtual memory and virtual I O adapters . Virtual server includes virtual processors virtual memory and virtual I O adapters

Each virtual server supports its own software environment including an operating system middleware and applications. The software environment of each virtual server can be different from the software environment of other virtual servers. For example the operating systems executed by each virtual server may differ from one another.

For example virtual server supports operating system middleware and applications . Virtual server supports operating system middleware and applications . Operating systems and may be the same or different operating systems.

A virtual server is a logical description of a server that defines a server environment that acts to a user as if it were a physical server being accessed and providing information in the same way as a physical server. The virtual processors virtual memory and virtual I O adapters that are defined for each virtual server are logical substitutes for physical processors memory and I O adapters.

Hypervisor manages the mapping between the virtual servers with their virtual processors virtual memory and virtual I O adapters and the physical hardware devices that are selected to implement these virtual devices. For example when a virtual processor is dispatched a physical processor such as one of physical processors is selected by hypervisor to be used to execute and implement that virtual processor. Hypervisor manages the selections of physical devices and their temporary assignment to virtual devices.

Hypervisor services all of the virtual servers virtual processors by interleaving the execution of virtual processors on physical processors . In order to prevent a virtual processor from monopolizing a physical processor the hypervisor sets a dispatch time slice of a particular length of time. During each dispatch time slice the hypervisor will allocate or assign the physical processor to a virtual processor and the physical processor will execute instructions for the virtual processor. This dispatch time slice limits the maximum amount of time a virtual processor may execute on a physical processor before the hypervisor resumes execution on the physical processor and might choose to dispatch a different virtual processor.

The hypervisor is responsible for dynamically creating managing and destroying virtual SMP servers. That is whole virtual processors virtual I O adapters and virtual memory blocks can be removed or added by the hypervisor . The hypervisor is also responsible for dynamic resource allocation managing time sharing of physical resources and altering the physical resource mapped to a processor without involving the operating system. Additionally the hypervisor is able to dedicate physical resources to virtual resources for situations where sharing is not desired. The hypervisor is responsible for managing the addition or removal of physical resources and makes these additions and deletions transparent to the upper level user applications.

Also connected to system bus is memory controller cache which provides an interface to local memory . The I O bus bridge is connected to system bus and provides an interface to the I O bus . The memory controller cache and the I O bus bridge may be integrated as depicted.

Peripheral component interconnect PCI bus bridge is connected to I O bus and provides an interface to PCI local bus . A number of modems may be connected to the PCI bus . Typical PCI bus implementations will support four PCI expansion slots or add in connectors. Communications links to network computers may be provided through modem and network adapter connected to PCI local bus through add in boards.

Network adapter includes a physical layer which conditions analog signals to go out to the network such as for example an Ethernet network for an R45 connector. A media access controller MAC is included within the network adapter is coupled to bus and processes digital network signals. The MAC serves as an interface between bus and physical layer and performs a number of functions involved in the transmission and reception of data packets. For example during the transmission of data the MAC assembles the data to be transmitted into a packet with address and error detection fields. Conversely during the reception of a packet the MAC disassembles the packet and performs address checking and error detection. In addition the MAC typically performs encoding decoding of digital signals transmitted and performs preamble generation removal as well as bit transmission reception.

Additional PCI bus bridges and provide interfaces for additional PCI buses and from which additional modems or network adapters may be supported. In this manner data processing system allows connections to multiple network computers. A memory mapped graphics adapter and hard disk may also be connected to I O bus as depicted either directly or indirectly.

Service processor interrogates system processors memory components and I O bridges to monitor data processing system . Service processor also executes Built In Self Tests BISTs Basic Assurance Tests BATs and memory tests on all elements found by interrogating a system processor memory controller and I O bridge. Any error information for failures detected during the BISTs BATs and memory tests are gathered and reported by service processor .

Those of ordinary skill in the art will appreciate that the hardware depicted in may vary. For example other peripheral devices such as optical disk drives and the like also may be used in addition to or in place of the hardware depicted. The depicted example is not meant to imply architectural limitations with respect to the present invention.

The present invention may be executed within various computers or data processing systems such as those depicted in . As a specific commercially available example a data processing system implementing an early time slice end to minimize virtual processor preemption such as described herein below can be built upon technologies found in IBM s z Architecture product line firmware and systemware as described in the z Architecture Principles of Operation IBM Publication No. SA22 7832 09 10th Edition September 2012 pages 1 1568 . IBM and z Architecture are registered trademarks of International Business Machines Corporation Armonk N.Y. U.S.A 

The hypervisor described above allows the running of multiple instances of operating systems such as Linux CMS z OS etc on a mainframe computer such as the computing system and does this by virtualizing a set of computer resources CPU memory I O devices for each instance of an operating system. So from the perspective of the operating system it appears to have real computer resources though the operating system is actually running in a virtual machine.

Such a virtual machine may have more than one virtual CPU which allows the corresponding operating system to concurrently execute two or more different sets of instructions. These different sets of instructions share access to a common memory for the virtual machine. As is typical in non virtualized environments operating systems and programs which run in a multiprocessor environment with shared memory require serialization to control access to read and write certain areas of memory with one exemplary serialization technique being a spin lock.

The hypervisor is responsible for among other things controlling access to the CPUs of the computing system by the virtual CPUs of the virtual machines and a system administrator may use a share setting to establish a policy for how much of the CPU resource each virtual machine should get when there are insufficient CPU resources to satisfy the needs of the virtual machines. This share setting can assign a priority to each virtual CPU and set forth that each virtual CPU is allowed to run for up to a certain amount of time called the time slice which can default to e.g. 5 ms before this priority is recalculated. The share setting can also be defined such that the time slice can be consumed in several discrete chunks. That is if a virtual CPU loads a wait state or becomes not runnable for some other reason the virtual CPU retains the remainder of its time slice and resumes when it again becomes ready to run. The priority assigned is a function of the share setting for the virtual machine how much CPU the virtual CPU has consumed and a time slice size.

In virtualized environments such as those described above with reference to a spin lock is a serialization technique in which one virtual CPU i.e. one of the virtual processors acquires a lock on a location in virtual memory using a serializing instruction to update the virtual memory location. All of the other virtual CPUs that wish to obtain this lock must enter into a loop waiting condition for the spin lock to enter a state where the other virtual CPUs are allowed to acquire the spin lock. While a virtual CPU holds a spin lock the virtual CPU is permitted to read or modify other virtual memory locations without using serialization instructions. With this arrangement multiprocessor guest use of spin locks can cause inefficient spinning if the hypervisor is not currently running the virtual CPU holding the spin lock. Existing technology can use certain algorithms e.g. SIGP sense running status SRS so that a virtual CPU that is spinning and waiting for the spin lock to become available can interrogate whether the virtual CPU which holds the spin lock is currently running. If not the program which is spinning for the spin lock can use a Diagnose x 9C hereinafter referred to as Diag x 9C instruction to instruct the hypervisor to prefer to run the virtual CPU currently holding the spin lock.

While such a solution may be effective in detecting and reacting to the problem of hypervisor preempting a guest CPU i.e. one of the virtual processors that currently holds a guest lock the solution can also lead to or cause context switches between the guest CPU and the hypervisor as well as to induce overhead in the hypervisor . This is because the Diag x 9C instruction requires hypervisor intervention which generally requires that the hypervisor acquires serialization of its own and incurs the related overhead. Thus as described herein the number of times the hypervisor preempts a guest CPU that holds a guest lock is minimized or reduced by an added ability to declare an early time slice end for a guest CPU in cases where the guest CPU loads a wait state program status word PSW and less than a certain threshold of the time slice remains. Doing so significantly reduces the number of times that a guest lock holding CPU is preempted and allows for more efficient guest CPU operation.

As noted above a problem that can occur in a virtualized environment is that one virtual CPU of a virtual machine might acquire a spin lock but then be preempted by the hypervisor choosing to stop the virtual CPU in the middle of the execution of its instructions and switch to running a different virtual CPU instead. If this happens another virtual CPU belonging to the same virtual machine as the virtual CPU which was preempted while holding the spin lock might find itself spinning for a lock which won t be released for an extended period of time due to the virtual CPU that had acquired the spin lock not currently running. Therefore the SIGP SRS and Diag x 9C options are available for operating systems running in virtual machines to detect that this has happened and to take action to instruct the hypervisor to give preference to running the virtual CPU currently holding the spin lock. This synergy between the hypervisor and the guest operating system has been shown to reduce unproductive spinning for spin locks and improves the performance of workloads but has an associated overhead caused by a break out of emulation mode where the virtual CPU s instructions are being directly executed by the real CPU and requires the hypervisor to acquire global serialization. An improvement on this issue would thus involve the hypervisor dispatching guest CPUs so as to reduce the number of times a guest virtual CPU determines it should use Diag x 9C .

Here one observation about the hypervisor operation is that when a virtual CPU concludes a time slice a new priority it receives will typically be worse than the one it previously held. This worsening of the priority relative to other virtual CPUs which are contending for the real CPUs means that this is a point where a virtual CPU is more likely to be preempted in order to run a different virtual CPU. By waiting to recalculate a virtual CPU s priority until it has consumed a certain amount of real CPU means that most of the time the virtual CPU will be in the middle of executing instructions unless it happens to load a wait state just as its time slice expires. In general the hypervisor will be unaware as to whether this virtual CPU is currently holding any guest spin locks.

Meanwhile an observation about spin lock usage is that operating systems tend to not enter wait states while holding spin locks. Therefore if the hypervisor observes a virtual CPU loading a wait state PSW it can logically conclude that the virtual CPU is not holding any spin locks.

With reference to an enhancement of the computing system described above is to set a threshold where if the virtual CPU enters a wait state and has less than this threshold amount of time remaining in its time slice then the virtual CPU will be rescheduled for a new time slice. This rescheduling takes into account the unused portion of the time slice that was ended early so the guest is not penalized for loading a wait state. The reason a threshold is used rather than doing this every time a virtual CPU loads a wait state is to reduce overhead if a virtual CPU were to frequently cycle between waiting and running.

Another benefit of the enhancement is that virtual CPUs that exit a wait state are guaranteed to have a certain minimum amount of time slice remaining. Prior to this invention there were times when a virtual CPU would load a wait state with a trivial amount of time left in the time slice. Later when that virtual CPU becomes active again it will quickly hit time slice end perhaps just as it has finished populating the memory cache with a significant amount of the guest s memory and risk being preempted. By ensuring that the virtual CPU has a minimum time slice length when it exits a wait state it will assuredly have sufficient time to take advantage of a memory cache should it make repeated references to the same memory locations or to other locations in the same cache line with those prior references.

One consequence of the enhancement is that a virtual CPU that enters a wait state and gets a new time slice and worse priority may be delayed longer to access real CPU than had it not had its priority recalculated. That is by recalculating the priority and possibly pushing it out past other competing virtual CPUs it might now have to wait longer to receive an interrupt to wake it from wait state because we will choose to run other virtual CPUs first.

As shown in the enhancement described above may be embodied as a method of operating the computing system . The method begins with a virtual CPU being ready to run with a full time slice at operation and the virtual CPU being delayed at operation until a real CPU becomes available. Once the real CPU becomes available at operation a control program dispatcher of the hypervisor runs the virtual CPU in emulation mode on the real CPU until the real CPU exits emulation mode and resumes execution in a control program at operation .

Next at operation a first determination is made as to whether or not the virtual CPU has exceeded its time slice. In an event that the virtual CPU has not exceeded its time slice control proceeds to operation at which point the virtual CPU retains the remainder of its current time slice with control then proceeding to operation . At operation a second determination is made as to whether the virtual CPU loaded a wait state. Conversely in an event that the virtual CPU has exceeded its time slice control proceeds to operation at which point the virtual CPU is rescheduled with a full time slice with control then proceeding to operation .

From operation in an event that the virtual CPU has not loaded a wait state control returns to operation with the virtual CPU being delayed until a real CPU becomes available. However in an event that the virtual CPU has loaded a wait state control proceeds to operation at which point a third determination is made as to whether or not the remaining time slice of the virtual CPU is below a predefined threshold e.g. 60 of the 5 ms time slice . Here in an event that the remaining time slice of the virtual CPU is not below the predefined threshold the virtual CPU is marked as idle at operation and control proceeds to operation at which point new work arrives for the virtual CPU. However in an event that the remaining time slice of the virtual CPU is below the predefined threshold the virtual CPU is rescheduled with a full time slice at operation before control proceeds to operation and then . In either case once new work arrives for the virtual CPU at operation control returns to operation with the virtual CPU being delayed until a real CPU becomes available.

In accordance with further embodiments and with reference to the predefined threshold may be set at varying selected or default levels at operation . Then during the course of an operation of the computing system the hypervisor may determine that the predefined threshold needs to be temporarily or permanently increased or decreased based on current performance data of the computing system . That is at operation the hypervisor determines whether current performance data of the computing system indicates that the predefined threshold needs to be increased or decreased and at operation the corresponding increase or decrease is executed. Thereafter the execution of operation see is executed with respect to the new predefined threshold.

In accordance with embodiments it will be understood that an increase in the predefined threshold may correspond to an incidence of for example the current performance data indicating that too many virtual CPUs are exceeding their time slice at operation see rather than loading a wait state PSW within the current threshold. By contrast it will be understood that a decrease in the predefined threshold may correspond to an incidence of the current performance data indicating that an excess of hypervisor overhead is occurring and thus slowing down the overall computing efficiency of the computing system . Of course it is to be understood that other reasons may exist as to why the thresholds may need to be increased or decreased.

With reference now to a computer program product is provided and includes a computer readable storage medium which is not a signal and program instructions for implementing a method for reducing virtual machine preemption in a virtualized environment as described above.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one more other features integers steps operations element components and or groups thereof.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

The flow diagrams depicted herein are just one example. There may be many variations to this diagram or the steps or operations described therein without departing from the spirit of the invention. For instance the steps may be performed in a differing order or steps may be added deleted or modified. All of these variations are considered a part of the claimed invention.

While the preferred embodiment to the invention had been described it will be understood that those skilled in the art both now and in the future may make various improvements and enhancements which fall within the scope of the claims which follow. These claims should be construed to maintain the proper protection for the invention first described.

