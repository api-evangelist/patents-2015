---

title: System for background subtraction with 3D camera
abstract: A system for background image subtraction includes a computing device coupled with a 3D video camera, a processor of the device programmed to receive a video feed from the camera containing images of one or more subject that include depth information. The processor, for an image: segments pixels and corresponding depth information into three different regions including foreground (FG), background (BG), and unclear (UC); categorizes UC pixels as FG or BG using a function that considers the color and background history (BGH) information associated with the UC pixels and the color and BGH information associated with pixels near the UC pixels; examines the pixels marked as FG and applies temporal and spatial filters to smooth boundaries of the FG regions; constructs a new image by overlaying the FG regions on top of a new background; displays a video feed of the new image in a display device; and continually maintains the BGH.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09530044&OS=09530044&RS=09530044
owner: THE BOARD OF TRUSTEES OF THE UNIVERSITY OF ILLINOIS
number: 09530044
owner_city: Urbana
owner_country: US
publication_date: 20150721
---
This application is a continuation of U.S. application Ser. No. 14 174 498 filed Feb. 6 2014 entitled SYSTEM FOR BACKGROUND SUBTRACTION WITH 3D CAMERA which is a continuation of U.S. application Ser. No. 12 871 428 filed Aug. 30 2010 entitled SYSTEM FOR BACKGROUND SUBTRACTION WITH 3D CAMERA now U.S. Pat. No. 8 649 592.

The present disclosure relates generally to 3D image processing and more particularly to a system for background subtraction from images in a video stream using a three dimensional camera.

Background subtraction BGS refers to the ability to remove unwanted background from a live video. Some current video conferencing programs use BGS technology to subtract and replace the background with another prerecorded still or moving background.

There have been several methods developed for BGS using color information only. These methods are either not robust for challenging but common situations such as a moving background and changing lighting or too computationally expensive to be able to run in real time. The recent emergency of depth cameras provides an opportunity to develop robust real time BGS systems using depth information. However due to current hardware limitations some of which are fundamental recorded depth video has poor quality. Notable problems with recorded depth are noisy and instable depth values around object boundaries and the loss of depth values in hair of a person or shiny object areas such as belt buckles. As a result background removal by a simple depth thresholding referred to as Basic BGS herein inherits a lot of annoying visual artifacts. Ideally a robust system will detect and eliminate visual artifacts and reduce jitter and roughness around edges contiguous with a removed background.

By way of introduction the present disclosure relates to a system having a computing device or other computer coupled with a three dimensional 3D camera for subtracting a background BG from a video feed. The system may also replace the removed background with a new background whether a still or video image. The system executes various or all of the steps executable by a background subtraction module disclosed herein to achieve step by step improvement in a robustness and quality of the result. That is the module as executed by a processor eliminates the artifacts noise and the instability of the depth information around edges of one or more target person also referred to as subject herein that is to remains as foreground FG when the background is subtracted.

The system receives a video feed from the 3D camera that contains colored images of the one or more subject that includes depth information. For each colored image extracted from the video feed the system segments colored pixels and corresponding depth information of the images into three different regions including foreground FG background BG and unclear UC . The system may then categorize UC pixels as FG or BG using a function that considers the color and background history BGH information associated with the UC pixels and the color and BGH information associated with pixels near the UC pixels. Pixels that are near other pixels may also be referred to herein as neighbor pixels which are pixels within a predetermined sized window that includes the pixel of reference.

The system may also examine the pixels marked as FG and apply temporal and spatial filters to smooth boundaries of the FG regions. The system may then construct a new image by overlaying the FG regions on top of a new background and display a video feed of the new image in a display device coupled with the computing device. The new background may include still images or video. The FG region that remains preferably includes one or more target subjects that are to be transferred from the processed image to the new image. The system may also continually maintain the BGH to keep it up to date for continued processing across multiple images within a video stream. Additional or different steps are contemplated and explained with reference to the Figures herein.

The 3D camera includes among other components a red green blue RGB sensor an infrared IR sensor and an IR illuminator . The IR illuminator shines light through a lens of the camera and the infrared sensor receives the depth information of the reflected light giving definition to objects within view or in the scene of the camera . The RGB sensor captures the colored pixel information in the scene of the captured video image. The 3D camera may also include synchronization hardware and or software embedded therein to temporally synchronize the IR illuminator the IR sensor and the RGB sensor together. The 3D camera may also include a 3D application programming interface API which may be programmed to receive the depth information Z the brightness B and RGB pixel information of a reflected video image as captured by the 3D camera . The 3D API provides the JO structure and interface programming required to pass this information and to the computer or computing device .

The computing device may further include or be coupled with a background subtraction module stored in memory and executable by a processor a post processing module background subtraction application programming interface API a background history BGH storage part of memory and a display such as a computer screen monitor or a plasma or LCD screen of a television or smart device. Accordingly the computing device may include a desktop laptop smart phone or other mobile or stationary computing device having sufficient processing power to execute the background subtraction module . Where X and Y axes may be referred to herein it is with reference to a two dimensional 2D plane cut through some point along the Z axis.

The computing device may process the background subtraction module with reference to sequential sets of images from the video feed continually in real time. The post processing module may for instance overlay the surviving FG regions onto a new background image whether from a still or a video to create a new image. Sequential real time processing may yield a series of such new images over the top of the new background to create a new video feed having the old Background replaced with the new background. The computer may then display the one or more subject in front of the new background on the display screen for viewing by the user.

During the process of processing sequential colored images from an incoming video feed background history of the sequential colored images may be kept up to date in the BGH storage . This history allows tracking the BG status of pixels in previous frames e.g. whether the pixels were previously categorized as BG. This process and the way the background module incorporates BGH into a decision whether to categorized UC regions as BG will be discussed in more detail below.

At block the system may receive depth and color information of a colored image and perform depth and IR thresholding thus segmenting colored pixels and corresponding depth information of the images into three different regions including foreground FG background BG and unclear UC . The result of the depth and IR thresholding of the image is a region map that shows the three regions pictorially. In block the system may identify and clean FG BG and UC three dimensional connected components. At block the system may enable a user to select a user mode that depends on how close a target subject is located with reference to the camera . At block the system may clean the UC region under a center of mass COM of the target subject. At block the system may warp the image from a depth point of view to a color point of view so that the depth and color information are aligned in 3D space. At block the system may receive RGB color information and clean the remaining UC region with background history BGH . At block the system may interpolate the region map to categorize uncategorized pixels in the RGB image which have unknown depth value and unknown region value as FG or UC depending on region information of neighbor pixels. At block the system may dilate the UC region outward to surrounding pixels that are not in the FG region. At block the system may detect a FG fringe which may include a thin area along the boundaries of the FG edges e.g. those edges between the FG region and the UC region or the BG region. At block the system may update the BGH.

At block the system may clean the UC region using neighbor pixels which step focuses on cleaning along the colored edge of the FG region. At block the system may clean the UC region under the COM of the target subject. At block the system may apply a median filter to the UC region to remove very small UC region then merge the remaining UC regions into the FG regions. At block the system may stabilize and smooth the edges of the FG region s . At block the system may check for reset conditions and if present sets a reset flag. At block the system determines if the reset flag is true and if so resets the flag. At block the system may reset both the BGH and a BG mask of the region map. Processing by the background subtraction module of the system may then continue with another image from the video feed. Sequential processing of colored images may lead to a continuous real time video feed having the BG subtracted therefrom. At block if the reset flag has not been set e.g. it has a false value the system continues operation at block again to continue processing sequential images. The same is true after resetting the BG mask and BGH at block .

As discussed earlier the z as used herein is with reference to a depth value of a particular pixel. A smaller value of z indicates that a pixel is closer to the camera . The term b refers to brightness or in other words the IR intensity collected by the IR sensor. With regards to a particular pixel the higher the intensity b value is the more confidently the system can differentiate the real signal from ambient noise and the more the system can trust the depth value. Values segmented into a FG or BG region are done with high confidence whereas pixels initially segmented into the UC region are pixels with regards to which the system is unsure how to categorize. Accordingly if pixels of a colored image are not categorizable as either FG or BG the pixels may be categorized as UC. Note that pixels in the same region do not need to be adjacent or near each other to be categorized as displayed in .

One set of rules to drive this segmentation of the pixels of an image is for the system to 1 categorize the pixel as foreground FG if a depth thereof is less than a predetermined threshold distance from the camera and a intensity thereof is greater than a predetermined threshold intensity 2 categorize the pixel as unclear UC if a depth thereof is less than the predetermined threshold distance and an intensity thereof is less than the predetermined threshold strength and 3 categorize all other pixels not categorized as FG or UC as background BG . These rules are cast below in Equation 1 which depicts a region map rmap i .

The system in executing block begins by detecting and labeling pixels that are adjacent to each other in the same region and that have similar depth values as region specific connected components. In other words the depth values of two adjacent pixels in the same component is smaller than a predetermined threshold. For instance the system may detect and label FG connected components in 3D space XY plane plus depth Z . The system thus groups pixels that are determined to be connected components for common processing. In the follow expressions D is the depth image p is a pixel R is the region labeled map N p are adjacent pixels around pixel p. A 3D connected component label C C is defined as C p D p N p R p R p ID p D p I

Note that there may be many components in a region however every pixel in the same component includes the same region label. When a UC component is referred to reference is being made to a connected component in the UC region for instance.

A meaningful component is a component whose area is larger than some threshold value . A large UC component however is most likely a meaningless component for example a part of a wall a ceiling or a floor. There are however some small but meaningful UC component such as human hair a belt and a cell phone because these objects tend to absorb infrared IR and are objects that should be kept for further processing. The trick is differentiating between meaningful UC components with other noisy small UC components. In general the meaningful UC components are going to be found adjacent to large meaningful FG components. From these observations the system is programmed to delete components based on the following rules 

Rule 1 Categorize as BG any FG connected component having a cross sectional area less than a predetermined threshold area .

Rule 2 Categorize as BG any UC connected component having a cross sectional area greater than where may be different than .

Rule 3 Categorize as BG any UC connected component having a cross sectional area less than and for which no adjacent component thereof includes a FG connected component having a cross sectional area greater than .

Note that categorizing FG or UC connected components as BG will have the result of ultimately removing those components when the BG is subtracted.

In preparation for image processing under other blocks the system may at or near block find the center of mass COM of large FG connected components such as a target subject and compute the average depth value for each FG component. In other words for a FG component C 

For each of the FG components the system categorizes all the UC pixels that lie under the COM as BG thus cleaning those portions from further processing within the UC region. The follow is example pseudo code for block 

The purpose of block is to help reduce errors caused by unexpected noise around the user and reduce processing time. Simultaneously the system is still able to keep a hair part for instance in the UC region for further processing in subsequent steps that the system may execute which are shown in .

More particularly each point of an image in 2D space can be mapped one to one with a ray in 3D space that goes through the camera position. Given a 2D image plane with basis vectors right arrow over s right arrow over t and a 3D space right arrow over i right arrow over j right arrow over k the 2D point to 3D ray mapping relation is 

Consider a point X in 3D space right arrow over i right arrow over j right arrow over k . Let right arrow over x and right arrow over x be homogeneous coordinates of X in the reference image plane and the desired image plane as shown in . Let P and Pbe mapping matrices of the reference camera and the desired camera. It has been proven that the warping equation between right arrow over x and right arrow over x is 

The BGH is a frame that contains only background BG pixels. The frame is built in an accumulated fashion from the previous frame. At block of for each UC pixel if the BGH is available for the pixel the system compares the RGB value of the pixel with the corresponding one in the BGH. If the BGH of the pixel is unavailable for some reason the system searches for the BG H of a neighbor of the pixel and compares the two. If they match the system sets the pixel to BG. Accordingly one function for categorizing the UC pixels may be based on color dissimilarity between UC pixels and neighbor pixels of the colored image and based on color dissimilarity between the UC pixels and neighbor pixels of the BGH.

Dilation is one of the two basic operators in the area of mathematical morphology the other being erosion. It is typically applied to binary images but there are versions that work on grayscale images. The basic effect of the mathematical morphology operator on a binary image is to gradually enlarge the boundaries of regions of foreground pixels i.e. white pixels typically . Thus areas of foreground pixels grow in size while holes within those regions become smaller.

The purpose of detecting the FG fringe and merging it into the UC region is as follows. Due to the tolerance in registration or warping between the depth information and color image depth resolution interpolation and flickering artifacts the region map edges shown in may not be good cutting edges. In fact there is usually a small mismatch between region map edges and the RGB edges assuming the RGB edges lie close to the region map edges. With the above opening operator the system can narrow down the area along the edge to perform further processing to get a FG BG cut at the RGB edges. This helps significantly reduce processing time.

Block repeats this cleaning step because the system expanded the UC region around the region map edges at block and after block there may still exist some unresolved UC pixels. Because after the next step the UC pixels are set to FG to recover the top part of the hair so block helps reduce errors caused by unexpected noisy edges around the user without affecting the hair part or other reflectance sensitive area .

To execute block the system may remove very small remaining UC connected components also referred to as fragments but keep and smoothen the edges of big UC connected components such as part or all of the hair of a target subject. A 7 7 support window may be applied by the median filter to the UC connected components for instance or another suitably sized window may be applied. Then the UC region may be merged with the FG region. Pseudo code to be executed by the system at block may include 

At block of the system may detect reset conditions which is a block available to the system throughout the background subtraction process. If a reset condition is detected a reset flat is set to true. A reset condition may include but not be limited to the following examples. 1 The system may receive an indication that the camera is shaken which makes the background history BGH useless. 2 The target subject may be too close to the camera which causes a large IR saturation area resulting in a large unknown or background area wherein the system may mistakenly update the BGH. 3 The user may move from the BG to the FG. When the target subject was in the background BG the BGH of corresponding pixels was updated. When the target subject moves into the FG of the scene the BGH behind the target subject is no longer correct and needs to be reset. 4 The system may detect a significant lighting change which also makes the BGH useless. At block of the system may detect whether the reset flag has been set. If it has the system resets the background BG mask and the BGH at block .

In a networked deployment the computer system may operate in the capacity of a server or as a client user computer in a server client user network environment or as a peer computer system in a peer to peer or distributed network environment. The computer system may also be implemented as or incorporated into various devices such as a personal computer or a mobile computing device capable of executing a set of instructions that specify actions to be taken by that machine including and not limited to accessing the Internet or Web through any form of browser. Further each of the systems described may include any collection of sub systems that individually or jointly execute a set or multiple sets of instructions to perform one or more computer functions.

The computer system may include a processor such as a central processing unit CPU and or a graphics processing unit GPU . The Processor may include one or more general processors digital signal processors application specific integrated circuits field programmable gate arrays digital circuits optical circuits analog circuits combinations thereof or other now known or later developed devices for analyzing and processing data. The processor may implement the set of instructions or other software program such as manually programmed or computer generated code for implementing logical functions. The logical function or any system element described. may among other functions process and or convert an analog data source such as an analog electrical audio or video signal or a combination thereof to a digital data source for audio visual purposes or other digital processing purposes such as for compatibility for computer processing.

The computer system may include a memory on a bus for communicating information. Code operable to cause the computer system to perform any of the acts or operations described herein may be stored in the memory . The memory may be a random access memory read only memory programmable memory hard disk drive or any other type of volatile or non volatile memory or storage device.

The computer system may also include a disk or optical drive unit . The disk drive unit may include a computer readable medium in which one or more sets of instructions e.g. software can be embedded. Further the instructions may perform one or more of the operations as described herein. The instructions may reside completely or at least partially within the memory and or within the processor during execution by the computer system . Accordingly the BGH database described above in may be stored in the memory and or the disk u nit .

The memory and the processor also may include computer readable media as discussed above. A computer readable medium computer readable storage medium machine readable medium propagated signal medium and or signal bearing medium may include any device that includes stores communicates propagates or transports software for use by or in connection with an instruction executable system apparatus or device. The machine readable medium may selectively be but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus device or propagation medium.

Additionally the computer system may include an input device such as a keyboard or mouse configured for a user to interact with any of the components of system . It may further include a display such as a liquid crystal display LCD a cathode ray tube CRT or any other display suitable for conveying information. The display may act as an interface for the user to see the functioning of the processor or specifically as an interface with the software stored in the memory or the drive unit .

The computer system may include a communication interface that enables communications via the communications network . The network may include wired networks wireless networks or combinations thereof. The communication interface network may enable communications via any number of communication standards such as 802.11 802.17 802.20 WiMax cellular telephone standards or other communication standards.

Accordingly the method and system may be realized in hardware software or a combination of hardware and software. The method and system may be realized in a centralized fashion in at least one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software may be a general purpose computer system with a computer program that when being loaded and executed controls the computer system such that it carries out the methods described herein. Such a programmed computer may be considered a special purpose computer.

The method and system may also be embedded in a computer program product which includes all the features enabling the implantation of the operations described herein and which when loaded in a computer system is able to carry out these operations. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

The above disclosed subject matter is to be considered illustrative and not restrictive and the appended claims are intended to cover all such modifications enhancements and other embodiments which fall within the true spirit and scope of the present disclosure. Thus to the maximum extent allowed by law the scope of the present embodiments are to be determined by the broadest permissible interpretation of the following claims and their equivalents and shall not be restricted or limited by the foregoing detailed description. While various embodiments have been described it will be apparent to those of ordinary skill in the art that many more embodiments and implementations are possible within the scope of the above detailed description. Accordingly the embodiments are not to be restricted except in light of the attached claims and their equivalents.

