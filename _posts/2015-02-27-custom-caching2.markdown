---

title: Custom caching
abstract: Methods and systems are presented for custom caching. Application threads define caches. The caches may be accessed through multiple index keys, which are mapped to multiple application thread-defined keys. Methods provide for the each index key and each application thread-defined key to be symmetrical. The index keys are used for loading data from one or more data sources into the cache stores on behalf of the application threads. Application threads access the data from the cache store by providing references to the caches and the application-supplied keys. Some data associated with some caches may be shared from the cache store by multiple application threads. Additionally, some caches are exclusively accessed by specific application threads.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09547601&OS=09547601&RS=09547601
owner: PAYPAL, INC.
number: 09547601
owner_city: San Jose
owner_country: US
publication_date: 20150227
---
This application is a continuation of U.S. patent application Ser. No. 13 619 270 filed on Sep. 14 2012 which is co pending with claims the benefit of and is a Continuation of U.S. patent application Ser. No. 13 026 909 filed Feb. 14 2011 which is a Continuation of U.S. patent application Ser. No. 11 965 458 filed on Dec. 27 2007 and which is set to issue as U.S Pat. No. 7 890 537 on Feb. 15 2011 U.S. patent application Ser. No. 11 965 458 is a continuation of and claims the benefit of U.S. patent application Ser. No. 11 007 061 filed on Dec. 8 2004 which is now issued as U.S. Pat. No. 7 406 464 on Jul. 29 2008 and which claims the benefit of U.S. Provisional Application No. 60 528 053 filed on Dec. 8 2003 U.S. Provisional Application No. 60 528 237 filed on Dec. 8 2003 and U.S. Provisional Application No. 60 528 238 filed on Dec. 8 2003 the disclosures of all of these applications are incorporated by reference herein.

Technological advancements have led to more sophisticated and powerful data management systems. Data processing in such systems require efficient handling a large number of data transactions e.g. data reads writes etc. .

The advent of the Internet and the World Wide Web WWW combined with the development of network based commerce system applications have enabled companies to transact business globally in real time. Such network based commerce systems may process large amounts of data transactions from geographically dispersed users. The users may enter transactional requests over communications lines that result in the persistent storage of data in a master information source data store . In several cases back end systems e.g. database servers etc. support the data store and interact with other systems or services to map the data between a persistent or relational form into an object form that is more readily utilized by software applications.

Furthermore software developers typically expend a large amount of time and energy in managing access related to the data store and in managing memory usage within their applications which is used to manipulate data acquired from the data store within the scope of their applications. One tool that developers rely on for improving processing throughput of their applications is a cache manager or cache service. However cache services are typically not customizable and are not extensible by the developers within their applications. This is so because caching services are generally implemented as black boxes having a defined set of generalized and static services or features. As a result data store access and memory management are tasks that developers still largely have to account for within their individual applications. These tasks are time consuming and in some cases not optimal because some developers may not be as adept as other developers in handling these tasks.

In various embodiments extensible caching services and techniques are presented. An application s access to a data store is transparent to the application. An application can customize extend and instantiate its own caching environment. The caching techniques or services utilized within the caching environment of the application provide seamless and transparent access to data residing within the data store while managing the applications memory usage in optimized manners.

In an embodiment the API which interfaces applications to the custom caching service is implemented in an Object Oriented OO fashion. Accordingly application source code includes class files that provide the API and that reference different aspects and features of the custom caching service through objects which are defined in classes and each class has a variety of methods which access the features. The objects abstract and encapsulate different aspects associated with caching. The API provides application specific environments which can custom control and access different features associated with caching these features and control have traditionally been unavailable to developers of applications.

Each application processes on a machine with its own processing space or memory heap. A single application may have a plurality of threads that share the same heap. Each thread may independently interact with the custom caching service. Alternatively collections of threads may jointly interact with the custom caching service. Moreover the custom caching service may synchronize data associated with different applications having different heaps.

The custom cache service provides instances of caches to the application threads. As used herein a cache is intended to include an abstracted and encapsulated version of a cache store. The cache store includes memory locations within heaps of the application threads. The application threads define their caches and access them through the API which interacts with the custom cache service. In this manner control over the actual cache stores is transparent to the application threads and managed by the custom cache service. Accordingly memory management of an application thread s heap can be delegated to the custom cache service while access control is provided through higher level interfaces associated with the API which is directly accessible to the application threads.

In addition to provide customized caching embodiments of the custom caching service reduces the load on data stores. This is so because transactions to the data store are substantially reduced by pre acquiring data frequently used by threads in single transactions to the data store and by sharing frequently used data across multiple threads through a single cache store. As a result threads that perform numerous data store transactions can reduce the number of their transactions to the data store. Moreover because data consumed by the threads are immediately available when used from the cache store the processing throughput of the threads can be substantially improved.

The custom caching service may support two types of caches a global cache and a thread local cache. The threads have control through calls to the API to define the type of cache that best suits the programming environment of the threads. A global cache includes data within its cache store that may be shared across multiple threads of an application. Data of a global cache is typically immutable meaning that the data is typically static to semi static in state and refreshed at intervals based on configurable policies enforced by the custom caching service. A thread local cache includes data within its cache store that is consumed and accessed only by a specific thread that creates the thread local cache. Furthermore a single thread may utilize both a global cache and a thread local cache. Data within a thread local cache may be changed dynamic and refreshed based on instructions issued by the thread that created the thread local cache.

Initially an application uses a call selected from the API to declare an instance of a cache within the programming logic of the application. The program also uses a variety of other calls within its programming logic to drive the processing of the custom caching service. The processing of the custom caching service is activated and executed during the execution of the application.

With this example context the processing of the custom caching service is now described with respect to . Accordingly at the custom caching service receives a request from the application to create a cache. Initially the cache is null having no data. At the custom caching service receives configuration information from the application for the cache. In an embodiment the configuration information is received when the cache is created for the application allowing the constructor of the cache to accept optional parameters from the application which can include the configuration information. The configuration information includes a pointer to a key manager and the key manager has one or more key definitions.

Initially before supplying configuration information the application defines a key manger that includes one or more key definitions. The key definitions may be viewed as an application supplied or pre configured service or object for key types such as an identifier used within a data object to uniquely identify that data object. For example one key type might be a social security number SSN field name of a data object that is a customer record from on online merchant s customer database. Each key definition accepts a value associated with a piece of data assigned to a specific key type. For example a particular key definition may accept the following parameter 555 55 5555 where the particular key definition is associated with a type SSN. In response to the value supplied to the key definitions the key definitions generate a unique key. The key manager also knows how to accept a data object parse it and call each unique key definition associated with the different key types included within that data object. In some embodiments the key manager is a generic object that is simply configured within the application by accessing the API to return a pointer to an instance of a generic key manager.

The configuration information may include a variety of information such as an identifier for a data source and a mechanism such as a search query for acquiring a subset of data from the data source. In an embodiment the custom caching service may provide a library of generic or known data sources having generic instances of key managers. In these embodiments the application can supply configuration information by identifying through the API the generic data sources and or the generic instances of key managers. Thus the application may directly or indirectly supply the configuration information acquired at by the custom caching service.

At the custom caching service loads or populates portions of data into a cache store associated with the newly created cache. Again the cache store is not directly accessible to the application. The application uses calls within its programming logic to define configure and access a cache. The custom caching service manages and maintains the data and the cache store operations to set get write flush and or update the data are provided through the API to the application for instructing the custom caching service to act on the cache store on the application s behalf.

During a load the custom caching service passes a data object or a portion of data to each key definition associated with that object through the key manager that was supplied by the application. The key definitions for each key type parse the data object and extract the necessary values for form their respective keys. This results in one or more keys from the key definitions. These keys are used within an index of the cache store. Each index entry includes a key and a pointer to where the data object physically resides within the cache store.

The portions of the data may be loaded by the custom caching service into the cache store in a variety of manners. In an embodiment at the custom caching service may have received via configuration information an identifier to a data store e.g. database collection of data bases organized as a data warehouse directories electronic files or various combinations of the any of these and a query which defines the portions of data to be loaded into the cache store.

In an embodiment at the custom caching service may be instructed by the application to load all portions of the data into the cache store. This is useful when the cache is a global cache and where the volume associated with the data is not excessive and is manageable within the heap or cache store.

In another embodiment the custom caching service may be instructed by the application to load the portions directly at a given point in time during the execution of the application. This is useful when the cache is a thread local cache and where the volume of the data is perhaps excessive and it is desired to have only a portion in memory at any one time or where the data is going to be changed by the application. It may also be that the application wants be sure that when it instructs the custom caching service to load a specific portion of the data that the portion is going to be as up to date as is possible.

Additionally at some portions of the data may be acquired by the custom caching service from a plurality of perhaps disparate data sources. For example the application may load data from one database for some data types being consumed by the application and simultaneously add data from one or more different databases for other data types being consumed.

Control of data types data sources and loading conditions of data which is loaded into the cache store is delegated to the application through calls to the API within the programming logic of the application. Some control is defined by the operations taken by the application such as when the application uses an API call or operation associated with a global cache implying that the all the data is to be immediately loaded by the custom caching service into the cache store. Other control is more explicitly exercised by the application such as when the application accesses calls or operations associated with a thread local cache indicating that the application is assuming direct control over when data is loaded and or modified within the cache store.

In an embodiment at the custom caching service may iterate the load processing in response to methods associated with the cache. These methods are accessed via the API during the execution of the application. Again this scenario is typically indicative of a thread local cache where the methods are calls or operations of the API and where the application selectively desires to perform loads during certain execution states of the application but may also be used to load a global cache as data is demanded assuming the data in the data source e.g. data store database etc. is typically static or semi static.

At the custom caching service returns to the application a reference to the cache. The application is now free to perform a variety of operations on the data loaded within the cache store via accessing methods associated with the reference to the cache. To do this the application will call specific key definitions within its programming logic and pass values for specific key types. The key definitions return a key that is symmetric with a key produced previously during the load by the custom caching service. The combined application generated key and the reference to the cache provide enough information to the custom caching service for locating the appropriate cache store associated with the cache reference and for locating the appropriate data within the cache store. Again the application supplied key provides a direct location within the cache store index and that location includes a pointer to the physical location of the data desired by the application.

Once the portions of the data are indexed and loaded within the cache store the custom caching service may also at refresh flush or update the data or selective portions of the data within the cache store or to a data store such as when an update is being performed. A variety of mechanisms may drive the custom caching service to perform these types of operations such as when intervals for performing these operations are configured into the custom caching service or defined within a policy that is dynamically enforced by the custom caching service. Other techniques may include performing the operations after receiving a specific instruction from the application performing the operations after receiving an event trigger raised by a data store when a field or table of the data store is modified and or performing the operations after receiving an event raised by a global administrator service that communicates with the custom caching service.

Some of the processing described above prior to may have or have not occurred before step . For example data may not have been loaded to the cache store and index keys to portions of the data may not have been established. In this manner the number and arrangement of the processing blocks of are not intended to impart any particular sequence.

As stated above and as indicated at in some embodiments where the cache is a global cache the custom caching service may share the cache store and the data loaded therein with other threads of the application. The threads acquire the reference to the cache from the application or parent thread and may access the cache store through the reference as desired.

In still another embodiment also at the custom caching service may synchronize loaded data within the cache store associated with the application s heap with other loaded data within other cache stores associated with different applications having different programming heaps. This may be useful when the same data acquired from the same data source is loaded within two different cache stores and there is a desire to maintain synchronicity of the data state.

As will be demonstrated below the key service provides novel features that permit data stored within cache stores to be uniquely associated grouped indexed shared and retrieved from the cache store. In this manner the key service extends the functionality of the custom caching service of providing added flexibility to applications. Again some features presented herein and below may be accessed via the API of the custom caching service and other features may be internal to the custom caching service.

At the key service acquires a key manager for a particular cache. Acquisition may be achieved via a pointer supplied by the application to the key service. The key manager includes a variety of key definitions. The key definitions provide symmetrical keys between the application and the key service. Each key definition operates on a particular key type associated with a data object. A key definition returns a symmetric version of a key in response to receive a particular value for a particular key type. The keys returned from the key definition provide name spacing that delineates between different key types for the same data object type and delineates between different key types for different data object types so that keys with equivalent data which pertain to different values and or objects can be delineated between the index. This is typically achieved through different actual class types for the key types which are used during index lookup. The key definition may be supplied the direct data or may parse the data from an instance of an object that it relates to in order to create a key object.

At the key service populates a cache store by using unique key definitions acquired from the key manager or processed through the key manager. Moreover for each portion of data received in a data object for loading an appropriate key definition is processed. In an embodiment at the key service may initiate the population processing in response to an instruction received from the application. In this manner the application may perform on demand loading such as when the application is using a cache associated with a thread local cache or loading a global cache based on demand for data.

At a key is received from an appropriate key definition for each portion of the data during the population at . The key is consumed by the key service for acquiring a specific unique location within an index associated with the cache store. At that key is stored in the index along with a reference to the data associated with the portion of the data that is being processed. The reference is stored within an entry value of the index the location of the entry value within the index is acquired via the key.

The processing at and is iterated at for identified key types and key definitions associated with the data. Thus a single piece of data or single data object may have a plurality of keys and entries housed in the index of the cache store and each of those keys include the reference to the physical location of the data object within the cache store.

At the storing of the reference may be modified to accommodate collection keys. A collection key is provided by a special type of a key definition. A collection key is a logical grouping of portions of the data associated with same key type. For example suppose that the application defined via a key definition that each set of data returned from the announcement data store where announcements associated with the same ISP be grouped into buckets and the buckets associated with a collection key. In the example maybe 100 announcement records are grouped into a first bucket with a first collection key because they all have an ISP value of AOL and another 10 customer records are grouped into a second bucket having a second collection key because they all have an ISP value of Roadrunner. The groupings that define a collection key may be established by the application through the API using key definitions operations or calls. One appreciates that this is a powerful feature since it can among other things preorder data within the cache and group data into buckets such that the order and buckets can be later accessed without having to perform an additional operation with the application when the order or buckets are needed.

To accommodate this novel collection key there may be multiple portions of the data that have the same key to the same entry value within the index. The key service knows that the key definition used to acquire the keys was a collection key definition. Therefore the key service will handle the situation during subsequent iterations of the key service when it encounters a portion of data that indexes to an already populated entry value which already has a reference included therein. Thus at the key service will acquire the pre existing reference which will typically be a list type structure and add the pointer of the new data object to the list. If an entry does not already exist for a collection key entry then it will create a new list structure and add the new data object to this list and put the collection key and list into the cache store. Thus the duplicate references are populated as a plurality of entries within the data structure. In this manner the application has logically grouped portions of its data loaded within the cache store into collections for future ease of reference and direct access.

In some embodiments an application may try to access data not yet loaded within the cache store. In these cases the key service may return a null reference indicating that an entry does not exist or generate an exception. At this point the application can attempt to acquire the data from a data store and then load the data into the cache store through methods associated with the cache reference and then properly acquire the data. Additionally if the data is not found in the data store then the application can put a marker object into the cache for this particular key to indicate that an object does not exist for this key so that subsequent thread requests will not bother calling the data store to look for the data that does not exist thus reducing load on the data store. Lazy loading may occur with both global cache types and thread local cache types.

Once data is loaded into the cache store the application can retrieve selective portions of the loaded data by passing the reference to the cache and a selective key within an API call or operation to acquire the data from the cache store. This is achieved by using the selective key to locate the entry value within the index of the cache store acquiring the populated reference and retrieving the selective portions of the loaded data from a physical location within the cache store identified by the reference.

In an embodiment the cache may also receive from a different thread of the application a specific key into the index along with a reference to the cache. In response to this the key service returns the data acquired by traversing the reference located at the entry value location within the index pointed to by the specific key which was supplied by the different thread.

In yet another embodiment at a write or update operation may be subsequently received from either the application or a different thread of the application. The operations include a reference to the cache a new piece of data and a key. The write operation at acquires the appropriate index entry value location pointed to by the key and acquires the reference to the location of the data within the cache store and replaces the data within the cache store with a new piece of data. This update is realized by all key entries for this object in the index as they hold a reference to a cache entry holder object which has a reference to the data object which is what is actually updated thus updating the data object reference for one key cache entry updates it for all key entries for that object. A write occurs with a cache reference associated with a thread local cache. An update operation acquires the matching location in the same manner but uses other services to write the matching data in the cache store to one or more data sources or data stores or is handled externally at the discretion of the application.

At the thread local cache service receives a request from a thread to obtain a reference to its thread local cache. The thread local cache instance is created for a thread if one does not already exist within the thread local cache service for it. The thread local cache instance receives an instruction from the application thread to begin caching data objects that the thread encounters during its processing. This instructs the internals of the thread local cache instance to enable operations or methods on this thread cache instance to read and put objects in the thread local cache s cache store. It is appreciated that other techniques are permissible to obtain an instance of a thread local cache. For instance in an OO implementation the cache may be utilize a specific thread local cache constructor may pass specific parameters or identifiers to a generic cache constructor or simply issue a begin or some other type of operation that identifies a generic cache as being associated with a thread local cache.

At the thread local cache acquires a key manager having key definitions from an application thread as it visits the objects that handle the loading of specific object types into the thread local cache. Techniques for generating and providing key definitions were discussed in detail with respect to the key service of . However in the case of the thread local cache the key manager or a specific key definition are received by the thread local cache when the thread wants put an object into the thread local cache. Thus key managers and their key definitions are discovered during runtime as objects are put into a thread local cache.

Additionally in some embodiments at some key definitions received from the application thread by the thread local cache may be for different object types. This means that multiple types of data objects from multiple data sources may be defined within the thread s thread local cache such the key definitions are significantly different from one another and are specific to the different object types.

At the thread instructs the thread local cache to selectively load portions of data to the cache store via API calls on thread local cache. At this point the thread local cache service interactively and in a piecemeal fashion as directed by the thread assembles the desired data into the cache store using the key manager and its key definitions in manners discussed above and in the process constructs an index into the cache store for acquiring the desired data. In addition at the key definitions supplied at may define a number of the key definitions as collection keys. Examples of a collection keys were provided above with respect to the key service of .

At some point during the execution life cycle of the thread it may reference at some specific portions of the loaded data within the cache store. A reference may be made with a variety of API operations or calls for such things as by way of example only getting flushing refreshing reading writing and or updating. The API operations or calls include parameters associated with a reference to the thread local cache and keys. The keys are symmetrical between the thread and the thread local cache service because both them used the same key definitions to acquire the keys. The keys possessed by the thread local cache service provide locations within an index of the cache store and the entry values identified at those locations includes references the actual locations within the cache store where the desired data resides.

For example at the thread may utilize API calls or operations to modify some portions of the loaded data or to update data sources associated with some portions of the loaded data housed and managed by the thread local cache service via the thread local cache reference. To do this the thread local cache service utilizes the reference to the thread local cache to identify the specific cache store and the keys into index locations. Armed with this information the thread local cache can use the keys to acquire the desired portions of data for purposes of retrieving flushing writing new data updating the existing data to one or more data stores etc.

It should be noted that the thread local cache service may manage a plurality of thread local caches for a variety of threads. In a similar manner the custom caching service and the key service of the may also manage a plurality of thread local caches for a variety of threads and or a plurality of global caches for a variety of applications.

The custom cache system abstracts cache stores memory heap space to a series of higher level references referred to as caches. An API associated with the caches permit applications or threads of an application to custom control and insert remove modify and or write data having a variety of data types to and from the cache stores via references to the caches and via calls to methods or operations of the API. The backend processing of the API calls drive some of the processing associated with the custom cache system . Other processing of the custom cache system may be driven by policies or administrator interfaces communicating with the thread local cache service.

The components of the custom cache system are presented for purposes of illustration only and are therefore not intended to limit embodiments of the custom cache system . Accordingly some components may be omitted some other components may be added and some collections of the components may be combined into single components. All such arrangements are intended to fall within the scope of the embodiments presented here if such arrangements provide for the novel techniques for custom caching as described herein.

In one implementation the custom cache system is implemented as an OO system having an OO API which is accessible to the applications and the threads of an application. The objects may be defined within a programming language as classes having public methods accessible to the applications and threads and protected methods accessible to certain objects defined within the custom cache system . The classes may inherit from one another and may permit polymorphism. Of course it is recognized that any software implementation of the custom cache system is intended to fall within the scope of the embodiments presented herein thus the custom cache system may also be implemented in other manners such as a series of services defined with traditional structured programming and the like.

The custom cache system includes a key manager A key definitions B a global cache manager B a thread local cache manager C a cache entry loader A a cache store B and a cache loader . In some embodiments the custom cache system also includes a cache maintainer a cache eviction policy and or a cache statistic collector . The operations and features of the custom cache system operate on and through instances of caches A. Each of these will now be discussed in turn.

The key manager A is responsible for managing key definitions B. The key manager A and the key definitions B and configured and or created through API operations processed by an application or thread in connection with an instance of a cache A. The key definitions B know how to parse different types of data or data objects to acquire values to generate keys. Each key definition B is associated with a specific key type and when supplied a value for data associated with a specific key type generates a symmetric key. The key is symmetric because both the application and the cache entry loader A can produce identical keys when using the same key type and the same value. Some key definitions may permit the creation of keys associated with logical collections of data. Techniques for achieving this were discussed in detail above.

The cache managers B and C create and manage instances of caches A with cache stores B on behalf of the application threads. The cache managers B and C return references to created cache instances A to the application threads. The application threads use the cache instance A reference to perform operations via its API. A cache instance A may be a global cache that is shared between threads or a cache may be a thread local cache that is exclusively associated and accessed by a particular thread. The cache managers B and C interact with the other components of the custom cache system via the references to specific cache instances A for purposes of managing the cache stores B and maintaining their integrity.

The cache entry loader A is provided references to the key manager A and cache loader by the cache instance A through which it accesses the key manager A and its key definitions B that it contains. The cache entry loader A holds the reference to the cache store B. The cache loader is responsible for determining where and how to acquire data from a variety of data sources where the data is potentially associated with a variety of disparate data types or objects. Thus the data access layer is abstracted and hidden from the application threads. This makes the application logic that interacts with the cache instance A more portable and less coupled to the specific data sources and their methods for acquiring data such that should a data source change its interface the application threads would still be operational because it is the cache loader that manages data stores and their interfaces.

The cache loader may cause all data identified by an application to be loaded into the cache store B when initially called such as when the cache instance A referenced by the application thread is a global shared cache. The cache instance A makes this happen via a call to the cache entry loader A and the cache entry loader A interacts directly with the cache loader to obtain data that the cache loader determines should be placed in the cache store B and then the cache entry loader A subsequently loads the data into the cache store B. Alternatively data may be loaded into the cache store B interactively or in a piecemeal fashion as instructed by the application thread via API calls on the cache instance s A public interface issued by the application thread such as when the cache instance A referenced by the application thread is a thread local cache or is a lazy loaded global cache.

The cache loader acquires data from the data sources associated with the data types or objects and causes units records or portions of the data acquired from the data sources to be indirectly passed on to the cache entry loader .

The cache entry loader A interacts with the key manager A to acquire symmetric keys for data being loaded. The keys identify locations within an index being maintained for the cache store B. Each location has a reference to specific data loaded in the cache store B. In some instances a location may include a pointer to a data structure where the data structure includes references to multiple data within the cache store B. This is done when a key definition B provided collection keys for some of the data as was discussed above in detail.

The cache instances A interacts with API calls initiated by the application threads to drive the loading retrieval and other potentially volatile operations e.g. writing flushing refreshing etc. against the data included within the cache stores B. The volatile operations by default are constrained on global cache instances A and not on thread local cache instances A and are enforced by the cache instances A should one of these operations be attempted by an application thread against a global shared cache type with this default configuration. The operations initiated by the application threads from the API can include a variety of parameters such as a reference to a cache a key a key manager and a data object. Moreover if the operation dictates the cache instance A may also consume a key for purposes of looking up in the cache store B a specific piece of data identified by the application thread via the key. These operations are driven via the cache instances A interfaces.

In an embodiment the custom cache system may also include a cache maintainer . The cache maintainer interacts directly with the cache instance A and is responsible for managing the state of the data in the cache instance A via configured refresh operations on global caches A. The cache maintainer will initiate a refresh sequence using a background thread based on configuration such as time. During this refresh sequence it will call the cache instance A which will subsequently call the cache loader which will determine if and what portion of the cache should be flushed and or updated. The cache instance A will then create a new cache entry loader A instance and cache store B instance and begin to populate it with the data provided by the cache loader . Once the new cache store B has been successfully loaded the old cache entry loader A reference will be replaced with a reference to the new cache entry loader A instance containing the new cache store B. This approach to refresh allows for the application threads to continue to use the cache while the new cache entry loader A and cache store B are created and it allows for these objects to be instantaneously replaced thus allowing for uninterrupted access to cache data for the application threads even during a refresh operation. Again a global cache also known as a shared cache is a cache instance A with an associated cache store B where data indexed therein is shared between pluralities of threads of a same application.

In still another embodiment the custom cache system may also include a cache eviction policy . The cache eviction policy may be consumed and evaluated by the cache instance A of the cache store B for purposes of determining the conditions and or events under which portions of the data within the cache stores B are to be removed from the cache stores B and or updated to one or more data sources associated with the portions of the data. In some cases the cache eviction policy may be an electronic data structure or file such as an extensible markup language XML formatted file. In other cases the cache eviction policy is dynamically communicated from a global administrator service.

In still another embodiment the custom cache system may also include a cache statistics collector . The cache statistics collector collects statistical data from the caches regarding usage and efficiency and reports the same to a configurable statistic collection service.

The custom cache system includes an API and a cache creator . In an embodiment the custom cache system may also include a key acquirer and or a cache synchronizer . Each of these will now be addressed in turn.

The API includes a suite of definitions operations calls or methods which are available to a developer to include in his her code associated with an application or a thread of an application. The definitions define abstracted views of cache stores via cache definitions. Each cache definition includes a set of operations for performing some operation against the cache store via references to a cache.

A cache can be of two types a global shared cache and a thread local cache. Other operations may permit cache configurations to acquire data of application defined types from a plurality of data sources. Still more operations may permit the communication of key managers associated with key definitions to be communicated. Some key definitions may be associated with collective keys. In yet other operations gets puts sets writes refreshes flushes updates and the like permit volatile and non volatile operations to be performed against data housed in the cache stores and referenced by the cache.

The front end of the API interfaces with applications or threads of an application. Conversely the back end of the API interfaces or communicates with the cache creator and optionally the key acquirer and or the cache synchronizer .

The cache creator handles API operations executed by the application or threads of the application. In an embodiment the cache creator is implemented within software on a machine accessible and readable medium as a means for creating and managing customized caches. Examples of objects services and systems for achieving this were discussed above with respect to the methods and and with respect to the system . Accordingly these descriptions associated with these methods and the system are incorporated by reference herein as the means for creating and managing customized caches. Some customized caches are shared between the same threads of an application while other ones of the customized caches are managed by specific threads to the exclusion of other ones of the threads.

In an embodiment the some features of the cache creator may be delegated to a key acquirer . The key acquirer manages keys for the customized caches. In an embodiment the key acquirer is implemented in software within a machine accessible and readable medium as a means for acquiring keys. The technique and features of managing and acquiring keys for customized caches where presented above with respect to the methods and the system . Accordingly the means for acquiring keys includes the descriptions and techniques presented above with respect to key creation key definitions and key management of the methods and the system . In an embodiment the means for acquiring keys provides a symmetrical key between both the application or threads and the customized caches.

In yet another embodiment the custom cache system includes a cache synchronizer . The cache synchronizer permits data associated with disparate cache stores disparate heaps to be synchronized with one another. In an embodiment the cache synchronizer is implemented in software within a machine accessible and readable medium as a means for synchronizing data between numbers of disparate customized caches via their corresponding cache stores. The means for synchronizing data may be defined within policies communicated to the affected cache stores. Alternatively the means for synchronizing data may be dynamically communicated by a global administrator service at predefined or dynamically instructed intervals.

It has now been demonstrated how custom caching can be achieved for purposes of improving processing throughput of an application and streamlining development time associated with developing the application. Additionally the techniques presented herein also reduce the number of data store transactions that may be needed to process an application to completion. Thus it is apparent that a significant reduction in data store transactions can be achieved when applications that access the data store employ the API and techniques presented herein.

The network based commerce system includes one or more of a number of types of front end servers that may each include at least one Dynamic Link Library DLL to provide selected functionality. The system may include page servers that deliver web pages e.g. mark up language documents picture servers that dynamically deliver images to be displayed within Web pages listing servers that facilitate category based browsing of listings search servers that handle search requests to the system and facilitate keyword based browsing of listings and ISAPI servers that provide an intelligent interface to a back end of the system . The system may also include e mail servers that provide among other things automated e mail communications to users of the network based commerce system .

In an embodiment one or more administrative application functions facilitate monitoring maintaining and managing the system . One or more API servers may provide a set of API functions for querying and writing to the network based commerce system . APIs may be called through the HTTP transport protocol. In an embodiment information is sent and received using a standard XML data format. Applications utilized to interact e.g. upload transaction listings review transaction listings manage transaction listings etc. with the network based commerce system may be designed to use the APIs. Such applications may be in an HTML form or be a CGI program written in C Perl Pascal or any other programming language.

The API servers page servers picture servers ISAPI servers search servers e mail servers and a database engine server e.g. provided by one or more of the application servers may individually or in combination act as a communication engine to facilitate communications between for example a client machine and the network based commerce system act as a transaction engine to facilitate transactions between for example the client machine and the network based commerce system and act as a display engine to facilitate the display of listings on for example the client machine .

The back end servers may include a database engine server a search index server and a credit card database server each of which maintains and facilitates access to a respective database.

In an embodiment the network based commerce system is accessed by a client program such as for example a browser e.g. the Internet Explorer distributed by Microsoft Corp. of Redmond Washington that executes on the client machine and accesses the network based commerce system via a network such as for example the Internet . Other examples of networks that a client may utilize to access the network based commerce system include a wide area network WAN a local area network LAN a wireless network e.g. a cellular network the Public Switched Telephone Network PSTN network or the like. The client program that executes on the client machine may also communicate with the network based commerce system via the API servers .

The database may in an embodiment be implemented as a relational database and includes a number of tables having entries or records that are linked by indices and keys. In an alternative embodiment the database may be implemented as collection of objects in an object oriented database as discussed by way of example in more detail below.

The database includes a user table that contains a record for each user of the network based commerce system . An example record for each user is shown in . A user may operate as a seller a buyer or both when utilizing the network based commerce system . The database also includes listings tables see that may be linked to the user table . The listings tables may include a seller listings table and a bidder listings table . A user record in the user table may be linked to multiple listings that are being or have been listed or offered for sale via the network based commerce system . In an embodiment a link indicates whether the user is a seller or a bidder or buyer with respect to listings for which records exist within the listings tables .

The database also includes one or more divisions in the form of categories provided in category tables . Each record within the category table may describe a respective category. In an embodiment listings provided by the system are arranged in the categories. These categories may be navigable by a user of the network based commerce system to locate listings in specific categories. Thus categories provide a mechanism to locate listings that may be browsed. In addition or instead an alphanumeric search mechanism may be provided by the search servers to allow a user to search for specific listings using search terms or phrases.

In an embodiment the category table describes multiple hierarchical category data structures and includes multiple category records each of which describes the context of a particular category within the multiple hierarchical category structures. For example the category table may describe a number of real or actual categories to which listing records within the listings tables may be linked.

The database is also shown to include one or more attributes tables . Each record within the attributes table describes a respective attribute associated with a listing. In an embodiment the attributes table describes multiple hierarchical attribute data structures and includes multiple attribute records each of which describes the context of a particular attribute within the multiple hierarchical attribute structures. For example the attributes table may describe a number of real or actual attributes to which listing records within the listings tables may be linked. Also the attributes table may describe a number of real or actual attributes to which categories within the category table may be linked.

The database may also include a note table populated with note records that may be linked to one or more listing records within the listings tables and or to one or more user records within the user table . Each note record within the note table may include among other things a comment description history or other information pertaining to a listing being offered via the network based commerce system to a user of the network based commerce system . The database may also include a targeted site table populated with targeted site records that may be linked to one or more listing records within the listings tables and or to one or more user records within the user table .

A number of other example tables may also be linked to the user table namely a user past aliases table a feedback table a feedback details table a bids table an accounts table and an account balances table . In an embodiment the database also includes a batch table a batch listings table and a listings wait table . The data may be partitioned across multiple database instances and queries may have to be executed against multiple database instances and query results may need to be aggregated.

The computer system includes a processor a main memory and a static memory which communicate with each other via a bus . The computer system may further include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT . The computer system also includes an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse a disk drive unit a signal generation device e.g. a speaker and a network interface device to interface the computer system to a network .

The disk drive unit includes a machine readable medium on which is stored a set of instructions or software embodying any one or all of the methodologies described herein. The software is also shown to reside completely or at least partially within the main memory and or within the processor . The software may further be transmitted or received via the network interface device .

For the purposes of this specification the term machine readable medium shall be taken to include any medium which is capable of storing or encoding a sequence of instructions for execution by the machine and that cause the machine to perform any one of the methodologies of the present invention. The term machine readable medium shall accordingly be taken to included but not be limited to solid state memories optical and magnetic disks and carrier wave signals. Further while the software is shown in to reside within a single device it will be appreciated that the software could be distributed across multiple machines or storage media which may include the machine readable medium.

It should also be noted that in some embodiment the methods and may be implemented on a machine accessible medium having instructions. The instructions when executed by a machine perform the methods and . The medium may be removable media that is interfaced to the machine. Alternatively the medium may be fixed media that is an integral part of the machine. In this manner the instructions of the methods and may be pressed on removable media and uploaded to a machine preloaded within a machine downloaded on a machine from a peripheral device or downloaded from a remote machine over a network to a different machine.

The above description is illustrative and not restrictive. Many other embodiments will be apparent to those of ordinary skill in the art upon reviewing the above description. The scope of embodiments should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

The Abstract is provided to comply with 37 C.F.R. 1.72 b and will allow the reader to quickly ascertain the nature and gist of the technical disclosure. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims.

In the foregoing description of the embodiments various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting that the claimed embodiments have more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate example embodiment.

