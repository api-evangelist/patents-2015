---

title: Methods and systems for visual replay of predictive engine performance
abstract: Disclosed are methods and systems for creating, evaluating, and tuning a predictive engine for machine learning, and for replaying performance of the predictive engine, including steps to deploy a variant of the predictive engine; receive a plurality of queries to the predictive engine variant from an end-user device; apply a query segment filter to the plurality of queries to generate a replay group of queries, wherein for each query in the replay group, the engine variant generates a corresponding predicted result, receives a corresponding actual result, and computes a corresponding prediction score; group queries in the replay group into one or more segments; and generate, as a replay of the performance of the engine variant, an accumulated prediction score over each of the one or more query segments of the replay group by applying an accumulation function. The present invention substantially improves on systems that utilize predictive engines.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09269095&OS=09269095&RS=09269095
owner: TappingStone Inc.
number: 09269095
owner_city: Walnut
owner_country: US
publication_date: 20150711
---
This application claims the benefit of priority from U.S. Ser. No. 14 684 418 filed on Apr. 12 2015 and entitled METHODS AND SYSTEMS FOR PREDICTIVE ENGINE EVALUATION TUNING AND REPLAY OF ENGINE PERFORMANCE which issued as U.S. Pat. No. 9 135 559 on Sep. 15 2015 and also is a non provisional of and claims the benefit of provisional application having U.S. Ser. No. 62 136 311 filed on Mar. 20 2015 and entitled METHODS AND SYSTEMS FOR PREDICTIVE ENGINE EVALUATION AND TUNING the entire disclosures of both of which are hereby incorporated by reference in their entireties herein.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. This patent document may show and or describe matter which is or may become tradedress of the owner. The copyright and tradedress owner has no objection to the facsimile reproduction by anyone of the patent disclosure as it appears in the U.S. Patent and Trademark Office files or records but otherwise reserves all copyright and tradedress rights whatsoever.

Embodiments of the present invention broadly relate to systems and methods for building and deploying machine learning systems for predictive analytics. More particularly embodiments of the present invention relate to creating evaluating tuning predictive engines in production and replaying the performance of predictive engines for predictive engine design and analysis. A predictive engine includes one or more predictive models that can be trained on collected data for predicting future user behaviors future events or other desired information. Such prediction results are useful in various business settings such as in marketing and sales. Embodiments of the present invention enable customization of engine components targeted for specific business needs allow systematic evaluation and tuning of multiple engines or engine variants and provide ways of replaying engine performances during or after the evaluation and tuning processes.

The statements in this section may serve as a background to help understand the invention and its application and uses but may not constitute prior art.

Machine learning systems analyze data and establish models to make predictions and decisions. Examples of machine learning tasks include classification regression and clustering. A predictive engine is a machine learning system that typically includes a data processing framework and one or more algorithms trained and configured based on collections of data. Such predictive engines are deployed to serve prediction results upon request. A simple example is a recommendation engine for suggesting a certain number of products to a customer based on pricing product availabilities product similarities current sales strategy and other factors. Such recommendations can also be personalized by taking into account user purchase history browsing history geographical location or other user preferences or settings. Some existing tools used for building machine learning systems include APACHE SPARK MLLIB MAHOUT SCIKIT LEARN and R.

Recently the advent of big data analytics has sparked more interest in the design of machine learning systems and smart applications. However even with the wide availability of processing frameworks algorithm libraries and data storage systems various issues exist in bringing machine learning applications from prototyping into production. In addition to data integration and system scalability real time deployment of predictive engines in a possibly distributed environment requires dynamic query responses live model update with new data inclusion of business logics and most importantly intelligent and possibly live evaluation and tuning of predictive engines to update the underlying predictive models or algorithms to generate new engine variants. In addition existing tools for building machine learning systems often provide encapsulated solutions. Such encapsulations while facilitating fast integration into deployment platforms and systems make it difficult to identify causes for inaccurate prediction results. It is also difficult to extensively track sequences of events that trigger particular prediction results.

Therefore in view of the aforementioned difficulties there is an unsolved need to make it easy and efficient for developers and data scientists to create deploy evaluate and tune machine learning systems.

The inventors of the present invention have created methods and systems for creating deploying evaluating and tuning predictive engines for machine learning applications and for replaying the performances of such predictive engines.

More specifically in one aspect one embodiment of the present invention is a method for replaying the performance of a predictive engine the method including steps to deploy a variant of the predictive engine based on an engine parameter set wherein the engine parameter set specifies a plurality of algorithms utilized by the engine variant a plurality of algorithm parameters and or other information to specify the engine variant its components and the components parameters the deployed engine variant listens to and receives queries from an end user device wherein each of the received query contains a timestamp. The method further includes steps to apply a query segment filter to received queries to generate a replay group for each query in the replay group the engine variant generates a corresponding predicted result receives a corresponding actual result which could be a variety of actual events user actions and or subsequent user behaviors at any time in the future and computes a corresponding prediction score based on the predicted result and the actual result. The method further includes steps to group queries in the replay group into one or more segments according to timestamps associated with each query and to generate an accumulated prediction score over each query segment of the replay group by applying an accumulation function. The accumulated prediction scores are a replay of the performance of the engine variant. In some embodiments the replay of the engine performance comprises a graphical display of the accumulated prediction scores.

In some embodiments of the present invention the query segment filter is an engine variant filter a user attribute filter an item attribute filter or a query attribute filter. In some embodiments the query segment filter is determined manually by an operator. In some embodiments the accumulation function is a weighted summation.

In some embodiments of the present invention the engine parameter set is generated manually by the operator. In other embodiments the engine parameter set is determined automatically by the system using one or more heuristics rules or other procedures. In yet other embodiments the engine parameter set may be determined automatically and later edited or modified by the operator before the engine variant is deployed.

In some embodiments the actual results comprise a sequence of user responses to the predicted results. In other embodiments the actual results are received from a datastore. In other embodiments the actual results are simulated. In yet other embodiments the actual results are correct values actual events user actions and or subsequent end user behaviors depending on the uses of the predictive engine.

In some embodiments the method for replaying the performance of the predictive engine further comprises steps to apply another query segment filter to the received queries to generate another replay group. For each query in the replay group the engine variant generates a corresponding predicted result receives a corresponding actual result and computes a corresponding prediction score based on the corresponding predicted result and actual result. In addition the method groups queries in the second replay group into one or more segments according to timestamps associated with the queries and generates accumulated prediction scores over each of the query segments of the second replay group by applying the accumulation function.

In another aspect the present invention is a non transitory computer readable storage medium storing executable instructions which when executed by a processor causes the processor to perform a process for replaying the performance of a predictive engine in a client server environment the instructions causing the processor to perform the aforementioned steps.

In another aspect the present invention is a system for replaying the performance of a predictive engine the system comprising a user device having a processor a display and a first memory a server comprising a second memory and a data repository a telecommunications link between said user device and said server and a plurality of computer codes embodied on said memory of said user device and said server said plurality of computer codes which when executed causes said server and said user device to execute a process comprising the aforementioned steps.

In yet another aspect the present invention is a computerized server comprising at least one processor memory and a plurality of computer codes embodied on said memory said plurality of computer codes which when executed causes said processor to execute a process comprising the aforementioned steps.

Yet other aspects of the present invention include the methods processes and algorithms comprising the steps described herein and also include the processes and modes of operation of the systems and servers described herein. Other aspects and embodiments of the present invention will become apparent from the detailed description of the invention when read in conjunction with the attached drawings.

Some illustrative definitions are provided to assist in understanding the present invention but these definitions are not to be read as restricting the scope of the present invention. The terms may be used in the form of nouns verbs or adjectives within the scope of the definitions.

In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the invention. It will be apparent however to one skilled in the art that the invention can be practiced without these specific details. In other instances structures devices activities and methods are shown using schematics use cases and or flow diagrams in order to avoid obscuring the invention. Although the following description contains many specifics for the purposes of illustration anyone skilled in the art will appreciate that many variations and or alterations to suggested details are within the scope of the present invention. Similarly although many of the features of the present invention are described in terms of each other or in conjunction with each other one skilled in the art will appreciate that many of these features can be provided independently of other features. Accordingly this description of the invention is set forth without any loss of generality to and without imposing limitations upon the invention.

Broadly embodiments of the present invention relate to methods and systems for building and deploying machine learning systems for data analytics. Such machine learning systems may reside on one or more dedicated servers or on on site client terminals such as desk PCs or mobile devices. More particularly embodiments of the present invention relate to creating and deploying predictive engines in production and systematically evaluating and tuning predictive engine parameters to compare different algorithms engine variants or engines. In addition embodiments of the present invention relate to tracking and replaying queries events prediction results and other necessary metrics for deducing and determining factors that affect the performance of a machine learning system of interest. A replay loop may serve to provide operators developers and data scientists insights into the selection and tuning of data sources algorithms algorithm parameters as well as other engine parameters that may affect the performance of a predictive engine.

Generally to create a smart application involving a machine learning system a developer needs to first establish and train machine learning models or algorithms using training data collected from one or more sources. Such training data may also be simulated by historical data collected internally or externally by the machine learning system. A system parameter may indicate how training data is prepared and sampled for training predictive models. Next training data are cleansed and unified into a consolidated format and may be further randomly sampled or additionally processed before being passed to and analyzed by the machine learning algorithms to determine system parameters that may specify which algorithms are to be evoked during deployment and the corresponding algorithmic parameters. The resulting algorithmic parameters provide a trained predictive model. Collectively parameters for a machine learning system control and specify data sources algorithms as well as other components within the system.

For example to establish an algorithmic trading system past prices and market trends may be analyzed to regress and extrapolate for future trading decisions. In this case analysis of training data may determine regression coefficients for computing future trading prices or volume thresholds. Another example of a machine learning system is a recommendation engine for predicting products that users of an e commerce website may potentially purchase. Such product recommendations may be personalized or filtered according to business rules such as inventory conditions and logistical costs. Analysis of training data may determine brand names price ranges or product features for selecting and ranking products for display to one or a group of customers. In this example system parameters may specify which sources are to be employed as training data what type of data cleansing is carried out which algorithms are to be used regression coefficients and what business rules are to be applied to prediction results.

Once a machine learning system is established it can be deployed as a service for example as a web service to receive dynamic user queries and to respond to such queries by generating and reporting prediction results to the user. Alternatively prediction results may be served in desired formats to other systems associated or not associated with the user. As subsequent user actions or actual correct results can be collected and additional data may become available a deployed machine learning system may be updated with new training data and may be re configured according to dynamic queries and corresponding event data. In addition predictive models may be configured to persist thus become re usable and maintainable.

In addition to creating and deploying machine learning systems the inventors of the present invention have created methods and systems for evaluating and tuning machine learning systems in production. In the present invention variants of predictive engines and algorithms are evaluated by an evaluator using one or more metrics with test data. Test data include user queries predicted results and actual results or corresponding subsequent user behaviors or sequences of user actions captured and reported to the evaluator. Test data including actual results can also be simulated using data collected internally or externally by the machine learning system. Evaluation results thus generated are used in automatic parameter set generation and selection for the machine learning system. Multiple instances of a predictive engine or engine variants may be evaluated at the same time and subsequently compared to determine a dynamic allocation of incoming traffic to the machine learning system. Furthermore the inventors of the present invention have created methods and systems for monitoring and replaying queries predicted results subsequence end user actions behaviors or actual results and internal tracking information for determining factors that affect the performance of the machine learning system. For example iterative replay of dynamic queries corresponding predicted results and subsequent actual user actions may provide to operators insights into the tuning of data sources algorithms algorithm parameters as well as other system parameter that may affect the performance of the machine learning system. Prediction performances may be evaluated in terms of prediction scores and visualized through plots and diagrams. By segmenting available replay data prediction performances of different engines or engine variants may be compared and studied conditionally for further engine parameter optimization.

In addition through an Application Programming Interface API these monitoring and replaying methods and systems may work for not only engines deployed on the machine learning system specified here but also external engines and algorithms. In other words implementations of monitoring and replaying of engine configuration and performances may be separate from the engine deployment platform thus allowing external monitoring and replaying services to be provided to existing predictive engines and algorithms.

One feature of the present invention is its focus on engine parameters instead of just algorithmic parameters. Engine parameters include hyperparameters such as data sources algorithms employed and business logic parameters in addition to configuration and data inputs to individual algorithms. Such engine level considerations allow engine level comparisons. Instead of tuning algorithmic parameters alone embodiments of the present invention allow additional selection of data sources algorithms business rules and any other characteristic of the engine under consideration. Engine variants may be chosen by an operator or developer based on a template with default values or generated automatically. Multiple variants of an engine deployed according to different engine parameter sets can thus utilize different algorithms or data sources offering a much wider variety of deployable engine instances for comparison and much more flexibility for performance optimization.

Another feature of the present invention is that it is capable of tracking multiple user actions behaviors or responses both immediately and over a delayed time frame. Sequences of user actions such as mouse clicks followed by an online purchase may be grouped and tracked under the same tracking tag or replay tag associated with a particular query. In addition user actions may be tracked across different sessions cohorts according to different segmentation rules.

With the ability to track and replay prediction history embodiments of the present invention not only allow developers and data scientists to track prediction accuracy but also enable them to troubleshoot and reconfigure the system as needed. Instead of just returning prediction success or failure rates for determining whether one variant performs better than another embodiments of the present invention can replay the whole prediction scenario from engine parameters queries prediction results to actual results user interactions and evaluation metrics to help developers understand particular behaviors of engine variants of interest and to tailor and improve prediction engine design. The graphical or textual visual replay of evaluation and tuning results not only makes the whole process easier to use but also allows interactive engine parameter tuning by an operator.

PredictionIO is a trademark name carrying embodiments of the present invention and hence the aforementioned trademark name may be interchangeably used in the specification and drawing to refer to the products services offered by embodiments of the present invention. The term PredictionIO may be used in this specification to describe the overall machine learning system creation evaluation and tuning processes of the invention. The term PredictionIO Enterprise Edition is one version of the PredictionIO platform offered and sold to enterprise customers with certain enhanced features above the baseline version. Of course the present invention is not limited to the trademark name PredictionIO and can be utilized by any naming convention or trademark name whatsoever.

In some embodiments of the present invention event server may be a component of predictive engine instead of being an independent entity. In addition not all input data to predictive engine must be streamed from event server . In some embodiments predictive engine may read data from another datastore instead of event server .

Based on unified data predictive engine can be created. Predictive algorithms can be selected to represent a given type of prediction problem or task. Examples of prediction tasks include recommendations and classifications. For instance a similar item recommendation task may seek to predict items that are similar to those on a given list a personalized recommendation task may seek to predict which items a given user or users are inclined or more likely to take actions on and a classification task may seek to predict whether a given document of text body is a suggestion or a complaint. PredictionIO server may provide template predictive engines that can be modified by a developer for rapid development of system . Predictive engine may contain one or more machine learning algorithms. It reads training data to build predictive models and may be deployed as a web service through a network configuration as shown in after being trained. A deployed engine responds to prediction queries from user application possibly in real time or over a given span of time.

After data are sent to event server continuously or in a batch mode predictive engine can be trained and deployed as a web service. User application may then communicate with engine by sending in a query through an Application Programming Interface API or a REST interface such interfaces may be automatically provided by Prediction IO platform . An exemplary query is a user ID. In response predictive engine returns predicted result in a pre defined format through a given interface. An exemplary predicted result is a list of product IDs. In the classification example previously discussed query may be a paragraph of text input and predicted result may be an alphanumerical string that indicates whether the input text is a suggestion or a complaint. In the similar item recommendation task query may be a set of item IDs such as P P P while predicted result may be another set of item IDs such as P P indicating that products P and P are similar to the given products P P and P. Similarity among different items may be defined through numerical scores and or non numerical criteria. In the personalized recommendation task query may be a user ID while predicted result may be a set of item IDS such as P P indicating that the user with the given ID is more likely to take actions on product P and P.

Similar to system shown in each of mobile application website and email campaign may communicate with engines by sending in data or query . A subset or all of the available predictive engines may be active depending on data or other engine parameter settings as configured through PredictionIO server . In response predictive engines return one or more predicted results individually or in combination in a possibly pre defined format.

Even though only three user applications and four predictive engines are shown in System may be scaled to include many more user applications and PredictionIO server may be scaled to include fewer or many more predictive models. Additional user applications may each reside on the same or separate devices or storage media. In addition PredictionIO server may be scaled to include multiple predictive engines of different types on the same platform. Event server may function to provide input data to all predictive engines or more than one event server may be implemented within PredictionIO server . For example depending on the type of prediction required subsets of data may be stored separately into multiple event servers and indexed correspondingly.

To facilitate the creation and deployment of a predictive engine a PredictionIO server such as may provide programming templates for creating each component of predictive engine . For example a read function of data source may be called directly to return training data and a prepare function of data preparator may be called to process training data into prepared data . Each of algorithms to processes prepared data to determine model or object parameters.

To facilitate evaluation and tuning of predictive engine its inputs outputs and internal parameters may be tagged and replayed. More detailed descriptions will be provided with reference to .

The first Data component refers to data source and data preparator . In data source and data preparator receive data from event server . Similarly in here data source imports application data possibly from an event server implemented on a PredictionIO platform. Data source functions as a reader of internal or external datastores while data preparator cleanses training data before passing prepared data to Algorithm component of predictive engine . Some exemplary functions of data preparator are to reformat and aggregate training data as desired and to sample a subset of training data using a pre defined random sampling strategy. In some embodiments data preparator may be excluded from Data component and training data may be passed directly from data source to Algorithm component of predictive engine . The inclusion or exclusion of data preparator may be useful in evaluating the performance of predictive engine under different settings or configurations.

The second Algorithm component of predictive engine comprises one or more algorithms denoted as algorithms to in . A very simple example of an algorithm within Algorithm component is a non personalized trending algorithm that recommends products which are most popular in the store at the moment. A more complicated example may be a personalized algorithm that takes into account products a particular customer has purchased in the past. A single predictive engine may contain multiple algorithms each can be trained as discussed previously with reference to and activated or called upon request as discussed previously with reference to . However not all algorithms have to be trained or called at the same time. The selection of algorithms within Algorithm component could depend on the availability of training data computing resources or other factors. The selection of algorithms is specified by parameters of predictive engine . In addition a subset of algorithms can be selected for best performance as will be discussed with reference to . Furthermore data from preparator may be sampled separately for each algorithm for best performance. In some embodiments the output of the training process includes a model part and a meta data part. The trained models and meta data are stored in a local file system in HDFS or another type of storage. Meta data may include model versions engine versions application ID mappings and evaluation results.

Predicted results such as and from activated algorithms are passed to Serving component . Serving component can combine filter and further process prediction results according to real time business rules to generate predicted result . Such business rules may be updated periodically or upon request.

In addition to evaluate the performance of the prediction process to compare different algorithms algorithm parameters settings as well as different engine variants an Evaluator component receives data from Serving component and applies one or more metrics to compute evaluation results as an output. An engine variant is a deployable instance of a predictive engine specified by an engine parameter set. The engine parameter set includes parameters that control each component of a predictive engine. An evaluation metric may quantify prediction accuracy with a numerical score. Evaluation metrics may be pre defined with default computation steps or may be customizable by developers that utilize the PredictionIO platform.

Although not explicitly shown in Evaluator may receive actual results including correct values user actions or actual user behaviors from a datastore or a user application for computing evaluation metrics. An actual result refers to a correct prediction result or an actual outcome of a prediction task. If a predicted result is the same as an actual result the predicted result can be considered as an excellent prediction. Recall the exemplary queries and corresponding predicted results discussed with reference to . In the classification task an actual result may be the string complaint which is a correct classification of the text input. In the similar item recommendation task an actual result may be product IDs P P indicating that products P and P are similar to given items P P P although the predictive engine suggests products P and P. In a personalized recommendation task an actual user behavior may be product IDs P P indicating that the user selected products P and P for further viewing and purchase after products P and P are recommended by the predictive engine. Another example of actual results is in algorithmic trading where an actual result may be the actual opening or closing price of a particular stock on the next day. Actual results may be collected through user devices read from storage or simulated.

Prediction result and evaluation result can be passed to other components within a PredictionIO server. As discussed previously a PredictionIO server is a predictive engine deployment platform that enables developers to customize engine components evaluate predictive models and tune predictive engine parameters to improve performance of prediction results. A PredictionIO server may also maintain adjustment history in addition to prediction and evaluation results for developers to further customize and improve each component of an engine for specific business needs.

In some embodiments of invention Apache Spark can be used to power the Data Algorithm Serving and Evaluator components. Apache Spark is a large scale data processing engine. In this case distributed algorithms and single machine algorithms may both be supported by the PredictionIO Server.

A predictive engine within a PredictionIO platform is governed by a set of engine parameters. Engine parameters determine which algorithms are used and what parameters are to be used for each algorithm chosen. In addition engine parameters dedicate the control of the Data component Algorithm component and Serving component of a predictive engine. In other words engine parameters include parameters for each component controller. As engine parameters essentially teach how an engine is to function engine parameters are hyperparameters. A given set of engine parameters specifies an engine variant.

The determination and tuning of engine parameters is the key to generating good predictive engines. The evaluator component also called an evaluation module facilitates the engine tuning process to obtain the best parameter set. For example in an classification application that uses a Bayesian algorithm an optimal smoothing parameter for making the model more adaptive to unseen data can be found by evaluating the prediction quality against a list of parameter values to find the best value.

In some embodiments to evaluate engine parameters available data can be split into two sets a training set and a validation set. The training set is used to train the engine as discussed with reference to while the validation set is used to validate the engine by querying the engine with the validation set data as discussed with reference to . Validation set data include actual results or action user behaviors. One or more metrics can be defined to compare predicted results returned from the engine with actual results among the validation data. The goal of engine parameter tuning is to determine an optimal engine parameter set that maximizes evaluation metric scores. The higher the score the better the engine parameter set. For example a precision score may be used to measure the portion of correct predictions among all data points. In some embodiments training and validation data are simulated by the PredictionIO platform.

The generated list of engine parameter sets are evaluated one by one at step according to a chosen evaluation metric or multiple chosen metrics until timeout or until a maximum number of tests is reached. In this example shown in the n th engine parameter set is represented as the tuple xn yn zn . . . where each element of the parameter set may take on different variable types. In some embodiments a baseline engine variant is presented as an optional input and is also evaluated. Baseline engine variant is of engine type and may take on default engine parameter values stored in a PredictionIO platform may be generated manually by an operator or may be generated automatically. The parameter value evaluation score and computation time of each of the engine parameter set and the baseline engine variant are reported at step as output . Subsequently a new predictive engine variant is created at step with the parameter set having the best score. If a baseline engine variant is present an engine variant is created only if the best score is better than the baseline engine variant s score. The whole engine and its complete parameter set entire DASE stack see definitions section or any sub component and its associated parameters may be tuned. This illustrative example shows the tuning of engine parameter sets. In other words the Data source data preparator Algorithm Serving and Evaluation components and their parameters can all be tuned in this manner as presented herein.

In some embodiments a PredictionIO platform may deploy a variant of a given predictive engine with an initial set of engine parameters or an initial engine parameter setting. The initial engine parameter set may take on default values stored in memory may be generated manually by an operator or may be determined automatically. The deployed engine variant then receives queries responds with predicted results and receives back actual results. Evaluation results are then generated and the current engine parameter set and evaluation results are passed to an engine parameter generator. From time to time the engine parameter generator generates a new parameter set based on evaluation results of the current variant and sometimes evaluation results of previously deployed variants. Such previously deployed variants may have been replaced by previously generated new engine parameter sets and evaluation results of previously deployed variants may have been stored by the PredictionIO platform. The new engine parameter set generated in the current round may then be deployed to replace the existing engine variant. Replacing old engine variants is an optional feature as old engine variants may also remain in memory for future analysis and comparison if desired or necessary.

In addition to evaluating the performance of predictive engines and tuning engine parameter sets a PredictionIO platform may record actual results including subsequent user actions actual correct results or actual information of the previously unknown event now revealed after a prediction has been made. Thus prediction history can be tracked for updating predictive engines during deployment. Such prediction history tracking may be performed in real time with live evaluation results returned as feedback to predictive engines for further engine parameter tuning and prediction accuracy improvement. Prediction history may also be individually or collectively replayed to operators of predictive engines for troubleshooting purposes.

In some embodiments a PredictionIO server generates and logs a unique tracking tag for each user query. Correspondingly predicted results generated in response to the current query and parameters of the engine variant deployed are associated with the same tracking tag. A tracking tag may be an alphanumerical string such as X or X1 a tuple of alphanumerical strings such as X 1 or any other identifier capable of identifying individual queries. Recall that in some embodiments a query may include identifying information including user ID product ID time and location. Similarly a tracking tag may be in the form of user device ID user ID time stamp . Subsequent actual results including user actions and behaviors and actual correct results revealed after the prediction result has been served are also logged under the same tracking tag. As a result prediction results and actual results can be segmented or categorized according to identifying information such as product name time day of week user categories and or attributes. User actions and or behaviors may be monitored over a long period of time such as several hours days or even months. User actions or behaviors may also be logged as sequences instead of a set of individual events. For example a user may click on five products before purchasing a particular product. All five user clicks and the purchase may be viewed together as a sequence of user actions. User actions or behaviors may also be further segmented according to connection sessions or even browsing windows. For example user actions performed on one webpage may be recorded separately from user actions performed on another webpage or they can be combined under the same user ID. Collectively such tracking data as identified by the possibly unique tracking tag can be replayed to a developer of a predictive engine automatically or upon request to assist in improving and understanding the performance of predictive engines. Tracking tags are thus also called replay tags. As previously discussed a user refers to any entity that interacts with a PredictionIO Server or predictive engines and may or may not be a person.

More specifically a PredictionIO server may include a replay loop to perform live evaluation of predictive engines with great details and high levels of accuracy. In some embodiments a PredictionIO server provides a special data source data reader or event datastore that can use the tracking data to replay how a prediction engine performs. This data source is able to reconstruct the complete history of each user that queries the system. In addition to tracking tags specific to individual queries other types of data characteristics or meta data can be employed to group and sort tracking data. Such meta data may or may not be part of the tracking tags themselves. A replay loop may be displayed graphically or textually to a developer of the system or an operator of the replay loop. Exemplary displays include event logs and graphs time series plots performance curves charts and so on. The Prediction server may also provide a special evaluator component that takes the complete history of each user and produce accurate and detailed reports of how each prediction performed. Besides obtaining a better picture of how the prediction engine performs in contrast to black box tests this level of detail enables fine tuning and troubleshooting of the prediction engine by data scientist and engine developers.

In this embodiment two variants of a predictive engine E are deployed through a PredictionIO platform. Each of the two variants receives queries from a user application and generates predicted results. Such predicted results are tagged with tracking or replay IDs and subsequently evaluated with their corresponding engine parameter sets tuned to generate two new variants of the predictive engine E. An engine variant is a deployable instance of a predictive engine specified by an engine parameter set. In the first variant of engine E is specified by engine parameter set while the second variant of engine E is specified by engine parameter set .

Parameter set states that variant uses DataSource x2 and Algorithms 4 and 2. The values of algorithm parameter1 and algorithm parameter2 of Algorithm 4 are set to b1 and a2 respectively while the value of the parameter Y of Algorithm 2 is set to 33.

Parameter set states that variant uses DataSource x1 and Algorithms 1 and 2. The values of algorithm parameter1 and algorithm parameter2 of Algorithm 1 are set to a1 and a2 while the value of the parameter Z of Algorithm 2 is set to 23.

In various embodiments of the present invention the evaluation and tuning process may start at either deployment platform or user application . For example after deployment platform deploys engine variant and engine variant user application may send three queries Q Q and Q to PredictionIO platform . In some embodiments a query may include identifying information including user ID product ID time and location. A split test controller determines which deployed variant each query is transferred to. In some embodiments a single query may be transferred to more than one deployed engine variants. In this example queries Q and Q are passed to first variant while query Q is passed to second variant . Deployed engine variant then generates predicted results including predicted result P with replay ID X and predicted result P with replay ID Z. Replay IDs in this example are alphanumeric tracking tags specific to individual queries. Similarly deployed engine variant generates predicted results including predicted result P with replay ID Y. Predicted results and are then passed back to split test controller to be exported as output to user application . In embodiments where more than one user applications are present the split test controller may track which user application a particular query has been generated from and corresponding predicted results should be transferred to. In some embodiments predicted results may be served to user applications other than the one where queries have been generated.

In addition to passing predicted results to the split test controller each deployed engine variant and also passes data and to datastore in this example shown in . Data include two sets of tracking data one specified by replay ID X and one specified by replay ID Z. The first set of tracking data specified by replay ID X includes query Q predicted result P and a description of engine variant V. This description of engine variant V may be engine parameter set itself or some meta data that uniquely identifies engine parameter set to event datastore . Similarly the second set of tracking data specified by replay ID Z includes query Q predicted result P and a description of engine variant V. Data include a single set of tracking data specified by replay ID Y and are comprised of query Q predicted result P and a description of engine variant V.

In this embodiment at user application user actions and or behaviors collected subsequent to receiving predicted results P P and P from the PredictionIO platform are considered as actual results A A and A respectively and tagged with corresponding Replay IDs. Such user actions may be collected in real time or over a given time span such as a few hours a day or a week. Recall that each query evokes a prediction process to generate a predicted result and each query is uniquely identified by a replay ID. Hence multiple user actions or actual results corresponding to a particular query with a given replay ID may be tagged with the same replay ID. For example actual result A shown in may represent a sequence of user clicks and browsed product pages all corresponding to query Q product recommendation P and replay ID X.

After actual results are transferred to datastore engine variant parameter sets queries predicted results and actual results corresponding to the same Replay ID are aggregated within datastore using the data source data reader or event datastore mentioned above. Aggregated data sets are sent to evaluator for evaluation. In this embodiment two metrics and are used within evaluator individually or in combination. Evaluation results are sent to auto parameter tuning variant generator . Auto parameter tuning variant generator functions in cooperation with evaluator according to one of the processes discussed with reference to before outputting updated engine parameter sets that specify two new variants V and V for Engine E. The newly generated engine variants may be subsequently deployed by deployment platform . The cycle of prediction evaluation and auto parameter tuning continues as more user queries are imported into the system.

In some embodiments engine variant V is generated based on engine variant V alone and engine variant V is generated based on engine variant V alone. In some embodiments both engine variants V and V are generated based on both engine variants V and V. For example as part of evaluator or auto parameter tuning variant generator variants V and V of engine E may be compared according to computed metrics and . Such pair wise comparison may provide a better performing engine variant the engine parameter set of which may in turn serve as a base parameter set for generating new variants V and V. In another example more than two variants may be deployed and evaluated at the same time. Evaluator may sort or rank the performances of such multiple engine variants with pair wise or multiple way comparisons before generating new engine variants for further deployment and evaluation.

In some embodiments one or more new engine variants may be determined manually by an operator. For example the operator may examine evaluation results output by evaluator and manually input a new set of engine parameters as new engine variant V. In another example the operator may directly modify the output of auto parameter tuning variant generator .

In addition to auto parameter turning a developer of the predictive engine E or an operator of the replay loop as shown in may prefer to examine prediction history to tune engine parameter sets directly and to troubleshoot issues in predictive engine design. For example Prediction platform may include an interface or a hook to such an interface for users or operators to provide actual results directly. PredictionIO platform may also allow operators to tag debug information so each prediction will have debug information that can be examined using a Replay feature as will be discussed next. Visual replay may replay tracking data from data store and available debug information to operators thus providing insights into the selection and tuning of data sources algorithms algorithm parameters as well as other engine parameters that may affect the performance of a predictive engine. Such extensive replay of prediction history allows operators to understand and deduce why particular prediction results are generated and how prediction performances can be improved.

The present invention allows users to replay prediction scenarios to analyze visualize and detect the change of prediction accuracy over various segmentations such as time. Take the following three types of prediction problems as examples shown in Table 1.

The Replay will allow operators to visualize the predicted results with actual results during the evaluation phase.

As prediction history and tracking data are collected and stored prediction scenarios may be replayed and the complete prediction history of each user that queries the system may be reconstructed allowing operators of the replay process to analyze visualize and detect changes of prediction accuracy over various segmentations such as different time periods. Recall from the discussion of evaluator in that actual results such as actual user behaviors may be received from a datastore or a user application during the evaluation phase. Such actual results may be visualized with predicted results through visual replay for comparative purposes. Given a particular replay ID visual replay may retrieve and selectively display associated query predicted result actual result additional auxiliary user information or meta data and possibly the corresponding engine variant as given by the engine parameter set. In some embodiments a selected subset of tracking data may be visually displayed where the subset is pre defined or manually configured by an operator of visual replay . Patterns anomalies and trends in tracking data may thus be analyzed by the system or by the operator directly. A replay of prediction history or engine performance may or may not be followed by further engine parameter tuning processes.

As the cycle of prediction evaluation and auto parameter tuning takes place visual replay may function as a task monitor allowing the operator to selectively and incrementally view tracking data thus collected. In some embodiments operators can be notified when user conversion decision to purchase drops below a certain predefined threshold for a particular engine or engine variant. The operator can then utilize the replay feature of the PredictionIO platform for troubleshooting and continuous prediction performance monitoring.

In this example actual user actions over a five minute time period of segmentation are plotted. In some embodiments actual results or other types of tracking data may be plotted over shorter or longer time segmentations. In some embodiments tracking data associated with multiple users multiple queries or multiple replay IDs are plotted on the same graph. Moreover data may be grouped by cohort session and other types of data characteristics. The PredictionIO platform may automatically detect patterns in tracking data and cluster them accordingly. On the other hand operators may also specify desired groupings directly. For example operators can select a specific user and session and see all the events associated with the user or session.

In addition to displaying tracking data directly the PredictionIO platform may produce detailed reports on prediction histories enabling the further fine tuning of prediction engines.

An operator of the replay process may further zoom in and out of a certain time period such as a single day as indicated by lines and to examine additional details and to further troubleshoot issues in predictive engine design and engine parameter tuning. Although only four data points are show for each time series data curve shown in in some embodiments number of prediction successes and failures may be statistically summarized over strategically generated samples and time spans. The PredictionIO platform may provide default values for the time scale. In some embodiments the PredictionIO platform may take into account the amount of data available to dynamically determine optimal time scale values for binning purposes. In yet some other embodiments the PredictionIO platform may further generate and display linear or non linear regression curves to model the observed tracking data. The Success and Failure metrics shown here are two examples of statistics useful for analyzing prediction performances. Operators may define additional metrics such as success rates and confidence statistics and more than two metrics may be provided in a report and shown graphically in a visualization.

As previously discussed data may be grouped by cohort session and other types of data characteristics in generating useful statistics for analyzing prediction results. is a bar chart of prediction successes and failures plotted against different genders. By considering different genders separately it becomes clear that the current engine or engine variant under consideration is more tailored for female users instead of male users. Consequently an operator or developer may decide to include gender as an additional variable in the predictive model. In some embodiments other types of charts such as histograms and scatter plots may be displayed.

In success and failure metrics are plotted against different genders. In some embodiments the PredictionIO platform provides a data augmentation feature for augmenting available user data with additional information such as gender. For example external information to be augmented may include ZIP code age group ethnicity occupation and family size. Additional information to be augmented may also be mined from behavior data. For example users may be classified into high spending and low spending groups or frequent on line shopping or non frequent on line shopping groups. Data augmentation provides new ways of categorizing tracking data for better performance monitoring and analysis.

Recall from the discussion with reference to that multiple engine variants may be tested and studied at the same time with a split test controller determining which engine variant a user query is dispatched to. Similarly shows a system for testing multiple engine variants at the same time according to an illustrative embodiment of the present invention.

In system input user traffic may be allocated dynamically through forward based on the performance of each engine variant under consideration. For example initially half of new user traffic or queries may be directed to the predictive engine while the remaining half are simply stored and thus not directed to a predictive engine as indicated by the No Engine placeholder . In some embodiments forward is a split test controller similar to component shown in . Predictive traffic through predictive engine may be equally shared among its three variants and . Thus each engine variant takes on one sixth of the overall user traffic. Overtime it may be determined that a specific variant such as engine variant provides higher prediction accuracy. As a result forward may automatically direct more than one sixth of overall traffic to engine variant to optimize overall system performance. The PredictionIO platform seeks to strike a balance between exploration and exploitation. In yet some other embodiments forward may direct the same predictive traffic to multiple engine variants thus enabling direct comparison of prediction results and prediction accuracy across the multiple engine variants.

In some embodiments a PredictionIO platform may deploy multiple engine variants with initial sets of engine parameters or initial engine parameter settings. The deployed engine variants then receive queries as allocated by a splitter and respond with predicted results. Corresponding actual results are also received. Evaluation results are then generated and the current engine parameter sets and evaluation results are passed to an engine parameter generator. From time to time the engine parameter generator generates one or more new parameter sets based on evaluation results of the current variants and sometimes evaluation results of some or all previously deployed variants. Such previously deployed variants may have been replaced by previously generated new engine parameter sets and evaluation results of previously deployed variants may have been stored by the PredictionIO platform. The one or more new engine parameter sets generated in the current round may then be deployed to replace the existing engine variants.

In yet other embodiments a PredictionIO platform may perform evaluation tuning and or comparison of multiple engines. For example multiple engines may be implemented by different developers and data scientists for a particular prediction problem such as classification of incoming mail as spam or non spam or recommendation of similar items. A PredictionIO platform may provide to externally or internally implemented predictive engines engine evaluation engine parameter set tuning prediction history tracking and replay services as discussed throughout the current disclosure. For multiple engines targeting the same prediction problem the PredictionIO platform may serve as an interface for cross comparison and engine selection. For multiple engines targeting different prediction problems based on queries from the same user PredictionIO platform may serve as an interface for cross examination selection and aggregation.

In addition to illustrative plots shown in and provide illustrative visual displays of prediction performances over one or more replay groups. A replay group refers to a pre defined or operator defined segment of queries that satisfy one or more conditions as provided through query segment filters. Replay groups may be created for textual or visual displays. Examples of query segment filters include engine variant filters user attribute filters item attribute filters query attribute filters and other property filters or conditional filters capable of selecting a subset of available queries for performance analysis and monitoring. For example an engine variant filter may select queries that have been or will be processed through a given engine variant and a single query may be assigned to multiple replay groups if it has been or will be processed through multiple engine variants a user attribute filter may be applied if queries contain at least a user and may be used to select queries associated with users in a particular age group an item attribute filter may be applied if queries contain at least an item and an query time attribute filter may be applied if queries have associated timestamps. Multiple query segment filters may be used jointly and filtered results may be combined as intersections or unions of query segments. Query segment filters may be pre defined or operator defined and may be applied automatically or upon request by an operator. In addition since query segment filters select subsets of queries without necessarily affecting the prediction process they may be applied during any stage of the predictive engine tuning evaluation and replay process. In one example a query segment filter may be applied to a query as the query is received from an end user device before the prediction process takes place. In another example a query segment filter may be applied to stored queries or query records after predictions have been made already. Each query may be associated with one or more replay group IDs as query segment filters are applied.

As a more specific example a recommendation engine may be deployed as an Engine Variant e v100 with an initial or default engine parameter set. A query to ask this engine to recommend five products to a user when the user is in San Francisco may look like userid 123 city SF num 5 . Since userid refers to a user a filter of a new replay group for Engine Variant e v100 may have user attribute options. User attributes can be anything that the system has stored about users. For instance age gender sign up date plan or service a user has signed up for range of user ids dates and so on. If the system contains users behavior data the filter can even go further to select queries that have targeted users who have performed certain actions during a certain time range. For example one or more filters may be applied to generate a replay group by selecting queries for recommending five products to female users when they are in San Francisco.

Depending on how such score functions are defined computed prediction scores may take on both positive and negative values in some embodiments but be non negative in some other embodiments. Computed prediction scores may also be normalized and may take on continuous or discrete values. For example consider an input predicted result containing two items such as P P and an input actual result also containing two items. In some embodiments a score function may return a value of 1 if the input actual result is exactly the same i.e. P P and otherwise. In some embodiments a score function may return a score of 0 1 or 2 depending on the number of overlapping items from the predicted result and the actual result. Such a score may also be normalized to 0 0.5 or 1 representing the percentage of correctly predicted items.

In this and subsequent illustrative examples shown in prediction performances are plotted in terms of accumulated prediction scores over time. Here an accumulated prediction score is calculated by an accumulation function that summarizes the prediction scores of all queries of a replay group within defined time intervals over a given time period. For example each query may have an associated timestamp representing the time at which the query was received by the predictive engine. According to such timestamps queries within a replay group may be segmented for computing accumulated prediction scores. In another example a timestamp may represent when a prediction has been made or a sign up date time at which a user has signed up for prediction service. Generally computation of accumulated predicted scores may be carried out over any categorization or segmentation of queries within a replay group. Furthermore when multiple score functions are defined multiple accumulated scores may be displayed on the same visualization chart or on separate charts.

An operator of the replay process may zoom in and out of the time period shown in to examine additional details in the prediction performance visualization thus further troubleshoot issues in predictive engine design. For example although prediction scores are accumulated over two day intervals during a single month in in some embodiments the system may allow an operator to manually configure the time interval s and time period for plotting. The PredictionIO platform may also take into account the amount of data available to dynamically determine optimal time intervals for prediction score accumulation and visualization.

In some other embodiments Replay Group 1 may be generated by selecting queries containing users who have signed up for prediction service during January 2015. Generally the time period may refer to any time related query attribute. In other embodiments prediction scores may be accumulated over different categories such as user gender leading to accumulated score plots similar to the diagram shown in . Moreover although accumulation has referred to a direct summarization operation in generating the plot shown in in some embodiments accumulation may refer to other algebraic or statistical operations such as averaging weighed summation and such. A direct summation operation is a weighed summation with weights equal to 1. An averaging operation is a weighed summation with weights equal to the reciprocal of the number of queries. A statistical sampling process followed by direct summation may be considered as a weighed summation with weights equal to 1 or 0. Non linear weighing is also possible in some embodiments of the present invention.

In only a single replay group has been visualized as curve and labeled by legend . shows an illustrative visual display of prediction performances over two replay groups according to one embodiment of the present invention. In addition to Replay Group 1 as represented by the curve accumulated scores for queries within Replay group 2 is visualized as curve . Both replay groups are labeled by legend . In addition visual display includes three checkboxes and placed below the plotting window. Checking and un checking boxes and turn the display of curves and on and off respectively. Box provides a Whole Period option which sets the time interval for prediction score accumulation to the entire time period of interest. Checking box turns each of curves and into a single data point. In other words under the whole period option all queries within the time period of the chart would be summarized to generate a single accumulated prediction score.

Once user attributes have been input by the operator Replay Group 1 may be updated automatically and accumulated prediction scores may be visualized in plotting window . Alternatively a request for updating the replay group and the corresponding accumulated prediction score visualization may be received by the system when the operator clicks on the Plot button .

In some embodiments operators can create as many replay groups on a visual chart as they like. Each replay group may be created through interfaces similar to interactive display or may be loaded from storage. Operators can assign a name label to each replay group for easy identification and can use different colors or symbols for each replay group.

In some embodiments accumulated prediction scores of one or more replay groups within the time period of interest can be displayed on the visual chart through different graphical representations such as line plots histograms bar charts and scatter plots. For example shows an illustrative histogram representing prediction performances over two replay groups according to one embodiment of the present invention. The same Replay Groups 1 and 2 from are shown here. Each bar such as bars and corresponds to prediction scores accumulated over one week intervals during the one month period of January 2015.

Although not shown explicitly in in some embodiments an operator may manually adjust the values of the time period and time interval as well as definitions for the score function and accumulation function. The visual chart may be updated automatically once these values are changed or upon request when such requests are received from the operator.

In addition show illustrative visual displays of prediction performances over multiple replay groups according to embodiments of the present invention. In visualization shows how well one engine variant performs over a given one month period for three different user segments divided by age groups. Curves and correspond to Replay Groups 1 2 and 3 respectively as indicated by legend . Queries are divided into below 30 30 to 60 and above 60 age groups and queries within each replay group are processed through engine variant e v111. In some embodiments Replay Groups 1 2 and 3 are generated by applying a user attribute filter that examines the user age attribute. All queries within each replay group are processed through engine variant e v111 either before or after the user attribute filter is applied.

In visualization compares how three engine variants perform over a given one month period for the below 30 age group. Curves and correspond to Replay groups 1 2 and 3 respectively as indicated by legend . In some embodiments Replay Groups 1 2 and 3 are obtained by applying a user attribute filter as well as an engine variant filter. Once a query is processed by an engine variant to generate a corresponding predicted result the query may include the engine variant information as part of the resulting query record. A query record may include the input query engine variant information predicted results actual results prediction score and or any other information relevant to the input query and how the input query has been processed by the prediction system. Thus a single input query to a predictive engine may lead to multiple query records and query records corresponding to the same input query may be segmented into different replay groups. An input query may also be associated with multiple replay group IDs depending on how it is processed by the prediction system.

Once visual replay of prediction performances are generated an operator of the replay process may further zoom in and out or mouse over the visualization to examine additional details in the prediction process hence further troubleshoot issues in predictive engine design. The PredictionIO platform thus provides method and systems for detailed prediction debugging.

Window provides a detailed and zoomed in view of table . In some embodiments window may be displayed on its own without the floating table . Label specifies the time interval and accumulated prediction score associated with data point and shows that query records displayed in this window have been processed through Engine Variant e v111. In this example query records include attributes such as Query Q Predicted Result P Actual Result A Query Time Time and Prediction Score Score . The displayed time interval and engine variant may also be part of the query records. In one specific embodiment in which no replay ID is utilized the system may replay based on time or other user defined condition and display the associated query records. In other embodiments dedicated replay IDs may be assigned to each individual query or individual query record and may or may not be displayed with other parts of the query records. A scrolling bar with up and down arrows allows the operator to scroll through query records when not enough space is available to display all query records at the same time.

In some embodiments the system also provides statistical features to summarize the prediction performance. For example the system may automatically select queries with outliner scores on the table. The system also provides statistical information such as mean variance and distribution about the scores. In label provides the total number of query records and the average accumulated score across the given time period between time and .

The languages in the examples or elaborations below are context specific embodiments and should not be construed to limit the broader spirit of the present invention.

Building machine learning an application from scratch is hard you need to have the ability to work with your own data and train your algorithm with it build a layer to serve the prediction results manage the different algorithms you are running their evaluations deploy your application in production manage the dependencies with your other tools etc.

The present invention is a Machine Learning server that addresses these concerns. It aims to be the key software stack for data analytics.

Let s take a classic recommender as an example usually predictive modeling is based on users behaviors to predict product recommendations.

This code will work in development environment but wouldn t work in production because of the following problems 

PredictionIO boasts an event server for storage that collects data say from a mobile app web etc. in a unified way from multiple channels.

An operator can plug multiple engines within PredictionIO each engine represents a type of prediction problem. Why is that important 

In a production system you will typically use multiple engines. For example the archetypal example of Amazon if you bought this recommend that. But you may also run a different algorithm on the front page for article discovery and another one for email campaign based on what you browsed for retargeting purposes.

How to deploy a predictive model service In a typical mobile app the user behavior data will send user actions. Your prediction model will be trained on these and the prediction engine will be deployed as a Web service. So now your mobile app can communicate with the engine via a REST API interface. If this was not sufficient there are other SDKs available in different languages. The engine will return a list of results in JSON format.

PredictionIO manages the dependencies of SPARK and HBASE and the algorithms automatically. You can launch it with a one line command.

The framework is written in Scala to take advantage of the JVM support and is a natural fit for distributed computing. R in comparison is not so easy to scale. Also PredictionIO uses Spark currently one of the best distributed system framework to use and is proven to scale in production. Algorithms are implemented via MLLib. Lastly events are store in Apache HBase as the NoSQL storage layer.

Preparing the data for model training is a matter of running the Event server launched via pio eventserver and interacting with it by defining the action i.e. change the product price product i.e. give a rating A for product x product name attribute name all in free format.

Building the engine is made easy because PredictionIO offers templates for recommendation and classification. The engine is built on an MVC architecture and has the following components 

PredictionIO Enterprise Edition is capable of performing live evaluation of its prediction performance. This is a lot more accurate because it is capable of tracking all subsequent actions of a user after a prediction has been presented to the user.

PredictionIO has two types of deployable servers event server and prediction engine server. In live evaluation mode a prediction engine server will do the following additional actions per query 

Subsequent actions of the user will be logged and tracked using the aforementioned unique tracking tag. This is called the tracking data. 

Utilizing the above features the present inventors built on top of it a replay loop to perform live evaluation of prediction engines with unmatched accuracy and level of details that otherwise A B testing or offline evaluation would not be able to provide.

PredictionIO Enterprise Edition provides a special data source data reader that can use the tracking data to replay how a prediction engine performs. This data source is able to reconstruct the complete history of each user that queried the system.

PredictionIO Enterprise Edition provides a special evaluator component that takes the complete history of each user and produce accurate and detailed reports of how each prediction performed. Besides obtaining a better picture of how the prediction engine performs in contrast to black box A B tests this level of detail enables fine tuning of the prediction engine by data scientists and engine developers.

The present invention helps data scientists and developers develop and deploy machine learning systems.

One embodiment provides a library engine templates gallery so developers can build their own engines or customize templates to their own needs ready to use right away and also customizable. All engines follow the same DASE architecture described above.

Engines are deployed as a web service which are deployed as a service. Unifying data for predictive analytics provide an event server to train the data. Event server can connect to existing systems like mail servers for example. Can be installed on premises. Can also be deployed on AWS or private cloud. Because of customizability makes sense for users to install on their own cloud.

These benefits are illustrative of some advantages of the present invention over the prior art and are not to be read as limiting or to limit the benefits of the present invention to those listed. Other benefits may also exist.

One of ordinary skill in the art knows that the use cases structures schematics and flow diagrams may be performed in other orders or combinations but the inventive concept of the present invention remains without departing from the broader spirit of the invention. Every embodiment may be unique and methods steps may be either shortened or lengthened overlapped with the other activities postponed delayed and continued after a time gap such that every user is accommodated to practice the methods of the present invention.

The present invention may be implemented in hardware and or in software. Many components of the system for example network interfaces etc. have not been shown so as not to obscure the present invention. However one of ordinary skill in the art would appreciate that the system necessarily includes these components. A user device is a hardware that includes at least one processor coupled to a memory. The processor may represent one or more processors e.g. microprocessors and the memory may represent random access memory RAM devices comprising a main storage of the hardware as well as any supplemental levels of memory e.g. cache memories non volatile or back up memories e.g. programmable or flash memories read only memories etc. In addition the memory may be considered to include memory storage physically located elsewhere in the hardware e.g. any cache memory in the processor as well as any storage capacity used as a virtual memory e.g. as stored on a mass storage device.

The hardware of a user device also typically receives a number of inputs and outputs for communicating information externally. For interface with a user the hardware may include one or more user input devices e.g. a keyboard a mouse a scanner a microphone a web camera etc. and a display e.g. a Liquid Crystal Display LCD panel . For additional storage the hardware my also include one or more mass storage devices e.g. a floppy or other removable disk drive a hard disk drive a Direct Access Storage Device DASD an optical drive e.g. a Compact Disk CD drive a Digital Versatile Disk DVD drive etc. and or a tape drive among others. Furthermore the hardware may include an interface with one or more networks e.g. a local area network LAN a wide area network WAN a wireless network and or the Internet among others to permit the communication of information with other computers coupled to the networks. It should be appreciated that the hardware typically includes suitable analog and or digital interfaces to communicate with each other.

In some embodiments of the present invention the entire system can be implemented and offered to the end users and operators over the Internet in a so called cloud implementation. No local installation of software or hardware would be needed and the end users and operators would be allowed access to the systems of the present invention directly over the Internet using either a web browser or similar software on a client which client could be a desktop laptop mobile device and so on. This eliminates any need for custom software installation on the client side and increases the flexibility of delivery of the service software as a service and increases user satisfaction and ease of use. Various business models revenue models and delivery mechanisms for the present invention are envisioned and are all to be considered within the scope of the present invention.

The hardware operates under the control of an operating system and executes various computer software applications components programs codes libraries objects modules etc. indicated collectively by reference numerals to perform the methods processes and techniques described above.

In general the method executed to implement the embodiments of the invention may be implemented as part of an operating system or a specific application component program object module or sequence of instructions referred to as computer program s or computer code s . The computer programs typically comprise one or more instructions set at various times in various memory and storage devices in a computer and that when read and executed by one or more processors in a computer cause the computer to perform operations necessary to execute elements involving the various aspects of the invention. Moreover while the invention has been described in the context of fully functioning computers and computer systems those skilled in the art will appreciate that the various embodiments of the invention are capable of being distributed as a program product in a variety of forms and that the invention applies equally regardless of the particular type of machine or computer readable media used to actually effect the distribution. Examples of computer readable media include but are not limited to recordable type media such as volatile and non volatile memory devices floppy and other removable disks hard disk drives optical disks e.g. Compact Disk Read Only Memory CD ROMS Digital Versatile Disks DVDs etc. and digital and analog communication media.

Although the present invention has been described with reference to specific exemplary embodiments it will be evident that the various modification and changes can be made to these embodiments without departing from the broader spirit of the invention. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than in a restrictive sense. It will also be apparent to the skilled artisan that the embodiments described above are specific examples of a single broader invention which may have greater scope than any of the singular descriptions taught. There may be many alterations made in the descriptions without departing from the spirit and scope of the present invention.

