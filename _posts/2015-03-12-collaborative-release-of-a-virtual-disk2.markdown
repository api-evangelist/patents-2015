---

title: Collaborative release of a virtual disk
abstract: A method for opening a virtual disk comprises reading information from a metadata file that identifies the current owner of the virtual disk. The method further includes sending a release request to the current owner of the virtual disk to release the virtual disk, writing information to the metadata file identifying the new owner, and then opening the virtual disk.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09575658&OS=09575658&RS=09575658
owner: VMware, Inc.
number: 09575658
owner_city: Palo Alto
owner_country: US
publication_date: 20150312
---
In most operating systems a file can be exclusively locked by an application or a host. If a second application tries to access the file the second application will not be able to do so. Similarly in virtualized environments only a single user is able to access a virtual disk VMDK at any one time to ensure consistency of the data on the virtual disk. For example a virtual disk may be exclusively locked by the single user and accesses to the virtual disk may be orchestrated by a virtual machine management module.

There may however be instances where the virtual disk may be opened without the knowledge of the virtual machine management module. In virtual environments employing input output IO filters a background thread or a daemon can access and exclusively lock a virtual disk without coordinating this action through the virtual machine management module. As a consequence virtual machine management module operations related to opening a virtual disk would fail without a solution to unlock the disk. As one example after a virtual machine VM crashes a daemon may access the virtual disk associated with the virtual machine to perform IO operations thereby locking the disk. When an attempt is made by the virtual machine management module to power on the VM the attempt fails because the virtual disk is locked and there is no existing solution for the virtual machine management module to unlock the disk. For some automated solutions a virtual machine management module that manages virtualized environments does not expect that a virtual disk will be locked and virtual machine management module operations would fail without a clear resolution of how to find the user that has locked the virtual disk and how to ask the user to release the virtual disk.

A virtual disk for a VM may consist of a collection of files stored on an underlying file system. For example the virtual disk may contain a metadata file referred to herein as the descriptor file and at least one data file referred to herein as the extent file. The extent file stores the virtual disk s data which is accessible to the VM. The descriptor file contains metadata accessible to the host s core virtualization stack for management of the extent file. For example the descriptor file points to one or more extent files to indicate which extent files belong to the virtual disk and to provide the layout of a virtual disk. The descriptor and extent files for a virtual disk are migrated cloned and otherwise copied together.

Virtual disk filters are plug ins pluggable code modules that are able to intercept and if applicable modify VM requests directed to corresponding virtual disks owned by the VMs. In doing so these filters may use and or create data metadata that needs to be stored. Filter data shouldn t be simply stored in a global location because it must be able to be easily migrated cloned and otherwise copied along with the virtual disk. Virtual disk I O filters however may create too much data metadata to be stored in the descriptor file. For example a filter may require many megabytes or even gigabytes of storage space e.g. for caching or storage of hashes but a typical descriptor file is very small on the order of a few kilobytes. Ballooning the size of the descriptor file to accommodate I O filter data would severely impact performance and maintainability of the virtual disk. Additionally such metadata should not be stored in the extent file which may be limited to the VM s data.

The I O filter framework allows for each plug in to own one or more metadata files sometimes known as sidecar files on a file system associated with the corresponding virtual disk. As described above a virtual disk includes descriptor and extent files. A sidecar file is separate from the descriptor and extent files and the framework tracks the sidecar file using an entry in the disk s descriptor file. The framework presents an application programming interface API to plug ins for creating or otherwise accessing sidecar files. The API further allows for discovery migration cloning creating a snapshot and other management of sidecar files e.g. in response to the migration cloning creating a snapshot or other management event for a corresponding virtual disk.

According to embodiments the metadata file sidecar file is stored alongside a virtual disk. The metadata file is separate from a file descriptor e.g. an inode and stores information about the current owner of a disk that allows other applications to request that the disk be released. To gain access to a locked virtual disk applications extract information about the current owner of a virtual disk from its associated metadata file and then send a request to the current owner of the virtual disk to close the virtual disk. In response the current owner writes the new owner information into the metadata file and then closes the disk. Upon receiving notification that the virtual disk has been released for its use the new owner opens the virtual disk.

As further shown in hypervisor is installed on top of hardware platform and supports the execution of applications . Applications may comprise any number of N virtual machines. These virtual machines may each have an associated virtual disk provisioned in storage device such as virtual disks . Input output operations IOs issued by the VMs are processed through IO stack of hypervisor .

At the top of IO stack is a SCSI virtualization layer which receives IOs from the VMs and targeting the issuing VM s virtual disk and translates them into IOs directed at one or more files managed by hypervisor e.g. virtual disk files representing the issuing VM s virtual disk.

Hypervisor employs filter drivers that are external to a file system driver of the hypervisor e.g. virtual machine file system VMFS driver to provide various IO filter functions such as encryption compression caching e.g. write back caching antivirus and others. Filter drivers examine the translated IOs from SCSI virtualization layer and intercept read write command and data for processing in various filters such as encryption or compression filters. Filter drivers illustrated here as F F F and F can each have an associated daemon as illustrated. Each daemon for an associated filter performs background tasks that may require the virtual disk to be locked. For example the daemon for the caching filter will need to lock the virtual disk when performing de staging operations.

The remaining layers of IO stack are additional layers managed by hypervisor . FS driver receives the IOs passed through filter drivers and translates them to block IOs issued to one or more logical volumes that are provisioned in file system VMFS .

As illustrated in each filter has an associated metadata file . The metadata files illustrated as SCF SCF for each disk are stored alongside and associated with the virtual disks . The virtual disk labeled A in may be a virtual disk associated with VM i.e. VM.VMDK . The virtual disk labeled B may be a virtual disk associated with another VM such as VM i.e. VM.VMDK . In addition to metadata files file descriptors are stored alongside and associated with virtual disks .

Not all IO filters are attached to virtual disks. When an IO filter is attached to a virtual disk metadata file is created for that virtual disk for purposes of storing data required to perform the functions of the IO filter. One type of information that a metadata file stores is current owner information for a virtual disk. In some embodiments metadata files may note in the ownership filed that an ownership transfer to a new owner is pending so that another party will not try to acquire ownership of the virtual disk before the new owner. These metadata files can be opened read and closed by the associated daemon and applications to determine ownership of the virtual disk. Once an owner is determined a request can be sent to the owner to close the virtual disk as discussed in further detail below.

The various terms layers and categorizations used to describe the components in may be referred to differently without departing from their functionality or the spirit or scope of the invention.

The method begins at step where the virtual disk open handler in response to an API call exclusively opens the metadata file in read write R W mode. The method proceeds to step where the virtual disk open handler writes information identifying the owner e.g. host IP address of the owner and the port or local socket the owner is monitoring in the metadata file. Writing the owner information into the metadata file occurs for example when a daemon needs to access the virtual disk to perform a background operation like flushing dirty data from a cache to a VMDK file. The daemon will open up the VMDK and will instruct its own filter to stamp its metadata file with the information that will identify itself as the owner so that the daemon can later receive a request to close the disk and release ownership. Filters only talk to their own daemon not to any other filter s daemon. In the case of a regular virtual machine power on the VMX is the owner which means no particular filter can own the virtual disk. Thus no identification information is stored in that instance since a filter cannot close the virtual disk.

The method then proceeds to step where the virtual disk open handler closes the metadata file. The owner information has been written into the metadata file and now other applications can identify the owner of the virtual disk. In addition the virtual disk is now exclusively opened to the owner.

If another application tries to open the virtual disk at this time it will fail because the virtual disk is exclusively opened. However the metadata file is closed. Therefore other applications can determine the owner of the virtual disk by checking the metadata files using the following procedures illustrated in .

The method begins at step where in response to a request to power on a virtual machine the VMX thread is launched. Method continues at step where the VMX thread calls a virtual disk handler to open a virtual disk associated with a virtual machine. As shown in decision block if the virtual disk is opened successfully the method moves to step wherein the VMX configures the virtual machine for use. At step the method ends.

If the open command was not successful in decision block the method proceeds to step . At step the VMX thread begins the process of selecting the IO filters in succession until the filter that owns the virtual disk is discovered. At step the VMX thread selects the next IO filter to be checked. In the first pass through the method the VMX thread selects the first IO filter at step .

At step the VMX thread sends a request to the selected IO filter to release the virtual disk. The process proceeds to decision block where the daemon for the selected IO filter determines if this filter owns the disk. If this filter does not own the disk the VMX thread then checks to see if this is the last filter at step i.e. all filters have been checked . If this is the last filter and the disk has not been successfully opened an error message is returned in step . If the result of step is that this is not the last filter the VMX thread selects the next IO filter in step to see if that filter owns the disk and the process continues.

If at decision block the filter verifies that its daemon does own the disk the process proceeds to step where the daemon associated with the filter begins the process to close the virtual disk. In some implementations the current owner of the disk may evaluate the request to close the disk and make a yes or no decision on whether to close the disk. This decision could be based on for example a priority of tasks being performed by the current owner compared to the priority of the requesting entity. Other criteria could also be used to make this decision. In this example embodiment however the current owner will close the disk upon receiving the request.

At step the daemon opens the metadata file that stores the current owner information in exclusive read write mode. At step the daemon writes the new owner information to the metadata file. Alternatively the daemon may clear the owner information in the metadata file without writing information identifying the new owner and may also indicate that transfer to the new owner is pending. At step the daemon closes the metadata file which now stores the new owner information. At step the daemon closes the virtual disk.

Control of the virtual disk is now relinquished to the new owner. At step the new owner or VMX thread in this example receives an acknowledgement that the virtual disk has been closed. The new owner can then reattempt to open the virtual disk as shown at step . The method then returns to step to determine if the open is successful. If the open is not successful the process of contacting the current owner of the virtual disk and requesting release of the virtual disk may be repeated.

Certain embodiments as described above involve a hardware abstraction layer on top of a host computer. The hardware abstraction layer allows multiple contexts to share the hardware resource. In one embodiment these contexts are isolated from each other each having at least a user application running therein. The hardware abstraction layer thus provides benefits of resource isolation and allocation among the contexts. In the foregoing embodiments virtual machines are used as an example for the contexts and hypervisors as an example for the hardware abstraction layer. As described above each virtual machine includes a guest operating system in which at least one application runs. These embodiments may also apply to other examples of contexts such as containers not including a guest operating system referred to herein as OS less containers see e.g. www.docker.com . OS less containers implement operating system level virtualization wherein an abstraction layer is provided on top of the kernel of an operating system on a host computer. The abstraction layer supports multiple OS less containers each including an application and its dependencies. Each OS less container runs as an isolated process in user space on the host operating system and shares the kernel with other containers. The OS less container relies on the kernel s functionality to make use of resource isolation CPU memory block I O network etc. and separate namespaces and to completely isolate the application s view of the operating environments. By using OS less containers resources can be isolated services restricted and processes provisioned to have a private view of the operating system with their own process ID space file system structure and network interfaces. Multiple containers can share the same kernel but each container can be constrained to only use a defined amount of resources such as CPU memory and I O.

The various embodiments described herein may employ various computer implemented operations involving data stored in computer systems. For example these operations may require physical manipulation of physical quantities usually though not necessarily these quantities may take the form of electrical or magnetic signals where they or representations of them are capable of being stored transferred combined compared or otherwise manipulated. Further such manipulations are often referred to in terms such as producing identifying determining or comparing. Any operations described herein that form part of one or more embodiments of the invention may be useful machine operations. In addition one or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines may be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The various embodiments described herein may be practiced with other computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like.

One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system computer readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive network attached storage NAS read only memory random access memory e.g. a flash memory device a CD Compact Discs CD ROM a CD R or a CD RW a DVD Digital Versatile Disc a magnetic tape and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

Although one or more embodiments of the present invention have been described in some detail for clarity of understanding it will be apparent that certain changes and modifications may be made within the scope of the claims. Accordingly the described embodiments are to be considered as illustrative and not restrictive and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims elements and or steps do not imply any particular order of operation unless explicitly stated in the claims.

Virtualization systems in accordance with the various embodiments may be implemented as hosted embodiments non hosted embodiments or as embodiments that tend to blur distinctions between the two are all envisioned. Furthermore various virtualization operations may be wholly or partially implemented in hardware. For example a hardware implementation may employ a look up table for modification of storage access requests to secure non disk data.

Many variations modifications additions and improvements are possible regardless the degree of virtualization. The virtualization software can therefore include components of a host console or guest operating system that performs virtualization functions. Plural instances may be provided for components operations or structures described herein as a single instance. Finally boundaries between various components operations and data stores are somewhat arbitrary and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention s . In general structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly structures and functionality presented as a single component may be implemented as separate components. These and other variations modifications additions and improvements may fall within the scope of the appended claim s .

