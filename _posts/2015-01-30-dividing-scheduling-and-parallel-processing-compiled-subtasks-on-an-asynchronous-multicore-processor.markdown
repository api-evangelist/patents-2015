---

title: Dividing, scheduling, and parallel processing compiled sub-tasks on an asynchronous multi-core processor
abstract: An asynchronous multiple-core processor may be adapted for carrying out sets of known tasks, such as the tasks in the LAPACK and BLAS packages. Conveniently, the known tasks may be handled by the asynchronous multiple-core processor in a manner that may be considered to be more power efficient than carrying out the same known tasks on a single-core processor. Indeed, some of the power savings are realized through the use of token-based single core processors. Use of such token-based single core processors may be considered to be power efficient due to the lack of a global clock tree.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09400685&OS=09400685&RS=09400685
owner: Huawei Technologies Co., Ltd.
number: 09400685
owner_city: Shenzhen
owner_country: CN
publication_date: 20150130
---
The present application relates generally to processors and more specifically to an asynchronous multiple core processor.

Modern processors may be considered to be following a trend toward forming what appears from the outside to be a single processor from multiple processors. That is a plurality of core processors or simply cores may be grouped to act as a single processor. Beneficially multiple core processors may be seen to have relatively small size and relatively low electrical power consumption when compared to single core processors. However obstacles related to use of multiple core processors include complicated development due to low compatibility. A given software application developed for a four core processor from one manufacturer may not work properly when executed on an eight core processor from a different manufacturer.

Linear Algebra PACKage LAPACK is a standard software library for numerical linear algebra. Conveniently LAPACK provides routines for solving systems of linear equations and linear least squares Eigen value problems and singular value decomposition.

Basic Linear Algebra Subprograms BLAS are a specified set of low level subroutines that carry out common linear algebra operations such as copying vector scaling vector dot products linear combinations and matrix multiplication. The BLAS were first published as a FORTRAN library in 1979 and are still used as building blocks in higher level math programming languages and libraries including LAPACK.

BLAS subroutines may be considered to be a de facto standard Application Programming Interface API for linear algebra libraries and routines. Several BLAS library implementations have been tuned for specific computer architectures. Highly optimized implementations have been developed by hardware vendors.

It has been noticed that LAPACK and BLAS although originally license free FORTAN Linear Algebra Libraries have become widely accepted industrial routines. Accordingly many commercial software packages make use of the LAPACK BLAS libraries. Many chip providers sell chip oriented LAPACK BLAS libraries. An advantage of LAPACK BLAS lies in the separation of high level software programmers from the low level chip related optimization of linear algebra problems.

However it may be considered that implementation of the LAPACK and BLAS packages are not performance power efficient on a single core processor.

In overview it is proposed herein to adapt an asynchronous multiple core processor for carrying out sets of known tasks such as the tasks in the LAPACK and BLAS packages. Conveniently the known tasks may be handled by the asynchronous multiple core processor in a manner that may be considered to be more power efficient than carrying out the same known tasks on a single core processor. Indeed some of the power savings are realized through the use of token based single core processors. Use of such token based single core processors may be considered to be power efficient mainly due to the lack of a global clock tree.

Several patent applications have been filed recently to protect token based single core processors. These patent applications include U.S. patent application Ser. No. 14 480 531 filed Sep. 8 2014 U.S. patent application Ser. No. 14 480 556 filed Sep. 8 2014 U.S. patent application Ser. No. 14 480 561 filed Sep. 8 2014 and U.S. patent application Ser. No. 14 325 117 filed Jul. 7 2014 the contents of all of which are incorporated herein by reference.

According to an aspect of the present disclosure there is provided an asynchronous multiple core processor. The asynchronous multiple core processor includes a plurality of self timed core processors linked with a network bus a task divider and a task scheduler. The task divider receives a task divides the task into a plurality of sub tasks transmits to a software library an indication of a selected sub task selected from among the plurality of sub tasks receives from the software library a compiled version of the selected sub task and transmit to a sub task scheduler the compiled version of the selected sub task. The sub task scheduler is configured to receive the compiled version of the selected sub task prepares a schedule for the execution of the compiled version of the selected sub task in context with other compiled versions of sub tasks formulates based on the schedule an instruction stream and broadcast the instruction stream to the plurality of self timed core processors.

According to another aspect of the present disclosure there is provided a method of handling a task in an asynchronous multiple core processor that includes a plurality of self timed core processors linked with a network bus. The method includes at a task divider receiving the task dividing the task into a plurality of sub tasks transmitting to a software library an indication of a selected sub task selected from among the plurality of sub tasks receiving from the software library a compiled version of the selected sub task and transmitting to a task scheduler the compiled version of the selected sub task. The method further includes at the task scheduler receiving a compiled version of the sub task sensing availability of the plurality of self timed core processors preparing a schedule for the execution of the compiled version of the sub task in context with other compiled versions of sub tasks formulating based on the schedule and the availability an instruction stream and broadcasting the instruction stream to the plurality of self timed core processors. In other aspects of the present application a computer readable medium is provided for adapting an asynchronous multiple core processor to carry out this method.

Other aspects and features of the present disclosure will become apparent to those of ordinary skill in the art upon review of the following description of specific implementations of the disclosure in conjunction with the accompanying figures.

Notably the plurality of self timed cores lack private instruction memory. All of the plurality of self timed cores share the same instruction fetch unit. That is the plurality of self timed cores are connected to a sub task scheduler dispatcher . The sub task scheduler dispatcher is connected to a task divider . The task divider maintains a connection to an instruction memory and to a software library . The software library contains pre compiled programs for carrying out sub tasks at the plurality of self timed cores .

The plurality of self timed cores maintain connections to select ones of each other and to a shared memory and to a common resource .

As illustrated in the example self timed core also includes a private memory an instruction queue and a plurality of ports. The plurality of ports includes a port to the shared memory of . The plurality of ports also includes a port to the common resource of . The plurality of ports further includes a port to the other self timed cores and to the sub task scheduler dispatcher in the self timed multiple core processor of .

Each ALU has its own link to the register file to the private memory to the port to the common resource to the port to the other self timed cores and to the port to the shared memory .

To prepare the self timed multiple core processor for use a LAPACK BLAS routine task is divided into sub tasks so that elements of the task as a whole may be performed in parallel where practical by the plurality of self timed cores . The manner in which a given task may be divided into sub tasks may be stored in the instruction memory . Furthermore versions of each of the sub tasks may be stored in software library . Such versions are compiled especially for execution by one of the plurality of self timed cores .

In operation a task is received at the self timed multiple core processor . illustrates example steps in a method carried out by the task divider of handling the received task. Initially the task is received step by the task divider . Using information stored in the instruction memory the task divider divides step the received task into a plurality of sub tasks.

The task divider then selects step one of the sub tasks and transmits step an indication of the selected sub task to the software library . From the software library the task divider receives step a version of the selected sub task. The version of the sub task has been compiled ahead of time for execution by one of the plurality of self timed cores . The task divider then transmits step the received version of the selected sub task to the sub task scheduler dispatcher .

The task divider may then determine step whether all of the sub tasks of the received task have been considered. Upon determining step that not all of the sub tasks of the received task have been considered the task divider selects step another one of the sub tasks and repeats the transmitting step receiving step transmitting step and determining step . Upon determining step that all of the sub tasks of the received task have been considered the task divider may consider the method to be complete.

In operation at the sub task scheduler dispatcher the compiled version of each of the sub tasks is received step from the task divider . The sub task scheduler dispatcher may prepare step a schedule for execution of the compiled sub tasks. It will be appreciated that some compiled sub tasks may be executed simultaneously with other compiled sub tasks while execution of other compiled sub tasks may depend on the complete execution of specific compiled sub tasks. Once the sub task scheduler dispatcher has prepared step a schedule for execution of the compiled sub tasks the sub task scheduler dispatcher may then associate step each compiled sub task with a core index for a respective one of the self timed cores . The sub task scheduler dispatcher may then formulate step based on the schedule and the associating an instruction stream. The sub task scheduler dispatcher may then broadcast step the instruction stream to all of the self timed cores .

In operation at each of the self timed cores the instruction stream is received step at the instruction queue . If the instruction queue is full the instruction queue transmits a queue full indication to the sub task scheduler dispatcher . For clarity the instruction stream is illustrated in as being received directly at the instruction queue . It should be clear that the instruction stream is received at the instruction queue via the port to the other self timed cores and to the sub task scheduler dispatcher .

The example self timed core may select step an instruction in the instruction stream and examine the core index associated with the selected instruction to determine step whether the core index associated with the instruction is a match for the core index associated with the example self timed core . Upon determining step that the core index associated with the instruction is a match for the core index associated with the example self timed core the example self timed core may determine step whether the instruction queue is full. Responsive to determining step that the instruction queue is full the example self timed core may send step a queue full indication to the sub task scheduler dispatcher . Responsive to determining step that the instruction queue is not full the example self timed core may add step the instruction to the instruction queue . Upon determining step that the core index associated with the compiled sub task is not a match for the core index associated with the example self timed core the example self timed core may ignore the compiled sub task.

Subsequent to the instruction being added to the instruction queue the feedback engine may fetch the instruction from the instruction queue . The feedback engine may maintain a scoreboard table to detect and register the data dependency among the instructions. Furthermore the feedback engine may dispatch a registered instruction to one ALU in a program counter order. To avoid resource conflicts among the ALUs tokens are used to allow only one ALU to access one resource at a given period of time. The output of an ALU can be immediately transmitted or multicast to any of the other ALUs .

To form a pipeline or something similar a processor may be equipped with mechanisms for 1 preserving the program counter PC order 2 detecting and resolving structural hazards and 3 detecting and resolving data hazards. In the token based self timed core processor the mechanism for 1 and the mechanism for 2 are realized by its token system and the mechanism for 3 by is realized by the crossbar interconnection bus and the feedback engine scoreboard .

A couple of the self timed ALUs may be serially linked by several special asynchronous signals named as tokens. A token is a special asynchronous edge sensitive signal that goes through the first ALU the second ALU up to the Nth ALU N 1. After being issued from the Nth ALU N 1 a token signal passes into an inverter that inverts the signal polarity and then passes the inverted token signal to the first ALU . When a token reaches a given ALU the given ALU is said to own the token. The property that only one ALU holds the ownership of a given token at any instant of time enables the token to be a good candidate to resolve a structural hazard for common resources. While owning a token an ALU may not consume it immediately. Instead the ALU may lock the token by a latch or SR flip flop logic until the consumption conditions for the token are satisfied. Alternatively the ALU may pass the token signal to the next ALU as quickly as possible upon deciding not to consume the token. Usually an ALU has made a decision about a particular token prior to the arrival of the particular token. The two ways the tokens are processed are referred to as consuming a token or bypassing a token.

The pipeline may be achieved by the token system in the following two aspects an intra ALU token gating system or an inter ALU token passing system.

In the intra ALU token gating system certain tokens gate other tokens. That is releasing one token becomes a condition to consuming another token. The gating signals from the preceding tokens are input into the consumption condition logic of the gated token. For example a launch token may generate an active signal to a register read token when released to the next ALU which establishes that any ALU will not read the register file until an instruction is officially started by the launch token.

In the inter ALU token passing system a consumed token signal may trigger a pulse to a common resource. For example a register access token may trigger a pulse to the register file . Meanwhile the token signal is delayed before the token signal is released to the next ALU for such a period that there is no structural hazard on a common resource between ALU n and ALU n 1 .

Tokens may be considered to not only preserve an ability for multiple ALUs to launch and commit instructions in the PC order but also to avoid structural hazards among the multiple ALUs .

The data hazard is detected and resolved by the feedback engine and the crossbar bus . Multiple ALUs are linked by the crossbar bus . In general each ALU has one output to the crossbar bus and three inputs for clarity not shown from the crossbar bus .

The RAW read after write hazard may be avoided as follows. When an ALU writes to the crossbar bus the ALU broadcasts a done signal on the crossbar bus to inform other ALUs . When an ALU requests data from the crossbar bus the ALU monitors the done signal from the targeted ALU . If the done signal has been broadcast the ALU pulls the data from the crossbar bus . If the done signal has not been broadcast the ALU waits for the done signal to be broadcast by the targeted ALU. In this way the data hazard among the instructions on different ALUs may be resolved.

The register and memory commitment may be considered to be in a post commit mode writing to the register file and or to the private memory take place after the commit token is released. The crossbar bus may play the role of register renaming to avoid a WAR write after read and a WAW write after write .

Instructions that come from the instruction queue may pass through the feedback engine that detects the data dependency by for example using a history table. The feedback engine may pre decode the instruction to decide how many input operands the instruction requires. Subsequently the feedback engine may look to the history table to find whether a given piece of data is in the crossbar bus or in the register file . If the data remains in the crossbar bus the feedback engine may calculate which ALU produces the data. This information may be tagged to the instruction dispatched to the ALUs .

At the end of each sub task there may be a return instruction and a barrier synchronization instruction. The feedback engine may receive a return signal from the ALU that completes the last instruction. Upon receipt of the return signal from the ALU the feedback engine may report to the sub task scheduler dispatcher . The results of the execution of the instructions in the sub task are maintained in an address in the shared memory . The address in the shared memory at which the results of the execution of the instructions in the sub task are maintained may be pre established by the task received in step see . Accordingly the instructions in each subsequent sub task can access the results.

Upon completion of execution of all the instructions in the compiled sub tasks by the self timed cores it may be considered that the task received by the task divider in step has been completed. It is expected that the entity from which the task is received in step can retrieve the results of the execution of the task from the pre established address in the shared memory .

If for example the task was a LAPACK BLAS routine the self timed multiple core processor may return the result in a format specified in an API for such LAPACK BLAS routines.

For example consider the task of matrix addition. As part of an API a processor external to the self timed multiple core processor may call a specific matrix addition API by providing two matrices and a request that the two matrices be added. The API call may be received step at the self timed multiple core processor as a task. The task divider may divide step the matrix addition task into sub tasks. Notably addition of two relatively large matrices may be divided into a plurality of distinct addition operations sub tasks on smaller matrixes.

The task divider may select step a sub task and transmit step the selected sub task to the software library . Notably the task divider may receive step compiled versions of many sub tasks from different tasks at the same time. It may be that some sub tasks request more resources e.g. more self timed cores than other sub tasks.

Based on the compiled versions of sub tasks received step from the task divider the sub task scheduler may formulate step an instruction stream for broadcast step to the self timed cores . Notably the sub task scheduler dispatcher may sense detect or otherwise determine the current availability of each of the self timed cores . Consequently the formulating step of the instruction stream may be carried out as a function of the current availability of the self timed cores .

These distinct addition operations sub tasks may be carried out in parallel by the plurality of self timed cores .

Subsequent to results being determined for each of the sub tasks the sub task scheduler dispatcher may formulate step and broadcast step an instruction stream including a further sub task to combine the sub task results to form a matrix that is the final result of the requested matrix addition operation.

The self timed multiple core processor may then return the result of the matrix addition operation as a reply to the matrix addition API call.

Conveniently the task divider the sub task scheduler dispatcher and the software library are programmable. Accordingly two distinct people may opt to divide a given task into sub tasks in two distinct manners. Correspondingly the compiled versions of the sub tasks in the software library will also be distinct. Furthermore the programming of the sub task scheduler to define a scheduling strategy may be tied to the manner in which the given task has been divided into sub tasks.

Conveniently the decisions regarding the manner in which a task is divided into sub tasks the programming and compiling of the sub tasks and the scheduling of the sub tasks may be left to experts thereby relieving programming effort from those who merely want to arrange that the tasks are carried out.

The performance of a multiple core processor may be attributed in part to parallelism. The parallelism can be enhanced on at least three different levels an instruction level a thread level and a processor level. Enhancement of the performance of a multiple core processor may be accomplished by improving parallelism.

In the framework of the present application the improvement of the parallelism may achieved by both software and hardware.

At the level of a single self timed core responsive to receiving step an instruction stream the self timed core uses the instruction queue to improve instruction level parallelism ILP . This is an example of achieving parallelism improvement through a hardware implementation.

At the level of the plurality of self timed cores it may be considered that software controls the manner in which a loop may be decomposed into multiple loop bodies where each loop body is executed by one of the self timed cores thereby improving thread level parallelism TLP . This is an example of achieving parallelism improvement through a software implementation in combination with a hardware implementation.

On top of the ILP and TLP aspects of the present application have introduced the task and the related sub tasks. A task may be considered to correspond to a LAPACK BLAS routine call. The combination of software and hardware proposed in the present application allows for reception and scheduling of several tasks at the same time if the tasks are independent of each other. Accordingly the processor has two further levels of parallelism task level parallelism and sub task level parallelism.

Each self timed core of a plurality of cores can work at a slower rate than a comparable single core processor. As a result of employing a plurality of such cores to carry out one routine it may be seen that power efficiency is enhanced. Conveniently if a given self timed core is not provided with a compiled sub task to execute then the given self timed core does not consume dynamic power.

Routines defined in the LAPACK and BLAS packages are widely used in many domains. The combination of software and hardware proposed in the present application may act as a replacement for a software implementation of a LAPACK BLAS library of routines.

It has been noted hereinbefore that the parallelization of the execution of the sub tasks is programmable and as such a programmer can focus on how to maximally parallelize a routine. However it is further noted that once the programmer is content with the manner in which a routine has been parallelized there is unlikely to be a need to rewrite code to accommodate a change from a multiple core processor with one number of cores to a multiple core processor with more cores.

The combination of software and hardware proposed in the present application may be seen to realize an advantage in that the bottleneck to access each instruction may be considered to have been overcome. Many consider that with modern processors memory access throughput rather than computational logics has become the primary performance bottleneck.

It has been noted that a self timed core does not need a global clock tree. It may be shown that eliminating a global clock tree may reduce the power of the processor by as much as save 30 . More importantly as the number of cores increases up to hundreds and thousands it may become increasingly less practical to have a global tree on a large die.

If the number of cores increases up to hundreds or even thousands heat reduction becomes an issue for a multi core processor. The heat reduction issue in turn presents a difficulty for backend routing. Conveniently in aspects of the present application it may be shown that idle self timed cores use little to no power and accordingly generate little to no heat.

Most probably a software application cannot use up all of the self timed cores . It is anticipated that some of the self timed cores will be idle at least some of the time. In a synchronous design power gating is required to switch off idle cores. However such power gating requires a certain level of granularity. For example it may be considered much more costly to allocate a power area for a single core than for a group of cores. Accordingly the granularity required for a synchronous design may be considered to be great. In contrast since there is no clock on the self timed core the granularity in aspects of the present application may be considered to be very small. Every self timed core that enters into idle when no compiled sub task is being executed consumes little to no power.

The synchronous core needs the clock signal to check the instruction availability and update the state of its own resource usage. However a self timed core can function like a queue when all of its resources computation and logic unit are busy the self timed core can automatically push a received compiled sub task back to the sub task scheduler dispatcher . This feature provides a natural indicator of the status of the self timed cores for the sub task scheduler dispatcher . Responsively the sub task scheduler dispatcher may dynamically schedule multiple sub tasks as a function of the instant status of the cores.

The above described implementations of the present application are intended to be examples only. Alterations modifications and variations may be effected to the particular implementations by those skilled in the art without departing from the scope of the application which is defined by the claims appended hereto.

