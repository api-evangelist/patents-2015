---

title: Emotion recognition in video conferencing
abstract: Methods and systems for videoconferencing include recognition of emotions related to one videoconference participant such as a customer. This ultimately enables another videoconference participant, such as a service provider or supervisor, to handle angry, annoyed, or distressed customers. One example method includes the steps of receiving a video that includes a sequence of images, detecting at least one object of interest (e.g., a face), locating feature reference points of the at least one object of interest, aligning a virtual face mesh to the at least one object of interest based on the feature reference points, finding over the sequence of images at least one deformation of the virtual face mesh that reflect face mimics, determining that the at least one deformation refers to a facial emotion selected from a plurality of reference facial emotions, and generating a communication bearing data associated with the facial emotion.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09576190&OS=09576190&RS=09576190
owner: Snap Inc.
number: 09576190
owner_city: Venice
owner_country: US
publication_date: 20150318
---
This disclosure relates generally to video conferencing and more particularly to systems and methods for recognizing emotions of participants in video conferencing.

Today video conferencing and videophone calls are popular tools for conducting two way video and audio communications over long distances. This technology has been developing rapidly due to the emergence of high speed networking solutions inexpensive hardware components and deployment of cellular networks. Typically video conferencing allows two or more individuals to communicate with each other using a variety of software applications such as video chat applications where the participants can view each other while talking. Video chats can be available on general purpose computers mobile devices and television systems as downloadable software applications or web services. Traditional hardware requirements for video conferencing include on each side an input audio module e.g. a microphone input video module e.g. a video camera output audio module e.g. speakers output video module e.g. a display or projector and a computing device that ties together input and output modules compresses and decompresses audio and video streams and initiates and maintains the data linkage via a communications network.

Videoconferencing has become popular in the customer service and support industry by providing direct communication with customers regardless of their location. For example video chat can enable face to face interactions between customer service representatives and customers. Typically chat sessions can be initiated from kiosks mobile devices and web and social media channels. This allows companies to provide personalized attention to each customer and conduct video interviews sales promotions services support and other video collaboration.

Although videoconferencing creates a useful channel to provide services to customers one challenging issue is to deal with irate annoyed and distressed customers. Customer anger is not always easy to spot even to professional service providers or sale representatives. One of the important strategies in sales and customer service is to detect when a customer is angry or annoyed and prevent difficult situations at a very early stage. Unfortunately it is a very difficult task even to experienced professionals.

This disclosure relates generally to the technology for video conferencing in which an emotional status of participating individuals can be recognized. The videoconferencing may include two or more participants including for example a customer and a customer service representative. If the recognized emotional status relates to a negative emotion such as anger stress irritation and annoyance the emotional status can be reported to one of the videoconference participants and or a third party such as a supervisor administrator or manager. Optionally the third party can enter into the videoconference between the individuals to resolve any issues. The technology allows determining the emotional status by analyzing a video channel to detect facial emotions and or an audio channel to detect speech emotions. The present technology can recognize facial emotions by locating feature reference points e.g. facial landmarks on the video aligning a virtual face mesh to the feature reference points and finding mesh deformations that reflect face mimics. Speech emotions can be recognized by extracting voice features and determining changes thereof.

According to one aspect of the technology a computer implemented method for videoconferencing is provided. The method comprises the steps of receiving a video including a sequence of images detecting at least one object of interest in one or more of the images e.g. a face locating feature reference points of the at least one object of interest aligning a virtual face mesh also referred herein to as mesh for simplicity to the at least one object of interest in one or more of the images based at least in part on the feature reference points finding over the sequence of images at least one deformation of the mesh that is associated and reflect at least one face mimic of the at least one object of interest determining that the at least one deformation refers to a facial emotion selected from a plurality of reference facial emotions and generating a communication bearing data associated with the facial emotion.

In some embodiments the determination that the at least one deformation refers to the facial emotion selected from the plurality of reference facial emotions can include comparing the at least one deformation of the mesh to reference facial parameters of the plurality of reference facial emotions and selecting the facial emotion based on the comparison of the at least one deformation of the mesh to the reference facial parameters of the plurality of reference facial emotions. In certain embodiments the comparing of the at least one deformation of the mesh to reference facial parameters can comprise applying a convolution neural network. In certain embodiments the comparing of the at least one deformation of the mesh to reference facial parameters can comprise applying a state vector machine.

In various embodiments the method may further comprise establishing a one way or two way videoconferencing between a service provider and a customer wherein the video is captured on a customer side. The method may further comprise transmitting the communication over a communications network to a third party. The method may further comprise allowing the third party to enter into the videoconference between the customer and the service provider if the facial emotion associated with the at least one deformation of the mesh relates to a negative facial emotion. In some embodiments the method may further comprise transmitting and presenting the communication to a customer service representative or a service provider.

In certain embodiments the at least one object of interest includes a face of an individual. In certain embodiments the feature reference points can include facial landmarks. In certain embodiments the feature reference points can include one or more facial landmarks indicating at least one of the following an eyebrows vertical position an eyes vertical position an eyes width an eyes height an eye separation distance a nose vertical position nose pointing up a mouth vertical position a mouth width a chin width a upper lip raiser a jaw drop a lip stretcher a left brow lowerer a right brow lowerer a lip corner depressor and an outer brow raiser.

In various embodiments the method may further comprise receiving a request to determine facial emotions of a video conferencing participant. In some embodiments the detecting of the at least one object of interest can include applying a Viola Jones algorithm to the images. In various embodiments the locating of the feature reference points can include applying an Active Shape Model algorithm to areas of the images associated with the at least one object of interest.

In certain embodiments the aligning of the mesh can be based on shape units SUs associated with a face shape of the at least one object of interest. In one example embodiment the step of aligning the mesh can comprise estimating intensities of the SUs associated with the face shape estimating intensities of action units AUs associated with the at least one face mimic and estimating of rotations of the virtual face mesh around three orthogonal axes and its translations along the axes. In certain embodiments the step of detecting the at least one object of interest can be based on a user input. In some embodiments the plurality of facial emotions can include at least a neutral facial emotion a positive facial emotion and a negative facial emotion. The negative facial emotion can include at least one of anger stress frustration embarrassment irritation and annoyance.

In various embodiments the method may further comprise receiving an audio stream associated with the video and recognizing a speech emotion of the at least one object of interest in the audio stream. The recognizing of the speech emotion may comprise extracting at least one voice feature from the audio stream comparing the extracted at least one voice feature to a plurality of reference voice features and selecting the speech emotion based on the comparison of the extracted at least one voice feature to the plurality of reference voice features. In some embodiments the recognizing of the speech emotion can comprise recognizing a speech in the audio stream. In some embodiments the communication can further include data associated with the speech emotion. In yet more embodiments the method may further comprise combining the facial emotion and the speech emotion to generate an emotional status of an individual associated with the at least one object of interest. In yet more embodiments the method may further comprise detecting one or more gestures determining that the one or more gestures refer to a predetermined emotion and generating an emotional status of an individual based on the facial emotion and determination that the one or more gestures refer to the predetermined emotion.

According to another aspect of the technology a computing system is provided. An example system comprises a computing device including at least one processor and a memory storing processor executable codes which when implemented by the least one processor cause the device to perform the method steps described above.

According to yet another aspect of the technology there is provided a non transitory processor readable medium having instructions stored thereon which when executed by one or more processors cause the one or more processors to implement the method steps described above.

Additional objects advantages and novel features will be set forth in part in the detailed description which follows and in part will become apparent to those skilled in the art upon examination of the following detailed description and the accompanying drawings or may be learned by production or operation of the example embodiments. The objects and advantages of the concepts may be realized and attained by means of the methodologies instrumentalities and combinations particularly pointed out in the appended claims.

The following detailed description includes references to the accompanying drawings which form a part of the detailed description. The drawings show illustrations in accordance with example embodiments. These example embodiments which are also referred to herein as examples are described in enough detail to enable those skilled in the art to practice the present subject matter.

The embodiments can be combined other embodiments can be utilized or structural logical and operational changes can be made without departing from the scope of what is claimed. The following detailed description is therefore not to be taken in a limiting sense and the scope is defined by the appended claims and their equivalents.

Present teachings may be implemented using a variety of technologies. For example the methods described herein may be implemented in software executing on a computer system or in hardware utilizing either a combination of microprocessors or other specially designed application specific integrated circuits ASICs programmable logic devices or various combinations thereof. In particular the methods described herein may be implemented by a series of computer executable instructions residing on a transitory or non transitory storage medium such as a disk drive or computer readable medium. It should be noted that methods disclosed herein can be implemented by a server network device general purpose computer e.g. a desktop computer tablet computer laptop computer mobile device e.g. cellular phone smart phone game console handheld gaming device television system set top box in vehicle computing device kiosk and so forth.

The present technology provides for methods and systems for videoconferencing which allow for determining an emotional status of at least one videoconference participant by analyzing video images and or audio associated with the videoconference participant. The videoconferencing may include two or more participants including for example a customer and a customer service representative. Emotional status can be determined by identifying facial emotions and or speech emotions. For these ends the present technology allows for tracking changes in facial expressions and or voice features over time. In various embodiments facial emotions can be identified by locating feature reference points of the videoconference participant aligning a virtual face mesh also referred to as mesh for simplicity based at least in part on the feature reference points finding or determining mesh changes that reflect one or more face mimics and comparing the mesh changes with reference facial parameters of a plurality of reference emotions stored in a database. Speech emotions can be identified by detecting and analyzing voice features and comparing voice features to a plurality of reference voice features stored in a database. Once the emotional status is identified it can be reported to one of the videoconference participants and or a third party such as a supervisor manager moderator or administrator. For example when the emotional status of one videoconference participant is identified and reported to relating to a negative emotion such as anger stress irritation and annoyance another videoconference participant can start adjusting his speech initiate managing of customer anger and the like. In addition a supervisor may be enabled to start watching the process of communicating between these participants and optionally enter into the videoconference to resolve negative emotion issues.

The term videoconferencing as used herein refers to a telecommunication technology which allows two or more people to communicate by simultaneous two way video and audio transmissions. The video transmissions include communicating a plurality of video images. In this disclosure the term video conferencing incorporates similar terms including for example videophone calling videotelephony video teleconferencing and video chat among others.

As discussed below in details the core element of this technology is locating and tracking individuals in video images and further detecting facial expressions and optionally individual gestures so as to determine an emotional status. According to various embodiments facial expressions can be detected and tracked using a variety of video processing algorithms. For example individual faces can be detected using the combination of Viola Jones algorithm which is targeted to detect a face in video images and an Active Shape Model ASM algorithm which is designed to locate feature reference points associated with the face. Once faces are detected a mesh based on the feature reference points can be aligned to the individuals in the video images. Further changes and deformations of the mesh can be found and analyzed. In some embodiments changes and deformations of the mesh that reflect and associated with face mimics can be compared to reference facial parameters and based on the comparison it can be determined that the mesh deformation refers to a facial emotion selected from a plurality of reference facial emotions. The emotional status of an individual can be based at least in part on the facial emotion. In some embodiments the emotional status is transmitted to one or more videoconference participants or third parties.

The video processing algorithm as described herein can be applied to a video stream in real time or it can be applied to a stored video file including progressive download solutions . Moreover in some embodiments the video processing is applied to each video images individually while in other embodiments the video processing can be applied to a video as a whole. It should be also noted that the video processing steps can be implemented on either a client side a server side or both depending on a particular system s architecture.

In general video conferencing can be implemented using one or more software applications running on a client side server side or both. In some embodiments the video conferencing can be implemented as a web service or as a cloud solution meaning it is available to videoconference participants via a website or web interface.

Client devices refer to but are not limited a user device terminal computing device e.g. laptop computer tablet computer desktop computer cellular phone smart phone personal digital assistant PDA gaming console remote control multimedia system television system set top box infotainment system in vehicle computing device informational kiosk and so forth. Each of client devices has a video chat application . The video chat applications are generally configured to enable video conferencing between two or more users. Video chat applications can be implemented as software middleware or firmware and can be separate application or constitute a part of larger software applications.

At least one of client devices further includes emotion recognition module which is configured to detect a particular emotional status of at least one user e.g. the customer and report it to at least one other user e.g. the customer service representative and or supervisor . In some embodiments emotion recognition module is a separate software application while in other embodiments emotion recognition module is a part of another software application such as video chat application web service and so forth.

As shown in client devices are connected into a peer to peer P2P network allowing their direct video teleconferencing with each other. Data between nodes can be exchanged directly using for example TCP IP Transmission Control Protocol Internet Protocol network communication standards. In some embodiments the P2P network can include more than three client devices .

In some embodiments video streaming between client devices can occur via server such that client devices are responsible for audio and video capture audio and video delivery and data transfer. In other embodiments server provides emotion recognition processes only while client devices implement the remaining communication tasks.

As shown in this figure system includes the following hardware components at least one processor at least one memory at least one storage device at least one input module at least one output module and at least one network interface . System also includes optional operating system video chat application and optional emotion recognition module .

In various embodiments processor implements functionality and or processes instructions for execution within the system . For example processor may process instructions stored in memory and or instructions stored on storage devices . Such instructions may include components of operating system and video chat application . System may include multiple processors such as a central processing unit CPU and graphic processing unit GPU which can share operational tasks with each other.

Memory is configured to store information within system during operation. Memory in some example embodiments refers to a non transitory computer readable storage medium or a computer readable storage device. In some examples memory is a temporary memory meaning that a primary purpose of memory may not be long term storage. Memory may also refer to a volatile memory meaning that memory does not maintain stored contents when memory is not receiving power. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples memory is used to store program instructions for execution by the processor . Memory may be also used to temporarily store information during program execution.

Storage device can include one or more transitory or non transitory computer readable storage media and or computer readable storage devices. In some embodiments storage device may be configured to store greater amounts of information than memory . Storage device can be further configured for long term storage of information. In some examples storage device includes non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs optical discs solid state discs flash memories forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM and other forms of non volatile memories known in the art.

Still referencing to system includes one or more input modules for receiving user inputs and one or more output modules for delivering data to a user. Input modules may include keyboard trackball touchscreen microphone video camera web camera and the like. Output modules may include any appropriate device to deliver data through visual or audio channels including displays monitors printers touchscreens speakers and so forth.

System further includes network interface which is configured to communicate with external devices servers and network systems via one or more communications networks . Network interface may be a network interface card such as an Ethernet card optical transceiver radio frequency transceiver or any other device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G Third Generation 4G Fourth Generation LTE Long Term Evolution and WiFi radios.

Operating system may control one or more functionalities of system or components thereof. For example operating system may interact with video chat application and may further facilitate interactions between video chat application and processor memory storage device input modules output modules and or network interface . Video chat application is configured to provide video conferencing services by implementing two way audio and video communications with another system. System may also include emotion recognition module for recognizing emotional statuses of video conference participants generating reports or notices associated with particular recognized emotional statuses and allowing third parties to enter invoke into a videoconference process. As described below emotion recognition module can determine emotional statuses by analyzing particular features in video and or audio streams. In some embodiments system may include additional software applications including for example web browsers encoders codecs encryption application and so forth.

A typical operation of system is as follows. Video chat application provides an interface including a graphical user interface enabling to initiate and conduct a videoconference between two or more individuals. A camera i.e. within input module captures a first video stream which may include a first individual e.g. a customer service representative . A microphone i.e. same input module captures a first audio stream which may include a speech of the first individual. The first audio and video streams are then transmitted to another system for video processing client device or a server via network interface . In some embodiments the first audio stream and or the first video stream can be modified by system such as by making visual or acoustic adjustments compression encryption and the like. System may also receive videoconference communications from another system for videoconferencing client device or server which communications may include second audio stream and or second video stream. These streams include video and audio content associated with a second individual e.g. a customer . The second audio and or video streams may be optionally modified for example by making visual or acoustic adjustments decompression decryption and the like. The second audio stream can be presented to the first individual via speakers output module . The second video stream can be played back on a display same output module . Upon receipt of the second audio and or video streams emotion recognition module may determine an emotional status of the second individual i.e. the customer . If it is determined that the emotional status refers to customer s anger stress irritation or annoyance emotion recognition module generates an alert or any other suitable communication and sends it to attention of the first individual customer service representative and or optionally to a remote device for attention of a third individual such as a manager supervisor administrator or moderator. The alert or communication regarding the emotional status of the second individual can be displayed via the graphical user interface of video chat application . Emotion recognition module can also allow the third individual to enter into the videoconference between the first individual and second individual. In certain embodiments the videoconferencing between the second individual and first individual can be switched to videoconferencing between the second individual i.e. the customer and third individual e.g. the supervisor .

The instant technology allows for recognizing emotional statuses of video conference participants. To these ends this technology detects individuals and more specifically individual faces presented in a video stream and tracks facial emotions over time. This section is focused on example processes for face detection as can be used in the present technology.

According to various embodiments a face in an image can be detected by application of a Viola Jones algorithm and an ASM algorithm. In particular a Viola Jones algorithm is a fast and quite accurate method for detecting a face region on an image. An ASM algorithm is applied to the face region to locate reference feature points associated with the face. These feature reference points can include one or more facial landmarks such as ala philtrum vermilion zonem vermilion border nasolabial sulcus labial commissures lip tubercle nasion outer canthos of eye inner canthos of eye and tragus of ear. Moreover the feature reference points can include one or more of the following facial points indicating eyebrows vertical position eyes vertical position eyes width eyes height eye separation distance nose s vertical position nose s pointing up mouth s vertical position mouth s width chin s width upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser. shows an example image of a face where some of reference feature points are illustrated.

Further an ASM algorithm starts searching for feature reference points of a mean facial shape which is aligned to the position and size of the face presented in the input video image. An ASM algorithm then repeats the following two steps until convergence i suggest a tentative shape by adjusting the locations of shape points by template matching of image texture around each point and ii conform the tentative shape to a global shape model. The shape model pools the results of weak template matchers to form a stronger overall classifier. The entire search is repeated at each level in an image pyramid from coarse to fine resolution. Thus two sub model types make up ASM a profile model and a shape model.

The profile models one for each landmark at each pyramid level are used to locate an approximate position of each feature reference point by template matching. Any template matcher can be used but the classical ASM forms a fixed length normalized gradient vector also known as a profile by sampling the image along a line also known as a whisker orthogonal to the shape boundary at a landmark. While training on manually landmarked faces at each landmark feature reference point the mean profile vector and the profile covariance matrix Sare calculated. While searching the landmark feature reference point along the whisker is displaced to the pixel whose profile g has lowest Mahalanobis distance from the mean profile where MahalanobisDistance 1 

Further the shape model specifies allowable constellations of landmarks. A shape of individual can be given by its shape vector x x where xis i th facial landmark. The shape model generates the shape circumflex over x with 2 where is the mean shape b is a parameter vector and is a matrix of selected eigenvectors of profile covariance matrix Sof the points of the aligned training shapes. Using a standard principal components approach the model has as many variations as is desired by ordering the eigenvalues i and keeping an appropriate number of corresponding eigenvectors in . In this process a single shape model for the entire ASM is used but it may be scaled for each pyramid level.

Further Equation 2 is used to generate various shapes by varying the vector parameter b. By keeping the elements of b within limits determined during model building it is possible to ensure that generated face shapes are lifelike.

Conversely given a suggested shape x the method can calculate the parameter b that allows Equation 2 to better approximate x with a model shape circumflex over x . The method can further use an iterative algorithm to minimize distance 3 where T is a similarity transform that maps the model space into the image space.

In one or more embodiments CANDIDE 3 shape and initial state can be estimated based on a mapping of CANDIDE 3 vertices to weighted combinations of reference feature points located by ASM. CANDIDE 3 is a parameterized three dimensional face mesh specifically developed for model based coding of human faces. It includes a small number of polygons approximately 100 and allows fast reconstruction. CANDIDE 3 is controlled by SUs AUs and a position vector. The SUs control mesh shape so that different face shapes can be obtained. The AUs control facial mimics so that different expressions can be obtained. The position vector corresponds to rotations around three orthogonal axes and translations along the axes.

Assuming that the observed face is frontal viewed in the image only yaw estimation is needed among the three rotation parameters. It can be found as an angle from the positive direction of the x axis to a vector joining the right eye center feature point with the left one. The following equation system can be created assuming that the observed face is neutral and frontal viewed in the image and the mesh points are projected on the image plane by scaled orthographic projection 

The solution of this linear equation system is 6 where cos sin 1 0 circumflex over x sin cos 0 1 cos sin sin cos . 7 

In some embodiments a Viola Jones algorithm and ASM algorithm can be used to improve tracking quality. Face tracking processes can lose face position under some circumstances such as fast movements and or illumination variations. In order to re initialize the tracking algorithm a Viola Jones algorithm and ASM algorithm are applied in such cases.

This section focuses on example processes for face tracking that can be used in the present technology. Face tracking is needed not only to detect facial expressions but also for monitoring a disposition of a face within a field of view of a camera. Because individuals can move in each video image frame make gestures and rotate or move their heads face tracking is required to accurately determine facial expressions.

CANDIDE 3 model can be used for face tracking. See Jorgen Ahlberg Candide 3 an updated parameterized face Technical report Link ping University Sweden 2001 . shows an exemplary mesh corresponding to CANDIDE 3 model aligned to the face shown in .

In one or more embodiments a state of CANDIDE 3 model can be described by an intensity vector of SUs intensity vector of AUs and a position vector. SUs refer to various parameters of head and face. For example the following SUs can be used vertical position of eyebrows vertical position of eyes eyes width eyes height eye separation distance nose vertical position nose pointing up mouth vertical position mouth width and chin width. AUs refer to face parameters that correspond to various face mimics. For example the following AUs can be used upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser.

The position of mesh such as one shown in can be described using six coordinates yaw pitch roll x y and z scale . Following the Dornaika et al. approach a mesh state can be determined by observing the region most likely to be a face. See Dornaika F. Davoine F. . IEEE Trans. Circuits Syst. Video Technol. 16 9 1107 1124 2006 . For each mesh state observation errors can be calculated. Observation errors refer to a value indicating the difference between an image under a current mesh state and a mean face. shows an example mean face. shows an example warped towards initial CANDIDE 3 state observation under a current state of the mesh illustrated in . More specifically shows an exemplary image having a face and a mesh aligned to the face.

In one or more embodiments a face modelled as a picture with a fixed size e.g. width 40 px height 46 px is referred to as a mean face. In one or more embodiments the observation process can be implemented as a warping process from the current CANDIDE 3 state towards its initial state and denoted by 8 where x denotes the observed image with the same size as of mean face y denotes the input image and b denotes the CANDIDE 3 AUs intensities and position parameters. Gaussian distribution proposed in original algorithms has shown worse results compared to a static image. Thus the difference between the current observation and mean face can be calculated as follows log 1 log 1 9 where Idenotes pixels of the mean face image and Idenotes observation pixels.

Logarithm function can make the tracking more stable and reliable. In one or more embodiments a Taylor series can be used to minimize error. The gradient matrix is given by

Here gis an element of matrix G. This matrix has size m n where m is larger than n e.g. m is about 1600 and n is about 14 . In case of straight forward calculating n m operations of division have to be completed. To reduce the number of divisions this matrix can be rewritten as a product of two matrices G A B. Here matrix A has the same size as G. Each element of matrix A can be represented as 12 

Yet another optimization can be used in this method. If matrix G is created and then multiplied by b it leads to nm operations but if the first Aand b are multiplied and then multiplied by B AA there will be only m n noperations which is much better because n

Thus face tracking in the video comprises CANDIDE 3 shape and initial state estimating that is based on located reference feature points associated with a particular face and aligning the mesh to the face in each video image. Notably this process can be applied not only to a face but also to other individual parts. In other words this process of localization and tracking of a video conferencing participant may include localization and tracking of one or more of the participant s face and his body limbs and or other parts. In some embodiments gesture detection and tracking processes can be also applied. In that case the method may create a virtual skeleton and a mesh aligned to these body parts.

It should be also noted that ARM advanced SIMD Single Instruction Multiple Data extensions also known as NEON provided by ARM Limited can be used for multiplication of matrices in order to increase tracking performance. Also a GPU Graphics Processing Unit can be used in addition to or instead of CPU Central Processing Unit whenever possible. Operations can be arranged in a particular way to get high performance of GPU.

According to some embodiments of the disclosure the face tracking process can include the following features. First a logarithm can be applied to grayscale the value of each pixel to track it. This transformation has a great impact to tracking performance. Second in the procedure of gradient matrix creation the step of each parameter can be based on the mesh scale.

In order to automatically re initialize the tracking algorithm in failure cases the following failure criterion can be used 14 where is Euclidean norm y bare indexed by an image number t.

As outlined above when faces or other parts of video conference participants are detected the present technology determines an emotional status of video conference participants. This may include identification of facial expressions or changes in facial expressions over time. The emotional status can be also partly based on speech recognition or voice analysis. If it is determined that the emotional status is negative an alert communication can be generated and transmitted to one of a video conference participant or a third party. These and other embodiments for emotion recognition in video conferencing are described below with reference to exemplary flow charts.

Method for video conferencing commences at step with establishing a video conference between a first individual such as a customer service representative or service provider and a second individual such as a customer. For these ends in one embodiment video chat applications can be used on each side. The establishment of a video conference means that video and audio streams are captured on each side and transmitted to another side and vice versa.

Accordingly at step a computing device receives a video of the second individual. As a general matter the video includes a sequence of video images also known as video frames and the video can be received as a video stream meaning it can be continually supplied to the computing device e.g. as progressive downloading or it can be stored in a memory of the computing device. The video can be captured for video conferencing purposes but not necessarily.

At optional step the computing device receives a request to determine an emotional status e.g. a facial emotion of at least one video conference participant e.g. second individual . In one example the request can be generated manually by a first individual such as customer service representative or service provider. The request may optionally include metadata associated with the video conference participant of interest. For example metadata may include a portion of the video where this individual appears.

At step the computing device detects localizes at least one object of interest in one or more video images. As discussed above the object of interest may refer to a human face or other parts of the body including limbs neck arms chest and so forth all related to a second individual i.e. customer . The detection can be based on a Viola Jones algorithm although other algorithms can be also used. In some other embodiments the detection of the at least one object of interest in one or more of the video images can be based on a user input. For example the user input can include data associated with an image area related to the at least one object of interest.

At step the computing device locates a plurality of feature reference points of at least one object of interest e.g. a face . The feature reference points can include one or more facial landmarks such as ala philtrum vermilion zonem vermilion border nasolabial sulcus labial commissures lip tubercle nasion outer canthos of eye inner canthos of eye and tragus of ear. Moreover the feature reference points can include one or more of the following facial points indicating eyebrows vertical position eyes vertical position eyes width eyes height eye separation distance nose vertical position nose pointing up mouth vertical position mouth width chin width upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser. The feature reference points can be located using ASM or extended ASM algorithms as explained above. However other procedures of facial landmark localization can be also used including but not limited to exemplar based graph matching EGM algorithm consensus of exemplars algorithm and so forth.

At step the computing device aligns a virtual face mesh to the at least one object of interest e.g. aligns the mesh to an image of the customer face based at least in part on the reference feature points. This procedure can be performed with respect to just some of the video images or all video images. As discussed above a parameterized face mesh such as CANDIDE 3 model can be aligned to the object of interest. CANDIDE 3 is a parameterized three dimensional face mesh that can be aligned to an individual face shape based on calculation of intensities of SUs. In some embodiments the aligning of the virtual face mesh can be further based on estimating intensities of SUs associated with the face shape intensities of AUs and rotations of the virtual face mesh around three orthogonal axes and its translations along the axes.

At step the computing device finds over a sequence of video images at least one deformation of the mesh that is associated and reflects at least one face mimic. Mesh deformation can include relative disposition of one or more mesh vertices because of a change in emotional expression by the second individual. For example the computing device can find modification of the mesh that replicate moving the labial commissure landmarks as well as the movement of eyebrows. In another example the computing device can find modification of the mesh that replicate moving the outer and inner canthus of the eyes as well as moving mouth landmarks. It should be appreciated that mesh can be deformed in a variety of different ways. In any case the computing device can track changes in position of each mesh point as well as a distance between each of the mesh points to determine changes in facial emotions.

At step the computing device compares the determined deformation of the mesh to reference facial parameters of a plurality of reference facial emotions. The reference facial parameters and the reference facial emotions can be stored in one or more databases located for example in a memory of computing device. The reference facial emotions may include for example neutral facial emotions positive facial emotions and negative facial emotions. In certain embodiments the negative facial emotions may include anger indignation dissatisfaction vexation frustration embarrassment irritation stress and annoyance.

The step of comparing may include applying at least one machine learning algorithm such as a convolution neural network CNN and or a state vector machine SVM . Generally CNN is a type of feed forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field. CNNs consist of multiple layers of small neuron collections which look at small portions of the input image called receptive fields. The results of these collections are then tiled so that they overlap to obtain a better representation of the original image this is repeated for every such layer. Convolutional networks may include local or global pooling layers which combine the outputs of neuron clusters. They also consist of various combinations of convolutional layers and fully connected layers with pointwise nonlinearity applied at the end of or after each layer. To avoid the situation that there exist billions of parameters if all layers are fully connected the idea of using a convolution operation on small regions has been introduced. One major advantage of convolutional networks is the use of shared weight in convolutional layers which means that the same filter weights bank is used for each pixel in the layer this both reduces required memory size and improves performance.

SVMs are supervised learning models with associated learning algorithms that are configured to recognize patterns. Given a set of training examples with each marked as belonging to one of two categories an SVM training algorithm builds a model that assigns new examples into one category or the other making it a non probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.

At step based on the result of comparison made at step the computing device selects a facial emotion from the plurality of reference facial emotions. In other words at the steps and the computing device determines that the at least one deformation of the mesh refers to a particular facial emotion. show two video images A and B respectively which illustrate example facial emotions that can be identified by this method . In particular shows an example positive facial emotion with a mesh aligned to a face area while shows an example negative facial emotion e.g. anger with the mesh aligned to a corresponding face area.

At step the computing device generates a communication which includes data associated with the particular facial emotion. In certain embodiments the communication is generated only if the particular facial emotion determined refers to a negative emotion such as anger stress annoyance irritation and the like. The communication can be also referred to as a notification alert indication or message. Accordingly the communication can be presented to the first individual customer service representative or service provider as a displayable message or audio message.

At the same step the computing device can optionally transmit the communication containing data related to the facial emotion over a communications network to a third party such as a supervisor manager administrator or moderator.

At optional step the computing device may allow the third party to review or watch the videoconference between the first individual customer service representative or service provider and second individual customer and or enter into the videoconference between the first individual and second individual. In this case the third party may handle a difficult situation and attempt to resolve those angry customer issues that could not be resolved by the first individual. Entering into the videoconference between the first individual and second individual may optionally mean that the first individual is suspended from continuing the videoconferencing.

In some embodiments the computing device may determine an emotional status of the second individual customer by combining data related to the determined facial emotion with other data. In one example the emotional status can be determined based on facial emotions and gestures of the customer. For these ends the computing device may track individual motions on the video identify one or more gestures and determine that at least one of the gestures relates to a predetermined emotion such as a negative emotion. Further the computing device may combine data associated with the determined facial emotion and data associated with the identified gesture and generate an emotional status of second individual. Similar to above the emotional status can be transmitted and presented to the first individual and or third party.

At step a computing device receives an audio stream associated with the video received at step . In other words at step the computing device receives the audio stream of a videoconferencing session between a first individual such as a customer service representative or service provider and a second individual such as a customer.

At step the computing device extracts at least one voice feature from the audio stream. Among voice features there can be one or more of the following a maximum value of fundamental frequency standard deviation of fundamental frequency range of fundamental frequency mean value of fundamental frequency mean of bandwidth of first formant mean of bandwidth of second formant standard deviation of energy speaking rate slope of fundamental frequency maximum value of first formant maximum value of second formant maximum value of energy range of energy range of second formant and range of first formant.

At step the computing device compares the extracted at least one voice feature to a plurality of reference voice features. Similar to method this step can be performed with the help of a machine learning algorithm such as SVM CNN and a statistical or heuristic algorithm.

At step the computing device selects the speech emotion based on the comparison of the extracted at least one voice feature to the plurality of reference voice features.

In some embodiments in addition to steps and or instead of steps and at optional step the computing device can identify and recognize a speech of the second individual i.e. transform speech input into text input for further processing . For example one or more natural language processing processes can be applied at step to detect speech and transform it into text.

Based on the analysis of recognized speech at step the computing device may select or facilitate selection of a particular speech emotion as the result of the presence of certain keywords or phrases in the recognized speech. For example when recognized speech includes vulgar offensive or vile slang words a negative emotion can be selected and attributed to the audio stream.

At step the computing device optionally combines the speech emotion and facial emotion as determined in method into a single emotional status of the second individual customer . Further the emotional status can be transmitted and presented to the first individual service provider and or third party supervisor for further action as discussed above.

Thus methods and systems for videoconferencing involving emotion recognition have been described. Although embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes can be made to these example embodiments without departing from the broader spirit and scope of the present application. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

