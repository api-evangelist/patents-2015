---

title: Hashing algorithm for network receive filtering
abstract: Roughly described, a network interface device is assigned a maximum extent-of-search. A hash function is applied to the header information of each incoming packet, to generate a hash code for the packet. The hash code designates a particular subset of the table within which the particular header information should be found, and an iterative search is made within that subset. If the search locates a matching entry before the search limit is exceeded, then the incoming data packet is delivered to the receive queue identified in the matching entry. But if the search reaches the search limit before a matching entry is located, then device delivers the packet to a default queue, such as a kernel queue, in the host computer system. The kernel is then responsible for delivering the packet to the correct endpoint.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09594842&OS=09594842&RS=09594842
owner: SOLARFLARE COMMUNICATIONS, INC.
number: 09594842
owner_city: Irvine
owner_country: US
publication_date: 20150130
---
This application is a continuation of prior U.S. application Ser. No. 13 162 421 filed Jun. 16 2011 which is a continuation of prior U.S. application Ser. No. 11 255 124 filed Oct. 20 2005 now U.S. Pat. No. 7 984 180 issued Jul. 19 2011 both of which applications are incorporated by reference herein.

The invention relates to network interfaces and more particularly to queue based network receive mechanisms supporting a plurality of receive queues in a host computer system.

When data is to be transferred between two devices over a data channel such as a network each of the devices must have a suitable network interface to allow it to communicate across the channel. Often the network is based on Ethernet technology. Devices that are to communicate over a network are equipped with network interfaces that are capable of supporting the physical and logical requirements of the network protocol. The physical hardware component of network interfaces are referred to as network interface cards NICs although they need not be in the form of cards for instance they could be in the form of integrated circuits ICs and connectors fitted directly onto a motherboard or in the form of macrocells fabricated on a single integrated circuit chip with other components of the computer system.

Most computer systems include an operating system OS through which user level applications communicate with the network. A portion of the operating system known as the kernel includes protocol stacks for translating commands and data between the applications and a device driver specific to the NIC and the device drivers for directly controlling the NIC. By providing these functions in the operating system kernel the complexities of and differences among NICs can be hidden from the user level application. In addition the network hardware and other system resources such as memory can be safely shared by many applications and the system can be secured against faulty or malicious applications.

It is desirable for the network interface device to be capable of supporting standard transport level protocols such as TCP UDP RDMA and ISCSI at user level i.e. in such a way that they can be made accessible to an application program running on the computer. TCP is defined in RFC 0793 Transmission Control Protocol. J. Postel. Sep. 1 1981 and UDP is defined in RFC 0768 User Datagram Protocol. J. Postel. Aug. 28 1980 both incorporated by reference herein. Support of transport protocols at user level enables data transfers which require use of standard protocols to be made without requiring data to traverse the kernel stack. Involving the kernel stack requires context switches which can significantly degrade performance of the computer system overall. To avoid this standard transport protocols can be implemented both within transport libraries accessible only to the operating system of the computer as well as within transport libraries accessible to user level applications.

There are a number of difficulties in implementing transport protocols at user level. Most implementations to date have been based on porting pre existing kernel code bases to user level. Examples of these are Arsenic and Jet stream. These have demonstrated the potential of user level transports but have not addressed a number of the problems required to achieve a complete robust high performance commercially viable implementation.

TCP receive side processing takes place and the destination port is identified from the packet. If the packet contains valid data for the port then the packet is engaged on the port s data queue step iii and that port marked which may involve the scheduler and the awakening of blocked process as holding valid data.

The TCP receive processing may require other packets to be transmitted step iv for example in the cases that previously transmitted data should be retransmitted or that previously enqueued data perhaps because the TCP window has opened can now be transmitted. In this case packets are enqueued with the OS NDIS driver for transmission.

In order for an application to retrieve a data buffer it must invoke the OS API step v for example by means of a call such as recv select or poll . This has the effect of informing the application that data has been received and in the case of a recv call copying the data from the kernel buffer to the application s buffer. The copy enables the kernel OS to reuse its network buffers which have special attributes such as being DMA accessible and means that the application does not necessarily have to handle data in units provided by the network or that the application needs to know a priori the final destination of the data or that the application must pre allocate buffers which can then be used for data reception.

It should be noted that on the receive side there are at least two distinct threads of control which interact asynchronously the up call from the interrupt and the system call from the application. Many operating systems will also split the up call to avoid executing too much code at interrupt priority for example by means of soft interrupt or deferred procedure call techniques.

The send process behaves similarly except that there is usually one path of execution. The application calls the operating system API e.g. using a send call with data to be transmitted Step vi . This call copies data into a kernel data buffer and invokes TCP send processing. Here protocol is applied and fully formed TCP IP packets are enqueued with the interface driver for transmission.

If successful the system call returns with an indication of the data scheduled by the hardware for transmission. However there are a number of circumstances where data does not become enqueued by the network interface device. For example the transport protocol may queue pending acknowledgments or window updates and the device driver may queue in software pending data transmission requests to the hardware.

A third flow of control through the system is generated by actions which must be performed on the passing of time. One example is the triggering of retransmission algorithms. Generally the operating system provides all OS modules with time and scheduling services driven by the hardware clock interrupt which enable the TCP stack to implement timers on a per connection basis.

If a standard kernel stack were implemented at user level then the structure might be generally as shown in . The application is linked with the transport library rather than directly with the OS interface. The structure is very similar to the kernel stack implementation with services such as timer support provided by user level packages and the device driver interface replaced with user level virtual interface module. However in order to provide the model of a asynchronous processing required by the TCP implementation there must be a number of active threads of execution within the transport library 

 iii Management of the virtual network interface and resultant upcalls into protocol code. ii and iii can be combined for some architectures 

 a The overheads of context switching between these threads and implementing locking to protect shared data structures can be significant costing a significant amount of processing time.

 b The user level timer code generally operates by using operating system provided timer time support. Large overheads caused by system calls from the timer module result in the system failing to satisfy the aim of preventing interaction between the operating system and the data path.

 c There may be a number of independent applications each of which manages a sub set of the network connections some via their own transport libraries and some by existing kernel stack transport libraries. The NIC must be able to efficiently parse packets and deliver them to the appropriate virtual interface or the OS based on protocol information such as IP port and host address bits.

 d It is possible for an application to pass control of a particular network connection to another application for example during a fork system call on a Unix operating system. This requires that a completely different transport library instance would be required to access connection state. Worse a number of applications may share a network connection which would mean transport libraries sharing ownership via inter process communication techniques. Existing transports at user level do not attempt to support this.

 e It is common for transport protocols to mandate that a network connection outlives the application to which it is tethered. For example using the TCP protocol the transport must endeavor to deliver sent but unacknowledged data and gracefully close a connection when a sending application exits or crashes. This is not a problem with a kernel stack implementation that is able to provide the timer input to the protocol stack no matter what the state or existence of the application but is an issue for a transport library which will disappear possibly ungracefully if the application exits crashes or stopped in a debugger.

In addition in order that incoming data packets be delivered to the data port of the correct application a mechanism is required which examines the header information of the packet and looks up the associated destination queue. Such filtering is commonly performed on a number of fields of the header including source and destination ports and addresses. In order to maximize efficiency it is preferable that the filtering be accomplished in hardware on the network interface device rather than in software within the kernel or user level drivers.

One means of filtering packets in the network interface card is by presenting the packet header information to a content addressable memory CAM which associates each combination of header bit information with a specified receive port. But in a TCP IP packet for example the header information may be as wide as 96 bits 32 source IP address bits 32 destination IP address bits 16 source port bits and 16 destination port bits. A CAM could be provided that is wide enough to accommodate the full width of all the fields against which filtering is to be performed but such a CAM becomes prohibitively large and expensive if it is desired that the number of potential destination ports is large and especially if it is to be implemented as only part of a custom integrated circuit. Modern servers in certain environments may require huge numbers of network endpoints such that filtering through a CAM is not practical.

In situations where CAM lookups are too expensive one might consider instead performing iterative lookups through a table comparing the input data the header bit information in the case of an incoming data packet filter with various entries in the table successively until a match is found. But iterative lookups can take a long time often more time than is available before the next incoming packet arrives. Again this problem worsens as the maximum number of network endpoints supported by the system increases.

In situations where CAM lookups are too expensive and iterative lookups will take too long one might consider a hashed lookup which is a form of hybrid between the parallel approach of a CAM and the sequential approach of an iterative lookup. In a hashed lookup the table is subdivided into a number of subsets of entries. The input data is passed through a consistent mathematical hashing function which converts it to an index called a hash code pointing to the particular list or bucket within which the data belongs. When new data is to be added to the table it is simply inserted into an available free entry of the list pointed to by its hash code. When input data is to be located in the table an iterative search is performed within the list pointed to by the hash code.

Many hashing algorithms are known and some examples may be found in Knuth Art of Computer Programming Volume 3 Sorting and Searching 2nd Edition incorporated herein by reference. In one variation the buckets pointed to by the hash codes are contiguous and of fixed length and when a bucket is filled additional entries are placed into a common overflow area. The buckets may even be limited to only one entry. In another variation the buckets pointed to by the hash codes are linked lists of entries which can have variable length. All the linked lists may be interspersed together within a single combined region of memory. In yet another variation known as open hashing each bucket has a fixed length for example a length of one and if the bucket is filled a secondary hash function is used to hop to a different next entry of the table. If that entry is filled then the secondary hash function is used to hop again to yet another entry and so on until an available entry is found. Multiple level hashing algorithms are also available. In the open hashing case the subset of entries pointed to by the primary hash function is considered herein to include all the entries that will be reached in multiple iterations of the secondary hash function so that the subset may be discontiguous. Whatever algorithm is used for placing entries into the table the same algorithm is used for searching the table.

One problem that arises in most hashing algorithms is the problem of clustering because the designer of the hashing algorithm does not know in advance what the input data will be that is to be stored in the table there is always some probability that the hash function will distribute the data unevenly among the various possible hash codes. Uneven data distribution means that while the average length of a list and therefore the average search time to find a matching entry may be small some hash codes will point to lists that are very long and therefore take a long time to search . In implementations having a maximum list length uneven data distributions increase the probability that some lists will overflow forcing the search algorithm into its back up mechanism such as searching a common overflow area.

Different hashing algorithms handle the clustering problem differently but most suffer from the problem that the iterative search time required to search the list pointed to by a heavily used hash code can become exorbitant. In the context of network receive packet filtering such a long search time means the network interface device may not be ready to handle a future packet when it arrives thereby potentially dropping the packet. Merely applying a hashing solution to the problem of filtering incoming data packets to identify the correct destination queue therefore may not be sufficient.

In order to address this issue roughly described a network interface device is assigned a maximum extent of search. A hash function is applied to the header information of each incoming packet to generate a hash code for the packet. The hash code designates a particular subset of the table within which the particular header information should be found and an iterative search is made within that subset. If the search locates a matching entry before the search limit is exceeded then the incoming data packet is delivered to the receive queue identified in the matching entry. But if the search reaches the search limit before a matching entry is located then device delivers the packet to a default queue preferably a kernel queue in the host computer system. The kernel is then responsible for delivering the packet to the correct endpoint.

The following description is presented to enable any person skilled in the art to make and use the invention and is provided in the context of a particular application and its requirements. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art and the general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present invention. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

The network interface card provides an interface to outside networks including an interface to the network and is coupled via network to corresponding interface devices in other computer systems. Network may comprise many interconnected computer systems and communication links. These communication links may be wireline links optical links wireless links or any other mechanism for communication of information. While in one embodiment network is the Internet in other embodiments network may be any suitable computer network or combination of networks. In and embodiment described herein network supports an Ethernet protocol.

Host memory subsystem typically includes a number of memories including a main random access memory RAM for storage of instructions and data during program execution and a read only memory ROM in which fixed instructions and data are stored. One or more levels of cache memory may also be included in the host memory subsystem . For simplicity of discussion the host memory subsystem is sometimes referred to herein simply as host memory . As used herein virtual memory is considered part of the host memory subsystem even though part of it may be stored physically at various times on a peripheral device.

The communication channel provides a mechanism for allowing the various components and subsystems of computer system to communicate with each other. In one embodiment the communication channel comprises a PCI Express bus. Other embodiments may include other buses and may also include multiple buses. The PCI bus and its progeny including the version known as PCI Express support burst transfer protocols such as that described above. PCI express is described in PCI Special Interest Group PCI Express Base Specification 1.0a Apr. 15 2003 incorporated herein by reference.

Computer system itself can be of varying types including a personal computer a portable computer a workstation a computer terminal a network computer a television a mainframe a server or any other data processing system or user devices. Due to the ever changing nature of computers and networks the description of computer system depicted in is intended only as a specific example for purposes of illustrating an embodiment of the present invention. Many other configurations of computer system are possible having more or less components and configured similarly or differently than the computer system depicted in .

The NIC supports resources of a number of types i.e. resources having capabilities of different natures. Examples include DMA queues event queues timers and support resources for remote apertures of the type described in WO2004 025477 incorporated by reference herein. Each type of resource is provided from a dedicated hardware resource pool which can support numerous instances of resources of the respective type. In order for such an instance to be made operational it is configured by means of instructions from the computing device as described in more detail below.

The NIC communicates with the computing device over the bus . In this example the bus is a PCI bus but the invention is not limited to such a bus. Data transmitted over the PCI bus is associated with a destination address and is received by whichever entity that is connected to the bus has had that address allocated to it. In a typical PC implementation the addresses are allocated in pages of 4 or 8 kB. One or more of these pages may be allocated to the NIC . Blocks and represent allocated pages on the PCI bus .

The NIC has a bus interface controller BIC a resource configuration unit RCU and a bus mapping table . The resource configuration unit processes communications received from the computer that provide instructions on the allocation re allocation and de allocation of resources on the NIC and configures the resources in accordance with such instructions. The kernel driver stores a record of which resources on the NIC are allocated. When a resource is to be allocated the driver identifies a suitable free resource of the required type on the NIC and transmits an allocation instruction to the NIC . The instruction identifies the resource and specifies the details of how it is to be allocated including details of the internal configuration of the resource e.g. in the case of a timer the amount of time it is to run for . That instruction is passed to the resource configuration unit. The resource configuration unit then loads the specified configuration into the identified resource. The instruction also includes an ownership string which may be an identification of which application or process on the computer is using the resource. The resource configuration unit stores these in a row of the bus mapping table. When a resource is to be re allocated the relevant entries in the resource s own configuration store and in the bus mapping table are altered as necessary. When a resource is to be de allocated it is disabled and any rows of the bus mapping table that relate to it are deleted.

During setup of the system one or more pages on the bus are allocated to the NIC . Part of this address space page can be used by the kernel driver to send instructions to the NIC . Other pages e.g. page can be used for communication between application processes such as application and the resources . The resource configuration unit stores a record of the pages that are allocated to the NIC for use by resources. Note that in some embodiments some or all of the functions of the resource configuration unit may alternatively be provided by the kernel driver itself

When an application wishes to open a data connection over the network it calls a routine in the user level transport library to cause the NIC resources that are required for the connection to be allocated. Standard types of network connection require standard sets of resources for example an event queue transmit and receive DMA command queues and a set of DMA able memory buffers. For example a typical set may contain one TX command queue one RX command queue two timers and on the order of 100 1000 DMA memory buffers.

The user level transport library includes routines that can be called directly by the application process and that initiate the allocation of such standard sets of resources including set numbers of resources of different types. The transport library also includes routines that allow a resource of each type to be allocated re allocated or de allocated individually. The presence of both these types of instruction means that standard connections can be set up efficiently and yet non standard groups of resources can be created and existing connections can be reconfigured on a resource by resource basis. As used herein a user level stack is any protocol processing software that runs in unprotected mode. A protocol stack is the set of data structures and logical entities associated with the networking interfaces. This includes sockets protocol drivers and the media device drivers.

The routines for allocation re allocation and de allocation of resources require access to restricted memory mapped addresses such as page for sending configuration instructions to the NIC . Since the user level transport library lacks the necessary privilege level to perform these accesses these routines in the user level transport library make calls to the kernel driver . In a Unix environment for example such calls might take the form of IOCtl system calls. These calls cause an initial context switch to a kernel level process which in turn communicate the instructions to the NIC for the allocation of the resources as specified in the routines. Those instructions specify the identity of the application or process with which the resources are to be associated and the nature of the resources. The instructions are processed by the resource configuration unit of the NIC .

The space on the bus that is allocated to the NIC can be split dynamically between the resources on the bus . Once one or more pages have been allocated to the NIC for use by resources those resources can be allocated one or more individual sub page addresses within that page corresponding to locations as illustrated at . Thus each resource can have a part of the total space allocated to it. A record of which part of the total space is allocated to which resource is stored in the bus mapping table . The effect is that a single page of the bus can be used for communication to resources of multiple types and or resources that relate to multiple connections and or resources that are associated with multiple applications or processes on the computer . As a result the total bus space can be used relatively efficiently.

The usage of the allocated bus space is managed by the kernel driver . When a resource is to be allocated the RCU identifies using a data store whose content it manages an unused block in the space on the bus that has already been allocated for use by resources of the NIC the space being of the size required for the resource. It then stores in that data store the identity of the resource resource ID the address of the block within the allocated space sub page ID and the identity of the application or process that is to use the resource process tag and sends a message to the resource configuration unit to cause it to store corresponding data in the bus mapping table . If the RCU finds that table indicates the address to be already occupied then it returns an error code to the driver. The sub page address may need to be supplemented with the address of the page in which the sub page lies if that cannot be inferred as a result of only a single page having been allocated for use by the resources. If the total space allocated for use by resources is insufficient then the kernel driver allocates it more space. Having allocated the resources the RCU returns a success message to the kernel driver. The allocated page and sub page addresses are returned to and mapped into the virtual address space of the user level process that requested the resources in order that it can access them by means of that data. Another context switch then takes place back to the user level calling process.

An application that has had resources allocated to it can access them by sending data e.g. by means of load store cycles through a virtual memory mapping to the relevant bus page at the sub page address corresponding to the respective resource. Since these addresses are part of the application s virtual address space no context switch to any kernel level processes are required in order to perform these accesses. Any data sent to pages allocated to resources is picked off the bus by the bus interface controller . It directs that data to the appropriate one of the resources by performing a look up in the table to identify the identity of the resource to which the sub page address has been allocated. An application can also access a resource by means other than a bus write for example by means of direct memory access DMA . In those instances the NIC checks that the identity of the application process from which the access has been received matches the identity indicated in the table for the resource. If it does not match the data is ignored. If it matches it is passed to the relevant resource. This adds to security and helps to prevent corruption of the resources by other applications.

The set of resources allocated to an application or process may be considered to constitute a virtual network interface VNIC .

Once a virtual interface has been composed it may be reconfigured dynamically. As one example of dynamic reconfiguration a resource that is no longer required may be freed up. To achieve this the application using the resource calls a de allocation routine in the user level transport library . The de allocation routine calls the kernel driver which instructs the RCU to de allocate the resource by disabling it clearing its status and deleting its row in the table .

As another example of dynamic reconfiguration additional resources may be added to the VNIC. The process is analogous to that described above for initial composition of the VNIC.

As yet another example of dynamic reconfiguration resources may be passed from one application or process to another. This is most useful in the situation where a single application has multiple processes and wants to pass control of a resource from one process to another for example if data from the network is to be received into and processed by a new process. To achieve this the application using the resource calls a re allocation routine in the transport library . The re allocation routine calls the kernel driver which instructs the RCU to re allocate the resource modifying its row in the table to specify the identity of the application or process that is taking over its control.

In some instances it may be desirable for resources of one type to communicate with resources of another type. For example data received from the network may be being passed to an application for processing. The application has a queue in a memory connected to the bus . The queue is managed in part by the transport library which provides a DMA queue resource on the NIC with an up to date pointer to the next available location on the queue . This is updated as the application reads data from the queue . When data is received from the network it is passed to an event queue resource which writes it to the location identified by the pointer and also triggers an event such as an interrupt on the computing device to indicate that data is available on the queue. In order for this to happen the event queue resource must learn the pointer details from the DMA queue resource . This requires data to be passed from the DMA queue resource to the event queue resource.

To achieve this the process tag column of the table can be treated more generally as an ownership tag and can link the DMA queue to the related event queue. To achieve this the ownership tag of the event queue can be set to the identity of the related DMA queue. When the DMA queue needs to pass data to the related event queue it can identify the event queue from the table by performing a look up on its own identity in the ownership tag column.

Data intended to be passed from one resource to another can be checked by the bus interface controller to ensure that it is compatible with the settings in the table . Specifically when data is to be sent from one resource to another the bus controller checks that there is a row in the table that has the identity of the resource that is the source of the data in the ownership tag field and the identity of the resource that is the intended destination of the data in the resource ID field. If there is no match then the data is prevented from reaching its destination. This provides additional security and protection against corruption. Alternatively or in addition it may be permitted for one resource to transmit data to another if both are in common ownership in this example if their resource ID fields indicate that they are owned by the same process application or other resource.

The identities of resources linked in this way can also be reconfigured dynamically by means of the re configuration routines in the transport library.

 i TCP code which performs protocol processing on behalf of a network connection is located both in the transport library and in the OS kernel. The fact that this code performs protocol processing is especially significant.

 ii Connection state and data buffers are held in kernel memory and memory mapped into the transport library s address space

 iii Both kernel and transport library code may access the virtual hardware interface for and on behalf of a particular network connection

 iv Timers may be managed through the virtual hardware interface these correspond to real timers on the network interface device without requiring system calls to set and clear them. The NIC generates timer events which are received by the network interface device driver and passed up to the TCP support code for the device.

It should be noted that the TCP support code for the network interface device is in addition to the generic OS TCP implementation. This is suitably able to co exist with the stack of the network interface device.

As a result of the above mechanisms the operating system and many application programs can each maintain multiple TX RX and Event DMA command queues. illustrates this feature. As can be seen the operating system maintains via kernel driver TX RX and Event data queues. Each such queue has an associated DMA command queue not shown in but maintained in the host memory by the kernel driver . Multiple applications can also be running in the computer system each with its own instance of user level driver . Each such application maintains via its respective user level driver instance TX RX and Event data queues. As with the kernel queues each such TX RX and Event data queue has an associated DMA command queue not shown in but maintained in the host memory by the respective user level driver . Note that the kernel driver is also able to communicate data packets received by the kernel to the user level driver of individual target applications. This communication occurs by standard interprocess communication mechanisms of the operating system.

Individual buffers may be either 4 k or 8 k bytes long in one embodiment and they are chained together into logically contiguous sequences by means of physically contiguous descriptors in a buffer descriptor table stored in the NIC . For example one receive queue might occupy buffers and in host memory which are discontiguous and possibly out of order regions of memory. They are chained together into a single logically contiguous space by the physically contiguous entries and in the buffer descriptor table . The entries and are written and managed by the host kernel driver and are viewed as a wrap around ring. So for example if the host wishes to define a receive queue having 64 k entries for receive data buffer descriptors and each buffer is 4 k in size then the host will allocate a physically contiguous sequence of 16 entries in buffer descriptor table for this receive queue. Similarly one event queue might occupy buffers and in host memory . These buffers are discontiguous and possibly out of order in host memory but are chained together into a single logically contiguous wrap around space by the physically contiguous entries and in the buffer descriptor table . The buffer descriptor table is indexed by buffer ID and each of its entries identifies among other things the base address of the corresponding buffer in host memory .

In order to keep track of the state of each of the transmit receive and event queues for the many user level applications that might be in communication with NIC at the same time the NIC includes a receive queue descriptor table a transmit queue descriptor table and an event queue descriptor table . The transmit receive and event queue descriptor tables are shown in as separate tables each containing the entire table but it will be appreciated that in different embodiments the three tables can be implemented as a single unified table or one of the tables can be implemented as separate sub tables divided by columns or by rows or by both or some combination of these variations might be implemented. Each receive queue has a corresponding receive queue ID which is used as an index into the receive queue descriptor table . The designated entry in the receive queue descriptor table is the starting point for describing the state and other characteristics of that particular receive queue as viewed by the NIC . Each such entry identifies among other things 

The host maintains host centric versions of the read and write pointers as well and when it has added additional receive buffers to the queue it so notifies the NIC by writing its updated host centric receive queue write pointer into the address on the NIC of the device centric receive queue write pointer for the particular receive queue.

As shown in the NIC also includes a filter table and logic block . Because the NIC can support multiple simultaneous connections between user level applications and remote agents on LAN and because the NIC supports these using multiple transmit and receive queues one function performed by the NIC is to direct each incoming data packet to the correct receive queue. The mechanisms used by NIC to make this determination are described in detail hereinafter but generally the filter table and logic maintains a correspondence between packet header information and destination receive queue ID. The filter table and logic thus uses the header information from the incoming packet to determine the ID of the proper destination receive queue and uses that receive queue ID to index into the receive queue descriptor table . The receive queue ID is the starting point for the NIC to obtain all required information about the destination receive queue for proper forwarding of the packet data.

Thus logically described in order to deliver a received data packet to the destination receive queue in host memory the NIC first uses the header information of the data packet to look up in the filter table the appropriate destination receive queue ID. It then uses the ID of the particular receive queue to look up in the receive queue descriptor table the buffer ID of the base buffer containing the receive descriptor queue. The NIC also obtains from the same place the current device centric read pointer into that receive descriptor queue. It then uses the base buffer ID as a base and the device centric read pointer high order bits as an offset into the buffer descriptor table to obtain the base address in host memory of the buffer that contains the particular receive queue. The NIC then uses that base address as a base and as an offset the device centric receive queue read pointer low order bits times the number of bytes taken up per descriptor as a starting host memory address for retrieving entries from the particular receive descriptor queue. The NIC does not allocate separate space for maintaining a write pointer into any local cache into which these entries will be written.

Entries for kernel receive descriptor queues can identify the buffer physical address itself rather than a buffer ID because the kernel is trusted to write the correct physical address whereas a user level queue is not.

The NIC then uses the buffer ID of the current receive data buffer as another index into buffer descriptor table to retrieve the buffer descriptor for the buffer into which the current receive data is to be written. Note this buffer descriptor is an individual entry in buffer descriptor table unlike the descriptors for buffers containing receive queues or event queues this buffer descriptor is not part of a ring. The NIC obtains the physical address in host memory of the current receive data buffer and then using that physical address as a base and the 2 byte aligned offset from the receive descriptor queue entry as an offset it determines the physical starting address in host memory into which the data transfer should begin. The NIC then transfers the received data packet into host memory beginning at that address.

The receive queue descriptor table entry designated by the receive queue ID as previously mentioned also contains the ID of the receive event queue associated with the particular receive queue. Similarly the transmit queue descriptor table entry designated by the transmit queue ID contains the ID of the event queue associated with the particular transmit queue. All of the event queues for all the applications are described by respective entries in the event queue descriptor table . The entry in the event queue descriptor table identified by a queue ID from the receive or transmit queue descriptor table or is the starting point for describing the state and other characteristics of that particular event queue as viewed by the NIC .

Note that as illustrated in whereas each slot e.g. shown in the buffer descriptor table represents a single descriptor each slot e.g. in the host memory represents a memory page of information. A page might be 4 k or 8 k bytes long for example so if a receive data buffer descriptor in a receive queue occupies either 4 or 8 bytes then each slot or as shown in might hold 512 1 k or 2 k receive data buffer descriptors.

After determining the amount of space currently available in the receive event queue in step the host subsystem determines a number M being the lesser of the number of data buffers available for queuing of receive data and the minimum number of receive data buffers that can be represented by receive completion events in the space available in the receive event queue as determined in step .

In step it is determined whether M is greater than or equal to some minimum threshold. Preferably the minimum threshold is 1 but in other embodiments a larger number may be chosen for the threshold. If M is less than the threshold then the host receive event queue management module simply goes inactive to await the next activation event step .

If M is greater than or equal to the minimum threshold then in step the host subsystem updates modulo increments its host centric receive queue write pointer by M entries. In step the host subsystem writes M available receive data buffer descriptors into the receive queue beginning at the entry previously before step designated by the host centric receive queue write pointer. In step the host subsystem notifies the NIC of the updated write pointer and in step the NIC updates its own device centric receive queue write pointer for the specified receive queue. In one embodiment steps and are combined into a single step in which the host subsystem writes the updated write pointer into a memory mapped location of the device centric receive queue write pointer. In step the host receive event queue management module goes inactive to await the next activation event.

In step the NIC writes data from the incoming packet into the receive data buffer designated by the retrieved descriptor beginning at the specified offset. Writing continues by DMA until either the end of the current data buffer is reached or the end of the incoming data packet is reached or both.

The NIC detects and reports a queue empty alert when it believes it has retrieved and used the last buffer descriptor in the particular receive queue. This alert is combined into a single event descriptor with the receive completion event. In particular the NIC determines in step whether it believes it has used the last receive buffer identified by a descriptor in the receive queue. The NIC can determine this by comparing its device centric receive queue read pointer to its device centric receive queue write pointer for the particular receive queue. If not that is the NIC knows there are more receive buffer descriptors in the receive queue then no alert is necessary and in step the NIC determines whether end of packet has been reached. If not then the NIC receive data module returns to step to retrieve the descriptor for the next receive data buffer. No event is asserted to indicate Receive Data Buffer Full in this embodiment. The host will become aware of which receive data buffers are full based on the receive data buffers identified consecutively in the receive queue beginning at the host centric RX queue read pointer.

If step determines that end of packet was reached then in step the NIC asserts a receive completion event to cover all the receive data buffers that contain data from the packet. The receive completion event descriptor format includes a receive descriptor queue empty flag rx desc q empty but in the receive completion event written in step this flag is not set because the NIC has determined in step that additional receive buffer descriptors remain in the receive queue. Note that in this embodiment only one receive completion event will be asserted even if the packet data spans multiple buffers in receive data buffers. Multiple buffers are chained together by consecutive entries in the receive queue. Note also that if end of packet does not coincide with the end of a receive buffer then the remaining space in the buffer is left unused.

Returning to step if the NIC believes that the last receive data buffer identified by a descriptor in the receive queue has been retrieved in step then the NIC does not wait until end of packet before reporting the receive completion event. Instead in step the NIC asserts a receive completion event to cover all the receive data buffers that contain data from the packet. In this receive completion event the rx desc q empty flag is set. If packet data remains in the NIC s RX FIFO when this occurs it is lost.

In an embodiment in which the NIC supports more than one network port the NIC does not batch receive completion events. Receive completion events do not indicate completion of more than one receive data buffer. This embodiment supports both standard size data packets in which data packets have a relatively small maximum length and the receive data buffers are at least as large as the maximum data packet length and jumbo data packets in which a data packet can be longer and can span more than one data buffer. A given receive queue is either in standard mode or jumbo mode. If the queue is in standard mode then absent an error every receive data buffer filled will contain an end of packet so no receive completion event will indicate completion of more than one data buffer and the problem will not arise. If the queue is in jumbo mode then it is still the case that no receive completion event will indicate completion of more than one data buffer since the NIC writes a receive completion event for each data buffer it fills. The receive completion event format includes a RX Jumbo Cont bit which the NIC sets in order to notify the host subsystem that the subject data buffer does not contain an end of packet i.e. there will be a continuation buffer . This embodiment therefore does not batch receive completion events. The receive completion event still includes a copy of the NIC s updated device centric receive queue read pointer which now points to the specific descriptor from the receive queue for whose data buffer the event indicates completion. The receive completion event format also indicates the NIC port number from which the packet was received.

Returning to the embodiment after both steps and once the NIC has asserted a receive completion event the NIC receive data module then returns to an inactive state step .

In both steps and the NIC asserts a receive completion event containing certain information. is a flowchart detail of this step. In step the NIC writes the receive completion event into the corresponding receive event queue beginning at the entry identified by the device centric receive event queue write pointer for that event queue. In step NIC correspondingly updates its own receive event queue write pointer. In step if enabled the NIC generates a wake up event for this event queue and writes it into an event queue associated with a char driver in the kernel. In step again if enabled the NIC generates an interrupt to activate the host char driver event queue handler then disables interrupts. In step the host char driver event queue handler upon reaching the wake up event activates the receive event handler in the process that owns the specified receive queue.

In step the host subsystem retrieves the event descriptor at the location in the event queue designated by the receive event queue read pointer. If this new event is not in the cleared state step then the receive event queue contains events for handling at this time. In step it is determined whether the new event is a receive completion event. In one embodiment receive event queue cannot contain any events other than receive completion events but in another embodiment it can. Thus if the current event is something other than a receive completion event such as a management event then it is handled in step .

If the current event is a receive completion event then in step the host determines whether the Receive Queue Empty flag is set. If so then the module in step triggers the host receive event queue management module in order to replenish the receive queue with additional receive data buffers. In step the host determines further whether any of a variety of error types are indicated by the receive completion event descriptor. If so then in step the host handles the error. Note that some of the error types included in step may actually be detected before or after the receive queue empty test of step some may bypass the replenishment triggered by step of receive buffer descriptors in receive queue for the time being and some may bypass processing of the data packet in step . The details of such error handling are not important for an understanding of the invention.

In step assuming no serious error has been detected the host processes the newly received packet data including protocol processing. This may require chaining together several receive data buffers in sequence as designated by consecutive receive queue entries. The host knows the starting buffer and offset of the packet from the buffer descriptor in the receive queue pointed to by the host centric receive queue read pointer and knows the end of the packet either from the receive packet byte count identified in the receive completion event or from the copy of the device centric receive queue read pointer that might be included in the receive completion event. After processing the packet data in these buffers the host may release the buffers back into a pool for eventually re writing into the receive queue for re use by different incoming packet data.

In step if the higher level software is so designed the host subsystem may reprogram the receive queue entry pointed to by the host centric receive queue read pointer with a descriptor for a new available receive data buffer and may do the same with respect to all consecutively subsequent receive queue entries up to but not including the receive queue entry pointing to the beginning of data for the next receive packet. In step the host subsystem modulo increments the host centric receive queue read pointer for the receive queue by the number of buffers represented in the current receive completion event. In step the host subsystem clears the event descriptor at the location in receive event queue identified by the current receive event queue read pointer and in step the host subsystem modulo increments the receive event queue read pointer. The module then loops back to step to retrieve the next event descriptor and so on until a cleared entry is retrieved and the module goes inactive step .

If in step it is determined that the retrieved next event descriptor is cleared then the receive event queue contains no more events for handling at this time. In one embodiment the host receive event handler would then simply go inactive to await the next activation trigger step . In another embodiment in step if the host centric receive event queue read pointer has changed then the host writes the updated pointer value into the NIC s device centric receive event queue read pointer. The host receive event handler then goes inactive in step .

In step the kernel driver programs initial search limits for four kinds of filter table hash searches into the NIC . Before explaining this step it will be useful to understand the organization of the receive filter table. There are many ways to organize the receive filter table but in the present embodiment all types of entries are intermixed in a single table address space. Four types of entries are supported TCP full TCP wildcard UDP full and UDP wildcard. Table 1 below illustrates the format for an entry in the receive filter table.

It can be seen that each entry contains up to five fields for identifying a particular TCP or UDP endpoint in the host subsystem protocol TCP or UDP source IP address source port number destination IP address destination port number plus one for the associated receive queue ID. The queue ID field points to the entry in the receive queue descriptor table into which an incoming packet should be delivered when the endpoint data specified in the entry matches that of the header of the incoming packet.

The four fields source IP address source port number destination IP address and destination port number are referred to herein as the endpoint data portion of the entry. For a TCP full entry all four fields of endpoint data are filled. An incoming data packet will not match such an entry unless it is a TCP packet and all four endpoint data fields match corresponding fields in the packet header. For a TCP wildcard entry the destination IP address and destination port number are filled in and the remaining fields contain zeros. An incoming data packet will match a TCP wildcard entry if the incoming packet is a TCP packet and the two filled in endpoint fields of the entry match corresponding fields in the packet header regardless of the source IP address and port. For a UDP full entry all four fields of endpoint data are filled. An incoming data packet will not match such an entry unless it is a UDP packet and all four endpoint data fields match corresponding fields in the packet header. For a UDP wildcard entry like the TCP wildcard entry only the destination IP address and ports are filled and the remaining fields contain zeros. The UDP wildcard entry format differs from that of the TCP wildcard entry format since the destination port number is located in the field in which for the TCP full entry contains the source port number. An incoming data packet will match a UDP wildcard entry if the incoming packet is a UDP packet and the two filled in endpoint data fields of the entry match corresponding fields in the packet header again regardless of the source IP address and port.

It will be appreciated that another implementation might support different table entry formats different protocols and or different entry types. As one example entries may contain other kinds of numeric range indications rather than a wildcard for the entire field. As another example particular bits of an address or port field may be wildcarded out such as the low order bits of an IPv6 address . As can be seen as used herein therefore the matching of an entry to particular header information does not necessarily require complete equality. It is sufficient that it satisfies all conditions specified for a match.

All invalid entries in the table are written with all zeros. Obsolete entries are removed tombstoned at least in the first instance by marking them with all zeros as well.

Although all four entry types co exist in the same filter table separate searches are performed through the table for each type. If the incoming data packet is a UDP packet then a search is performed first for a matching UDP full entry and if that fails then for a matching UDP wildcard entry. If the incoming data packet is a TCP packet then a search is performed first for a matching TCP full entry and if that fails then for a matching TCP wildcard entry. All four kinds of searches are hashed searches described in detail below. It will be seen that these searches will proceed through only a limited number of iterations before aborting. In one embodiment the search limits for all four types of entries are the same but in the present embodiment they can be different. In particular the UDP wildcard search limit is kept relatively small because UDP packets are relatively rare. It is not likely that the receive filter table will develop very long search chains of valid UDP entries since not many software processes will need to program very many UDP filters. Similarly the TCP wildcard search limit is typically kept relatively small because again few software processes are expected to require more than a few TCP wildcard filter entries. The only one of these four types of entries that might require a high search limit is the TCP full entry type since a separate filter table entry might be required for example for each TCP connect call made by an application or kernel process. In any event if these assumptions are incorrect in a particular environment or at a particular point in time the search limits can be adjusted dynamically as described hereinafter.

Accordingly in step the kernel driver initialization routine programs into the NIC the initial search limits for each of the four types of filter table entries. In step the driver routine returns to the kernel.

In a step when the application first starts up its libraries are initialized. This includes the user level transport library which is initialized into the application s virtual address space.

Step begins an example sequence of steps in which the application process uses a UDP transport protocol. In step the application makes a call to the socket routine of the user level transport library specifying that it would like a UDP socket. In step the application process calls the bind routine of the user level transport library in order to bind the socket to a port. In step the application process makes a call to the recvfrom routine of the user level transport library specifying the socket handle the receive buffer the buffer length and the source IP address and port from which an incoming packet is expected. As described below the recvfrom routine of the User Level Transport Library among other things writes an appropriate UDP full type tuple into the receive filter table on the NIC and in the host . If the application wishes to receive packets from any IP address and Port these latter two fields may contain zeros. In this case the recvfrom routine of the User Level Transport Library will write an appropriate UDP wildcard type tuple into the receive filter table on the NIC and in the host .

After the recvfrom call the application blocks until a packet satisfying the specified criteria comes up from the NIC or from the kernel driver . At that time in step the application processes the received data and returns in this example to step to await the next incoming data packet.

Steps and are repeated many times most likely interspersed with many other functions performed by the application process. When the application has finished with the socket that it had created in step then in step the application makes a call to the close routine of the user level transport library in order to close the socket.

Alternatively to the UDP sequence beginning with step step begins an example sequence of steps in which the application process uses a TCP transport protocol. In step instead of calling the socket routine of the user level transport library to specify the UDP protocol it calls the socket routine to specify the TCP protocol. In step the application process calls the bind routine similarly to step in order to bind the socket to a port. In step since the transport protocol is now TCP the application process calls the listen routine of the user level transport library in order to listen for an incoming TCP connection request. Alternatively in step the application process may call the accept routine of the user level transport library . Both the listen and accept routines invoke the kernel to write into the receive filter table on the NIC and in the host a new IP tuple of type TCP wildcard so that any incoming TCP connection request SYN having the appropriate destination IP address and port number in its header will be sent up to the present application process for handling.

In step the application process makes a call to the recv function of the user level transport library specifying a socket handle the buffer into which data should be delivered and the buffer length. At this point the application blocks until a packet satisfying the specified criteria comes up from the NIC or from the kernel driver . At that time in step new incoming data arrives in the buffer and the application processes it. The application then may return to the recv step to await the next incoming data packet. Again many other steps may take place in between those illustrated in the flow chart. In step the application terminates or the connection may be terminated at which time the application makes a call to the close routine of the user level transport library in order to close the socket.

Note that for TCP connections initiated by the present application process instead of calling the listen routine in step typically the application will make a call to the connect routine of the User Level Transport Library specifying the socket handle and the destination IP address and port number. The connect routine of the User Level Transport Library will among other things invoke the kernel driver to write into the receive filter table on the NIC and the host a new IP tuple of type TCP full so that only those incoming TCP packets having four matching endpoint data fields will be sent up into the present application process.

In step as part of the initialization of the user level transport library a resource allocation routine in the kernel driver is invoked. The kernel level routine is required for allocating resources in the network interface card and the host memory subsystem since these resources are outside the virtual address space of the application or involve direct hardware accesses that advisedly are restricted to kernel processes. After resource allocation the user level driver initialization routine may perform a number of other steps before it returns to the application in step .

In step the kernel routine allocates a minimum set of the buffers for each of the transmit receive and event queues requested and programs their buffer IDs into the transmit receive and event queue descriptor tables and . In step the kernel routine determines the doorbell address in the NIC for each of the transmit and receive queues and maps them as well into the application s virtual address space. The doorbell address is the address to which the user level transport library will write a value in order to notify the NIC either that a transmit buffer is ready or that a receive buffer can be released. For transmit queues the doorbell address is the address of the device centric transmit queue read pointer in the transmit queue descriptor table entry for the particular transmit queue. For receive queues the doorbell address is the address of the device centric receive queue write pointer in the receive queue descriptor table entry for the particular receive queue.

In step the resource allocation routine returns to the application with handles for the resources allocated with the base virtual addresses of the transmit receive and event queues and virtual memory addresses corresponding to the doorbells allocated in the transmit and receive queue descriptor tables and .

In a second mechanism during the resource allocation step performed during initialization of a particular instance of the user level driver the kernel allocates a unique IP address for that instance to use as it wishes. If each instance of the user level driver has its own exclusively assigned IP address then the instance can manage the available port numbers for use with that IP address without risking interference with any other process. This mechanism is useful only if there are sufficient numbers of IP addresses available to the computer system to allocate to the various requesting processes. Again this mechanism can be used in conjunction with the first to reject or pass on to the kernel all user level requests to bind to a kernel only port number regardless of the exclusivity of an assigned IP address.

In a third mechanism again during initialization of a particular instance of the user level driver the initialization routine makes a number of anticipatory bind calls to the kernel in order to form a pool of port numbers that the user level driver instance can later allocate to the application program upon receipt of bind calls to the user level driver. This mechanism can succeed with far fewer IP addresses available to the computer system but also undesirably involves a context switch during library initialization for each port number to be added to the pool.

In yet a fourth mechanism no IP address port number combinations are pre allocated to the particular instance of the user level driver. Instead the user level bind routine invokes the kernel bind routine for each user level bind call received. This mechanism utilizes IP address port number combinations most conservatively but may require more context switches than any of the first second and third mechanisms. In an embodiment this fourth mechanism is used only as a backup for example if the user level process requires more port numbers than were made available using the anticipatory bind calls in the third mechanism.

If in step the user level bind routine determines that the requested port number is not available to the current instance of the user level driver or otherwise cannot determine whether is available then in step the routine makes a call to the kernel bind routine to pass the request on to the kernel to handle. If the fourth mechanism above is the only way that the particular embodiment avoids conflicting or illegal allocation of address port number combinations then step will be taken during every user level call to the bind routine . Otherwise step will be taken only as a backup if pre allocated port numbers have been exhausted or if the routine otherwise cannot determine that the requested port number is available.

If the specified port number is legal or if a port number was assigned by the routine in step then in step the routine updates the application s state internally to bind the port number with the specified socket. The routine returns to the caller in step .

Note that the user level recv routine is in pertinent part the same as the recvFrom routine of except that since the filter has already been applied steps and are omitted.

The user level accept routine is in pertinent part the same as the listen routine of except that the routine will more likely program the new TCP full entry into the filter table because the application is more likely to follow up with a fork into a new address space.

Note that the User Level Transport Library routines that invoke the kernel to set up a new filter also maintain a local copy of the filters that they already had set up. In this way they can avoid the context switch to the kernel to duplicate work that has already been performed.

If the protocol is TCP or UDP then in step the routine performs a hashed search in the host receive filter table for an empty slot. For UDP filters a search of the UDP wildcard entries is performed if the ip src and src port arguments are null. If either the ip src or src port argument contains a value then a search of the UDP full entries is performed. Alternatively the API can include a separate parameter to indicate whether a full or wildcard search should be performed. For TCP filters a search of the TCP wildcard entries is performed if the ip src and src port arguments are null. If either the ip src or src port argument contains a value then a search of the TCP full entries is performed. In each case the search is bounded by the appropriate search limit for the TCP full TCP wildcard UDP full or UDP wildcard protocol as programmed in step . The search algorithm itself is described in detail with respect to and is the same as that used by the NIC against the NIC copy of the filter table upon receipt of a packet from the network.

In step if an empty slot was found before the appropriate search limit was reached then the routine programs both the NIC and the host receive filter tables at the identified entry with the queue ID and the endpoint data as provided in step . The kernel routine then returns to the caller in the user level transport library.

If the search limit was reached before an empty slot was found then the routine makes a decision whether to increase the search limit for the particular kind of entry step . If not then in step the routine simply sets up a software redirect for this set of endpoint data. The redirect is a data structure that the kernel driver consults on receipt of every packet to see whether the packet should be delivered to the kernel stack or a user transport managed endpoint. In one embodiment it is a separate table of filter entries structured similarly to the host receive filter table.

If the routine does decide to increase the search limit then in step the routine simply continues the hashed search of step until an empty slot is found. When one is found then in step the routine programs the NIC and host receive filter tables at the identified entry with the queue ID and endpoint data from step . Then in step the routine programs the new search limit for the particular type of entry into the NIC and then in the kernel. It will be appreciated that steps and may be reversed in sequence or combined so that the number of additional hops required to find an available entry can be taken into account in the decision of whether to increase the search limit.

In the close function of the user level driver called both at steps and of one of the functions that must be performed is the removal of the socket s entries from the IP filter table. The routine in the user level driver accomplishes this by making a request to the RemoveFilterEntry routine of the kernel illustrated in . In step the routine looks up the filter table entry from the data associated with the socket handle specified. In step the routine determines whether the entry is located in the software redirect table or in the host filter table. If it is in the software redirect table then in step the routine removes the entry from the software redirect table and returns step . If the entry is in the host filter table when the entry is identified in step then in one embodiment the routine can simply remove the identified entry in both the NIC receive filter table and the host receive filter table in that order step . As mentioned removal involves simply writing a tombstone value into the entry for example all zeros. However it would be useful at this time to also perform certain garbage collection functions in order to keep the search chain lengths reasonable. These garbage collection functions can be performed at other times in the same or different embodiments but the present embodiment conveniently performs them at least whenever a filter entry is to be removed. Accordingly before the step of tombstoning the identified entry step the routine first determines in step whether removal of the identified entry and all tombstones previously written in the present chain would reduce the overall maximum chain length. If not then only the single entry removal is effected step . Garbage collection is typically an expensive process so step limits the number of times that garbage collection is performed to only those times when a significant performance improvement can be achieved such as through a reduction in the overall maximum chain length. Hysteresis can be applied here to avoid flapping and a weight can be applied so that the chain length is reduced more aggressively if it has become longer than the maximum allowed for full line rate performance.

If the maximum chain length would be reduced then in step the routine removes the identified entry from the table as well as all tombstones in the table or only those in the chain if the chain is very long compared to others in the table reprogramming the table to bring up into the previously tombstone the locations entries and that were previously farther down in the search chain. Finally in step the routine programs the new smaller search limit for the particular kind of entry into the NIC and then makes it effective in the kernel as well. In an embodiment the routine may skip step in the event that the new search limit would be smaller than some predetermined minimum chain length for the particular kind of entry. Many other optimizations will be apparent.

As mentioned when packets arrive the filter table and logic unit first determines the queue ID of the receive queue into which the packet should be delivered. is a detail of the filter table and logic unit . The filter table itself is implemented as two sub tables and collectively . The software is unaware of this implementation detail and instead sees only a single table. The hardware in the NIC decodes the write address from the kernel driver software and places even numbered entries in filter sub table and odd numbered entries in filter sub table . Thus filter sub table contains entries 0 2 4 6 and so on whereas filter sub table contains entries 1 3 5 and so on. The implementation of the filter table as two sub tables enables parallel entry lookup per cycle to reduce the total lookup time. It will be appreciated that other implementations can use a single sub table or more than two sub tables.

Both of the filter sub tables and are addressed by a 13 bit index provided by filter table search logic . A 13 bit index can address up to 8192 entries which for two sub tables comes to 16 384 entries numbered 0 through 16 383. Four index values are reserved for the kernel NET driver queues so only 16 380 entries are represented in the table. The filter table search logic is described hereinafter but basically it receives the header data of the incoming data packet and uses it to derive a hash key then uses the hash key to derive a hash code which is the first 13 bit index to be searched in the filter table . The filter table search logic also calculates subsequent entries to search if the first entry in neither of the two filter sub tables matches that of the incoming header data and also forces the selection of a default kernel queue for delivery of the current data packet if the search limit is reached before a match is found. The filter table search logic also determines a match type TCP full TCP wildcard UDP full or UDP wildcard in dependence upon the header data and the state of the search algorithm.

The various formats for an entry in the filter table are set forth in the table above. As shown in the endpoint data from the selected entry of filter sub table is provided to one input of match logic and the endpoint data from the selected entry of filter sub table is provided to the corresponding input of match logic . The other input of each of the match logic units and collectively receives the header data of the incoming data packet. The match type is provided from the filter table search logic to both match logic units each of which then outputs a match signal to a hit logic unit . If the match type is TCP full then match logic units at will indicate a match only if the incoming packet type is TCP and all four fields of endpoint data match the corresponding fields of the incoming header data. If the match type is TCP wildcard then the match logic units will indicate a match if the incoming packet type is TCP and bits of the endpoint data in the table contains the same destination IP address and destination port as the incoming header data. The source IP address and source port as indicated in the header data are ignored. If the match type is UDP full then match logic units at will indicate a match only if the incoming packet type is UDP and all four fields of endpoint data match the corresponding fields of the incoming header data. If the match type is UDP wildcard then match logic units will indicate a match if the incoming packet type is UDP and bits of the filter endpoint data contain the same destination IP address and bits of the endpoint data contain the same destination port number as indicated in the header data of the incoming packet.

If either match logic unit or indicates a match then hit logic so notifies the filter table search logic . The Q ID fields of the currently selected entries in both filter sub tables are provided to two of three inputs of a multiplexer and hit logic provides a select input so that the multiplexer will select the queue ID from the currently selected entry of the correct filter sub table or . As mentioned if no matching entry has been found after the search limit has been reached then the filter table search logic provides a signal to the multiplexer to select to the default queue ID provided on a third input of the multiplexer . The default queue ID in one embodiment is queue 0 which is defined to be a kernel queue. In other embodiments the default queue ID can be programmable. In any event whether or not a match has been found the multiplexer outputs the queue ID indicating the particular receive queue to which the NIC should deliver the incoming data packet.

Note that in a multiple CPU system there can be multiple kernel queues. In such a system it is advantageous to distribute failed search traffic as evenly as possible among the different kernel queues so as not to overload any particular one of them. It is also desirable that all failed search traffic involving a particular connection always be delivered to a single one of the kernel queues. In order to accomplish both of these goals the low order bit or bits of the initial hash key code already calculated is used to select the default kernel queue for delivery of failed search packets. In particular the kernel driver first programs a register in the NIC not shown to indicate the number of kernel queues in use. If a search fails the NIC then uses the low order 1 or 2 bits depending on whether there are 2 or 4 processors of the initial hash code in order to select the particular CPU whose kernel queue will receive the incoming data packet. Other methods may be used instead to select the kernel queue to which a failed search packet will be delivered. As one alternative a different hash function can be used different from the one described herein for searching the filter table. For example the Toeplitz hash function described in Microsoft Scalable Networking with RSS Win HEC 2005 Update Apr. 19 2005 incorporated by reference herein can be used.

If a data packet has arrived without an IP header or containing a mal formed IP header such as with an IP header checksum error then the NIC delivers this packet to an overall default kernel queue which is queue 0.

If the protocol of the incoming data packet is TCP then in step the filter table search logic performs a hashed TCP full search as described in more detail hereinafter. If this search finds a match before the TCP full search limit is reached then in step the filter table search logic cause the multiplexer to select the queue ID from the matching filter sub table or . If the TCP full search limit is reached before a match is found in step then in step a TCP wildcard search is performed. By performing a TCP full search before a TCP wildcard search the more specific filter is given precedence over the more general filter. If the TCP wildcard search finds a match before the TCP wildcard search limit is reached then in step the filter table search logic causes the multiplexer to select the queue ID from the matching filter sub table or . Otherwise in step the filter table search logic causes the multiplexer to select the default queue ID.

If the protocol of the incoming data packet is UDP then in step the filter table search logic performs a hashed UDP full search as described in more detail hereinafter. If this search finds a match before the UDP full search limit is reached then in step the filter table search logic cause the multiplexer to select the queue ID from the matching filter sub table or . If the UDP full search limit is reached before a match is found in step then in step a UDP wildcard search is performed. If the UDP wildcard search finds a match before the UDP wildcard search limit is reached then in step the filter table search logic causes the multiplexer to select the queue ID from the matching filter sub table or . Otherwise in step the filter table search logic causes the multiplexer to select the default queue ID.

As with all flow charts herein it will be appreciated that many of the steps in the flow chart of can be combined performed in parallel or performed in a different sequence without affecting the functions achieved.

In step the key is passed through the first hash function h1 key to determine the initial hash code which is then to be used as an index into the filter table as previously described. The first hash function is implemented through a linear feedback shifting register LFSR sequence with characteristic polynomial X 16 X 3 1 on the 32 bit key. This value is taken modulo 2 3 by taking only the low order 13 bits of the LFSR output to form the hash code. This function randomizes the key sufficiently so that even packets sharing the same IP address or port number are not likely to be mapped into the same initial code. The modulus 2 13 is the number of entries that can be held in one of the filter sub tables so the truncation to only the low order 13 bits effectively keeps the hash code within the total index range of the table. In an implementation the hash function can be performed in a single cycle by flattening the serial shift register into parallel XOR functions of the 32 bit key. Other first hash functions can be used instead of this one but this one is preferred.

Now that the initial index has been calculated in step in step the match logic units and hit logic unit indicate to the filter table search logic whether the header data matches either the endpoint data in the currently selected entry of filter sub table or that of the currently selected entry of filter sub table . The filter table search logic indicates TCP full as the match type provided to match logic units and . If the current entry of one of the two filter sub tables matches then a match has been found step and the filter table search logic cause the multiplexer to select the appropriate queue ID step . If not then in step the filter table search logic increments the iteration count k and determines whether the TCP full search limit has been reached. If so then the routine exits step such that a TCP wildcard search can be performed step in . If the search limit has not been reached then in step the index is incremented by a second hash function h2 of the key and truncated again to 13 bits. The second hash function need not be as expensive as the first but it is advantageously a function of the key in order to provide sufficient randomness to minimize overlapping hop sequences for different incoming header data. In an embodiment the second hash function is simply 2 key 13 LSB of key 2 1 .

Combining the two has functions it can be seen that the filter table index for the k th search is given by index 1 key 2 key mod 2 13. If the filter table had not been divided into sub tables then the modulus would be 2 14 instead of 2 13. In general if the overall filter table can hold N entries and they are divided into P parallel tables where P is an integer greater than or equal to 1 then the modulus would be M N P. Note that while the embodiment described herein maintains a current value for k it uses this value only for determining when the search limit is reached. The different index values are maintained and calculated separately without using the current value of k. Another embodiment could instead use the current value of k to calculate each index value for example by using the combined mathematical function above for index k .

Once the next index has been calculated in step the TCP full search logic returns to step to determine whether the header data matches the endpoint data in either filter table entry now pointed to by the new index. Eventually either a match will be found step or the TCP full search limit will be reached and the TCP full search step will terminate.

In step the key is passed through the first hash function h1 key to determine the initial hash code which is then to be used as an index into the filter table as previously described. The same first hash function is used as is set forth above for the TCP full search although in another embodiment a different first hash function might be used.

The initial index is applied to the filter sub tables and in step the match logic units and hit logic unit indicate to the filter table search logic whether the header data matches either the endpoint data in the currently selected entry of filter sub table or that of the currently selected entry of filter sub table . The filter table search logic indicates TCP wildcard as the match type provided to match logic units and . If the current entry of one of the two filter sub tables matches then a match has been found step and the filter table search logic cause the multiplexer to select the appropriate queue ID step . If not then in step the filter table search logic increments the iteration count k and determines whether the TCP wildcard search limit has been reached. If so then the routine exits step such that the default queue ID can be selected step of . If the search limit has not been reached then in step the index is incremented by the second hash function h2 of the key and truncated again to 13 bits. The second hash function is again the same as for the TCP full search but it need not be in a different embodiment. Once the next index has been calculated the TCP wildcard search logic returns to step to determine whether the header data matches the endpoint data in either filter table entry now pointed to by the new index. Eventually either a match will be found step or the TCP wildcard search limit will be reached and the TCP wildcard search step will terminate.

In step the key is passed through the first hash function h1 key to determine the initial hash code which is then to be used as an index into the filter table as previously described. The same first hash function is used as is set forth above for the TCP full and TCP wildcard searches although again in another embodiment a different first hash function might be used for UDP full searches.

The initial index is applied to the filter sub tables and in step the match logic units and hit logic unit indicate to the filter table search logic whether the header data matches either the endpoint data in the currently selected entry of filter sub table or that of the currently selected entry of filter sub table . The filter table search logic indicates UDP full as the match type provided to match logic units and . If the current entry of one of the two filter sub tables matches then a match has been found step and the filter table search logic cause the multiplexer to select the appropriate queue ID step . If not then in step the filter table search logic increments the iteration count k and determines whether the UDP full search limit has been reached. If so then the routine exits step such that the default queue ID can be selected step of . If the search limit has not been reached then in step the index is incremented by the second hash function h2 of the key and truncated again to 13 bits. The second hash function is again the same as for the TCP full and TCP wildcard searches but it need not be in a different embodiment. Once the next index has been calculated the UDP full search logic returns to step to determine whether the header data matches the endpoint data in either filter table entry now pointed to by the new index. Eventually either a match will be found step or the UDP full search limit will be reached and the UDP full search step will terminate.

It can be seen that in a compact hardware implementation of the search algorithm all three fold by three functions can be performed with common XOR hardware by preceding it with a multiplexer to select to the XOR logic the particular fields of the incoming packet header data that are required for the particular search type.

In step the key is passed through the first hash function h1 key to determine the initial hash code which is then to be used as an index into the filter table as previously described. The same first hash function is used as is set forth above for the TCP full and TCP wildcard searches although again in another embodiment a different first hash function might be used for UDP wildcard searches.

The initial index is applied to the filter sub tables and in step the match logic units and hit logic unit indicate to the filter table search logic whether the header data matches either the endpoint data in the currently selected entry of filter sub table or that of the currently selected entry of filter sub table . The filter table search logic indicates UDP wildcard as the match type provided to match logic units and . If the current entry of one of the two filter sub tables matches then a match has been found step and the filter table search logic cause the multiplexer to select the appropriate queue ID step . If not then in step the filter table search logic increments the iteration count k and determines whether the UDP wildcard search limit has been reached. If so then the routine exits step such that the default queue ID can be selected step of . If the search limit has not been reached then in step the index is incremented by the second hash function h2 of the key and truncated again to 13 bits. The second hash function is again the same as for the TCP full and TCP wildcard searches but it need not be in a different embodiment. Once the next index has been calculated the UDP wildcard search logic returns to step to determine whether the header data matches the endpoint data in either filter table entry now pointed to by the new index. Eventually either a match will be found step or the UDP wildcard search limit will be reached and the UDP wildcard search step will terminate.

As previously mentioned if the incoming data packet is mal formed or uses a protocol that is not supported in the filter table or if it uses the supported protocol but a match was not found in the filter table before the appropriate search limit s was were reached then the NIC will deliver the incoming data packet to a receive queue of the kernel driver . is a flow chart showing pertinent steps that the kernel driver performs upon receipt of such a data packet. Initially in step the kernel routine determines whether the incoming data packet uses the TCP or UDP protocol. If not then in step the kernel driver processes the packet in whatever manner is appropriate for the particular packet format. If the incoming data packet does use TCP or UDP then in step the kernel driver performs a hashed search with no search limit imposed of the software redirect table. In step if no match was found then the kernel driver simply delivers the packet to a normal kernel network stack step . If a match was found then in step the kernel driver delivers the packet to the proper user level receive process. In order to avoid contention with the NIC attempting to deliver its own data packets to receive queues in an application s transport library the delivery of the packet from the kernel driver to the user level transport library occurs by some communication channel other than through the use of the receive queue. Typical standard operating system mechanisms can be used to notify the user level driver of the availability of this packet.

As used herein the identification of an item of information does not necessarily require the direct specification of that item of information. Information can be identified in a field by simply referring to the actual information through one or more layers of indirection or by identifying one or more items of different information which are together sufficient to determine the actual item of information. In addition the term indicate is used herein to mean the same as identify .

Additionally as used herein a given signal event or value is responsive to a predecessor signal event or value if the predecessor signal event or value influenced the given signal event or value. If there is an intervening processing element step or time period the given signal event or value can still be responsive to the predecessor signal event or value. If the intervening processing element or step combines more than one signal event or value the signal output of the processing element or step is considered responsive to each of the signal event or value inputs. If the given signal event or value is the same as the predecessor signal event or value this is merely a degenerate case in which the given signal event or value is still considered to be responsive to the predecessor signal event or value. Dependency of a given signal event or value upon another signal event or value is defined similarly.

The foregoing description of preferred embodiments of the present invention has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Obviously many modifications and variations will be apparent to practitioners skilled in this art. As an example whereas in the embodiments described herein it is the header fields of an incoming packet which are compared to fields in the filter table to detect a matching filter table entry in another embodiment other aspects of the content of the incoming packet can be compared instead. As another example whereas the filter table in the NIC in the embodiments described herein have a tabular format it will be appreciated that a table is only one possible format for what is more generally known as a database. Another embodiment might implement a filter database having a different structure that need not be tabular. The embodiments were chosen and described in order to best explain the principles of the invention and its practical application thereby enabling others skilled in the art to understand the invention for various embodiments and with various modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalents.

