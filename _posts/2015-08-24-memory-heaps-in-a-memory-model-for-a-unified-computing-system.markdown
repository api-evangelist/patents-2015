---

title: Memory heaps in a memory model for a unified computing system
abstract: A method and system for allocating memory to a memory operation executed by a processor in a computer arrangement having a first processor configured for unified operation with a second processor. The method includes receiving a memory operation from a processor and mapping the memory operation to one of a plurality of memory heaps. The mapping produces a mapping result. The method also includes providing the mapping result to the processor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09448930&OS=09448930&RS=09448930
owner: Advanced Micro Devices, Inc.
number: 09448930
owner_city: Sunnyvale
owner_country: US
publication_date: 20150824
---
This application is a continuation of U.S. patent application Ser. No. 13 724 879 filed Dec. 21 2012 which claims the benefit of U.S. Provisional Application No. 61 617 405 filed on Mar. 29 2012 which are incorporated herein by reference in their entirety.

The present invention is generally directed to computer systems. More particularly the present invention is directed towards an architecture for unifying the computational components within a computer system.

The desire to use a graphics processing unit GPU for general computation has become much more pronounced recently due to the GPU s exemplary performance per unit power and or cost. The computational capabilities for GPUs generally have grown at a rate exceeding that of the corresponding central processing unit CPU platforms. This growth coupled with the explosion of the mobile computing market e.g. notebooks mobile smart phones tablets etc. and its necessary supporting server enterprise systems has been used to provide a specified quality of desired user experience. Consequently the combined use of CPUs and GPUs for executing workloads with data parallel content is becoming a volume technology.

However GPUs have traditionally operated in a constrained programming environment available primarily for the acceleration of graphics. These constraints arose from the fact that GPUs did not have as rich a programming ecosystem as CPUs. Their use therefore has been mostly limited to two dimensional 2 D and three dimensional 3 D graphics and a few leading edge multimedia applications which are already accustomed to dealing with graphics and video application programming interfaces APIs .

With the advent of multi vendor supported OpenCL and DirectCompute standard APIs and supporting tools the limitations of the GPUs in traditional applications has been extended beyond traditional graphics. Although OpenCL and DirectCompute are a promising start there are many hurdles remaining to creating an environment and ecosystem that allows the combination of a CPU and a GPU to be used as fluidly as the CPU for most programming tasks.

Existing computing systems often include multiple processing devices. For example some computing systems include both a CPU and a GPU on separate chips e.g. the CPU might be located on a motherboard and the GPU might be located on a graphics card or in a single chip package. Both of these arrangements however still include significant challenges associated with i efficient scheduling ii programming model iii compiling to multiple target instruction set architectures iv providing quality of service QoS guarantees between processes ISAs and v separate memory systems all while minimizing power consumption.

In conventional systems e.g. CPU and GPU computing systems programmers were required to explicit marshal memory between separate address spaces associated with each of the client devices. This among other things introduced a constraint to the programmer.

What is needed therefore is a method and system providing a memory configured to operate in a multi client computing system environment that frees the programmer from the above noted constraint. More particularly what is needed is a region of memory allocated from a single memory space with common access and storage properties.

Although GPUs accelerated processing units APUs and general purpose use of the graphics processing unit GPGPU are commonly used terms in this field the expression accelerated processing device APD is considered to be a broader expression. For example APD refers to any cooperating collection of hardware and or software that performs those functions and computations associated with accelerating graphics processing tasks data parallel tasks or nested data parallel tasks in an accelerated manner compared to conventional CPUs conventional GPUs software and or combinations thereof.

More specifically embodiments of the invention in certain circumstances provide a method and apparatus for allocating memory to a memory operation executed by a processor in a computer arrangement having an APD configured for unified operation with a CPU. The method includes receiving a memory operation from a processor and mapping the memory operation to one of a plurality of memory heaps. The mapping produces a mapping result. The method also includes providing the mapping result to the processor.

Further features and advantages of the invention as well as the structure and operation of various embodiments of the invention are described in detail below with reference to the accompanying drawings. The invention is not limited to the specific embodiments described herein. The embodiments are presented for illustrative purposes only and so that readers will have multiple views enabling better perception of the invention which is broader than any particular embodiment. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

The invention is described with reference to the accompanying drawings. The drawing in which an element first appears is typically indicated by the leftmost digit s in the corresponding reference number.

In the detailed description that follows references to one embodiment an embodiment an example embodiment etc. indicate that the embodiment described may include a particular feature structure or characteristic but every embodiment may not necessarily include the particular feature structure or characteristic. Moreover such phrases are not necessarily referring to the same embodiment. Further when a particular feature structure or characteristic is described in connection with an embodiment it is submitted that it is within the knowledge of one skilled in the art to affect such feature structure or characteristic in connection with other embodiments whether or not explicitly described.

The term embodiments of the invention does not require that all embodiments of the invention include the discussed feature advantage or mode of operation. Alternate embodiments may be devised without departing from the scope of the invention and well known elements of the invention may not be described in detail or may be omitted so as not to obscure the relevant details of the invention. In addition the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. For example as used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises comprising includes and or including when used herein specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

In one example system also includes a memory an operating system and a communication infrastructure . The operating system and the communication infrastructure are discussed in greater detail below.

The system also includes a kernel mode driver KMD a software scheduler SWS and a memory management unit such as input output memory management unit IOMMU . Components of system can be implemented as hardware firmware software or any combination thereof. A person of ordinary skill in the art will appreciate that system may include one or more software hardware and firmware components in addition to or different from that shown in the embodiment shown in .

In one example a driver such as KMD typically communicates with a device through a computer bus or communications subsystem to which the hardware connects. When a calling program invokes a routine in the driver the driver issues commands to the device. Once the device sends data back to the driver the driver may invoke routines in the original calling program. In one example drivers are hardware dependent and operating system specific. They usually provide the interrupt handling required for any necessary asynchronous time dependent hardware interface.

Device drivers particularly on modern Microsoft Window platforms can run in kernel mode Ring 0 or in user mode Ring 3 . The primary benefit of running a driver in user mode is improved stability since a poorly written user mode device driver cannot crash the system by overwriting kernel memory. On the other hand user kernel mode transitions usually impose a considerable performance overhead thereby prohibiting user mode drivers for low latency and high throughput requirements. Kernel space can be accessed by user module only through the use of system calls. End user programs like the UNIX shell or other GUI based applications are part of the user space. These applications interact with hardware through kernel supported functions.

CPU can include not shown one or more of a control processor field programmable gate array FPGA application specific integrated circuit ASIC or digital signal processor DSP . CPU for example executes the control logic including the operating system KMD SWS and applications that control the operation of computing system . In this illustrative embodiment CPU according to one embodiment initiates and controls the execution of applications by for example distributing the processing associated with that application across the CPU and other processing resources such as the APD .

APD among other things executes commands and programs for selected functions such as graphics operations and other operations that may be for example particularly suited for parallel processing. In general APD can be frequently used for executing graphics pipeline operations such as pixel operations geometric computations and rendering an image to a display. In various embodiments of the present invention APD can also execute compute processing operations e.g. those operations unrelated to graphics such as for example video operations physics simulations computational fluid dynamics etc. based on commands or instructions received from CPU .

For example commands can be considered as special instructions that are not typically defined in the ISA. A command may be executed by a special processor such a dispatch processor command processor or network controller. On the other hand instructions can be considered for example a single operation of a processor within a computer architecture. In one example when using two sets of ISAs some instructions are used to execute x86 programs and some instructions are used to execute kernels on an APD unit.

In an illustrative embodiment CPU transmits selected commands to APD . These selected commands can include graphics commands and other commands amenable to parallel execution. These selected commands that can also include compute processing commands can be executed substantially independently from CPU .

APD can include its own compute units not shown such as but not limited to one or more SIMD processing cores. As referred to herein a SIMD is a pipeline or programming model where a kernel is executed concurrently on multiple processing elements each with its own data and a shared program counter. All processing elements execute an identical set of instructions. The use of predication enables work items to participate or not for each issued command.

In one example each APD compute unit can include one or more scalar and or vector floating point units and or arithmetic and logic units ALUs . The APD compute unit can also include special purpose processing units not shown such as inverse square root units and sine cosine units. In one example the APD compute units are referred to herein collectively as shader core .

Having one or more SIMDs in general makes APD ideally suited for execution of data parallel tasks such as those that are common in graphics processing.

Some graphics pipeline operations such as pixel processing and other parallel computation operations can require that the same command stream or compute kernel be performed on streams or collections of input data elements. Respective instantiations of the same compute kernel can be executed concurrently on multiple compute units in shader core in older to process such data elements in parallel. As referred to herein for example a compute kernel is a function containing instructions declared in a program and executed on an APD. This function is also referred to as a kernel a shader a shader program or a program.

In one illustrative embodiment each compute unit e.g. SIMD processing core can execute a respective instantiation of a particular work item to process incoming data. A work item is one of a collection is of parallel executions of a kernel invoked on a device by a command. A work item can be executed by one or more processing elements as part of a work group executing on a compute unit.

A work item is distinguished from other executions within the collection by its global ID and local ID. In one example a subset of work items in a workgroup that execute simultaneously together on a SIMD can be referred to as a wavefront . The width of a wavefront is a characteristic of the hardware of the compute unit e.g. SIMD processing core . As referred to herein a workgroup is a collection of related work items that execute on a single compute unit. The work items in the group execute the same kernel and share local memory and work group barriers.

In the exemplary embodiment all wavefronts from a workgroup are processed on the same SIMD processing core. Instructions across a wavefront are issued one at a time and when all work items follow the same control flow each work item executes the same program. Wavefronts can also be referred to as warps vectors or threads.

An execution mask and work item predication are used to enable divergent control flow within a wavefront where each individual work item can actually take a unique code path through the kernel. Partially populated wavefronts can be processed when a full set of work items is not available at wavefront start time. For example shader core can simultaneously execute a predetermined number of wavefronts each wavefront comprising a multiple work items.

Within the system APD includes its own memory such as graphics memory although memory is not limited to graphics only use . Graphics memory provides a local memory for use during computations in APD . Individual compute units not shown within shader core can have their own local data store not shown . In one embodiment APD includes access to local graphics memory as well as access to the memory . In another embodiment APD can include access to dynamic random access memory DRAM or other such memories not shown attached directly to the APD and separately from memory .

In the example shown APD also includes one or n number of command processors CPs . CP controls the processing within APD . CP also retrieves commands to be executed from command buffers in memory and coordinates the execution of those commands on APD .

In one example CPU inputs commands based on applications into appropriate command buffers . As referred to herein an application is the combination of the program parts that will execute on the compute units within the CPU and APD.

A plurality of command buffers can be maintained with each process scheduled for execution on the APD .

CP can be implemented in hardware firmware or software or a combination thereof. In one embodiment CP is implemented as a reduced instruction set computer RISC engine with microcode for implementing logic including scheduling logic.

APD also includes one or n number of dispatch controllers DCs . In the present application the term dispatch refers to a command executed by a dispatch controller that uses the context state to initiate the start of the execution of a kernel for a set of work groups on a set of compute units. DC includes logic to initiate workgroups in the shader core . In some embodiments DC can be implemented as part of CP .

System also includes a hardware scheduler HWS for selecting a process from a ran list for execution on APD . HWS can select processes from run list using round robin methodology priority level or based on other scheduling policies. The priority level for example can be dynamically determined. HWS can also include functionality to manage the run list for example by adding new processes and by deleting existing processes from run list . The run list management logic of HWS is sometimes referred to as a run list controller RLC .

In various embodiments of the present invention when HWS initiates the execution of a process from run list CP begins retrieving and executing commands from the corresponding command buffer . In some instances CP can generate one or more commands to be executed within APD which correspond with commands received from CPU . In one embodiment CP together with other components implements a prioritizing and scheduling of commands on APD in a manner that improves or maximizes the utilization of the resources of APD resources and or system .

APD can have access to or may include an interrupt generator . Interrupt generator can be configured by APD to interrupt the operating system when interrupt events such as page faults are encountered by APD . For example APD can rely on interrupt generation logic within IOMMU to create the page fault interrupts noted above.

APD can also include preemption and context switch logic for preempting a process currently running within shader core . Context switch logic for example includes functionality to stop the process and save its current state e.g. shades core state and CP state .

As referred to herein the term state can include an initial state an intermediate state and or a final state. An initial state is a starting point for a machine to process an input data set according to a programming order to create an output set of data. There is an intermediate state for example that needs to be stored at several points to enable the processing to make forward progress. This intermediate state is sometimes stored to allow a continuation of execution at a later time when interrupted by some other process. There is also final state that can be recorded as part of the output data set.

Preemption and context switch logic can also include logic to context switch another process into the APD . The functionality to context switch another process into running on the APD may include instantiating the process for example through the CP and DC to run on APD restoring any previously saved state for that process and starting its execution.

Memory can include non persistent memory such as DRAM not shown . Memory can store e.g. processing logic instructions constant values and variable values during execution of portions of applications or other processing logic. For example in one embodiment parts of control logic to perform one or more operations on CPU can reside within memory during execution of the respective portions of the operation by CPU .

During execution respective applications operating system functions processing logic commands and system software can reside in memory . Control logic commands fundamental to operating system will generally reside in memory during execution. Other software commands including for example kernel mode driver and software scheduler can also reside in memory during execution of system .

In this example memory includes command buffers that are used by CPU to send commands to APD . Memory also contains process lists and process information e.g. active list and process control blocks . These lists as well as the information are used by scheduling software executing on CPU to communicate scheduling information to APD and or related scheduling hardware. Access to memory can be managed by a memory controller which is coupled to memory . For example requests from CPU or from other devices for reading from or for writing to memory are managed by the memory controller .

As used herein context can be considered the environment within which the kernels execute and the domain in which synchronization and memory management is defined. The context includes a set of devices the memory accessible to those devices the corresponding memory properties and one or more command queues used to schedule execution of a kernel s or operations on memory objects.

Referring back to the example shown in IOMMU includes logic to perform virtual to physical address translation for memory page access for devices including APD . IOMMU may also include logic to generate interrupts for example when a page access by a device such as APD results in a page fault. IOMMU may also include or have access to a translation lookaside buffer TLB . TLB as an example can be implemented in a content addressable memory CAM to accelerate translation of logical i.e. virtual memory addresses to physical memory addresses for requests made by APD for data in memory .

In the example shown communication infrastructure interconnects the components of system as needed. Communication infrastructure can include not shown one or more of a peripheral component interconnect PCI bus extended PCI PCI E bus advanced microcontroller bus architecture AMBA bus advanced graphics port AGP or other such communication infrastructure. Communications infrastructure can also include an Ethernet or similar network or any suitable physical communications infrastructure that satisfies an application s data transfer rate requirements. Communication infrastructure includes the functionality to interconnect components including components of computing system .

In this example operating system includes functionality to manage the hardware components of system and to provide common services. In various embodiments operating system can execute on CPU and provide common services. These common services can include for example scheduling applications for execution within CPU fault management interrupt service as well as processing the input and output of other applications.

In some embodiments based on interrupts generated by an interrupt controller such as interrupt controller operating system invokes an appropriate interrupt handling routine. For example upon detecting a page fault interrupt operating system may invoke an interrupt handler to initiate loading of the relevant page into memory and to update corresponding page tables.

Operating system may also include functionality to protect system by ensuring that access to hardware components is mediated through operating system managed kernel functionality. In effect operating system ensures that applications such as applications run on CPU in user space. Operating system also ensures that applications invoke kernel functionality provided by the operating system to access hardware and or input output functionality.

By way of example applications include various programs or commands to perform user computations that are also executed on CPU . CPU can seamlessly send selected commands for processing on the APD . In one example KMD implements an application program interface API through which CPU or applications executing on CPU or other logic can invoke APD functionality. For example KMD can enqueue commands from CPU to command buffers from which APD will subsequently retrieve the commands. Additionally KMD can together with SWS perform scheduling of processes to be executed on APD . SWS for example can include logic to maintain a prioritized list of processes to be executed on the APD.

In other embodiments of the present invention applications executing on CPU can entirely bypass KMD when enqueuing commands.

In some embodiments SWS maintains an active list in memory of processes to be executed on APD . SWS also selects a subset of the processes in active list to be managed by HWS in the hardware. Information relevant for running each process on APD is communicated from CPU to APD through process control blocks PCB .

Processing logic for applications operating system and system software can include commands specified in a programming language such as C and or in a hardware description language such as Verilog RTL or netlists to enable ultimately configuring a manufacturing process through the generation of maskworks photomasks to generate a hardware device embodying aspects of the invention described herein.

Computing system can include one or more input interfaces non volatile storage one or more output interfaces network interfaces and one or more displays or display interfaces.

In graphics pipeline can include a set of blocks referred to herein as ordered pipeline . As an example ordered pipeline includes a vertex group translator VGT a primitive assembler PA a scan converter SC and a shader export render back unit SX RB . Each block within ordered pipeline may represent a different stage of graphics processing within graphics pipeline . Ordered pipeline can be a fixed function hardware pipeline. Other implementations can be used that would also be within the spirit and scope of the present invention.

Although only a small amount of data may be provided as an input to graphics pipeline this data will be amplified by the time it is provided as an output from graphics pipeline . Graphics pipeline also includes DC for counting through ranges within work item groups received from CP pipeline . Compute work submitted through DC is semi synchronous with graphics pipeline .

Compute pipeline includes shader DCs and . Each of the DCs and is configured to count through compute ranges within work groups received from CP pipelines and

The DCs and illustrated in receive the input ranges break the ranges down into workgroups and then forward the workgroups to shader core .

Since graphics pipeline is generally a fixed function pipeline it is difficult to save and restore its state and as a result the graphics pipeline is difficult to context switch. Therefore in most cases context switching as discussed herein does not pertain to context switching among graphics processes. An exception is for graphics work in shader core which can be context switched.

After the processing of work within graphics pipeline has been completed the completed work is processed through a render back unit which does depth and color calculations and then writes its final results to memory .

Shader core can be shared by graphics pipeline and compute pipeline . Shader core can be a general processor configured to run wavefronts. In one example all work within compute pipeline is processed within shader core . Shader core runs programmable software code and includes various forms of data such as state data.

A disruption in the QoS occurs when all work items are unable to access APD resources. Embodiments of the present invention facilitate efficiently and simultaneously launching two or more tasks to resources within APD enabling all work items to access various APD resources. In one embodiment an APD input scheme enables all work items to have access to the APD s resources in parallel by managing the APD s workload. When the APD s workload approaches maximum levels e.g. during attainment of maximum I O rates this APD input scheme assists in that otherwise unused processing resources can be simultaneously utilized in many scenarios. A serial input stream for example can be abstracted to appear as parallel simultaneous inputs to the APD.

By way of example each of the CPs can have one or more tasks to submit as inputs to other resources within APD where each task can represent multiple wavefronts. After a first task is submitted as an input this task may be allowed to ramp up over a period of time to utilize all the APD resources necessary for completion of the task. By itself this first task may or may not reach a maximum APD utilization threshold. However as other tasks are enqueued and are waiting to be processed within the APD allocation of the APD resources can be managed to ensure that all of the tasks can simultaneously use the APD each achieving a percentage of the APD s maximum utilization. This simultaneous use of the APD by multiple tasks and their combined utilization percentages ensures that a predetermined maximum APD utilization threshold is achieved.

In embodiments described herein methods and systems relating to hardware assisted software managed task scheduling are provided. For example embodiments described herein relate to an accelerated processing device controlling the scheduling based on a set of priorities and execution of a set of given processes. In an embodiment the software maintains a list of processes to be run by the APD. Further the APD maintains a subset list of processes wherein the APD controls the priority and execution of the subset list of processes without the need for intervention from the software. In this manner the APD offloads a portion of the burden of the software to monitor APD utilization and control the execution of processes executed by the APD thereby freeing software resources.

As used in this patent document a unified computing system USC memory model broadly describes the permissible interactions of memory operations e.g. load and store operations from multiple threads operating in computing system using CPU and APD described above.

In the UCS memory model there is a shared memory for example shared memory address space having a single unique virtual address for memory heaps mapped into the application. This property is independent of whether CPU or APD is granted access to the region. All memory heaps are accessible with a unique virtual address in the application address space which allows pointers to be passed directly between the CPU and APD without modification.

As described in embodiments below the UCS programming model provides memory resources and memory coherency between cooperating APD and CPU processing components using different memory heaps.

UCS also allows implementation flexibility for dynamic partitioning of the APD hardware coherent and local memory. This can be achieved in implementations that contain a APD probe filter that keeps track of all lines currently exported from the APD local memory. Cache lines that are never touched by the CPU do not have a footprint in the probe filter and have the same performance as local non coherent APD memory.

As used typically herein a thread is a program ordered sequence of operations through a processing element. A thread can also be characterized as a single instance of a program execution with a unique data state. Examples of threads of different types include a CPU thread and a graphics processor GP thread. As used with some embodiments herein CPU threads are conventional x86 threads operating with a traditional x86 memory model using a conventional CPU e.g. CPU depicted in . As also used typically herein a GP thread is a thread that executes on a graphics processor e.g. APD .

Typically a GP thread is a set of work items across which the instructions of a compute kernel are applied in lock step within a single SIMD processing core. An example of a work item in an accelerated processing device includes an OpenCL style thread. As would be appreciated by one having skill in the relevant art s teachings herein can apply to any method of processing programmatic operations using one or more processing elements in one or more APDs CPUs or other processing elements e.g. co processors etc.

An aspect of UCS is a virtual machine language technique which is designed for parallel processing and can be translated on the fly into native machine language. Each UCS computational unit includes two types of cores. For convenience as used herein UCS virtual machine language is referred to as UCS Intermediate Language UCSIL .

UCSIL is an intermediate language. All software visible mechanisms are described as UCSIL primitives. The implementation of UCSIL primitives by hardware will include primitives not described here. Separate documents may describe cache or buffer flushes fences arbitration points cache states etc.

The UCSIL abstraction that runs on top of a virtual machine VM one goal of UCSIL being to abstract system hardware resources from the high level programming languages HLL and programmer while not unduly restricting compiler JIT HW optimizations and allowing a range of possible HW instruction sets ISA and architectures. UCSIL threads may operate in one or more in workgroups with shared access to a set of memory heaps via a shared memory address space.

By way of background is a block diagram of an example system architecture in which embodiments of the present invention or portions thereof may be implemented. System architecture includes application coupled computer arrangement . In an embodiment computer arrangement has characteristics of unified computing system described above. Some embodiments described herein describe aspects of a memory model for unified computing system described above e.g. a unified memory model. Computer arrangement includes CPU APD shared memory address space mapper memory heaps system memory and graphics memory .

One aspect of the memory model for unified computing system described herein is the use of memory heaps to provide access to different portions of memory in computer arrangement . As used typically herein a UCS memory heap refers to an approach to allocating physical memory storage resources in the UCS. More specifically memory heaps are virtual memory pools that can be mapped to physical memory pools. Examples of physical memory pools include system memory and graphics memory . Because of characteristics of the UCS different memory heaps are used to allocate memory resources to different processing elements using different approaches.

As used typically herein a memory resource is an electronic device that stores data for an electronic computer. This memory resource can be either a physical memory resource or a virtual memory resource. It should be appreciated that memory resources as used typically herein includes caches and other associated memory devices.

Computer arrangement includes memory resources including system memory and graphics memory . System memory is also known typically as CPU memory and is directly accessible to and associated with CPU . Graphics memory for example can be APD DRAM and is directly accessible to and associated with APD .

Generally speaking in an embodiment a method of providing memory to a memory operation operating in a computer arrangement having an APD and CPU processor is provided. A memory operation for example from application is received from a processor and the memory operation is mapped using mapper to one of a plurality of memory heaps . The result of the mapping is provided to the processor. The processor uses the mapping result to perform the memory operation using memory resources.

Memory heaps used in embodiments described herein can have several general characteristics. Different characteristics allow memory heaps to provide different services to computer arrangement . A non limiting list of example characteristics C1 C8 is listed below 

C1. An example memory heap can be beneficially configured to access only one physical memory resource in the computer arrangement. Certain memory resources have higher performance capabilities than others for example graphics memory has higher performance for certain types of operations. One example of a memory heap configured to only access graphics memory is the APD coherent memory heap discussed with the description of below. Providing application access to graphics memory enables the beneficial acceleration of programming environments such as OpenCL as noted above.

C2. An example memory heap can be beneficially configured to access more than one physical memory resource in the computer arrangement. One example of a memory heap configured to access more than one physical memory resource in the computer arrangement is the APD local memory heap discussed with the description of below such memory heap configured to access both graphics memory and system memory . Providing access to multiple memory resources can beneficially allow a memory heap to perform memory operations in the UCS.

C3. An example memory heap can be beneficially configured to only accept memory operations from one type of processing element in the UCS e.g. from either APD or CPU . One example of a memory heap configured to only accept memory operations from APD is the APD local memory heap discussed with the description of below. It should be noted that as described with item C2 above while only accepting memory operations from APD the APD local memory heap can access both graphics memory and system memory .

C4. An example memory heap can be beneficially configured to accept memory operations from multiple types of processing elements in the UCS e.g. from both APD and CPU . One example of a memory heap configured to accept memory operations from both APD and CPU is the APD coherent memory heap discussed with the description of below.

C5. Notwithstanding a memory heap capability to accept memory operations from multiple types of processing element as described with item C4 above not all memory resources are accessible by all types of processing elements. For example there can be memory regions exported by APD that are only accessible by APD . If CPU attempts to access to these regions a not present fault can result.

C6. Some memory heaps use different memory managing components in computer arrangement to access the physical memory resources. The memory manager used for the memory heap to enable access to a physical memory resource can depend upon the processor from which the memory operation originates. For example as discussed with the descriptions of below for some memory heaps either IOMMU or GPUVM can be used to access system memory based on the processor from which a memory operation originates.

C7. Memory heaps that are accessible to multiple types of processing elements e.g. as discussed with item C4 above can also have different attributes applied to the mapped memory depending upon the processor from which the memory operation originates. For example when accessed by a memory heap memory in system memory can be set to read only for memory operations originating in APD but be write eligible for memory operations that originate in CPU . System coherent memory discussed with the description of below and APD coherent memory discussed with the description of below can use this approach to memory attributes.

C8. Memory allocated to an example memory heap can beneficially be pinned memory. As used typically herein a pinned block of memory is marked as unmovable that is not subject to the standard types of performance based moving performed by aspects of computer arrangement . Pinning is used for example so that unmanaged code can use managed pointer types without having the data moved unexpectedly. For example pointers for passing buffer references to Win32 API functions can be invalidated if the referenced buffer is relocated in memory without the knowledge of the Win32 API.

Example characteristics C1 C8 described above are used below to describe example memory heaps used in the UCS. As would be appreciated by one having skill in the relevant art s given the description herein characteristics noted above can have additional implementation specific aspects and additional or fewer characteristics can apply to an embodiment of a memory heap.

One approach to accessing memory heaps is to use a memory instruction mapper mapper to map to memory heaps using shared memory address space . Memory heaps mapped into shared memory address space by embodiments can be accessed by a memory reference in a memory instruction from application . The memory instruction defines the address space. An example of a mapper that can be used by embodiments is detailed in the Virtual Memory Application noted above.

In an embodiment memory heaps in the UCS each are referenced by a range of addresses in shared memory address space to which they are mapped. Memory operations executed by CPU or APD can reference an address in shared memory address space such address being mapped to a memory heap and memory resources can be provided to application via the mapped heap.

Memory heaps that are accessible by a unique shared address in shared memory address space allow pointers to be passed directly between CPU and APD without modification. Memory heaps that are accessible by addresses in shared memory address space enable beneficial results for example allowing pointers to be stored in generic pointer containers native to a given language e.g. the void container in the C programming language.

Example memory heaps listed above are intended to be a non limiting list of example memory heaps that can be used with embodiments. Each example memory heap is detailed below with example characteristics discussed. As would be appreciated by one having skill in the relevant art s given the description herein without departing from the spirit of embodiments herein any characteristic noted below can be beneficially varied based on implementation specific factors. Additional memory heap structures may also be used by embodiments having characteristics detailed herein as well as other beneficial characteristics.

It also should be appreciated that the location of memory heaps in computer arrangement is implementation specific and can vary in different embodiments. Though depicted as discrete blocks in the approaches described with embodiments of memory heaps herein can be logically implemented in other components of computer arrangement e.g. IOMMU TLB memory controller system memory APD and CPU .

Page tables are shown as exchanged between IOMMU and system memory page tables via connection . Couplings with solid lines are used to exchange information between components in some embodiments illustrated by . Couplings with dotted lines are used to exchange information in some embodiments but not in embodiments depicted on . The use of dotted and solid lines on is meant to be illustrative of the use of a coupling by an embodiment. As would be appreciated by one having skill in the relevant art s given the description herein couplings depicted can have different uses additional couplings can be used and couplings shown may not exist in embodiments.

In an embodiment LDS memory heap is a private memory region that is directly accessible only to APD i.e. not directly accessible to CPU . In an embodiment if CPU attempts to reference an address in shared memory address space that is mapped to LDS memory heap CPU is required to take a protection fault. In an embodiment to avoid this type of protection fault library calls can be supplied application to guide and control shared address references by processing resources.

In another embodiment application is prevented from using CPU to reference shared memory addresses allocated to memory heaps that are only accessible to APD . Similarly in another embodiment application is prevented from using APD to reference shared memory addresses allocated to memory heaps that are only accessible to CPU . In either case page faulting is the mechanism used to prevent reference to the shared memory addresses.

In an example of the operation of LDS memory heap an APD memory operation references an address in shared memory address space and this address is mapped by mapper to LDS memory heap . LDS memory heap then uses a memory manager to access different memory resources in computer arrangement . In an embodiment LDS heap uses a memory manager that is associated with APD such memory manager having access to both memory resource associated with APD APD memory and memory resources associated with CPU system memory .

According to an embodiment a memory manager that is associated with APD and accesses both APD memory and system memory is an IOMMU . One approach taken to access system memory used by an embodiment of IOMMU uses replicated system memory page tables . Page tables are exchanged via connection updating page tables in a fashion that would be appreciated by one having skill in the relevant art s given the description herein.

As described in above IOMMU includes logic to perform virtual to physical address translation for memory page access for devices including APD . One approach used by IOMMU to enable the use of shared memory address space uses full system memory page tables to allow conventional x86 user code and APD code to share the same memory page tables. Because of this page table sharing in an embodiment a APD context can use the format of a standard x86 user context. Using IOMMU each APD context can participate fully in system memory page tables paging translations protections and permissions.

Using IOMMU APD is also enabled to use standard TLB translation caching techniques according to an embodiment. In an embodiment because of this expanded use of system memory page tables by embodiments operating system from may be required to propagate page invalidations and page table flushes to IOMMU e.g. using pages tables via connection .

In an embodiment LDS is private to an APD work group the size and memory resources used by the LDS memory heap varying based on the application with which the memory heap is associated application . Because of the allocation of memory for application LDS memory heap can be termed a compiler managed data heap. In an embodiment memory in memory resources accessed by LDS is pinned memory.

As further shown in global data store memory heap GDS memory heap has similar characteristics to LDS memory heap . In an embodiment GDS memory heap has the same characteristics noted with respect to LDS memory heap with at least one difference Unlike LDS memory heap which is private to a single APD work group GDS memory heap can be used by multiple work groups in a single application e.g. application .

Couplings with solid lines are used to exchange information between components in some embodiments illustrated by . Couplings with dotted lines are used to exchange information in some embodiments but not in embodiments depicted on . The use of dotted and solid lines on is meant to be illustrative of the use of a coupling by an embodiment. As would be appreciated by one having skill in the relevant art s given the description herein couplings depicted can have different uses additional couplings can be used and couplings shown may not exist in embodiments.

In an embodiment scratch memory heap is directly accessible only to APD . In an embodiment scratch memory heap can be used as a per work item extension to general purpose registers in APD .

One feature implemented in an embodiment of scratch memory heap involves address replication. In this embodiment shared memory addresses referenced in scratch memory heap undergo a transformation so that for each APD work item each shared memory address can refer to a different address in a memory resource. Stated a different way each shared memory address SMA that maps to scratch memory heap is replicated for each APD processor thread and each replicated SMA maps to a portion of the memory resource the portion being only accessible to the APD processor thread to which the replicated SMA is mapped.

In an embodiment scratch memory can directly access both APD memory and system memory . In another embodiment scratch memory heap can map to other memory heaps not shown . Example memory heaps that can be used by scratch memory heap include APD local memory heap APD coherent memory heap and system coherent memory heap each memory heap being discussed further below.

Using IOMMU scratch memory heap can access APD memory and system memory . Similar to the process noted above with the description of IOMMU with to access system memory for scratch memory heap IOMMU can use page tables such page tables being exchanged via connection .

In an embodiment this local memory heap is only directly accessible to API and mapped using APD internal virtual memory hardware for example GPUVM . In an embodiment a portion of APD memory can be allocated to APD local memory heap .

In another embodiment APD local memory heap accesses system memory via GPUVM . GPUVM accesses in an example memory that is unsnooped in system memory . GPUVM can access system memory without referencing system memory page tables . In this example system memory has specific reserved portions for APD local memory heap system memory page tables table entries corresponding being marked as not present. As would be appreciated by one having skill in the relevant art s given the description herein portions of system memory allocated to GPUVM can be termed aperture memory. 

In an embodiment the mapping of APD local memory heap to specific memory resources in APD memory and or system memory does not occur until the initiation of the process threads associated with application .

In an embodiment system coherent memory heap is a memory region that is directly accessible by both APD and CPU . In an embodiment the memory manager used by system coherent memory heap to access a memory resource can depend upon the processor from which the memory operation originates. For example when a memory operation from application originates at CPU system memory manager is used by system coherent memory heap to access system memory . Similarly when a memory operation from application originates at APD IOMMU is used by system coherent memory heap to access system memory . More specifically IOMMU is used to translate virtual addresses in system coherent memory heap to physical addresses in system memory . System memory manager controls allocation of system memory and sets up system memory page tables that provide the mappings between the virtual and physical addresses.

As noted with the discussion of above in an embodiment IOMMU uses replicated page tables to access system memory . In an embodiment the allocated memory in system memory referenced by system coherent memory heap is not pinned. Unlike conventional memory used by APD memory resources accessed by system coherent memory heap can generate standard page faults.

Like system coherent memory heap discussed with the description of above APD coherent memory heap is accessible to both CPU and APD . Unlike system coherent memory heap APD coherent memory heap accesses APD memory instead of system memory .

Similar to system coherent memory heap discussed above in an embodiment APD coherent memory heap uses either IOMMU or system memory manager to access a memory resource. The memory manager used by APD coherent memory heap to access a memory resource depends upon the processor from which the memory operation originates. For example when a memory operation from application originates at CPU system memory manager is used by APD coherent memory heap to access APD memory . Similarly when a memory operation from application originates at APD IOMMU is used by APD coherent memory heap to access APD memory .

Also similar to system coherent memory heap memory referenced by APD coherent memory heap is not required to be pinned and can generate memory page faults if referenced by page tables .

Initially as shown in stage in a memory operation is received from a processor. In one example in an embodiment a memory operation from an application for example application from is received at mapper from a processor e.g. CPU . In another example the memory operation is received at mapper from APD .

At stage the received memory operation is mapped to a one of a plurality of memory heaps such mapping resulting in a mapping result. For example in an embodiment the memory operation from stage is mapped by mapper to a one of memory heaps . In another embodiment the memory operation includes a reference to shared memory address space and mapper uses the reference to map to memory heaps .

At stage the mapping result is provided to the processor and the processor uses the mapping result to perform the memory operation. For example in an embodiment mapper provides a mapping result to a processor for example CPU . In another example the mapping result is provided to APD . After stage method ends.

Embodiments described herein relate to providing memory to a memory operation in a computer arrangement. The summary and abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventors and thus are not intended to limit the present invention and the claims in any way.

The embodiments herein have been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries may be defined so long as the specified functions and relationships thereof are appropriately performed.

The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others may by applying knowledge within the skill of the art readily modify and or adapt for various applications such specific embodiments without undue experimentation without departing from the general concept of the present invention. Therefore such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.

The breadth and scope of the present invention should not be limited by any of the above described exemplary embodiments but should be defined only in accordance with the claims and their equivalents.

