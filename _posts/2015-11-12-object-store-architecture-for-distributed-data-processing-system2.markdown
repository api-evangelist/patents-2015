---

title: Object store architecture for distributed data processing system
abstract: Embodiments described herein provide an object store that efficiently manages and services objects for use by clients of a distributed data processing system. Illustratively, the object store may be embodied as a quasi-shared storage system that interacts with nodes of the distributed data processing system to service the objects as blocks of data stored on a plurality of storage devices, such as disks, of the storage system. To that end, an architecture of the object store may include an on-disk layout, e.g., of the storage system, and an incore layout, e.g., of the nodes, that cooperate to illustratively convert the blocks to objects for access by the clients.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582213&OS=09582213&RS=09582213
owner: NetApp, Inc.
number: 09582213
owner_city: Sunnyvale
owner_country: US
publication_date: 20151112
---
This application is a divisional application of and claims priority to U.S. patent application Ser. No. 13 773 119 filed on Feb. 21 2013 the entirety of which is incorporated herein by reference.

The present disclosure relates to object stores and more specifically to an architecture of an object store deployed in a distributed data processing system.

In many current analytics frameworks distributed data processing systems may be used to process and analyze large datasets. An example of such a framework is Hadoop which provides data storage services to clients using a distributed file system and data processing services though a cluster of commodity computers or nodes. The distributed file system e.g. the Hadoop Distributed File System HDFS executes on the cluster of nodes to enable client access to the data in the form of logical constructs organized as blocks e.g. HDFS blocks. Each node of the cluster typically has its own private i.e. shared nothing storage and employs a native file system such as ext3 4 or XFS. The native file system typically has a plurality of features directed to management of the data in the form of logical constructs organized as files. As a result the distributed file system may be employed to access the data as blocks while the native file systems executing on the cluster of nodes may be employed to store and process the blocks as one or more files.

Often it may be desirable to avoid the use of a native file system in certain deployments of distributed data processing systems because many of the features provided by the native file system may not be required. For example a feature of a native file system is its compliance with the Portable Operating System Interface POSIX standard which requires exposing file handles to clients to enable access e.g. reading and writing to files in accordance with a full set of operations. The distributed data processing system may not require POSIX compliance because there may only be a limited set of operations such as open read and verify checksum needed by the distributed file system to access blocks. Thus the overhead associated with the many features provided by a native file system may not be appropriate for the distributed data processing system deployment.

Accordingly it may be desirable to provide a storage solution to distributed data processing systems that eliminates the overhead associated with native file systems. In addition it may be desirable to provide a generic storage solution that may be deployed in distributed data processing systems that employ data management systems such as distributed file systems and distributed database management systems.

Embodiments described herein provide an object store that efficiently manages and services objects for use by clients of a distributed data processing system. Illustratively the object store may be embodied as a quasi shared storage system that interacts with nodes of the distributed data processing system to service i.e. access the objects as blocks of data stored on a plurality of storage devices such as disks of the storage system. To that end an architecture of the object store may include an on disk layout e.g. of the storage system and an incore layout e.g. of the nodes that cooperate to illustratively convert the blocks to objects for access by the clients.

In one or more embodiments the on disk layout of the object store may be implemented as one or more volumes wherein each volume is a container of objects. Each volume may include a plurality of segments wherein one segment is a master segment and the other segments are data segments. Each segment may be formed from a range of bytes within a logical unit number lun constructed from one or more disks of the storage system. A lun range thus defines a location of a segment within a volume. Illustratively a chunk is a unit of storage within the on disk layout of the object store. The data segments of a volume are provided to allocate store retrieve and recover chunks including their data and metadata. The chunks stored on a data segment may be chained linked together to form one or more blocks of the volume. The master segment contains block layout information for the volume including information that links a collection of chunks together as blocks.

In addition the incore layout of the object store may be implemented as incore data structures of the nodes. One or more blocks of a volume may be retrieved from disk and loaded into memory incore of a node where each block is represented as an object. That is an object is an on disk block which in turn is a collection of linked chunks. Data structures such as an object accessor an object descriptor and a chunk descriptor are maintained incore to describe and enable access to the object and its constituent chunks. Illustratively the chunk descriptor describes a chunk with respect to its size an amount of data and a current state whereas the object accessor and object descriptor enable access to the chunks of the object via a linked list of chunk descriptors.

Advantageously the object store may be configured to service objects in a manner that is adaptable to various data management system deployments including distributed file systems and or distributed database management systems that support object level management utilizing e.g. the quasi shared storage system of the distributed data processing system.

The storage system illustratively includes a processor a memory one or more network adapters and a storage adapter interconnected by a bus . Each network adapter includes the mechanical electrical and signaling circuitry needed to connect the storage system to the nodes over network . The storage system may also include a storage operating system that illustratively provides a file system to logically organize the data as a hierarchical structure of e.g. named directory file and lun storage elements on disks . The file system may be configured to provide volume management capabilities for use in block based access to the data stored on disks . These capabilites may include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as synchronous mirroring and or parity RAID .

Storage of data on the storage system may be implemented as one or more storage volumes that include a group of the disks defining an overall logical arrangement of disk space. The disks within a volume are typically organized as one or more RAID groups. RAID implementations enhance the reliability integrity of data storage through the writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of redundant information with respect to the striped data. The redundant information enables recovery of data lost when one or more storage devices e.g. disks fails.

In an embodiment the storage system may interact with the nodes to provide a quasi shared storage system infrastructure of the distributed system as described herein. To that end the storage adapter may cooperate with the storage operating system to access e.g. retrieve via a read operation or store via a write operation data requested by the nodes. The storage adapter may include I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC serial link topology. The data may be retrieved or stored on disk by the storage adapter and upon completion either the retrieved data or an acknowledgement generated by the processor or the adapter may be forwarded over the bus to the network adapter where it is formatted into one or more packets or messages and forwarded to the nodes.

The memory includes a plurality of storage locations addressable by the processor and or network interface for storing software programs e.g. processes and or services and data structures associated with the embodiments described herein. The processor and interface may in turn include processing elements and or logic circuitry configured to execute the software programs and manipulate the data structures such as virtual replica map and incore data structures . An operating system portions of which are typically resident in the memory and executed by the processor functionally organizes the node by inter alia invoking operations in support of the software processes services and or application programming interfaces APIs executing on the node. A suitable operating system may include the UNIX series of operating systems the Microsoft Windows series of operating systems or other similar operating system however in an embodiment described herein the operating system is illustratively the Linux operating system.

Besides the operating system a data management system such as a distributed database management system or illustratively a distributed file system provides data storage services in support of an analytics framework of the distributed data processing system . A distributed file system that may be advantageously used with the embodiments described herein is the Hadoop Distributed File System HDSF which illustratively performs write once read many WORM high throughput parallel streaming access to a workload e.g. a dataset. The distributed data processing system illustratively provides an architecture that facilitates distributed data analytics wherein multiple analytics jobs may be run on the dataset. To that end the architecture may employ data analytic processes modules to store the dataset on the storage system and partition the dataset into blocks e.g. HDFS blocks for distribution among the nodes and to enable processing of the blocks by the nodes. In one or more embodiments the architecture may further employ a distributed hash algorithm to calculate the locations of the blocks in the system. If a block is not available in a particular calculated location e.g. in the memory of a respective node the block may be fetched from the dataset stored on the storage system and forwarded to the respective node.

In the case of a node embodied as the job distribution node the software processes and or services may include data analytic processes such as a metadata coordinator and a job coordinator whereas in the case of the compute node the data analytic processes may include a task executor and a data accessor . It will be apparent to those skilled in the art that other processor and memory types including various computer readable media may be used to store and execute program instructions pertaining to the disclosure described herein. Also while the description illustrates various processes it is expressly contemplated that the various processes may be embodied as modules configured to operate in accordance with the disclosure herein e.g. according to the functionality of a similar process .

In one or more embodiments the metadata coordinator contains computer executable instructions executed by the processor to perform operations that manage the distributed file system namespace and control access to objects such as partitioned blocks of the dataset residing on the storage system . Illustratively the management and control operations may include e.g. retrieving the partitioned blocks of a dataset from the storage system for distribution to the compute nodes and tracking the locations of those blocks in the system. The job coordinator contains computer executable instructions executed by the processor to perform operations that manage each analytics request or job received from a client of the system . The job coordinator may further perform operations to divide the job into sub jobs or tasks assign schedule the tasks among the compute nodes and communicate with the task executors running on the nodes. As used herein a task illustratively involves issuing a request for a block object that the task may subsequently process to produce a result.

Each task executor contains computer executable instructions executed by the processor to perform the tasks assigned to the compute node. The task executor may communicate the data accessor to retrieve one or more blocks needed to process the assigned task. The data accessor contains computer executable instructions executed by the processor to perform operations that manage access to the storage system . Illustratively the management operations may include access e.g. read write operations directed to the blocks stored on the system and serviced by the data accessor as well as block creation deletion and replication.

As noted a distributed file system that may be advantageously used with the embodiments described herein is HDFS which is typically configured to service data as HDFS blocks. HDFS typically runs on top of a native general purpose file system e.g. ext3 4 or XFS. The HDSF blocks are served as files by the native file system which typically has a file like interface such as POSIX to HDFS. The embodiments described herein essentially replace the native file system with an object store. Accordingly the HDFS blocks are stored as objects which in turn are stored as chunks. Functionality related to files block management and block metadata management with respect to the native file system are replaced with the object store which manages its own block metadata block data and chunk data.

In an embodiment described herein a first API layer i.e. object store interface is provided to enable access to data stored on the object store. Note that in the context of Hadoop the first API layer may be referred to as FSDataset. Illustratively the object store interface is an API layer interface to the data accessor of compute node . Unlike the POSIX like interface the object store interface manages HDFS blocks using operations such as read HDFS block write HDFS block check HDFS block metadata and read HDFS block metadata. In essence the object store interface transforms the HDFS blocks into objects blocks and chunks for storage on the object store. In addition a DataStorage interface not shown to the data accessor may provide meta operations such as formatting a volume checking for consistency of a volume and similar fsck utilities.

In a traditional Hadoop type distributed data processing system each compute node has its own local private storage that services HDFS blocks. Unlike the embodiments described herein the Hadoop type of distributed data processing system does not assemble the compute nodes into compute groups that include mapped volumes. A second API layer i.e. a storage management interface illustratively transforms a traditional Hadoop type system into a distributed data processing system having compute nodes coupled to a quasi shared storage system of the object store to form one or more compute groups. Note that in the context of Hadoop the second API layer may be referred to as StoragePlugin. is a block diagram of a compute group of the distributed data processing system that may be advantageously used with one or more embodiments described herein. The storage management interface is illustratively another API layer interface to the data accessor that enables read write RW read only RO mappings of compute nodes and corresponding volumes within the compute group . For example each compute node of the compute group may have RW access denoted by the solid lines to a specified volume mapped into the compute node and RO access denoted by the dashed lines to the remaining volumes mapped into the compute node. Illustratively the storage management interface provides the functionality of differentiated mapping e.g. the specified volume V being mounted mapped as RW access for compute node C of the compute group and that volume V being mounted mapped as RO access for the other compute nodes C C of the compute group.

If a client of the traditional Hadoop type distributed data processing system requests processing e.g. reading or writing of a block the job coordinator of the job distribution node determines which compute node may process the block and then schedules processing at the compute node. However in the embodiments described herein write operations directed to a specified volume are processed by one compute node in a compute group while read operations to that volume may be processed by all other compute nodes in the group. Accordingly the job coordinator may be configured for this architectural change to the distributed data processing system. For example in response to a write request received from a client the job coordinator may schedule the write to only one compute node of a local compute group that has RW access to the specified volume. Yet for read requests the job coordinator may balance those requests load balance across all compute nodes of the local group each of which has RO access to the volume. Furthermore compute nodes of a remote compute group may also have RO access to the volume but only over network . Those nodes of the remote compute group illustratively have a lower priority to the specified volume as opposed to the nodes of local compute group which have a higher priority . A third API layer i.e. Topology provides information to the job coordinator about the organization of the distributed data processing system e.g. the local remote compute groups and scheduling of reads writes to nodes of a compute group. To that end the Topology is illustratively an API layer interface to the job coordinator .

Furthermore a traditional Hadoop type distributed data processing system typically utilizes the notion of replicas to provide e.g. reliability to the system. In such a traditional system each compute node has its own local private storage to service HDFS blocks. If a compute node that stores a block and is scheduled to process that block fails the block can be read from the local storage of another node that stores a replica of that block. Thus the traditional system has the notion of a replication factor which is typically three 3 i.e. 3 copies of the data may be stored on the compute nodes of the distributed data processing system. However the embodiments described herein enable storage of only one copy of the data on a volume of the object store which improves the efficiency of storage but allows all of the e.g. four 4 compute nodes of a compute group read access to that data within the volume thus improving data availability. The improved data availability is manifested as virtual replicas that obviate the use of replication factors. Illustratively a storage administrator may alter the replication factor e.g. from 3 to 1 and the job coordinator of the job distribution node may use virtual replica map to schedule reads of the data at any of the compute nodes of the compute group.

In an embodiment the virtual replica map is a data structure maintained by the job coordinator to identify the RW and or RO mappings of each compute node to each volume of a compute group. The virtual replica map is illustratively populated by block maps e.g. a list of blocks or objects stored on mapped volumes provided by the compute nodes of each compute group in the distributed data processing system . That is each of the compute nodes of a compute group may report all of their virtual e.g. RO and physical e.g. RW mappings of blocks objects within a volume to the job coordinator. Armed with the virtual replica map the job coordinator may resolve the virtual physical mappings of each compute node of a compute group. Thus the virtual replica map enables the job coordinator to provide load balancing and redirecting of read and write traffic across the nodes of the distributed data processing system.

Embodiments described herein provide an object store that efficiently manages and services objects for use by clients of the distributed data processing system. Illustratively the object store may be embodied as a quasi shared storage system that interacts with the nodes of the distributed data processing system to service i.e. access the objects as blocks of data stored on a plurality of storage devices such as disks of the storage system. To that end an architecture of the object store may include an on disk layout e.g. of the storage system and an incore layout e.g. of the nodes that cooperate to illustratively convert the blocks to objects for access service by the clients. As described further herein the on disk layout of the object store may be implemented as one or more volumes wherein each volume is a container of objects. The volumes including their respective objects may be cross mapped into each of the compute nodes in a compute group so that any compute node in the group can access any of the objects or chunks stored on the respective volume of the storage system . Therefore the quasi shared storage system may be configured to relax the restriction of private shared nothing storage of the compute nodes to allow shared storage access of the object store within a compute group .

For example each compute node C C illustratively has RW access to a specified volume V V mapped into the compute node and RO access to the remaining volumes mapped into the compute node. The mapping of the compute node is illustratively indicated by a configuration file of the node having a plurality of configuration parameters including a storage identifier associated with a volume identifier of the specified volume and a list of the remaining volumes within the compute group e.g. by path names. Upon boot or restart the volume identifier of a volume is loaded into memory of the compute node and compared with the storage identifier of the configuration file. If the storage identifier matches the volume identifier the compute node has by default RW access to the volume. Otherwise the node has RO access to the volume.

The RW and RO mapping access of compute nodes within a compute group obviates a need for monitoring type of communication such as heartbeat messages among the nodes to ensure high availability of the distributed data processing system . For example if a compute node with RW access to a specified volume fails read traffic requests directed to objects stored on the volume may be routed e.g. by cluster management software to the remaining nodes of the compute group which have RO access to the volume. A write request creates a new HDFS block. If the write request succeeds in finalizing the block before a node failure then the block is fully written and available. If a node fails while a block is being written the entire write fails and is retried e.g. by the cluster management software . Retried write requests can be routed to another node and can be stored on another volume to which that node has RW access. Data access is thus contention free i.e. lock free because only one compute node of the compute group may modify the objects or chunks stored in a specified volume. An example of a distributed processing system that is configured to provide compute nodes with high availability contention free access to chunks stored on a storage system is disclosed and described in U.S. patent application Ser. No. 13 558 061 filed Jul. 25 2012 and titled Contention Free Multi Path Storage Access in Distributed Compute Systems by Gaurav Makkar et al.

Accordingly the volume may contain multiple lun ranges one for each data and master segment of the volume. Illustratively each segment does not have to be contained within the same lun to form the volume . That is different segments i.e. different lun ranges of different luns may be organized as the volume . As an example the volume may constitute four segments carved out formed from two luns wherein three of the segments may be formed from a first lun and one of the segments may be formed from a second lun. As a result a size or capacity of the volume may be grown or shrunk quickly and efficiently. For instance to grow the capacity of the volume a system administrator need only add more data segments to the volume. The various segments of the various luns may then be organized as the volume using identifiers as discussed further herein.

In one or more embodiments each data segment has an on disk layout organized as a plurality of fields including a header field a chunk metadata field a chunk field and a recovery area field . Illustratively a chunk is a unit of storage within the on disk layout of the object store. The data segments of volume are provided to allocate store retrieve and recover chunks including their data and metadata. To that end the header field generally contains information about the data segment including a size of the segment a size of the chunk s stored within the segment a storage location of different components areas of the segment i.e. the lun within which the segment is stored and the volume to which the segment belongs. Each data segment is associated with other data segments to form the volume using various identifiers contained in the header field . One such identifier is a lun identifier lun id that provides the association of the data segment to a lun. The lun id includes an offset within the lun and a size of the segment constituting the lun range. Illustratively the lun id is located in the header field of each data segment because that lun id may be different for each segment. Another identifier is a volume identifier volume id that among other things operates to associate the data segments collectively as volume .

The chunks e.g. data content are stored in the chunk field of the data segment . As used herein allocation of a chunk denotes adding a chunk to the chunk field and deallocation of a chunk denotes deleting or removing a chunk from the chunk field . Allocation and deallocation of chunks are fundamental operations in the object store because of the WORM workload environment within which the object store illustratively operates. Once written and allocated a chunk cannot be modified written again it can only be deleted deallocated. Therefore a simple chaining of chunks is all that is needed. To modify its data content the chunk is deallocated deleted and then allocated written that is the chunk is not modified in place. Metadata is provided for managing and tracking the allocation deallocation of chunks within the chunk field of the data segment . The chunk metadata information i.e. allocation deallocation information is stored in the chunk metadata field . Illustratively there is chunk metadata stored in the chunk metadata field for each corresponding chunk stored in the chunk field . Each chunk metadata may specify whether the corresponding chunk has been allocated or deallocated and if allocated to which client or application it has been allocated. Thus the client that allocated or deallocated the corresponding chunk may be identified by the chunk metadata e.g by context .

The recovery area field contains recovery information that identifies any change made to the chunk metadata. Illustratively the recovery information includes a transaction identifier that uniquely identifies the recovery information an action such as an allocation or deallocation operation and a checksum for use in error correction during replay recovery of checksum related data. The allocation or deallocation operation including a change of state of a chunk in the data segment is written to or logged in the recovery area field . A change of state of the chunk denotes a change or transition to the chunk metadata of the chunk the allocation deallocation of a chunk illustratively transitions through certain states. For example the states of a chunk may include partially allocated partially filled not yet finalized allocated and deallocated. Information about the transition through those states is illustratively logged in the recovery area field .

In an embodiment one compute node of a compute group i.e. the compute node with RW access or mapping to a volume may mount the volume and log the recovery information in the recovery area field of a data segment of the volume while the other compute nodes of the compute group may only read that information via their RO mappings to the mounted volume. Once the recovery information is read the other compute nodes can replay the metadata from the recovery area field in accordance with a logical replay and reconstruct the view logged by that metadata to update catch up with the operations rendered by the RW mapped compute node. Notably a logical replay occurs during steady state when a RO mapped node needs to catch up with the metadata advancements of a RW mapped node. The other compute nodes may read the recovery information and reconstruct the chunk metadata changes e.g. allocation deallocation change of state within their own compute environments thereby avoiding modifications at the storage system and obviating any contention among the compute nodes.

However if the RW mapped node writes the recovery information at substantially the same time as an RO mapped node reads that information the RO mapped node may read stale data. Accordingly the RO mapped node may be configured to check the correctness of the updated recovery information it reads. Illustratively the RO mapped node may check the transaction identifier and operation as well as the checksum to verify it is reading the correct recovery information. The transaction identifier is illustratively a monotonically increasing value the node may thus check a previous transaction to ensure that the current recovery information being read has an incremented value and is thus correct. The checksum value is illustratively based on the recovery information contents and is also checked by the node to verify correctness. If any of these checks fail the RO mapped node may perform a software reboot e.g. remount the volume to reload the information.

In addition it is possible that the RW mapped node may fail crash during flushing writing of the metadata recovery information to one or more disks of a volume e.g. upon swapping of recovery log areas such that the updates are not complete. Here some of the recovery information may have been stored to disk while the rest of the recovery information may need to be replayed e.g. based on operations stored in a non volatile buffer of the node s memory after a software reboot and remount of the volume. In response the RW and RO mapped nodes may perform a physical replay to retrieve the persisted recovery information from the mounted volume and compare that retrieved information with the recovery information associated with the operations stored in the non volatile buffer to determine what information needs to be replayed. Note that as opposed to a logical replay which occurs in steady state a physical replay occurs in response to a crash. The RW and RO mapped nodes do not replay the recovery information that was stored on disk they abort those operations and only replay the operations that did not result in recovery information being stored on disk.

Illustratively the volumes of a compute group may be remounted to be lock free free running and to assist error handling. Assume a RO mapped compute node of the compute group loads a RO volume including metadata associated with one or more chunks e.g. of a block object and metadata recovery information to enable the node to replay the transitions operations for that block object that were persistently stored on disk. This in turn allows the RO mapped node to update the metadata to be current with the transitions operations performed by the RW mapped node. If an error is detected during update of the metadata the RO mapped node may perform a software reboot to reload the incore data structures as well as the recovery information. Illustratively error handling involves the RO mapped node retrieving the recovery information from the RO volume so that it catch up with the changes updates made by the RW mapped node. Advantageously this process avoids any locking of storage on the object store.

Assume also that the RW mapped node renders updates to the recovery information in memory or other media and those updates are not immediately written flushed to disk s of the mounted volume. If the RO mapped node reads the recovery information of the mounted volume it will not be able to catch up with the updates rendered by the RW mapped node because those updates are still in memory of the RW mapped node and not accessible to the RO mapped node. To obviate this situation the RW mapped node may be configured to perform direct I O operations on its mapped volume to ensure that the updates e.g. to both data and recovery information are written directly to disk bypassing memory or other intermediate media copies. In this manner direct I O operations allow other compute nodes to share and synchronize data information. That is the updated data and or recovery information may be written directly to the appropriate data segment including the recovery area of the volume by the RW mapped node so the RO mapped nodes can immediately access the data information to e.g. catch up with the recovery information updates and or read the data to process a compute task.

Illustratively a data segment is not dependent upon and thus is unaware of the presence of another data segment within a volume the data segments of the volume are totally isolated. Accordingly the data segment may be defined as a fault boundary which means that if a failure occurs to the segment the failure is totally contained within the segment. Such a fault boundary eliminates a single recovery area and single point of failure for an entire volume. Only the master segment is aware of the presence other data segments in the volume . Illustratively the master segment contains information sufficient to organize all of the data segments within the volume. The organization of the volume ensures that each data segment is independent of each other data segment and if a failure occurs on a storage device e.g. disk or lun range that constitutes the particular data segment the failure is isolated within that data segment . Although this organization advantageously obviates the destruction of the volume in response to a failure of a data segment the content of the failed data segment can still be recovered using error detection and correction techniques such as RAID on the storage system .

In one or more embodiments the master segment has an on disk layout that is generally similar to that of the data segment . That is the master segment is organized as a plurality of fields including a header field a chunk metadata field a chunk field and a recovery area field . The header field generally contains metadata information about the master segment including a size of the segment a storage location of the segment i.e. the lun id within which the segment is stored and the volume id to which the segment belongs. The chunk metadata field identifies changes e.g. allocation or deallocation to the master segment and the recovery area field logs those changes.

However the chunks field of the master segment is specially formatted to include a master header sub field and a block layout sub field pertaining to the layout of a block. Illustratively a block is a logical construct that resides in volume and includes a collection of chunks. The size of a block can vary e.g. the block can include one chunk or a substantially large number of chunks. A chunk may be allocated to a block otherwise the chunk is freed i.e. deallocated . The allocation deallocation information of a chunk is tracked and managed at the data segment level and thus is not maintain in the master segment .

The chunks stored on a data segment may be chained linked together to form one or more blocks of the volume illustratively the master segment contains block layout information for the blocks contained in its volume. In an embodiment the chunks stored in the data segment may be organized as one or more blocks and metadata information related to the data chunks of blocks contained within the data segment may be represented as a block layout. Note that the block layout contains a description of the blocks that are stored in a particular data segment of the volume i.e. each data segment has its own block layout. Note further that a block may generally span one data segment a block generally does not contain chunks from different segments . Thus the block layout information may be provided per data segment.

Illustratively the master header sub field contains information such as the number of data segments in the volume the number of block layouts for the data segments and the offset at which each block layout starts. The block layout sub field contains block layout information including client specific metadata for all blocks that are stored within a corresponding data segment . Assume a client requests writing of a block the block layout information may include an identification of the block block id the size of the block client user permissions and checksum of the data of the block. In an embodiment the block layout sub field may include information e.g. metadata that specifies the linking of chunks of a block via pointers to the chunks of the block as well as a 64 bit block id that uniquely identifies names the block. The sub field may also contain information as to whether the block is allocated or deallocated and information about the block layout on the segment.

In an embodiment alignment of the various fields of the master and data segments of a volume may provide substantial improvement to read write performance of the object store. Illustratively the fields e.g. header chunk metadata chunk and recovery area of the master and data segments are discrete disk locations e.g. sectors within the luns of the disks . Starting offsets of these fields may be aligned to e.g. a stripe width across the disks of a RAID group to ensure that only required information is read written when accessing a stripe. For example if the stripe width is a multiple of the disk sector size e.g. 512 KB the starting offsets may be multiples of 512K. In particular the starting offsets of one or more fields of the segments may be aligned with the striped width so that all of the required information from the field s may be accessed with e.g. one read access.

In addition by separating the master segment from the data segments of a volume the contents of the volume may be implemented on mixed media to improve performance of the on disk layout of the object store. For example the master segment which stores critical metadata content can be stored on a medium e.g. solid state disk SSD such as Flash that is different from the medium used to store some or all of the data contents of the data segments e.g. hard disk drive HDD . To that end information such as metadata content related to random input output I O operations and or to sequential small write operations may be stored on SSD whereas information such as data content related to sequential I O operations may be stored on HDD. In an embodiment the storage system may be constructed of mixed media in quantities such as approximately 80 of HDD disks and 20 of SSD or Flash card although it will be understood to those of skill in the art that other quantities may be advantageously utilized.

The information stored on the master segment of a volume is persistently stored on disk and upon a startup or boot of a node that information may be loaded into the memory i.e. incore of the node. For instance when the node is booted the information may be loaded from the master segment into memory to populate incore data structures used to identify and access data chunks of a block. is a block diagram of the incore layout of the object store that may be advantageously used with one or more embodiments described herein. Illustratively the incore layout may be implemented as incore data structures of the nodes. In core a chunk is illustratively a location or area of memory that stores the data content of the chunk. The memory area of the chunk may be referenced pointed to by a corresponding data structure e.g. a chunk descriptor which contains chunk metadata that describes the chunk with respect to e.g. its size an amount of data in the chunk and or a current state of the chunk i.e. locked or unlocked .

As noted the master segment has an on disk layout that contains information used to organize chunks as one or more blocks. In core however each block is illustratively converted to an object. In other words the on disk layout of the object store utilizes a construct of a block stored on disk whereas an incore layout of the object store utilizes a construct of an object stored in memory. Thus in one or more embodiments an on disk block is equivalent to an incore object. Data structures e.g. an object accessor and an object descriptor may be used to enable access to the object incore. Illustratively the object accessor may be a map or table data structure that contains one or more pointers e.g. object desc pointer to one or more object descriptors each of which is essentially a linked list of chunk descriptors . Note that the chunk descriptors may be linked together incore similar to the way chunks are linked together on disk. However the object descriptor is not published provided to e.g a client to enable access to an object rather an object identifier object id may be provided to the client. In one or more embodiments the object id is a 64 bit identifier that uniquely identifies an object as used herein the object id is illustratively equivalent to the 64 bit block id.

Advantageously the object store described herein may be configured to service objects in a manner that is adaptable to various data management system deployments including distributed file systems and or distributed database management systems that support object level management utilizing e.g. the quasi shared storage system of the distributed data processing system.

While there have been shown and described illustrative embodiments that provide an object store that efficiently manages and services objects for use by clients of a distributed data processing system it is to be understood that various other adaptations and modifications may be made within the spirit and scope of the embodiments herein. For example embodiments have been shown and described herein with relation to deployment of the object store in an analytics framework such as Hadoop which services data in the form of HDFS blocks for WORM workloads but which may be configured to transform the HDFS blocks into objects of the object store. However the embodiments in their broader sense are not so limited and may in fact allow deployment of the object store in other frameworks that may access the data directly as objects without the use the HDFS protocol and or for non WORM workloads. For these other deployments the object store functions substantially similar as with Hadoop. For example a client e.g. an application running on a computer external to the distributed data processing system may request access e.g. reading or writing to an object serviced by a compute node of the distributed data processing system using a storage protocol e.g. other than HDFS that employs the 64 bit object id. The object id may then be presented to the compute node and in particular to the incore data structures of the object store to identify and access the chunk s constituting the object as described herein.

The foregoing description has been directed to specific embodiments. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. For instance it is expressly contemplated that the components and or elements described herein can be implemented as software encoded on a tangible non transitory computer readable medium e.g. disks and or CDs having program instructions executing on a computer hardware firmware or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the embodiments herein. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the embodiments herein.

