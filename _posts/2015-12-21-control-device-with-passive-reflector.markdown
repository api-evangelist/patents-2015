---

title: Control device with passive reflector
abstract: A control device includes a housing. The control device also includes a wireless communicator interior the housing. The wireless communicator wirelessly sends commands for controlling an electronic device, such as a game console. The control device also includes a reflector positioned to reflect light directed at the housing.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09524554&OS=09524554&RS=09524554
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09524554
owner_city: Redmond
owner_country: US
publication_date: 20151221
---
The present application is a divisional of U.S. patent application Ser. No. 13 767 634 filed Feb. 14 2013 the entire contents of which are incorporated herein by reference for all purposes.

Association of a control device with a specific user typically involves multiple levels of interaction between the control device the user and a computing system. The association may also remain until a user specifically requests to update the control device with a new association or the computing system determines that the player association has changed and needs to be updated.

A control device includes a housing. The control device also includes a wireless communicator interior the housing. The wireless communicator wirelessly sends commands for controlling an electronic device such as a game console. The control device also includes a reflector such as a retroreflector positioned to reflect light directed at the housing.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

The present disclosure is directed to a control device that includes a reflector capable of reflecting light that is emitted towards the reflector. For example the control device may include a retroreflector capable of retroreflecting infrared light emitted from a depth camera system or other environmental illuminator. An infrared camera may then recognize this reflected infrared light and use it to locate and track the control device that includes the retroreflector. This type of passive locating and tracking may be performed instead of or in addition to active locating and tracking in which the control device actively emits an infrared light from a beacon.

Computing system may include an imaging device such as depth camera . For example depth camera may include one or more sensors lens elements and or light sources that are configured to image environment . Depth camera may utilize any suitable depth imaging technology including but not limited to a time of flight depth camera and or a structured light depth camera. As such the depth camera may include an infrared camera. Depth camera may include additional sensors including but not limited to a visible light camera and or one or more microphones. Depth camera may generate and send infrared images and or depth images to computing system . The depth images may indicate a depth of a surface imaged by each pixel of the depth images in order to provide information regarding one or more features of environment .

As further illustrated in environment may include one or more human subjects such as game player and game player and one or more non human subjects such as table . In some embodiments one or more users such as game player may interact with computing system . For example Computing system may provide visual output to game player in any suitable manner such as through display of display device . Computing system may also provide audio output to game player in any suitable manner including through one or more audio devices such as audio headphones of control device and or speakers of display device .

Game player may provide input to computing system via one or more user input devices such as control device depth camera microphone of control device and or any other suitable user input device. User input devices such as those described above may communicate with computing system in any suitable manner including but not limited to wired and or wireless communication. For example a control device may provide user input to computing system in the form of commands that are sent over a radiofrequency RF communication channel. In some embodiments each of a plurality of control devices and or other objects may communicate over a particular RF channel and or provide a unique identifier during communication in order to distinguish a particular control device from another control device.

Objects such as control devices user input output devices and the like may also be configured to interact with computing system by emitting a beacon. For example as illustrated in control device may emit beacon . Beacon may comprise any suitable beacon including but not limited to an audio and or optical beacon. For example beacon may include an infrared light beacon a visible light beacon a sonar beacon and or any suitable electromagnetic signal. In some embodiments the beacon may only be emitted in response to a request or other message from a computing system. Beacon may be used to determine a source of user input and or a location of an object. Determining a source of user input may allow computing system to provide a dynamically adjustable user experience. For example content of an application may be customized based on a particular user providing input. In some embodiments content of an application may be tied to a control device. For example an audio output of an application providing sound to an audio headphone may be adjusted based on a position of the audio headphone.

Control devices and or other objects may also be distinguished from one another via a directed round robin and or time sliced scheduling of message transmission. For example a control device may be assigned one or more time slices during which the controller will emit active infrared light. In response the computing system may distinguish each control device by determining a time and or order of received signals and associating the source of the signal with the control device assigned to that time. Accordingly the computing system may detect and associate the beacon that it observes with an object that it communicates with via another communications medium such as RF wired cable etc.

Identifying a source of user input may also enable cooperation between multiple user input and or control devices. For example an audio headset may be associated with a control device such that audio input provided to the audio headset may supplement input provided to the control device. Gestures that are detected by a control device depth camera and or other suitable gesture detection device may also be associated with a particular control device such that gesture based commands may supplement input provided to the control device. For example computing system may determine that a gesture is performed by a particular user and or in a particular location and associate the gesture with a control device based on the determined user and or location.

Accordingly in multi user environments a first user such as game player may provide input via control device and control device while a second user such as game player provides input via control device . By associating each control device with a respective user computing system may provide a multiplayer gaming experience. For example user input provided by control device and or control device may be applied to a first player representation and user input provided by control device may be applied to a second player representation . Likewise audio associated with a first player representation may be directed to headphones of control device while audio associated with a second player representation may be directed to speakers of display device and or another suitable audio device.

At method includes identifying a subject imaged by the depth images. In some scenarios the subject is a human subject such as game player of . In some scenarios the subject may be an inanimate object such as table and or control devices and or of . For embodiments in which a computing system includes an infrared sensor as discussed above the computing system may identify a subject imaged by the infrared images. For example the computing system may analyze an infrared reflectivity of each pixel of an infrared image in order to determine a position of an object beacon retroreflector and or subject. In some embodiments the computing system may utilize an infrared sensor to track and determine a two dimensional location and a depth camera to calculate a depth value for the two dimensional location. The computing system may analyze virtually any type of electromagnetic information to determine a location of an object beacon retroreflector and or subject without departing from the scope of this disclosure.

In the case of a human subject identification may be specific to a particular individual. In other words the identity of a particular person may be identified. For example computing system of may identify game player as being John where John may be described by a user profile stored locally and or remotely on a storage machine of computing system and or a database. The user profile may include any suitable information to identify a game player including but not limited to virtual skeleton data facial recognition data audio parameters etc. The user profile may be locally saved and or accessed via a network such as the Internet. In other cases identification is not specific to a particular individual. For example computing system of may identify and track game player without differentiating game player from any other game player.

In some embodiments a computing system may differentiate a game player from other game players without directly identifying the game player. For example the computing system of may perform facial recognition described in more detail below or any other identification technique without tying the game player to a particular user profile. The identification may be utilized to associate the game player with a particular representation within an application and or other context of the computing system such that input provided by the game player regardless of the input mechanism utilized is provided to the particular representation. For example a game player may be identified via facial recognition while providing input associated with a particular character in a game. Thereafter even if the game player utilizes a different input device such as a different controller the input provided by the game player may be associated with the particular character.

As indicated at identifying a human subject optionally may include modeling the human subject with a virtual skeleton. Skeletal modeling may optionally be used to find the particular identity of an observed subject. For example one or more characteristics detected via skeletal model e.g. bone lengths bone posture etc. may be used to look up a user identity in a look up table or other database. Such modeling is described in more detail with reference to .

At shows game player of from the perspective of a depth camera such as depth camera of . A depth camera such as depth camera may include one or more sensors e.g. infrared light sensors that are configured to observe a human subject such as game player . The depth camera may also include one or more sources for illuminating the scene that is to be imaged e.g. infrared diodes or lasers . The depth camera may also observe a beacon that is emitted from a control device such as control device . For example beacon may include an optical beacon such as a visible light beacon and or an infrared light beacon.

At shows a schematic representation of the observation data collected by a depth camera such as depth camera . The types of observation data collected will vary depending on the number and types of sensors included in the depth camera. For example the depth camera may include an infrared camera configured to produce an infrared image.

A depth camera may determine for each pixel of the depth camera the depth of a surface in the observed scene relative to the depth camera. schematically shows the three dimensional x y z coordinates observed for a DPixel v h of a depth camera . Similar three dimensional x y z coordinates may be recorded for every pixel of the depth camera. The three dimensional x y z coordinates for all of the pixels collectively constitute a depth map. The three dimensional x y z coordinates may be determined in any suitable manner without departing from the scope of this disclosure.

A visible light camera may determine for each pixel of the visible light camera the relative light intensity of a surface in the observed scene for one or more light channels e.g. red green blue grayscale etc. . schematically shows the red green blue color values observed for a V LPixel v h of a visible light camera of depth camera . Similar red green blue color values may be recorded for every pixel of the visible light camera. The red green blue color values for all of the pixels collectively constitute a digital color image. The red green blue color values may be determined in any suitable manner without departing from the scope of this disclosure.

The depth camera and visible light camera may have the same resolutions although this is not required. Whether the cameras have the same or different resolutions the pixels of the visible light camera may be registered to the pixels of the depth camera. In this way both color and depth information may be determined for each portion of an observed scene by considering the registered pixels from the visible light camera and the depth camera e.g. V LPixel v h and DPixel v h .

As illustrated at beacon may be configured such that observation data clearly differentiates the beacon from surrounding objects imaged by the depth camera. For example beacon may include an infrared light beacon that is represented in infrared images as a series of white or bright pixels arranged in accordance with a beacon pattern emitted by control device . Similarly the infrared images may clearly differentiate a retroreflector reflecting passive infrared light.

One or more microphones may determine directional and or nondirectional sounds coming from an observed human subject and or other sources. schematically shows audio data recorded by a microphone such as microphone of control device . Such audio data may be determined in any suitable manner without departing from the scope of this disclosure. Sound source localization could also be used to find and or locate a controlling device if instead of a light beacon the control device produces a sound beacon.

The collected data may take the form of virtually any suitable data structure s including but not limited to one or more matrices that include an infrared intensity value for every pixel imaged by an infrared camera a three dimensional x y z coordinate for every pixel imaged by the depth camera red green blue color values for every pixel imaged by the visible light camera and or time resolved digital audio data. While depicts a single frame it is to be understood that a human subject may be continuously observed and modeled e.g. at 30 frames per second . Accordingly data may be collected for each such observed frame. The collected data may be made available via one or more Application Programming Interfaces APIs and or further analyzed as described below.

A depth camera and or cooperating computing system optionally may analyze the depth map to distinguish human subjects and or other targets that are to be tracked from non target elements in the observed depth map. Each pixel of the depth map may be assigned a player index that identifies that pixel as imaging a particular target or non target element. As an example pixels corresponding to a first player can be assigned a player index equal to one pixels corresponding to a second player can be assigned a player index equal to two and pixels that do not correspond to a target player can be assigned a player index equal to zero. Such player indices may be determined assigned and saved in any suitable manner without departing from the scope of this disclosure.

A depth camera and or cooperating computing system optionally may further analyze the pixels of the depth map of a human subject in order to determine a part of that subject s body that each such pixel is likely to image. A variety of different body part assignment techniques may be used to assess which part of a human subject s body a particular pixel is likely to image. Each pixel of the depth map with an appropriate player index may be assigned a body part index . The body part index may include a discrete identifier confidence value and or body part probability distribution indicating the body part or parts to which that pixel is likely to image. Body part indices may be determined assigned and saved in any suitable manner without departing from the scope of this disclosure.

As one nonlimiting example machine learning can be used to assign each pixel a body part index and or body part probability distribution. The machine learning approach analyzes a human subject using information learned from a prior trained collection of known poses. In other words during a supervised training phase a variety of different people are observed in a variety of different poses and human trainers provide ground truth annotations labeling different machine learning classifiers in the observed data. The observed data and annotations are used to generate one or more machine learning algorithms that map inputs e.g. observation data from a tracking device to desired outputs e.g. body part indices for relevant pixels .

At shows a schematic representation of a virtual skeleton that serves as a machine readable representation of game player . Virtual skeleton includes twenty virtual joints head shoulder center spine hip center right shoulder right elbow right wrist right hand left shoulder left elbow left wrist left hand right hip right knee right ankle right foot left hip left knee left ankle and left foot. This twenty joint virtual skeleton is provided as a nonlimiting example. Virtual skeletons in accordance with the present disclosure may have virtually any number of joints.

The various skeletal joints may correspond to actual joints of a human subject centroids of the human subject s body parts terminal ends of a human subject s extremities and or points without a direct anatomical link to the human subject. Each joint may have at least three degrees of freedom e.g. world space x y z . As such each joint of the virtual skeleton may be defined with a three dimensional position. For example a left shoulder virtual joint is defined with an x coordinate position a y coordinate position and a z coordinate position . The position of the joints may be defined relative to any suitable origin. As one example a depth camera may serve as the origin and all joint positions are defined relative to the depth camera. Joints may be defined with a three dimensional position in any suitable manner without departing from the scope of this disclosure.

A variety of techniques may be used to determine the three dimensional position of each joint. Skeletal fitting techniques may use depth information color information body part information and or prior trained anatomical and kinetic information to deduce one or more skeleton s that closely model a human subject. As one nonlimiting example the above described body part indices may be used to find a three dimensional position of each skeletal joint.

A joint orientation may be used to further define one or more of the virtual joints. Whereas joint positions may describe the position of joints and virtual bones that span between joints joint orientations may describe the orientation of such joints and virtual bones at their respective positions. As an example the orientation of a wrist joint may be used to describe if a hand located at a given position is facing up or down.

Joint orientations may be encoded for example in one or more normalized three dimensional orientation vector s . The orientation vector s may provide the orientation of a joint relative to the tracking device or another reference e.g. another joint . Furthermore the orientation vector s may be defined in terms of a world space coordinate system or another suitable coordinate system e.g. the coordinate system of another joint . Joint orientations also may be encoded via other means. As non limiting examples quaternions and or Euler angles may be used to encode joint orientations.

Joint positions orientations and or other information may be encoded in any suitable data structure s . Furthermore the position orientation and or other parameters associated with any particular joint may be made available via one or more APIs.

As seen in virtual skeleton may optionally include a plurality of virtual bones e.g. a left forearm bone . The various skeletal bones may extend from one skeletal joint to another and may correspond to actual bones limbs or portions of bones and or limbs of a human subject. The joint orientations discussed herein may be applied to these bones. For example an elbow orientation may be used to define a forearm orientation.

The virtual skeleton may be used to identify a human subject by comparing the values associated with each element of the skeleton e.g. relative locations of joints and or bones with known virtual skeleton values for a particular human subject. For example a game player may be imaged and a virtual skeleton associated with that player may be stored in a look up table. Thereafter when the game player is imaged a generated virtual skeleton may be compared to the stored virtual skeleton. For example a computing system may include a user profile for each member of a family including in order of decreasing virtual skeleton size a father mother son and daughter. In order to identify a particular user and select a profile associated with that user a size of the virtual skeleton of the user may be compared to the size of the virtual skeleton associated with each user profile.

In some embodiments the virtual skeleton may be used to compare a location of an object and or a beacon such as beacon as represented at to a location on a game player. For example a position of beacon as represented at may be determined based on imaged surfaces near the beacon. Accordingly an imaged surface representing a controller may be used to determine one or more coordinates associated with a detected beacon. Upon determining the position of the beacon the proximity of beacon or a known location of control device to a hand or other joint of virtual skeleton may be analyzed to determine whether the control device may be associated with game player . The association of the control device with a game player is discussed further with reference to below.

In some embodiments association of a control device and or other object to a subject may be performed based on a suitable parameter of the control device in relation to the subject. For example a game player may place different control devices in different location of an environment such as environment of . The game player may then perform a gesture or otherwise indicate a particular control device to which a user input is to be associated. Accordingly a control device may be associated with a subject based on a gesture of the subject even if another subject is in closer proximity to the control device. It is to be understood however that a virtual skeleton may be used for additional and or alternative purposes without departing from the scope of this disclosure.

Turning back to as indicated at identifying the human subject optionally may include facial recognition. For example facial recognition algorithms may be applied to depth images raw infrared images color images and or images from a visible light camera. Facial recognition may optionally be used to find the particular identity of an observed subject. For example one or more characteristics detected via facial recognition e.g. eye separation eye eye mouth triangle etc. may be used to look up a user identity in a look up table or other database. To reduce computing power needed the parts of images that are analyzed for facial recognition may be limited to an area around a head joint of a virtual skeleton such as virtual skeleton of .

Additionally as indicated at identifying the human subject optionally may include voice recognition. For example audio input may be received from a microphone such as microphone of and parameters associated with the audio input such as frequency amplitude breathing patterns etc. may be analyzed and or compared to audio parameters associated with a user and stored in a look up table or other database to identify the user.

As indicated at the method may optionally include outputting a message requesting an object to emit a beacon. For example computing system of may request via one or more RF frequencies that one or more of the control devices and or emit a beacon so that the computing system may resolve a respective location of the control device in environment . In some embodiments the computing system may request that the object emits a particular beacon. For example computing system may output a message to each object individually requesting a different beacon from each object. Computing system may also output one or more messages to assign time slices to the control devices for emitting the same beacon at different times. Likewise computing system may output any other suitable sequence of messages that will enable the system to detect and associate the beacon that it observes with an object that it communicates with via another communication medium such as RF wired cable etc. In additional or alternative embodiments computing system may output and or broadcast a general message that is not directed to a particular object requesting an object to emit a beacon.

As indicated at method includes recognizing within one or more of the depth images and or infrared images a beacon emitted from an object. A beacon may be recognized within depth images and or infrared images in any suitable manner. For example as shown in a beacon from control device may be represented in observational data as schematically represented at as a bright spot illustrated at that is distinguishable from surrounding elements imaged by a depth camera. One or more parameters of the beacon may be analyzed to identify and distinguish a first object from a plurality of different objects. These parameters may include virtually any suitable parameter of the beacon including but not limited to an intensity a pattern and or a frequency of the beacon. The parameters may be used to find the first object in a look up table that includes one or more objects and one or more parameters associated with the objects. For example a first object may emit a beacon with one or more different parameters from a beacon emitted by a second object. Objects may additionally or alternatively emit beacons in a time slice and or sequential manner such that a time and or order parameter may be different between objects.

As indicated at method includes assessing a position of the object in three dimensions. For example computing system of may utilize pixel information from depth camera to determine x y and z coordinates of a control device within environment . The position of the object may be relative to the position of depth camera a subject imaged by depth camera and or any other suitable reference point. In some embodiments assessing the position of the control device may include identifying the control device within the depth images and or infrared images and or may include tracking the position of the control device over time to identify and interpret control gestures from the tracked position. In some embodiments multiple beacons may be emitted at known locations on the controlling device in order to enable the computing system to compute the controller location. Additional beacons also add redundancy in case some of the beacons are occluded from the sensor. For example fingers of a game player may be inadvertently placed over a location of a beacon emitter on the control device.

Method further includes associating the object with the human subject based on a proximity of the control device to the human subject as indicated at . For example control device of may be associated with game player due to the proximity of the control device to the game player. Conversely control device may not be associated with either game player or due to the distance between control device and game players and . Instead control device may be associated with table . The proximity may be determined based on any suitable threshold.

In some embodiments the location of the object may be compared to a location of a particular part of a human subject. As indicated at associating the object with the human subject may optionally include analysis of a hand joint position. Turning briefly to a distance between a hand joint of virtual skeleton and a location of control device e.g. as determined at of method may be compared to a distance threshold. Although the hand joint is analyzed at it is understood that virtually any suitable joint and or portion of the virtual skeleton may be analyzed to determine a distance between the joint and or portion and an object. In some embodiments an object may be associated with a human subject if the object is within a threshold distance of a hand joint of the human subject. However the object may not be associated with the human subject if the distance between the object and the hand joint exceeds the threshold distance. In some embodiments without full skeletal tracking a location of the head face and the controller along the horizontal axis can be used to determine association.

Associating the control device with the human subject may include attributing a gesture input of the virtual skeleton and a control input of the control device to a same source. For example such association may allow a user to provide multiple types of user input utilizing multiple devices to control a single player representation. Associating the object with the human subject may additionally or alternatively include applying a settings profile of the human subject to an application controlled by the object. In some embodiments associating the object with the human subject may include presenting user specific content via an application controlled by the object. For example the user specific content may be game content media programming advertising and or any suitable content selected for the human subject. In some embodiments association of the object may change when passing an object from one user to another. For example association may dynamically update based on a user that is nearest and or providing input to the object.

A control device or other object may also emit a beacon to perform a binding operation with a computing system. For example computing system of may not accept input from control device until a binding operation is successfully completed. As used herein the term unbound may refer to an object that is not bound to a particular computing system and or an object that is not bound to any computing system.

As indicated at method includes determining if the object is bound to the computing system. For example the computing system may identify the object and determine whether the identified object is bound to the computing system. In some embodiments the computing system may determine whether the object is bound to the computing system based on a beacon emitted by the object such as the beacon detected at . For example control devices that are not bound to the computing system may emit a beacon and or may emit a particular beacon to advertise an unbound state. Therefore if the object emits a beacon and or emits a particular beacon the computing system may determine that the object is not bound to the computing system. Conversely if the object does not emit a beacon the object may be determined to be bound to the computing system. If the object is bound to the computing system no binding operation is necessary therefore the method returns to the start of method .

If the object is determined to be unbound method proceeds to where the method includes detecting an optical beacon emitted from the object. For example an object such as control device of may automatically emit an optical beacon in response to changing states if the control device is not bound to computing system . Changing states may include changing from an inactive state to an active state. For example changing from an inactive state to an active state may include changing from a power off state to a power on state. In some embodiments changing from an inactive state to an active state may additionally or alternatively be performed as a result of detecting interaction with the control device by a user. Detecting interaction may include detecting actuation of a control element of the control device detecting a signal from a sensor of the control device and or be performed in any other suitable manner. For example control device may change from an inactive state to an active state when a user removes control device from table .

The computing system may detect an optical beacon in a similar manner to those described above. For example the computing system may recognize an optical beacon in one or more images and or depth images received from a visible light camera and or a depth camera. In some embodiments the computing system may include an infrared sensor to detect an infrared light beacon and or a visible light sensor to detect a visible light beacon that is emitted from the object. For example a depth camera may include an infrared pass filter in front of an infrared sensor in order to filter out visible light and allow infrared light having a particular range of wavelengths to be detected by the infrared sensor. Accordingly an object may emit a beacon with a wavelength that is matched to an infrared pass filter so that it may be detected by the depth camera.

As indicated at method includes binding the object to the computing system. For example the computing system may automatically enter a binding mode upon detection of an optical beacon and or an object that is unbound in order to bind the object to the computing system.

As indicated at binding the object to the computing system may optionally include performing a handshake operation to verify the identity of the control device. The handshake operation may be performed in virtually any suitable manner. In some embodiments performing a handshake operation may optionally include sending a request for a particular beacon pattern as indicated at . For example computing system of may send a request to control device to emit a particular beacon pattern. The request may identify any suitable pattern including but not limited to number duration and or frequency of optical signals to be emitted from the control device. A control device may also include a plurality of light sources that may be independently controlled to emit a particular beacon. Accordingly the request may identify a particular beacon pattern for one or more light sources of the plurality of light sources such that a first light source may be requested to emit a different pattern than a second light source. If the requested beacon pattern and or patterns are detected by the computing system such as through analysis of depth images the computing system may identify and or determine the position of the control device.

Performing a handshake operation may additionally or alternatively include sending a request for an identifier associated with the object as indicated at . For example computing system of may send a request to control device for an identifier associated with control device . Upon receiving an identifier for control device computing system may authenticate the control device and may bind the control device to the computing system. In some embodiments computing system may assign a unique identifier to the object to bind the object upon completing a handshake operation. Binding an object to a computing system may be performed in any suitable manner including but not limited to storing an identifier for the object in a storage machine and or database.

In some embodiments binding may occur before the computing system provides any commands or requests to the control device. For example computing system may automatically bind with control device A upon receipt of the beacon emitted at . In additional or alternative embodiments computing system may initiate a handshake operation by requesting a particular beacon pattern from control device A at . For example computing system may determine or negotiate an RF channel over which control device A may communicate. In some embodiments the beacon emitted at and or may be a different type beacon than the beacon requested at . For example the beacon emitted at may be an infrared beacon while the beacon requested at may be an RF signal. In some embodiments the RF channel may be specific to control device A. Computing system may then specifically address control device A by sending a request for the particular beacon pattern over the channel. In response control device A emits the particular beacon pattern at . Upon confirming that the beacon pattern emitted by control device A is the same and or within a threshold of the requested particular beacon pattern computing system may bind control device A and or confirm or finalize a previously executed binding operation. For example as indicated at computing system may assign and send a first unique ID to control device A and or store the unique ID in order to bind control device A. Upon binding control device A and computing system may freely communicate with one another in any suitable manner such as utilizing a particular RF channel. In other words computing system may allow control device A to issue commands once the binding operation has been performed.

As indicated at computing system may also request that control device B emit a particular beacon pattern. In some embodiments the particular pattern may be the same as the particular beacon pattern requested at while in other embodiments the particular pattern may be different than the particular beacon pattern requested at . For example if the particular pattern is the same the computing system may associate a time value to the pattern to differentiate a pattern received from control device A from a pattern received from control device B. Conversely if the particular pattern is different for each control device the computing system may be able to differentiate two beacons received at the same time. After the request sent at control device B may emit a beacon having an unrequested pattern at . For example while the request sent at may be intended for control device B control device B may not receive the request. Therefore control device B may emit a beacon having a pattern that is different than and or outside of a similarity threshold of the pattern requested at . In response computing system may determine that reliable communication may not be established with control device B and thus may not bind control device B. In some embodiments computing system may attempt to bind with control device B again by sending another request for a particular beacon pattern. In additional or alternative embodiments computing system may automatically bind with control device B after receiving a number of beacons from control device B that exceeds a threshold. In some embodiments the threshold number of beacons may be one such that computing system binds a control device or other object immediately upon detecting a beacon emitted by the control device or other object.

As described above an infrared beacon may be used to actively emit infrared light that can be detected by an infrared camera and used to assess the three dimensional position of the control device emitting the infrared light. As described below at least some of the benefits of this type of locating and or tracking alternatively and or additionally may be achieved using a passive reflector that reflects infrared light that is emitted towards the control device including the reflector. Such passive reflection can decrease energy consumed by the control device because active emission of infrared light can be reduced or eliminated. Such energy reduction can increase battery life of the control device. In some embodiments the environmental illuminator used to generate the light for reflection may be the same illuminator used for depth imaging. As such passive reflection need not use light that is not already serving another purpose.

At method optionally includes commanding an infrared beacon of a wireless control device to emit infrared light. Such a command may be delivered via radiofrequency communication for example. At method includes recognizing within one or more of the infrared images an active infrared light emitted from the beacon of the wireless control device. At method optionally includes commanding the beacon to stop emitting infrared light. The beacon control and recognition of and or may be implemented as described above with reference to . As one nonlimiting example the active beacon may be used to locate a wireless control device upon startup or whenever a position of the wireless controller is lost.

At method optionally includes commanding an environmental illuminator to emit infrared light. The beacon recognition described above may optionally be performed when an environmental illuminator e.g. IR blaster time of flight infrared projector structured light infrared projector etc. is not emitting infrared light so that passive reflectors will not be confused with active beacons. In other words the environment may be temporarily controlled so as to limit the amount of infrared light that is likely to be detected from the reflectors by the infrared camera. As such the infrared light that is detected may more confidently be attributed to the active beacon as opposed to a passive reflector. However once the active beacon is identified and located it may be turned off and the environmental illuminator s may be used to emit infrared light that can be reflected by the reflector s of the wireless control device. In some embodiments the beacon recognition may optionally be performed while the environmental illuminator remains active. For example modulation and or other parameters of the beacon light may be used to differentiate the active beacon from passive reflectors.

At method includes recognizing within one or more of the infrared images a passive infrared light reflected from a reflector of the control device. At method optionally includes using passive infrared light to track the three dimensional position of the control device. The recognition and tracking of and may occur after the beacon of the wireless control device stops emitting the active infrared light. As such the system is able to locate and or track a control device in an energy efficient manner in which the beacon of the wireless control device does not continuously signal. The active beacon may be used intermittently on an as needed basis to associate and or confirm a unique ID for a particular passive reflector and or corresponding control device. It is to be understood that all of the active location tracking association and other functions described above with reference to may also be implemented when the camera recognizes passive reflected light instead of or in addition to active beacon light.

Control device includes a housing that includes a light transmissive portion . The light transmissive portion may be configured to transmit light having different wavelengths to different degrees. As one nonlimiting example light transmissive portion may be greater than 50 transmissive to infrared light but less than 50 transmissive to visible light. While shows a single light transmissive portion on a front side of control device it is to be understood that one or more different light transmissive portions may be located virtually anywhere on a control device. Further while shows a light transmissive window surrounded by opaque portion s the entire housing may be light transmissive.

Control device includes an infrared beacon interior the housing and positioned to emit infrared light through light transmissive portion of housing . In other words the infrared beacon may be positioned behind the light transmissive window so that it is protected but remains visible. The infrared beacon may be mounted directly to the light transmissive portion or the infrared beacon may be spaced away from the light transmissive portion. The infrared beacon may include a light emitting diode laser diode and or other light source. The infrared beacon may also include one or more optics to focus filter or otherwise condition light for easy detection. As one nonlimiting example light transmissive portion may optionally be configured to focus filter or otherwise condition light.

Control device includes a reflector interior the housing. Reflector is positioned to reflect light entering the housing through light transmissive portion . For example reflector may reflect infrared light from an IR blaster time of flight infrared projector structured light infrared projector and or other environmental illuminator. As discussed above such an environmental illuminator may be intelligently controlled for example by computing system of or computing system of . Reflector may reflect the infrared light back out of the housing through the light transmissive portion so that it can be detected by a camera such as an infrared and or depth camera.

Reflector may be a retroreflector. As an example the reflector may be configured as a retroreflective tape configured as a sheeting that includes cube corner microprism retroreflective elements integrally bonded to a flexible polymeric film. The prism surfaces may be coated with a vacuum deposition of aluminum to provide a mirror surface to the prism facets. In some embodiments the retroreflective tape is characterized by a specular gloss of not less than 40 when tested in accordance with ASTM method D523 at an angle of 85 . Further the coefficient of retroreflection may be at least 15 for an observation angle of 0.5 degrees and an entrance angle of 30 degrees and at least 75 for an observation angle of 0.2 degrees and an entrance angle of 4 degrees.

The reflector may be affixed directly to the light transmissive portion or the reflector may be spaced away from the light transmissive portion. While the infrared beacon and reflector are illustrated as being behind the same light transmissive portion this is not required. The reflector and the infrared beacon may be positioned behind different light transmissive portions. Further while a single infrared beacon and a single reflector are illustrated it is to be understood that two or more infrared beacons and or two or more reflectors may be used. Similarly zero infrared beacons or zero reflectors may be used in some embodiments. Infrared beacons and reflectors may be positioned virtually anywhere about the control device internally and or externally without departing from the scope of this disclosure.

Control device includes a wireless communicator interior the housing. Communicator may be configured to wirelessly send and wirelessly receive commands via radiofrequency communication. For example the communicator may be configured to communicate with computing system of or computing system of . In this way a computing device may send commands to the control device e.g. emit light via infrared beacon rumble etc. and or the control device may send commands to the computing device. The commands sent from the control device to another computing device may take a variety of different forms depending on the configuration of the control device and or the current function of the computing device. As nonlimiting examples the commands may include a game command for controlling an electronic game facilitated by the electronic device. As another example the command may include an audio visual command for controlling an audio visual experience e.g. television movie etc. facilitated by the electronic device.

A control device may further include button s directional pad s analog stick s rumble motor s dial s display s touch display s track pad s scroll wheel s track ball s microphone s camera s and or other mechanisms for receiving user input.

In some embodiments the methods and processes described herein may be tied to a computing system of one or more computing devices. In particular such methods and processes may be implemented as a computer application program or service an application programming interface API a library and or other computer program product.

Computing system includes a logic machine and a storage machine . Computing system may optionally include a display subsystem input subsystem communication subsystem and or other components not shown in .

Logic machine includes one or more physical devices configured to execute instructions. For example the logic machine may be configured to execute instructions that are part of one or more applications services programs routines libraries objects components data structures or other logical constructs. Such instructions may be implemented to perform a task implement a data type transform the state of one or more components achieve a technical effect or otherwise arrive at a desired result.

The logic machine may include one or more processors configured to execute software instructions. Additionally or alternatively the logic machine may include one or more hardware or firmware logic machines configured to execute hardware or firmware instructions. Processors of the logic machine may be single core or multi core and the instructions executed thereon may be configured for sequential parallel and or distributed processing. Individual components of the logic machine optionally may be distributed among two or more separate devices which may be remotely located and or configured for coordinated processing. Aspects of the logic machine may be virtualized and executed by remotely accessible networked computing devices configured in a cloud computing configuration.

Storage machine includes one or more physical devices configured to hold instructions executable by the logic machine to implement the methods and processes described herein. For example logic machine may be in operative communication with a depth camera interface such as an interface of depth camera and storage machine . When such methods and processes are implemented the state of storage machine may be transformed e.g. to hold different data.

Storage machine may include removable and or built in devices. Storage machine may include optical memory e.g. CD DVD HD DVD Blu Ray Disc etc. semiconductor memory e.g. RAM EPROM EEPROM etc. and or magnetic memory e.g. hard disk drive floppy disk drive tape drive MRAM etc. among others. Storage machine may include volatile nonvolatile dynamic static read write read only random access sequential access location addressable file addressable and or content addressable devices.

It will be appreciated that storage machine includes one or more physical devices. However aspects of the instructions described herein alternatively may be propagated by a communication medium e.g. an electromagnetic signal an optical signal etc. that is not held by a physical device for a finite duration.

Aspects of logic machine and storage machine may be integrated together into one or more hardware logic components. Such hardware logic components may include field programmable gate arrays FPGAs program and application specific integrated circuits PASIC ASICs program and application specific standard products PSSP ASSPs system on a chip SOC and complex programmable logic devices CPLDs for example.

When included display subsystem may be used to present a visual representation of data held by storage machine . This visual representation may take the form of a graphical user interface GUI . As the herein described methods and processes change the data held by the storage machine and thus transform the state of the storage machine the state of display subsystem may likewise be transformed to visually represent changes in the underlying data. Display subsystem may include one or more display devices utilizing virtually any type of technology. Such display devices may be combined with logic machine and or storage machine in a shared enclosure or such display devices may be peripheral display devices. For example display subsystem may include display device of .

When included input subsystem may comprise or interface with one or more user input devices such as a keyboard mouse touch screen microphone or game controller. For example input subsystem may include or interface with control devices and or of . In some embodiments the input subsystem may comprise or interface with selected natural user input NUI componentry. Such componentry may be integrated or peripheral and the transduction and or processing of input actions may be handled on or off board. Example NUI componentry may include a microphone for speech and or voice recognition an infrared color stereoscopic and or depth camera for machine vision and or gesture recognition a head tracker eye tracker accelerometer and or gyroscope for motion detection and or intent recognition as well as electric field sensing componentry for assessing brain activity.

When included communication subsystem may be configured to communicatively couple computing system with one or more other computing devices. Communication subsystem may include wired and or wireless communication devices compatible with one or more different communication protocols. As non limiting examples the communication subsystem may be configured for communication via a wireless telephone network or a wired or wireless local or wide area network. In some embodiments the communication subsystem may allow computing system to send and or receive messages to and or from other devices via a network such as the Internet.

It will be understood that the configurations and or approaches described herein are exemplary in nature and that these specific embodiments or examples are not to be considered in a limiting sense because numerous variations are possible. The specific routines or methods described herein may represent one or more of any number of processing strategies. As such various acts illustrated and or described may be performed in the sequence illustrated and or described in other sequences in parallel or omitted. Likewise the order of the above described processes may be changed.

The subject matter of the present disclosure includes all novel and nonobvious combinations and subcombinations of the various processes systems and configurations and other features functions acts and or properties disclosed herein as well as any and all equivalents thereof.

