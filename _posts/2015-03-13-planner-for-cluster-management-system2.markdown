---

title: Planner for cluster management system
abstract: A processing device receives a starting cluster layout for a first cluster, a target cluster layout for the first cluster, and a cluster management operation for creating or modifying the first cluster. The processing device determines a plurality of node level tasks to transition the first cluster from the starting cluster layout to the target cluster layout, determines dependencies between the plurality of node level tasks, and determines an order for the plurality of node level tasks based on the cluster management operation and the dependencies. The processing device then creates an execution plan that divides the cluster management operation into the plurality of node level tasks in the determined order.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09594601&OS=09594601&RS=09594601
owner: Cask Data, Inc.
number: 09594601
owner_city: Palo Alto
owner_country: US
publication_date: 20150313
---
This patent application claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Application No. 61 953 106 filed Mar. 14 2014.

Embodiments of the present invention relate to provisioning and managing a cluster and more specifically to provisioning and managing a cluster as a single entity.

Web applications may be implemented as a multi tiered application stack configured in a cluster. For example a common multi tiered application may include a database layer an application server layer and a web application layer. Components within each of these layers may have their own constraints and dependencies. Provisioning and managing the cluster for such a multi tiered application stack can be difficult and time consuming for an administrator.

Embodiments are directed to a cluster management system that provisions multi tiered applications as clusters. The cluster management system receives a cluster management operation and determines how to perform the cluster management operation using a sequence of node level tasks. The cluster management system exposes simple interfaces to administrators and allows easy customization of the properties of provisioned clusters including software services hardware configurations and infrastructure providers. An administrator may submit a cluster management operation request with a specified cluster template desired number of nodes and custom configurations. The cluster management system may then determine a valid cluster layout that satisfies all constraints determined from the cluster template and configuration determine a sequence of node level tasks to achieve the determined cluster layout and then interface with third party infrastructure providers to implement the node level tasks. Accordingly most of the decisions and tasks for provisioning a cluster and or modifying an existing cluster are automated in embodiments significantly decreasing the complexity difficulty and time associated with setting up or modifying a cluster for a multi tiered application stack.

The cluster management system described in embodiments allows users to manage clusters as single entities. As a result users do not have to know information about nodes that are included in a cluster. A node is a machine real or virtual that consists of a service or collection of services running on hardware or a virtual machine. The service is an application process or other piece of software. A cluster is a collection of nodes. Typically nodes in a cluster can communicate with one another to provide some functionality that each individual node is unable to fully provide. An example of a cluster is a collection of nodes that work together to provide a distributed file system like Hadoop HDFS.

The cluster management system described in embodiments includes a server and a collection of provisioners. The server is responsible for determining what should be done to achieve different cluster management operations. A cluster management operation is an action that is performed on a cluster affecting some or all of the nodes in the cluster. Some example cluster management operations are creating deleting shrinking expanding upgrading rollback configuring starting and stopping a cluster or its services. Cluster level operations typically involve many node level tasks. For example configuring a cluster usually requires configuring services on each node in the cluster and may also involve stopping and starting those services in a particular order. Cluster management operations usually obey some constraints as well. For example when expanding a cluster an administrator usually does not want the system to make any changes to existing nodes and usually wants the new nodes to mirror the types of nodes already existing in the cluster.

The cluster management system first takes the cluster management operation and cluster and solves the problem of how to perform the cluster management operation while obeying all constraints that apply to the cluster. The result is that cluster level operations are broken down into node level tasks that are coordinated and performed in a way that all constraints are satisfied. A node level task also referred to herein simply as a task is an action that is performed on a node. Some examples of node level tasks are creation and deletion of a node and the installation initialization configuration start stop or removal of a service on the node.

The cluster management system may also store the state of all provisioned clusters as well as a history of all operations performed. In embodiment the server does not perform any of the node level tasks. Instead the server may place tasks onto a task queue for provisioners to execute. In one embodiment provisioners are responsible for actually executing the node level tasks on the desired node and then reporting back to the server success or failure of a given node level task.

Referring now to the figures illustrates an example system architecture for a cluster management system in accordance with embodiments described herein. In one embodiment the system architecture includes a server and a collection of provisioners A N connected to one or more providers A N and a client computing device via a network . The network may be a public network e.g. the Internet a private network e.g. a local area network LAN wide area network WAN intranet etc. or a combination thereof.

The server computing device may be a physical machine and or a virtual machine hosted by a physical machine. The physical machine may be a rackmount server computer a desktop computer a blade server computer a portable computer or other computing device.

Each provider A N also referred to herein as an infrastructure provider platform provider or cloud provider provides infrastructure for creating and running nodes A N A N A N. Each provider A N may be a third party service or system with its own set of application programming interfaces APIs for interacting with that provider. Some providers A N may be public cloud providers such as Rackspace Google Cloud Platform Google App Engine Joyent Amazon Web Services AWS including Amazon Elastic Compute Cloud EC2 Amazon Simple Storage Service S3 and so on . Other providers A N may be private providers such as instances of Openstack . Each provider A N may provide physical machines and or host virtual machines that may be configured as nodes for a cluster. Providers A N may provide infrastructure as a service IaaS . Providers A N may also provide a platform used by an infrastructure as a service IaaS .

Client computing device may be any type of computing device such as a laptop computer desktop computer mobile phone tablet computer and so on. Client computing device may host or run an application that interfaces with server . The application may be a generic web browser or may be an application specifically configured for communicating with server e.g. for interfacing with one or more APIs of server . Via the application a user may generate and or select a cluster template select a desired configuration for a cluster select a desired cluster management operation and so on.

Responsive to receipt of a cluster management operation server may invoke solver and or planner to determine how to implement the cluster management operation. If the cluster management operation will create a new cluster or cause a cluster layout for an existing cluster to change solver determines a target cluster layout that will satisfy the cluster management operation. Planner then determines what node level tasks should be performed to transition the cluster from a starting state e.g. an initial cluster layout to the target cluster layout. Planner then assigns the node level tasks to the provisioners A N until all node level tasks have been performed and the cluster management operation has been completed. For each cluster management task a provisioner A N determines an appropriate provider A N to satisfy the node level task and uses an API for that provider A N to instruct the provider to perform the node level task. For example provisioner A may receive a create node task and may instruct provider A to create node A to satisfy the node level task. The created node A may have one or more properties indicated in the node level task and specified to the provider A by the provisioner A. These properties may include specified hardware properties and or specified software properties. Upon completion of all of the node level tasks server may send a success message to application . The success message may include information about the state of the cluster after the cluster management operation such as a deployed cluster layout information about provisioned nodes e.g. addresses of nodes names of nodes hardware types of the nodes services running on the nodes etc. and so on.

Server may perform several separate but related roles. The first role is to interact with administrators to define providers hardware images services and templates that can be used for cluster management purposes. These definitions may be persistently stored in persistent data store . The second role of the server is to create plans to perform cluster management operations. In short the second role is to transform cluster level operations into a plan of node level tasks all of which can be wrapped in a transaction. The final role is to interact with provisioners A N to coordinate the execution of those node level tasks.

Server includes one or more APIs which may include user APIs and administrator APIs . Administrator APIs may be used to create and modify cluster templates set cluster configurations create new users and so on. The administrator is responsible for defining cluster entities such as providers hardware types image types services and cluster templates. These cluster entities are used to perform cluster management operations and may be defined through the administrator APIs .

Server can create and delete machines real or virtual through different providers. As previously mentioned a provider can be an instance of an infrastructure as a service IaaS provider such as Rackspace Joyent Google or AWS. A provider can also be a platform used by an IaaS such as Rackspace. A provider can also be a custom provider created by the user that manages bare metal or any other system that can provide a machine to run services on. Server is not concerned with how to create and delete machines. Instead the provisioners may be relied upon to handle such minutia. An administrator creates a provider entity by specifying a unique name a description and a list of key value pairs that a provisioner plugin uses when communicating with a particular provider. The key value pairs may vary for different provisioner plugins. For example one key value pair may specify the universal resource locator URL to use to communicate with a particular provider. Another key value pair may specify the user identifier ID to use with the provider and another may specify a password to use.

A hardware type is an entity that unifies hardware from different providers into a single entity that can be referenced elsewhere. Administrators specify a unique name a description and a mapping of provider name to provider specific key value pairs. Hardware types may include multiple different mappings or definitions associated with different providers. For example an administrator can define a small hardware type. In it the admin maps for example the amazon provider to have a key value pair with flavor as a key and m1.small as a value. The amazon provider understands what a flavor is and understands what the m1.small flavor refers to. Similarly the admin may map the rackspace provider to have a key value pair with flavor as key and 2 as value. The rackspace provider understands what a flavor is and what the 2 flavor refers to. Now elsewhere in the server the administrator can simply refer to the small hardware type instead of referring to different flavor identities that depend on what provider is in use. Accordingly a first hardware definition may be used for a node if a first provider is used to create the node having the hardware type and a second hardware definition may be used for the node if a second provider is used to create the node having the hardware type.

An image type is an entity that it unifies images from different providers into a single entity that can be referenced elsewhere. An image is the state of a computer or virtual machine and usually contains an operating system and some basic software. Administrators specify a unique name a description and a mapping of provider name to provider specific key value pairs. For example an administrator can create a centos6 image type that maps the Amazon provider to some identifier for an image that uses centos6 as its operating system and that maps the rackspace provider to some other identifier for an image that uses centos6 as its operating system. Now elsewhere in server the administrator can simply refer to the centos6 image type instead of referring to different image identities that depend on what provider is in use.

A service is a piece of software that can run on a node. An administrator defines a service by giving it a unique name a description a set of other services it depends on and a set of provisioner actions that can be run with the service. Provisioner operations contain an action that can be performed on a service on a node such as install remove initialize configure start and stop. Provisioner operations may also contain a script and any optional data that an automator plugin will use to actually perform a node level task.

A cluster template is a blueprint that describes how a cluster should be laid out by specifying how different services image types and hardware types can be placed on nodes in a cluster. Cluster templates contain enough information that a user can specify just a template and a number of machines to create a cluster. Each cluster template may have a unique name a short description and a section devoted to compatibilities defaults and constraints. When a user chooses to create a cluster with some number of machines based off a cluster template the solver solves for a layout of nodes and services that satisfies the compatibilities and constraints given in the template. The solved layout is then sent to the planner which coordinates the execution of all tasks that are to be completed in order to create the cluster. These operations are described more fully below.

A cluster template may include a compatibility section. In one embodiment the compatibility section defines three things. The first information defined in the compatibility section is a set of services that are compatible with the template. This means that when a user goes to create a cluster with this cluster template the user is allowed to specify any service from the defined set as services to place on the cluster. Server may not automatically pull in service dependencies in some instances so the full set of compatible services are defined in some embodiments. Next a set of compatible hardware types may be defined. This means only hardware types in the compatible set can be used to create a cluster. Similarly the compatible image types may be defined where only image types in a compatible set can be used to create a cluster.

A cluster template may include a defaults section that describes what will be used to create a cluster if the user does not specifically specify anything beyond the number of machines to use in a cluster. In some embodiments everything in this section can be overwritten by the user though it is likely only advanced users will want to do so. Cluster templates may contain a set of default services a default provider and a default configuration. Optionally a hardware type to use across the entire cluster and an image type to use across the entire cluster may be specified. The default services should be a subset of the services defined in the compatibility section. Similarly if a hardware type or image type is specified it should be one of the types given in the compatibility section. Lastly the configuration usually describes different configuration settings for the services that will be placed on the cluster. In some embodiments the configuration may be separate from the cluster template.

User APIs may be used to perform initiate management operations using preconfigured cluster templates and or cluster configurations. Users make cluster management operation requests through the user APIs . This may ensure that once an administrator sets up the constraints for an organizations clusters users may later perform cluster management operations to create or modify clusters and those cluster management operations will not cause those clusters to deviate from the initially established constraints. Thus the administrator may set up criteria for different layers in a multi tiered application stack and users may later repeatedly provision clusters in accordance with the criteria. Once the cluster template and configuration have been set up users may use them to create as many clusters as desired. For example a user may use the user API with minimal inputs to generate a new cluster that closely or exactly matches other clusters that may have been generated for an organization. The administrator may be guaranteed that the user is not misconfiguring the cluster.

In one embodiment server includes an orchestrator a permissions manager a solver a planner a janitor a task queue and a transaction manager . Alternatively the functionality of one or more of the orchestrator permissions manager solver planner janitor task queue and or transaction manager may be divided into additional modules or may be combined into a single module. Server may additionally include one or more provisioner APIs for interfacing with provisioners A N.

Permissions manager may perform authentication and authorization operations to authenticate users and or determine their permissions. Permissions manager may maintain user accounts in persistent store each of which may be associated with a particular group or tenant. Users may provide credentials to permissions manager to login to a particular user account.

The Permissions Manager is in charge of deciding which users have permission to perform which action on which entity. An entity here is any of the previously mentioned administrator defined cluster entities e.g. hardware types image types services etc. as well as actual cluster instances. Once permissions manager authenticates the user the user is permitted to access data e.g. entities associated with that user s access level and a particular group or tenant associated with that user account. Each user account may have access to one or more entities e.g. cluster templates cluster configurations etc. cluster management operations and so on associated with a particular group or tenant. The existence of other tenants may not be made known to the user. Moreover entities such as cluster templates image types hardware types clusters cluster configurations cluster layouts etc. associated with other tenants may not be visible to the user.

The permissions manager additionally ensures that users can only perform actions they are authorized to perform. For example an admin from a first tenant group A may only read and write entities belonging to the first tenant and not entities belonging to a second tenant group B . For each user and entity pair the permissions manager determines what actions the user can perform on the entity. Some examples of actions are reading writing executing and granting. A read permission gives the user the ability to read a specified entity. A write permission gives the user the ability to change the entity. An execute permission applies to certain types of entities and gives the user the ability to perform one or more operations on those entities. For example an execute permission on a cluster template means the user is able to create a cluster with the template. A grant permission means the user is able to grant other users permission to perform certain actions on the entity. Actions are not limited to those listed above and can include many more.

By default server may support a super admin that has all permissions on all entities. The super admin is able to create admins that have permission to write admin entities and perform cluster operations as well as grant permissions to other users in their group. A group also referred to as a tenant is a set of users that share access to a particular set of entities.

The default permissions policy is not the only possible policy. For example it is possible to create a policy where the super admin only has permission to create regular admins and grant them permission but no permission to write any admin entities or perform any cluster operation. This may be desired if the role of the super admin is simply to delegate responsibility to admins for each user group.

Another policy may limit all admin permissions to be just that of writing admin entities but leave cluster operations completely for users. Admins therefore would not be able to read cluster information or perform cluster operations. A setup like this may be desired if privacy is important and each user needs to make sure only they are able to access their cluster and the data on their cluster. Users may have various access levels to a cluster and or the data on a cluster. A user may have access to specific data in a cluster and or to specific actions. For example a user may have access to perform one or more of view create delete and or modify a cluster. The type of permissions policy implemented by the permissions manager should not be limited to the scope mentioned in the previous examples.

In one embodiment permissions manager hooks into existing user management systems such as lightweight directory access protocol LDAP . The permissions manager in one embodiment has a pluggable interface for integration with existing user management systems such as LDAP. This enables the permissions manager to leverage existing systems to obtain a list of users as well as the group or groups the one or more users belong to.

Cluster management operation requests received by the user API are provided to orchestrator . The orchestrator is able to determine from a high level what is performed when implementing different cluster management operations. Some operations like cluster creation should first be sent to the solver to find a suitable cluster layout for the planner to work with. Other operations like cluster deletion can be sent directly to the planner .

Some examples of types of cluster management operations that may be requested include creating deleting shrinking expanding configuring starting stopping and upgrading clusters. Some of these operations change a cluster layout while others are performed on an existing cluster without any layout change. Orchestrator determines which operations require constraints to be solved and or which operations will change a cluster layout for a cluster. In one embodiment orchestrator determines whether a received cluster management operation will change a cluster layout or require constraints to be solved based on a current cluster state and a nature of the cluster management operation.

In general operations that add services or nodes to a cluster will change constraints or a cluster layout. Operations that remove services from nodes generally will not change constraints or a cluster layout. Some operations may be more complicated and depend on the cluster state. One example of such a cluster management operation is one that modifies dependencies of a service. For example a cluster update operation may change dependencies e.g. may change service dependencies via a service update and these changed dependencies may modify constraints and ultimately cause a cluster layout to change. However a cluster update operation that does not change dependencies may not cause constraints or a cluster layout to change.

Operations that may change a cluster layout such as those that require constraints to be solved are sent to the solver . A cluster layout defines the exact set of nodes for a cluster where each node definition contains which hardware and image types to use as well as the set of services that should be placed on the node. Those operations for which no constraints need to be solved and or that will not change a cluster layout can be directly sent to planner .

In one embodiment the orchestrator also handles the case where the solver determines that an operation is impossible to complete without breaking one or more constraints. In such a case a meaningful error message should be generated and provided to the user and various internal logs and states may be updated accordingly.

Solver is responsible for taking an existing cluster layout or an empty cluster if provisioning a new cluster the cluster template associated with the cluster and in some instances user specified properties or configurations and finding a valid cluster layout that satisfies all inputs and constraints from the cluster template configurations and or existing cluster layout.

As mentioned a cluster template is included with the cluster management operation request. Cluster templates make it so that administrators don t have to specify every single detail for every cluster. To set up a cluster using traditional techniques an administrator finds out that a certain type of cluster needs to be created. The admin then manually gets some hardware installs some operating system on it then installs the necessary software on each node. The admin then goes and manually configures the services on each node then manually starts and initializes the services in the correct order depending on which services depend on which others. Everything is a manual process and small tweaks to the cluster require manual changes and specialized knowledge. For example creating a cluster with five nodes may require a different layout than a cluster with fifty nodes. The administrator must then be involved in creating the five node cluster and then the fifty node cluster. With cluster templates small tweaks are automatically taken care of and manual steps are removed.

Cluster templates also give administrators power and flexibility. An administrator can make a cluster template completely rigid where every service hardware type image type and configuration setting is specified and unchangeable by end users. An administrator can also make a flexible cluster template that allows end users to specify properties they are interested in such as which services should be placed on the cluster and what hardware and image to use.

The cluster template and or user provided configuration may include one or more constraints or may include data that is usable to derive one or more constraints. In one embodiment server supports two classes of constraints layout constraints and service constraints. However the general idea of a constraint based cluster template is not restricted to just these types of constraints. Many additional types of constraints can be thought of and potentially added.

Layout constraints define which services must coexist with other services on the same node and which services cannot coexist on the same node. For example in a Hadoop cluster you generally want data nodes datanode region servers regionserver and node managers nodemanager to all be placed together. To achieve this an administrator would put all three services in the same must coexist constraint.

In one embodiment must coexist constraints are not transitive. For example if there is one constraint saying service A must coexist with service B and another constraint saying service B must coexist with service C this does not mean that service A must coexist with service C. This is to prevent unintended links between services especially as the number of must coexist constraints increase.

In one embodiment if a must coexist rule contains a service that is not on the cluster it is shrunk to ignore the service that is not on the cluster. For example a cluster template may be compatible with data nodes node managers and region servers. However in one example by default only data nodes and node managers are placed on the cluster. In such an embodiment a constraint stating that data nodes node managers and region servers must coexist on the same node would get transformed into a constraint that just says data nodes and node managers must coexist on the same node.

The other type of layout constraint are can t coexist constraints. For example in a Hadoop cluster you generally do not want your name node namenode to be on the same node as a data node datanode . Specifying more than two services in a cannot coexist rule means the entire set cannot exist on the same node. For example if there is a constraint that service A service B and service C cannot coexist service A and service B can still coexist on the same node. Though supported this can be confusing so the best practice is to keep the cannot coexist constraints binary. Anything not mentioned in the must or can t coexist constraints are generally allowed.

Service constraints define hardware types image types and quantities for a specific service that can be placed on the cluster. A service constraint can contain a set of hardware types that a service must be placed with. Any node with that service should or must use one of the hardware types in the set. If the hardware type field is empty for a service the service can go on a node with any type of hardware. Similarly a service constraint can be a set of image types that a service should or must be placed with. Any node with that service should use one of the image types in the set. If the service type field is empty for a service the service can go on a node with any type of image e.g. an image having any type of operating system .

A service constraint can also limit the quantities of a particular service across the entire cluster. The service constraint can specify a minimum and maximum number of nodes that are to contain the service across the entire cluster. A ratio can also be specified stating that a service should be placed on at least a lower threshold percentage of nodes across the entire cluster or at most an upper threshold percentage of nodes across the entire cluster. Other types of constraints are possible. For example a constraint could be added stating that there should always be an odd number of nodes with the specified service or that the service is only allowed if there are at least a specified number of nodes that have another service.

Solver determines all of the layout constraints and service constraints associated with a particular cluster management operation and then solves for a cluster layout that satisfies all of these constraints. In one embodiment there are three stages involved in solving for a cluster layout. The first stage of solving for a cluster layout is finding valid service sets an example of which is illustrated in . The second stage of solving for a cluster layout is finding valid node layouts an example of which is illustrated in . The third stage in solving for a cluster layout is finding a valid cluster layout an example of which is illustrated in . It should be noted that what is described is just one way to find a cluster layout. There are many ways this constraint satisfaction problem could be solved.

The cluster layout solution is deterministic in embodiments. This ensures that given the same set of inputs the same cluster layout result will be achieved. Thus even if there are multiple valid cluster layouts that may satisfy all constraints the same valid cluster layout will be chosen each time given the same set of inputs. Once a cluster layout is created solver may provide this target cluster layout to planner . Solver may also provide a starting cluster layout to the planner in some embodiments. Alternatively the starting cluster layout and or target cluster layout may be stored in persistent store or in memory and planner may access the persistent store or memory to obtain the starting cluster layout and the target cluster layout.

Planner takes a starting cluster layout a target cluster layout and a cluster management operation and creates an execution plan of node level tasks that can be performed in order to perform the cluster management operation. Accordingly planner divides a cluster level operation into a collection of node level tasks.

Planner coordinates which tasks must occur before other tasks and which tasks can be run in parallel. Ordering of tasks is based on action dependencies that are inherent to the type of cluster operation being performed and also based on the service dependencies defined by the administrator. For example when creating a cluster creation of nodes must always happen before installing services on those nodes. That is an example of a dependency that is inherent to the create cluster operation. An example of a dependency derived from services is if service A depends on service B then starting service A must happen after service B is started. In one embodiment the planner examines the cluster layout and action dependencies and creates an execution plan that is a directed acyclic graphed DAG based on the cluster action and cluster layout.

In one embodiment planner arranges node level tasks in a series of stages. Each node level task in a particular stage can be executed in parallel. Thus planner may group tasks that can be run in parallel into stages. Planner may set a current stage determine the node level tasks in the current stage and place those tasks that can currently be run onto task queue for consumption by the provisioners A N. In one embodiment planner does not know how to perform node level tasks. Planner may instead include information identifying results of node level tasks and may rely on provisioners A N to handle implementation. Once all tasks in a stage are complete the planner may place all tasks in the next stage onto the task queue .

Planner may place tasks e.g. all tasks in a stage onto the task queue . Provisioners A N periodically poll the server asking for tasks to execute. If the task queue has tasks the planner will assign a task to the provisioner A N and wait for it to report back with a success or failure. Based on the success or failure the planner can decide to wait for more tasks to finish move on to the next stage or initiate rollback plans in case the cluster operation cannot be successfully completed. Note that in alternative embodiments a data structure other than a queue may be used for keeping track of node level tasks to be performed. Since in some embodiments the order of the node level tasks in the queue is unimportant the task queue may instead be implemented as an unordered set of tasks for example.

Coordination of node level tasks may be performed though provisioner APIs task queue and janitor . The janitor will periodically time out tasks if they have been taken from the queue but the server has not heard back about the task for more than a configurable threshold of time. In case a task fails it may be retried a configurable amount of times. Almost all tasks are idempotent with the exception of the create node task. If a create note tasks fails it is possible that the actual machine was provisioned but there was an issue with the machine. In this case the machine may be deleted before another is created to prevent resource leaks. In case a provisioner fails to reply back with a task failure or success after some configurable timeout the planner will assume a failure and retry the task up to the configurable retry limit. Janitor may run in the background to perform the timeout.

Cluster operations are transactional in embodiments. Accordingly server will try to ensure that either the entire operation completes successfully or it does not complete at all. What this means is that there is a way to roll back changes if an operation is unsuccessful. This is accomplished by the planner working in conjunction with transaction manager .

In one embodiment the planner works with the transaction manager throughout the execution of a cluster management operation to provide a complete audit log of all tasks performed and to store a cluster state for rollback purposes. The audit log may include a record of every single operation. For each stage and each node level task the planner may write an entry to the audit log . For example planner may write to the audit log when a task is placed onto the queue for consumption by a provisioner again when the task is taken from the queue and once more when a provisioner comes back with the task status. Retries may be logged in the same way as separate tasks so a full and complete history is kept. The planner also works with the transaction manager to ensure that each task is tied to the correct cluster transaction so that audits can be performed for periods of time or by cluster action.

Planner may also work with the transaction manager to store job states and or task states . Job states define the state of a cluster management operation. In one embodiment there is one job state for each cluster management operation with each job state containing the status of the job and the DAG of tasks for carrying out the cluster management operation. There can be many statuses for a job. In one embodiment job status is one of running paused completed or failed. Other embodiments may include additional job statuses such as waiting for approval or blocked. Task states define the state of a node level task. In one embodiment there is one task state for each node level task with each task state containing the status of the task and details needed to complete the task. In one embodiment task status is one of submitted in progress successful or failed. Other embodiments may include additional task statuses such as paused or cancelled. Task state also includes details needed to complete the task. In one embodiment the details include the type of node level task such as create node or start service and settings required to complete the task such as provider credentials or service arguments.

The transaction manager ensures that operations are atomic performing rollbacks if needed. Transaction manager also records all actions performed for auditing purposes. The transaction manager is responsible for maintaining and managing cluster state. Transaction manager may store a snapshot of the cluster after each successful cluster operation. The snapshot contains every detail about a cluster and so includes the entire cluster state . For example the snapshot may contain the full cluster layout every single configuration setting hostnames internet protocol IP addresses secure shell SSH keys and many other details.

The transaction manager is able to rollback to any previous cluster state in case of an operation failure. In one embodiment rolling back to a previous state is a functional rollback where the cluster layout is exactly the same but where some node information may change depending on the operation. For example when shrinking a cluster from ten nodes to five nodes it is possible that four nodes are deleted but some issue happens on the fifth node and the operation must be rolled back. In that case the transaction manager is able to tell the planner that it needs to recreate four nodes with specified hardware specified images and specified services on each node with specified configuration settings. However in some embodiments transaction manager does not guarantee that the same IP addresses are used and the same hostnames are used. So functionally the rolled back cluster is the same and the layout is the same but some details may change. After a rollback the cluster state is saved again as a separate entry to preserve the full cluster history.

A cluster operation can fail if a given task is retried past a maximum number of retries configured for the server . If this happens the planner will notify the transaction manager that the cluster operation has failed and the transaction manager will provide the planner with the full state of the cluster prior to the start of the transaction prior to the cluster management operation . The planner is then able to create another execution plan to roll back the state of the cluster.

The planner may create the execution plan to roll back the state of the cluster by creating a new DAG based on the current failed DAG. In one embodiment the planner starts in the current failed stage and for each successfully completed task it creates a corresponding rollback task. The planner then works backwards in the original DAG adding rollback tasks for each successfully completed task. For example for a configure service A task on node X the rollback task would be a configure service A task on node X but with the previous configuration settings as given by the transaction manager . Similarly for an install service B task on node Y the rollback task would be to remove service B from node Y. As another example a create node Z task has a rollback task of delete node Z.

The planner is thus able to work backwards to create a rollback task plan from the original failed plan. Before actually starting the rollback the transaction manager may store a snapshot of the cluster state at the point of failure for historical purposes. The planner then proceeds to coordinate the rollback tasks as before e.g. by dividing the DAG into stages and placing the tasks onto the queue for consumption by provisioners . If the rollback task plan also fails the transaction manager may store a snapshot of the state of the cluster at that moment and mark the cluster for examination by an administrator. Cluster operations can be tried again in the future once the errors have been investigated.

In one embodiment solver may attempt to create a new solution for the cluster management operation after a failure. A particular cluster management operation may be implemented using multiple different valid solutions e.g. multiple different cluster layouts . However one such valid solution is initially selected deterministically. Solver may generate a new constraint that causes the failed cluster layout to become an invalid solution. Thus a different cluster layout will be solved for on the second attempt to create a solution. Planner may then generate a new execution plan for the new cluster layout.

The provisioners A N are worker daemons that interface with server through provisioner APIs . Provisioners A N are responsible for taking node level tasks from the server executing the tasks and reporting back to the server whether or not the tasks were successfully performed. A provisioner A N may poll server for a task and then receive assignment of a node level task from planner . The provisioner A N then executes the received task e.g. by invoking an appropriate task handler plugin for the task . Once the received task is executed the provisioner A N reports back results of the operation including an indication of success or failure and any appropriate metadata e.g. IP address host name etc. .

At a high level provisioners A N simply perform tasks given to them by planner . Alternatively provisioners A N provide an API to manage data and worker processes. Provisioners A N may each manage one or more worker processes also referred to herein as workers or worker daemons . Workers are lightweight daemons that perform tasks and poll the server. In an embodiment that uses workers the worker processes perform operations such as server polling and tasks given to the provisioners A N by planner . In the below description embodiments are described in regards to a provisioner polling the planner receiving tasks and performing operations to complete those tasks. However it should be understood that all such actions and operations that are described as being performed by the provisioners A N may instead be performed by workers that are managed by the provisioners.

These are the tasks that are performed to orchestrate cluster operations. Such tasks may include for example provisioning nodes e.g. creating nodes from cloud providers installing configuring software e.g. bootstrapping nodes configuring services on nodes starting services on nodes initializing services on nodes stopping services running custom commands and so on. Each instance of the provisioner A N may poll server for the next task in the queue. Once a provisioner A N receives a node level task that provisioner handles the node level task to completion. In one embodiment a plugin framework is utilized to handle any task for extensibility. In one embodiment the provisioners A N are lightweight and stateless. Therefore many provisioners A N can be run in parallel.

In one embodiment each provisioner A N manages one or more worker processes. Each worker process may be capable of performing a node level task e.g. invoking an appropriate plugin to perform a node level task . The workers may poll the planner to request node level tasks. When a worker completes a node level task the worker may report the completion to the planner .

Each running provisioner A N instance may continually poll the server for tasks. In one embodiment when a task is received it consists of a Javascript object notation JSON task definition. This task definition may contain all the information needed by the provisioner A N to carry out the task.

Consider a typical scenario for provisioning a node on a cloud provider asynchronously. A node having given attributes is requested. These attributes may include a node size an operating system to be installed on the node a region etc. The provisioner uses a provider API to cause a provider to perform a requested action on the node. The provider accepts the request and returns an internal ID for a new node it is going to create. During creation the requesting provisioner may continually poll the provider for the new node s status and public IP address using the internal ID. The requesting provisioner then may perform some additional validation using the IP address and declares success. The internal provider ID may be used later if the node is ever to be deleted. Similarly the IP address provided for the node may be used in subsequent tasks.

In one embodiment provisioners A N perform tasks by determining a correct provisioner plugin appropriate for the task and then using that correct provisioner plugin to complete the task. There are two types of provisioner plugins in one embodiment provider plugins and automator plugins. Provider plugins are used to allocate delete and manage machines e.g. virtual machines using different infrastructure providers such as OpenStack Rackspace Amazon Web Services Google Compute Engine and Joyent.

Automator plugins are responsible for implementing the various services defined on a cluster. For example a Chef automator plugin could be used to invoke Chef recipes that install configure initialize start or stop a service. Various plugins may be implemented to support desired technologies such as a Puppet plugin a Docker plugin or even shell commands.

In one embodiment provisioners A N are not directly installed on the target host or node but rather use SSH e.g. SSHD to interact with the remote host making the described architecture simple and secure. Moreover no software associated with cluster management or the provisioners e.g. no server components provisioners daemons etc. may be installed on the hosts for nodes. This may enable provisioners to interface with any provider without having to install special software on those providers for node creation service installation and or cluster management.

Since multiple provisioners can work concurrently this layer of provisioners support execution of thousands of concurrent tasks. Provisioners A N can also be managed by the server to automatically scale according to workload.

Planner may track queue length completion time for tasks and or other metrics. Planner may automatically increase or decrease the number of provisioners and or workers that are available for a user or tenant based on one or more criteria in view of these metrics. For example if a queue length is consistently high it can be an indication that there are not enough workers to handle the workload. As another example if the task queue is constantly growing in size over a long period of time it is a strong indication that there are not enough workers to handle a normal workload. Based on the size of the queue and or some other possible metrics such as average task completion time and rate of growth for the queue the server can estimate how many more workers are needed to handle the workload and spin them up directly or notify an administrator with the suggested number of workers to add. Similarly if the task queue is consistently empty it may be an indication that there are more workers running than required to handle the normal workload. Based on metrics like the average time a task stays in the queue before being taken by a worker the server can estimate how many workers are actually required and either shut down some running workers or notify an administrator with the suggested number of workers to shut down. Thus server may monitor metrics such as average queue length and average time to complete node level tasks and may then determine an appropriate number of workers based on these metrics. The server may then increase or decrease the number of available workers accordingly.

Periodically updates for plugins used by provisioners A N are created. Additionally or alternatively new plugins may be added to the provisioners existing plugins may be removed from provisioners and or different plugin versions may be selected for provisioners. Server and provisioners A N may coordinate with one another to synchronize plugins data user by plugins and or other resources of the provisioners. In one embodiment planner sends an update message to provisioners A N that causes the provisioners to pause some or all tasks and or to stop taking new tasks. The plugins and or the data used by the plugins in the provisioners A N may then be updated. Once the update is complete the provisioners may resume existing tasks and or resume receiving new tasks.

In one embodiment there are different classes of provisioners A N. Each provisioner class may be responsible for a subset of possible node level tasks. For example provisioner A may be responsive for creating and deleting nodes and provisioner B may be responsible for installing configuring starting etc. services. In such an embodiment provisioners of a first class may have different plugins from provisioners of a second class.

If multiple classes of provisioners are used planner may determine class of a provisioner requesting a task and assign an appropriate task based on the provisioner task. In one embodiment to ensure that appropriate tasks are assigned to provisioners separate queues are maintained for tasks associated with different provisioner classes. Alternatively flags may be included in task entries in a queue where the flags indicate a provisioner class that may be assigned to the tasks.

In one embodiment the server is a multi tenant server. Accordingly a single instance of the server may be presented to multiple tenants as if the server manages clusters only for that tenant. Server may divide resources and apportion percentages of those resources to each tenant. Each tenant may be guaranteed that resources of the server will meet specified quality of service and availability constraints specified by the tenant. Accordingly solver and planner may each concurrently perform respective operations of creating a cluster layout and creating an execution plan for multiple tenants.

For embodiments in which multi tenancy is implemented a separate task queue may be maintained for each tenant. Additionally each tenant may have its own cluster templates hardware types image types services and plugins. The cluster entities clusters users task queues etc. of one tenant are not viewable to another tenant. Thus each tenant may be completely unaware of the fact that the server is serving multiple different tenants.

Each tenant can select which plugins they want to use for provisioners. Since provisioners associated with a first tenant may differ from provisioners associated with a second tenant due to having different plugins each plugin instance is assigned to a particular tenant.

In an example server may receive a first cluster management operation request from a first tenant and concurrently receive a second cluster management operation from a second tenant. The solver may separately determine a first cluster layout for the first cluster management operation and a second cluster layout for the second cluster management operation. The planner may then separately determine a first execution plan for the first cluster management operation and a second execution plan for the second cluster management operation. The planner may populate a first task queue with node level tasks of the first execution plan for the first tenant and may populate a second task queue with node level tasks of the second section plan for the second tenant. The planner may then coordinate with a first set of provisioners assigned to the first tenant for carrying out the first cluster management operation and may at the same time coordinate with a second set of provisioners assigned to the second tenant for carrying out the second cluster management operation. Throughout all of these steps the first tenant is not aware of the work being performed for the second tenant and the second tenant is not aware of the work being performed for the first tenant.

For simplicity of explanation the methods are depicted and described as a series of acts. However acts in accordance with this disclosure can occur in various orders and or concurrently and with other acts not presented and described herein. Furthermore not all illustrated acts may be performed to implement the methods in accordance with the disclosed subject matter. In addition those skilled in the art will understand and appreciate that the methods could alternatively be represented as a series of interrelated states via a state diagram or events.

At block processing logic determines constraints that should be followed for the cluster management operation. These constraints may include a set of layout constraints and or a set of service constraints. The constraints may be determined from the cluster template and or the provided configuration data.

At block processing logic solves for a cluster layout based on the determined constraints and the provided number of nodes. At block processing logic then outputs the created cluster layout. This cluster layout may then be sent to a planner and used to automatically provision a new cluster having the cluster layout.

At block processing logic determines whether the cluster management operation will modify an existing cluster layout of the cluster. Cluster management operations that will not modify the cluster layout may be sent directly to a planner without solving for a new cluster layout. Cluster management operations that will cause the cluster layout to change are sent to a solver so that the solver can solve for a new target cluster layout.

At block processing logic determines constraints from a cluster template included in the cluster management operation request and or in configuration data provided with the cluster management operation request. At block processing logic also determines additional constraints from the existing cluster layout. Modifying the existing cluster layout could cause an interruption of an already deployed multi tiered application. However by using the existing cluster layout to define additional constraints processing logic ensures that only minimal changes will be made to the existing cluster layout e.g. only changes that need to be made in view of new constraints thus minimizing or eliminating such an interruption.

At block processing logic solves for a new cluster layout based on the constraints the additional constraints and the number of nodes specified in the cluster management operation request. At block processing logic outputs the new cluster layout. Processing logic may also output the starting preexisting cluster layout . A planner may then determine a sequence of node level operations that should be performed to transition the cluster from the starting cluster layout to the new target cluster layout.

In an example we define N as the number of services that must be placed on the cluster and n as the number of services in a particular service set. For each n from 1 to N the solver goes through every possible service combination and checks if the service combination is valid given the constraints defined in the cluster template. If the service set is valid it is added to the list of valid service sets .

An example of solving for valid service sets for a cluster with three serves s s and s services is shown in . In this example there are three layout constraints . The constraints include a must coexist constraint for s and s a cannot coexist constraint for s and s and a cannot coexist constraint for s and s.

We start with a first service set of three services n 3 which has only one combination s s s . This first service set is invalid because it violates the constraint that s cannot coexist with s so it is not added to the valid service sets . Next the solver moves on to service sets with two services n 2 which has three combinations. Of these service set s s is invalid because s cannot coexist with s. Service set s s is valid because it satisfies all the constraints . Accordingly service set s s is added to the valid service sets . Service set s s is invalid because s cannot coexist with s.

Finally the solver moves on to service sets that include only a single service n 1 which has 3 possibilities. Service set s is invalid because s must coexist with s. Service set s is valid because it satisfies all the constraints and so is added to the valid service sets . Service set s is invalid because s must coexist with s. Thus we end up with two valid service sets in this scenario s s and s. If there are no valid service sets there is no solution and the cluster operation fails.

For each valid service set each combination of service set hardware type and image type is examined. If the node layout satisfies all service constraints it is added to a valid node layout . After that if there are multiple valid node layouts for a service set one valid node layout is chosen for a final node layout and the others are discarded.

In one embodiment which node layout is chosen is determined by a comparator that compares node layouts. In this embodiment the comparator deterministically chooses a node layout based on preferred image and hardware types in order to ensure that the same image is chosen across the entire cluster when possible and to prefer cheaper hardware when possible. In one embodiment the comparator is a pluggable component so different users can define their own comparator to match their needs.

In the example shown in there are two hardware types that can be used hw and hw. Also there are two image types that can be used img and img. The starting valid service sets s s and s are taken from the previous example. Every possible node layout combination of service set hardware type and image type is examined. Since there are two hardware types and two image types this means there are four possible node layouts for each service set. Service set s s includes node layouts and service set s includes node layouts .

Each node layout is checked against the service constraints . In this example the service constraints specify that s must be placed on a node with hw and s must be placed on a node with img. After each possible node layout is examined the solver ends up with four valid node layouts. The valid node layouts are and . There are two valid node layouts for each service set which enables the solver to narrow down the final set until we end up with two final node layouts and . One layout is then chosen deterministically e.g. by a pluggable comparator .

After the final set of node layouts is determined the solver finds how many of each node layout there should be based on the number of nodes in the cluster. It does this by first ordering the node layouts by preference then searching through every possible cluster layout until it finds a cluster layout that satisfies all constraints . The search is done in a deterministic fashion by trying to use as many of the more preferred node layouts as possible. Again the preference order may be determined using a pluggable comparator.

In the illustrated example the cluster will have five nodes and there is a first constraint that s must only be placed on one node and a second constraint that there must be at least one node with s. The comparator decides that the node layout with s and s is preferred over the node layout with just s. The search then begins with as many of the first node as possible. At each step if the current cluster layout is invalid a single node is taken away from the most preferred node and given to the next most preferred node. The search continues in this way until a valid cluster layout is found or until the search space is completely exhausted. In embodiments there are some search optimizations that may occur. In the example there can only be at most one node of the first node layout since there can only be one node with s. Solver can therefore skip ahead to a cluster layout with only one of the first node layout and continue searching from there. Once a valid cluster layout has been found it is sent to the planner to determine what node level tasks should be performed to execute the cluster operation. If no valid cluster layout is found the operation fails.

It should be noted that the above examples only illustrate a small number of constraints whereas many more constraints are possible. In fact when shrinking and expanding a cluster or when removing or adding services from an existing cluster the current cluster itself may be used as a constraint. That is in one embodiment the hardware and image types on existing nodes should not change and are enforced as constraints. Similarly services uninvolved in the cluster operation are not allowed to move to a different node in embodiments. Thus when a cluster is expanded for example the current nodes in the cluster may be unchanged.

For simplicity of explanation the methods are depicted and described as a series of acts. However acts in accordance with this disclosure can occur in various orders and or concurrently and with other acts not presented and described herein. Furthermore not all illustrated acts may be performed to implement the methods in accordance with the disclosed subject matter. In addition those skilled in the art will understand and appreciate that the methods could alternatively be represented as a series of interrelated states via a state diagram or events.

At block processing logic determines node level tasks that can be performed to transition the cluster from the starting cluster layout to the target cluster layout. Such node level tasks may include for example create node install service configure service initialize service start service and so on.

At block processing logic determines dependencies between the node level tasks. For example an install service task may depend on a create node task and or on another install service task on an configure service task and so on. At block processing logic determines an order for the node level tasks based on the dependencies and the cluster management operation. The same tasks may have different orders based on different cluster management operations. For instance in one embodiment when starting all services on a cluster if service A depends on service B service B must be started before service A is started. However in one embodiment when stopping all services on a cluster service A must be stopped before service B is stopped. In the first operation start the dependency between service A and service B results in the task for service B occurring before the task for service A. In the second operation stop the dependency results in the task for service A occurring before the task for service B.

At block processing logic creates an execution plan that divides the cluster management operation into the ordered node level tasks. In one embodiment the execution plan is a directed acyclic graph DAG of the node level tasks. The DAG may show numerous tasks that can be run in parallel as well as tasks that should be run in sequence. In one embodiment processing logic divides the node level tasks into stages block . All node level tasks in a particular stage may be performed concurrently and the order in which any of these node level tasks are started or completed may be unimportant.

At block processing logic populates a task queue with node level tasks for a current stage. At block processing logic receives a request for a node level task from a provisioner. Alternatively processing logic may proactively identify an idle provisioner and assign a node level task to that provisioner without waiting for the provisioner to request a task.

At block processing logic assigns a node level task from the task queue to the provisioner. Alternatively processing logic may make the task queue visible to provisioners and provisioners may select node level tasks from the task queue. In such an embodiment the provisioners may notify a planner that they have selected a particular node level task.

At block processing logic determines if there are any additional node level tasks in the task queue. If so the method returns to block so that processing logic can assign additional node level tasks from the task queue to provisioners. If there are no node level tasks left in the task queue and all provisioners have reported completion of their assigned node level tasks then all of the node level tasks of a current stage have been completed. Accordingly the method proceeds to block .

At block processing logic determines whether there are any additional stages in the execution plan. If there are not additional stages then the cluster management operation has been completed and the method ends. If there are additional stages then the method continues to block and processing logic proceeds to the next stage in the execution plan. At this point processing logic populates the task queue with the node level tasks in the new stage and then returns to block to assign the tasks.

As previously mentioned a transaction manager may manage a cluster management operation as a transaction. Thus a log may be created for each of the node level tasks that are performed to execute the cluster management operation. If at any point during execution of the cluster management operation a node level task fails then the entire cluster management operation may in turn fail. It may then be desirable to roll back the cluster management operation to a starting state of the cluster before the cluster management operation was performed.

At block processing logic determines whether the node level task has failed a threshold number of times. A node level task that has failed more than once may be indicative of a problem with the task itself rather than with a particular provisioner that was assigned the task. If the node level task has failed the threshold number of times e.g. once twice five times etc. the method continues to block . If the node level task has not yet failed the threshold number of times the method proceeds to block and the node level task is reassigned to a different provisioner.

At block processing logic determines a new cluster management operation to roll back the original cluster management operation. At block processing logic determines new node level tasks to transition from an intermediate cluster layout to the starting cluster layout. The intermediate cluster layout represents a state of the cluster at the time that the node level task failed. The intermediate cluster layout may not match either the starting cluster layout or the target cluster layout.

At block processing logic determines dependencies between the new node level tasks. At block processing logic determines an order of the new node level tasks. At block processing logic creates a new execution plan to execute the new cluster management operation and roll back the original cluster management operation. The new node level tasks may be inverses of original node level tasks that were executed as part of the execution plan to transition the cluster from the starting cluster layout to the target cluster layout. Some node level tasks in the execution plan may not need to be undone or redone. For example a restart node level task that was performed as part of the execution plan may not have an inverse that needs to be performed to roll back the cluster management operation. Accordingly the new execution plan may not simply be an inverse of the original execution plan.

Accordingly the DAG starts with node level tasks of create node create node create node create node and create . The node level tasks of install service s and install service s follow from create node . The node level task of install service s follows from create node . The node level task of install service s follows from create node . The node level task of install service s follows from create node . The node level task of install service s follows from create node .

The node level task of configure service s follows from the node level task of install service s . The node level task of configure service s follows from the node level task of install service s . The node level task of configure service s follows from the node level task of install service s . The node level task of configure service s follows from the node level task of install service s . The node level task of configure service s follows from the node level task of install service s . The node level task of configure service s follows from the node level task of install service s .

The node level task of initialize service s follows from the node level task of configure service s . The node level task of initialize service s follows from the node level task of configure service s . The node level task of initialize service s follows from the node level task of configure service s . The node level task of initialize service s follows from the node level task of configure service s . The node level task of initialize service s follows from the node level task of configure service s .

The node level task of start service s follows from the node level task of initialize service s . The node level task of start service s follows from the node level task of initialize service s . The node level task of start service s follows from the node level task of initialize service s . The node level task of start service s follows from the node level task of initialize service s . The node level task of start service s follows from the node level task of initialize service s .

The node level task of initialize service s depends from node level task configure service s start service s start service s start service s start service s and start service s . Start service s then follows from the node level task of initialize service s .

In the above example many of the tasks can be performed in parallel while some tasks can only be performed after others have completed. For example all of the create node tasks can be done in parallel but the install s task on node can only be done after the create node task has completed successfully. In one embodiment the planner takes the DAG and divides it into stages based on what can be done in parallel.

For example in the first iteration all the create node tasks are sources and are therefore grouped into a first stage . Once the create node tasks and their edges are removed from the DAG the next iteration begins. In one embodiment all the install tasks are identified as sources and grouped together into a second stage . All configure node tasks are included in a third stage . The initialize s task and all initialize s tasks are grouped into a fourth stage . The start s task and all start s tasks are grouped into a fifth stage . A sixth stage is created for the initialize s task. A seventh stage is created for the start s task .

In example the first stage includes all create node tasks . The second stage includes the install s task and all install s tasks . The third stage includes the install s task . The fourth stage includes the configure s task . The fifth stage includes the configure s task and all configure s tasks . The sixth stage includes the initialize s task and all initialize s tasks . The seventh stage includes the start s task and all start s tasks . The eighth stage includes the initialize s task . The ninth stage includes the start s task .

Each task in a stage can be performed concurrently and in one embodiment all tasks in a stage must be completed before moving on to the next stage. That is tasks in stage i 1 are not performed until all tasks in stage i have completed successfully. Note that this staged approach is not the only way to coordinate execution of the tasks. For example from the original DAG there is nothing wrong with performing the install s task on node once the create node task has completed but the staged approach will wait until all other create node tasks have completed before performing the install s task. Execution order and parallelization can be done in many ways this is just one way to do it.

For simplicity of explanation the methods are depicted and described as a series of acts. However acts in accordance with this disclosure can occur in various orders and or concurrently and with other acts not presented and described herein. Furthermore not all illustrated acts may be performed to implement the methods in accordance with the disclosed subject matter. In addition those skilled in the art will understand and appreciate that the methods could alternatively be represented as a series of interrelated states via a state diagram or events.

At block of method processing logic requests a node level task from a server e.g. from a planner of the server . At block processing logic receives an assignment of a node level task from the server. Alternatively processing logic may access a task queue and select an available task from the task queue. In such an instance processing logic may notify the server of the selected node level task.

At block processing logic determines a plugin capable of performing the assigned or selected node level task. At block processing logic invokes the plugin and uses the plugin to perform the assigned node level task. At block processing logic then reports a result e.g. a success or failure of the node level task to the server. Processing logic may also report appropriate metadata associated with a success or a failure.

At block processing logic requests a new node level task. At block if the cluster management operation is complete then there are no additional tasks to be assigned and the method ends. If at block the cluster management operation is not complete the method returns to block and a new node level task assignment is received.

The provisioner then executes the node create request through a provider API for the specific provider and receives a provider response including the new node s provider ID e.g. abc321 as a result. Since this provider ID will used for future operations against this node provisioner may report the provider ID back to server . This report may additionally include the task ID of the node level task and a status of the task. In one embodiment the provider ID is reported by populating a result key value hash in a task result JSON. The server may preserve these key values in a config hash on all subsequent tasks for this node.

A subsequent node level task e.g. a CONFIRM task may then be sent from the server to the provisioner . This subsequent node level task may include the provider ID for the node the task ID and a task name as well as additional information. Provisioner may then use the provider API to send a new provider API request as to the status of the node. Provider may then provide a provider response including a status of the node. The provider response may include an IP address of the node. The provisioner may then report a result obtained from the provider back to server . This result may include the task ID status the IP address and or additional information.

Server may additionally assign a third node level task to provisioner . The request for the third task may include all metadata discovered thus far about the node e.g. the provider ID for the node and the IP address for the node . In this way the server is building up a persistent payload of metadata about a node which can be used by any subsequent task. For example many providers send back an identifier upon creation of a node. The DELETE task may use this provider specific identifier in order to tell the provider to delete the node. In this case the CREATE task would send back the provider identifier in its payload so that the DELETE task can use it when it eventually runs. As another example the IP address of a node may not be known until completion of the CONFIRM task. However future services may need to be configured using the IP address so provisioners can return the IP address in the payload to provide the information to future tasks.

In addition to this payload of key value pairs provisioner and or server may also automatically provide additional metadata regarding cluster layout. For example once the nodes of a cluster are established server may include a nodes hash in a task JSON which contains the hostnames and IP addresses of every node in the cluster. This can be readily used by any task requiring cluster information. For example node level tasks for configuring software on a node may need a list of all peer nodes in the cluster. Accordingly such information may be included with such node level tasks.

A plugin is a self contained program designed to perform a specific set of tasks. Plugins may be written in Ruby Javascript Perl or other languages. Each plugin may have a name and a type. The name uniquely identifies each plugin while the type groups related plugins together. The type also corresponds to the list of tasks the plugin is capable of handling.

Referring to two tasks being consumed by provisioners A B are shown. Provisioner A and provisioner B are instances of the same provisioner. Provisioner A receives node level task and provisioner B receives node level task . Each provisioner A B includes all of the plugins that might be needed for any task associated with a cluster management operation for a tenant. In the illustrated example each provisioner includes a provider plugin for the cloud provider Rackspace and a provider plugin for the cloud provider Joyent. Each provisioner additionally includes an automator plugin for the Chef application and an automator plugin for the Shell application. However other applications than Chef may also be used.

The provisioners A B each invoke an appropriate plugin to perform the received node level task. When a task is received the provisioner A B first determines from the taskName which type of plugin is required to handle the task. In the first example task includes a CREATE taskName which indicates that the task should be handled by a provider plugin. Provisioner A then checks the task e.g. a task JSON for the providertype field to determine which plugin to invoke. The determined provider type in the example is Rackspace and so provider plugin is invoked and used to interface with the Rackspace API . In the second example task includes an INSTALL taskName which indicates the task should be handled by an automator plugin. Provisioner B then checks the task e.g. the task JSON for the service action type field to determine which plugin to invoke. The determined provider type in the example is Chef and so automator plugin is invoked and used to interface with the Chef application to install the Chef service on the node . Alternatively other services than the Chef service may be used.

In one embodiment each plugin provides a descriptor file in which it declares its name type and execution class. Upon startup the provisioner may scan its own directories looking for these descriptor files. Upon successful verification the plugin is considered registered.

In one embodiment a plugin can contain any arbitrary data it uses to perform its tasks. For example a provider plugin may store API credentials locally or a Chef plugin may keep a local repository of cookbooks. This data can be packaged with and considered as part of the plugin. Alternatively API credentials and other arbitrary data that a plugin might use to perform its tasks may be managed centrally by the server. The server may manage such data per tenant and distribute data for a tenant to appropriate provisioners associated with that tenant. A plugin may also specify certain configuration parameters that it expects to be filled in by users e.g. through a user interface . For example there are variances among cloud providers regarding the credentials needed to access their API. Some require a password some require a key on disk etc. The provisioner allows a plugin to specify the necessary configuration fields so that an admin can simply fill in the values. Then when a task is received by that particular plugin it will have the key value pairs it expects.

This plugin model is useful for supporting many providers and custom installation procedures. It makes it easy to leverage existing provider plugins or community code as plugins within the provisioners.

The example computing device includes a processing device a main memory e.g. read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM etc. a static memory e.g. flash memory static random access memory SRAM etc. and a secondary memory e.g. a data storage device which communicate with each other via a bus .

Processing device represents one or more general purpose processors such as a microprocessor central processing unit or the like. More particularly the processing device may be a complex instruction set computing CISC microprocessor reduced instruction set computing RISC microprocessor very long instruction word VLIW microprocessor processor implementing other instruction sets or processors implementing a combination of instruction sets. Processing device may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like. Processing device is configured to execute the processing logic instructions for performing the operations discussed herein.

The computing device may further include a network interface device . The computing device also may include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse and a signal generation device e.g. a speaker .

The data storage device may include a machine readable storage medium or more specifically a computer readable storage medium on which is stored one or more sets of instructions embodying any one or more of the methodologies or functions described herein. The instructions may also reside completely or at least partially within the main memory and or within the processing device during execution thereof by the computer system the main memory and the processing device also constituting computer readable storage media.

The computer readable storage medium may be used to store instructions for a solver a planner a provisioner etc. and or a software library containing methods that call such a solver planner and or provisioner . While the computer readable storage medium is shown in an example embodiment to be a single medium the term computer readable storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term computer readable storage medium shall also be taken to include any medium other than a carrier wave that is capable of storing or encoding a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies described herein. The term computer readable storage medium shall accordingly be taken to include but not be limited to the non transitory media including solid state memories and optical and magnetic media.

The modules components and other features described herein can be implemented as discrete hardware components or integrated in the functionality of hardware components such as ASICS FPGAs DSPs or similar devices. In addition the modules can be implemented as firmware or functional circuitry within hardware devices. Further the modules can be implemented in any combination of hardware devices and software components or only in software.

Some portions of the detailed description have been presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as receiving determining solving outputting creating requesting performing reporting or the like refer to the actions and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

Embodiments of the present invention also relate to an apparatus for performing the operations herein. This apparatus may be specially constructed for the discussed purposes or it may comprise a general purpose computer system selectively programmed by a computer program stored in the computer system. Such a computer program may be stored in a non transitory computer readable storage medium such as but not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs random access memories RAMs EPROMs EEPROMs magnetic disk storage media optical storage media flash memory devices or other type of machine accessible storage media.

It is to be understood that the above description is intended to be illustrative and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. Although the present invention has been described with reference to specific example embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense. The scope of the invention should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

