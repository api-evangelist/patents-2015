---

title: Disaster recovery framework for cloud delivery
abstract: In an example embodiment, an instance of a first database and an instance of a first application in a first data center of a managed cloud are replicated to a second data center of the managed cloud. Then state information regarding the first application is stored in a network file system in the first data center. Interactions between a user and the first application are directed to the instance of the first application in the first data center. The state information is then updated based on the interactions, and any changes to the instance of the first database based on the interactions are replicated to the second data center. Then a disaster is detected in the first data center, and all interactions between the user and the first application are redirected to a second instance of the first application in the second data center caused by the replication of the instance of the first application in the first data center.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09639439&OS=09639439&RS=09639439
owner: SAP SE
number: 09639439
owner_city: Walldorf
owner_country: DE
publication_date: 20150414
---
This document generally relates to methods and systems for use with computer networks and customer business solutions deployed in it. More particularly this document relates to a disaster recovery framework for cloud delivery.

High availability and disaster recovery are key elements of a business continuity service. High availability protects against a single point of failure by introducing technical measures and related services that safeguard application continuity from a business perspective in case of occurrence of this single failure. The target is to ensure the maximum business continuity in the event of a failure by either avoiding or at least minimizing the unplanned outage.

Disaster recovery helps ensure that in the event of a total failure of a particular data center there remains the ability to ensure maximum business continuity by providing a secondary site. The target is to ensure the maximum business continuity in the case of a disaster recovery occurrence by introducing technical and organizational measures as well as procedures to allow recovery in the fastest possible timeframe of all the business functions that are vital to the customer. In contrast to high availability disaster recovery is embedded in an overall disaster recovery plan that addresses and prepares not only the technical measures but also the organizational procedures for all involved parties.

The description that follows includes illustrative systems methods techniques instruction sequences and computing machine program products that embody illustrative embodiments. In the following description for purposes of explanation numerous specific details are set forth in order to provide an understanding of various embodiments of the inventive subject matter. It will be evident however to those skilled in the art that embodiments of the inventive subject matter may be practiced without these specific details. In general well known instruction instances protocols structures and techniques have not been shown in detail.

In an example embodiment aspects are provided that describe a disaster recovery architecture framework that recognizes the optimal realization of a business continuity service. The disaster recovery architecture framework allows a standardized setup and delivery of disaster recovery for any product in a managed cloud delivery environment.

In an example embodiment disaster recovery is achieved on the database level via replication mechanisms. For each database in a product there is a dedicated server available in a second location hosting a replication target of the primary database which is the replication source. The replication target is continuously kept up to date by the replication source regardless whether the source is setup as singleton or in a high availability setup. Thus in the case of a disaster affecting the database the database at the second location is ready to take over.

In an example embodiment disaster recovery is achieved on the application tier via storage replication. For each service in a product there is a dedicated server available at a second location to perform the same service. Those two servers are peers together and any change applied on the server on the primary side is automatically mirrored to the second server on the disaster recovery side. Any application data not stored on the database side may be stored in a network file system. Asynchronous replication of the network file system may be performed continuously via for example storage mirroring technology. In the case of a disaster affecting the primary server the server at the second location is in principle ready to provide the same services.

Customers may not wish to invest twice in both a high availability setup and a disaster recovery setup. In an example embodiment the setups may be shared using the same hardware considering the distance between the primary site and the secondary site.

The cloud integration component may also comprise additional components other than the infrastructure controller . A data security component may manage the data security of the cloud . A development and lifecycle management component may manage the development deployment and updates for various applications in the cloud including for example applications A D. A solution manager may be positioned as a monitoring tool used for solution availability monitoring. A connectivity service may manage connectivity between the users A C and the cloud . A governance and service level agreements SLAs component may manage various rules and regulations for cloud data including adherence to one or more SLAs. A monitoring and administration component may monitor conditions in the cloud and generate alerts as well as providing an interface to the landscape virtualization manager to allow the administrator to alter conditions for the monitoring and or alerts. All these management components refer to the primary side which is by default the primary data center but may get failed over to the secondary data center in the case of a disaster . The disaster recovery architecture is able to foresee that all the central cloud managing entities while applied on the primary side are also then automatically or semi automatically applied to the secondary side in order to ensure that the secondary side can become the primary side in the event of an disaster in the fastest possible way.

In order to understand the overall disaster recovery architecture it is useful to understand how customer landscapes are set up in a managed cloud. is a block diagram illustrating a managed cloud delivery customer landscape system and its connection to a remaining on premise deployment in accordance with an example embodiment. The managed cloud delivery customer landscape system includes one or more customers A B and C which may be businesses or other large organizations as well as a managed cloud. Access from the customers A B and C is channeled through one or more management networks which is an extension of the customer s on premise network. The customer by this always only can access their network. On the other hand the cloud Service provider has to access all of this customer networks. This is done by establishing the communication server as the single entry point to the customer network from cloud management network side. There is virtually one management network that is physically separated in the different data center locations. This virtual one management network shares one virtual administrative infrastructure . The access control may be managed by for example a landscape virtualization manager such as the landscape virtualization manager of . While high availability shall be realized from within a single customer network in an example embodiment disaster recovery as well as an extension of disaster recovery is realized using a dedicated additional customer network each having its own data center specific domain. Thus while each network domain contains the data center specific appendix the domain used by the customer is cross data center and thus data center domain independent. In the event of an disaster the URLs towards customer remain stable while the mapping of the customer facing URL to the data center specific domain is done via respective DNS management.

Each management network A B could be thought of as a different data center perhaps located in a geographically distant location from the other data center. Each data center and hence each management network A B may be assigned its own domain. In order to derive the domain in an example embodiment respective master data is modeled available and retrievable from respective services and considered in the respective operations entities such as the DNS management. Network creation may be initiated by for example the infrastructure controller of . All relevant information may be stored by the infrastructure controller .

In cases where the systems e.g. application servers are distributed equally between multiple management networks A B and accessible in parallel for customer usage the domain of each management network A B is different.

In an example embodiment for end to end disaster recovery five phases maybe implemented. In the context of the failover real disaster the key performance indicators KPIs of recovery point objective RPO and recovery time objective RTO determine the overall service quality is a diagram illustrating the operations phases of high availability in accordance with an example embodiment. These phases include a prepare phase a detect phase a recover phase a ramp up phase or performance ramp and a failback phase . In the prepare phase actions may be undertaken to prepare a network for the disaster recover setup. These actions may be designed based on the business objective for data loss e.g. recovery point objective and the business objective for the time to recover from a single failure e.g. recovery time objective . This may include setting up both databases and applications according to the disaster recovery architecture in separate networks. In an example embodiment this setup may be performed by one or more automation scripts in order to achieve high standardization. In an example embodiment the setup may include replicating databases and or applications onto different servers within the network as well as establishing formalized procedures for synchronizing the databases in replication mode e.g. replicate when a change occurs as well as the applications leveraging storage mirroring and applications replicate at a periodic rate etc. . Aspects of this replication will be described in more detail below.

In the detect phase a standard monitoring operations architecture is altered to include specialized monitoring of all systems relevant for high availability as defined by the customer. Alerts may be established that clearly outline the current issue and data needs as well as defining the proper problem resolution. Additionally the alerts may define the impact to service quality such as SLA violations due to unplanned unavailability.

In an example embodiment managed cloud delivery primary database instances are continuously monitored but this standard monitoring is enhanced to check the state and the replication status of the secondary replication target database s instance for high availability setup as well as disaster recovery setup. This helps to ensure that the secondary instance is always in the desired state and ready to take over whenever a failure occurs. Monitoring of availability metrics can also be performed on the application side. Depending on the kind of failure the issue can be captured either by the redundant service e.g. dialog instance or by the explicitly triggered failover e.g. central services for high availability. For disaster recovery an outage affecting more than one single entities is occurring. Also in this case monitoring would detect this but in contrast of restoring the single service a failover to the disaster recovery side would be initiated.

In the recover phase an automated process may be executed by leveraging as much as possible the adaptive computing principles and functionality embedded into the landscape virtualization manager of as well as additional cloud automation principles to allow a failover transparent of the customer with the fulfillment of the relevant service KPIs in regards to Recovery Point Objective and Recovery Time Objective. As will be described in more detail below for in memory databases additional in memory database specific automation content may be put in place to achieve the same level of automation for all different components on the database and application levels.

In the ramp up phase the objective is to keep the ramp up of the secondary instance as short as possible to allow business operations to run as much as possible with minimal interruption. In an example embodiment the ramp up phase may be integrated into a proactive monitoring approach to achieve both a controlled recovery procedure and reference data for further optimization of recovery speed in the future.

In the failback phase after successful recovery the objective is to ensure that resilience is added again to prepare for the next possible failure. Rebuilding of the high availability setup for all affected entities is targeted to be done according to database application specifics. Clear guidelines can be provided on how to perform this setup.

High availability on the database level is achieved via replication mechanisms. For in memory database environments this means the setup and configuration of synchronous in memory database system replication in the case of a single node deployment and the addition of a dedicated standby server in the case of a multi node deployment. For other databases this means the setup and configuration of replication leveraging a system replication solution SRS . In both scenarios the respective standby server is continuously updated by the primary instance to ensure that in the case of a failure the standby server can take over any time the failover procedure has been invoked.

In an example embodiment high availability at the application tier is achieved via a redundancy layer. For each of the services at least one redundant server is set up. The redundant services are provisioned on a separate hypervisor so that in case one hypervisor fails the services deployed on the second hypervisor are still available. Depending on the application specific requirements stored data in a file system repository is shared between those services and therefore kept up to date. In the case of failure of one of the services the secondary redundant service resumes the productive role so from an end user perspective no change is visible with new requests being performed by the secondary redundant service. In the case where an application has its own application specific features that are assigned statically to one service additional measures can be introduced to assign such features automatically to the secondary redundant service.

As to the disaster recovery in contrast to the system landscape on the primary side the main target for the disaster recovery setup is to keep it in sync with the productively used customer landscape e.g. system configured in the respective disaster recovery scenario not just in terms of system setup but also in terms of any configuration and operation performed by the customer. Hence it is useful for the respective application s configuration information to be stored on the database level.

In an optimal setup the disaster recovery landscape does not require any additional effort as monitoring only becomes relevant at the time of permanent failover and any other maintenance and operations activity is performed via the primary system and the respective mirroring to the disaster recovery side. In instances where it is useful to log on to the disaster recovery side such as applying database patches this may be kept to a minimum.

The phases of the end to end high availability operational procedure applied above can also be applied to the disaster recovery setup. The main difference applies infrastructure wise in a separated dedicated customer network setup and operation wise in a more complex procedure because instead of a single failure a disaster implies a major outage of multiple systems landscapes and thus different SLAs in particular.

In an example embodiment the wide area network WAN connectivity to the cloud integration cloud components is the responsibility of the customer. The customer has an independent network connection to each management network e.g. data center . Each management network shall utilize a different range of network addresses. A failover event within an in network database environment usually only results in DNS changes and therefore any customer specific DNS sub domain should be delegated to the in network database. Each management network gets its own site specific DNS sub domain.

Disaster recovery on the database level is achieved via replication mechanisms. For each of the databases there is a dedicated server available in the secondary management network. The database in the secondary management network is connected with the database in the primary management network using a supported replication mechanism.

Disaster recovery on the application tier is achieved via storage replication. For each of the services there is a dedicated server available in the secondary management network. Any application data not stored on the database side can be stored in an NFS storage volume. Asynchronous replication of the NFS storage volume can then be performed via mirroring technology. In the case of a disaster the services on the secondary management network are ready to take over.

In order to ensure high standardization and automation it is useful if there is clear orchestration of the tools. Referring back to the infrastructure controller may control the infrastructure as a service layer and is therefore responsible for the server storage network provisioning and management. The infrastructure controller may also own the respective master data used to map the business applications of a particular customer to respective hardware and network resources.

The landscape virtualization manager may provide basic availability monitoring on the system level and is the central tool for any system management related activity. The landscape virtualization manager is also the main orchestration framework when it comes to the automatic setup of systems instrumentation for monitoring and automation of failover procedures. The landscape virtualization manager may provide instructions to the infrastructure controller as to how to react to the detection of a disaster. Specifically the instructions may include instructions on attempting to restart the applications in the first data center after a disaster occurs and these instructions may vary depending on the type of alert detected by the solution manager .

The solution manager may be positioned as a monitoring tool used for solution availability and system monitoring. The collected availability monitoring data is used for generating alerts but also as a basis for service quality reporting in regards to SLA compliance such as maximal allowed RTO month SLA month.

As described earlier in an example embodiment disaster recovery is utilized in an in memory database environment. A general implementation of an in memory database will now be described prior to additional details being provided of how the disaster recovery aspects may be extended into the in memory database environment.

Also depicted is a studio used to perform modeling or basic database access and operations management by accessing the in memory database management system .

The in memory database management system may comprise a number of different components including an index server an XS engine a statistics server a preprocessor server and a name server . These components may operate on a single computing device or may be spread among multiple computing devices e.g. separate servers .

The index server contains the actual data and the engines for processing the data. It also coordinates and uses all the other servers.

The statistics server collects information about status performance and resource consumption from all the other server components. The statistics server can be accessed from the studio to obtain the status of various alert monitors.

The preprocessor server is used for analyzing text data and extracting the information on which text search capabilities are based.

The name server holds information about the database topology. This is used in a distributed system with instances of the database on different hosts. The name server knows where the components are running and which data is located on which server.

The client requests can be analyzed and executed by a set of components summarized as request processing and execution control . An SQL processor checks the syntax and semantics of the client SQL statements and generates a logical execution plan. Multidimensional expressions MDX are a language for querying and manipulating multidimensional data stored in online analytical processing OLAP cubes. As such an MDX engine is provided to allow for the parsing and executing of MDX commands. A planning engine allows applications e.g. financial planning applications to execute basic planning operations in the database layer. One such operation is to create a new version of a dataset as a copy of an existing dataset while applying filters and transformations.

A calc engine implements the various SQL script and planning operations. The calc engine creates a logical execution plan for calculation models derived from SQL scripts MDX planning and domain specific models. This logical execution plan may include for example breaking up a model into operations that can be processed in parallel.

Each SQL statement may be processed in the context of a transaction. New sessions are implicitly assigned to a new transaction. A transaction manager coordinates database transactions controls transactional isolation and keeps track of running and closed transactions. When a transaction is committed or rolled back the transaction manager informs the involved engines about this event so they can execute needed actions. The transaction manager also cooperates with a persistence layer to achieve atomic and durable transactions.

An authorization manager is invoked by other database system components to check whether the user has the specified privileges to execute the requested operations. The database system allows for the granting of privileges to users or roles. A privilege grants the right to perform a specified operation on a specified object.

The persistence layer ensures that the database is restored to the most recent committed state after a restart and that transactions are either completely executed or completely undone. To achieve this goal in an efficient way the persistence layer uses a combination of write ahead logs shadow paging and save points. The persistence layer also offers a page management interface for writing and reading data to a separate disk storage and also contains a logger that manages the transaction log. Log entries can be written implicitly by the persistence layer when data is written via the persistence interface or explicitly by using a log interface.

An L infrastructure includes a number of components to aid in the running of L procedures including an L runtime system mode an L compiler and an L runtime user mode .

Turning now to the details of high availability and disaster recovery within an in memory database high availability within an in memory database scale out cluster can be achieved by adding additional standby servers. At the point of failure of one server the standby server takes over. This takeover handling can be embedded into the in memory database platform application and initiated automatically without any manual intervention. is a block diagram illustrating a system for providing high availability in an in memory database platform in accordance with an example embodiment. Here there are one or more active servers A F in a cluster in addition to one or more standby servers A. A shared file system is provided for all servers in the cluster . All of the active servers A F may contain at least a name server A F and an index server A F. A statistics server is contained on only one active server here the active server A. A name server is contained on the standby server A. When a particular active server such as the active server E fails the first available standby server A reads indexes from the shared file system and connects to a logical connection of the failed active server E.

The shared file system is useful to allow the synchronization of the data and log files between the different active server A F and standby servers A within the cluster . In an example embodiment the General Parallel File System GPFS is utilized to achieve synchronization.

In an example embodiment instead of GPFS a Network File System NFS based shared storage mirroring solution is utilized. is a block diagram illustrating a system utilizing an NFS based shared storage mirroring solution in accordance with an example embodiment.

The system includes a cluster manager management entity and a plurality of hosts labeled master host A worker host B and standby host C as well as a network attached storage NAS . In this scenario assume that the worker host B has failed. The take over procedure would work as follows. First the master host A may ping the worker host B repeatedly and not receive an answer within a certain timeout period. Then the master host A decides that the standby host C should take over the worker host s B role and triggers the failover. The standby host C then calls a custom storage connector with the hostname of the worker host B as the parameter. The custom storage connector sends a power cycle request to the cluster manager management entity which in turn triggers a power cycle command to the worker host B. Once the custom storage connector returns without error the standby host C acquires the persistence of the worker host B from the NAS .

Turning now to disaster recovery in an in memory database environment there may be a dedicated in memory database scale out cluster setup on a secondary management network data center . As outlined previously this secondary management network may have its own network segment with its own network address range and domain name. is a block diagram illustrating a system for disaster recovery in an in memory database environment in accordance with an example embodiment. A primary data center A and a secondary data center B are shown. Each of one or more in memory databases A in the primary data center A is connected with a corresponding in memory database B in the secondary data center B excluding perhaps a standby server used for high availability which for cost optimization purposes may only exist in the primary data center A. Likewise an NFS A in the primary data center A is connected to an NFS B in the secondary data center B.

As long as replication is switched on the replication target data recovery site is monitored via the replication source primary or active site Replication status may also be displayed in the central management tools such as the central system monitoring component.

At operation the file system may be switched to read write. This operation may be performed by for example the infrastructure controller of orchestrated by the Landscape Virtualization Manager. At operation registration of an in memory scale out cluster to become primary gets performed. This operation may be performed by for example the landscape virtualization manager of . At operation application specific parameters e.g. FQDN in host profile may be updated. This operation may be performed by for example the landscape virtualization manager of .

At operation application specific parameters may optionally be updated. This operation may be performed by for example the landscape virtualization manager of . At operation applications may be started up. This operation may be performed by for example the landscape virtualization manager of . At operation the DNS caches can be updated. This operation may be performed by for example the technical infrastructure manager orchestrated by the LVM of .

At operation a final connectivity test may optionally be performed. At operation the landscape may be released to the customer. Both are manual activities. At operation the landscape virtualization manager and solution manager monitoring setup can be finalized. This operation may be performed by for example the landscape virtualization manager of .

At operation interactions between a user and the first application are directed to the instance of the first application in the first data center. This may include using access control parameters stored by an infrastructure controller in a cloud integration component the access control parameters defining an access control policy for the user. At operation the state information is updated based on the interactions. At operation replication to the second data center of any changes to the instance of the first database based on the interactions is caused.

At operation a disaster in the first data center is detected. At operation in response to the detecting of the disaster all interactions between the user and the first application are redirected to an instance of the first application in the second data center caused by the replication of the first application. This may include using the access control parameters. Specifically the access control parameters can include a domain that is to be used when the user attempts to interact with the first application and these access control parameters can be modified to update the domain to match the domain of the second data center.

Connection for this mobile device to the disaster recovery site would follow the same principles than for an customer on premise or an third party Internet application

Certain embodiments are described herein as including logic or a number of components modules or mechanisms. Modules may constitute either software modules e.g. code embodied 1 on a non transitory machine readable medium or 2 in a transmission signal or hardware implemented modules. A hardware implemented module is a tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments one or more computer systems e.g. a standalone client or server computer system or one or more processors e.g. processor may be configured by software e.g. an application or application portion as a hardware implemented module that operates to perform certain operations as described herein.

In various embodiments a hardware implemented module may be implemented mechanically or electronically. For example a hardware implemented module may comprise dedicated circuitry or logic that is permanently configured e.g. as a special purpose processor such as a field programmable gate array FPGA or an application specific integrated circuit ASIC to perform certain operations. A hardware implemented module may also comprise programmable logic or circuitry e.g. as encompassed within a general purpose processor or another programmable processor that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware implemented module mechanically in dedicated and permanently configured circuitry or in temporarily configured circuitry e.g. configured by software may be driven by cost and time considerations.

Accordingly the term hardware implemented module should be understood to encompass a tangible entity be that an entity that is physically constructed permanently configured e.g. hardwired or temporarily or transitorily configured e.g. programmed to operate in a certain manner and or to perform certain operations described herein. Considering embodiments in which hardware implemented modules are temporarily configured e.g. programmed each of the hardware implemented modules need not be configured or instantiated at any one instance in time. For example where the hardware implemented modules comprise a general purpose processor configured using software the general purpose processor may be configured as respective different hardware implemented modules at different times. Software may accordingly configure a processor for example to constitute a particular hardware implemented module at one instance of time and to constitute a different hardware implemented module at a different instance of time.

Hardware implemented modules can provide information to and receive information from other hardware implemented modules. Accordingly the described hardware implemented modules may be regarded as being communicatively coupled. Where multiple of such hardware implemented modules exist contemporaneously communications may be achieved through signal transmission e.g. over appropriate circuits and buses that connect the hardware implemented modules . In embodiments in which multiple hardware implemented modules are configured or instantiated at different times communications among such hardware implemented modules may be achieved for example through the storage and retrieval of information in memory structures to which the multiple hardware implemented modules have access. For example one hardware implemented module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware implemented module may then at a later time access the memory device to retrieve and process the stored output. Hardware implemented modules may also initiate communications with input or output devices and can operate on a resource e.g. a collection of information .

The various operations of example methods described herein may be performed at least partially by one or more processors that are temporarily configured e.g. by software or permanently configured to perform the relevant operations. Whether temporarily or permanently configured such processors may constitute processor implemented modules that operate to perform one or more operations or functions. The modules referred to herein may in some example embodiments comprise processor implemented modules.

Similarly the methods described herein may be at least partially processor implemented. For example at least some of the operations of a method may be performed by one or more processors or processor implemented modules. The performance of certain of the operations may be distributed among the one or more processors not only residing within a single machine but deployed across a number of machines. In some example embodiments the processor or processors may be located in a single location e.g. within a home environment an office environment or a server farm while in other embodiments the processors may be distributed across a number of locations.

The one or more processors may also operate to support performance of the relevant operations in a cloud computing environment or as a software as a service SaaS . For example at least some of the operations may be performed by a group of computers as examples of machines including processors these operations being accessible via a network e.g. the Internet and via one or more appropriate interfaces e.g. application programming interfaces APIs .

Example embodiments may be implemented in digital electronic circuitry or in computer hardware firmware or software or in combinations of them. Example embodiments may be implemented using a computer program product e.g. a computer program tangibly embodied in an information carrier e.g. in a machine readable medium for execution by or to control the operation of data processing apparatus e.g. a programmable processor a computer or multiple computers.

A computer program can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a standalone program or as a module subroutine or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.

In example embodiments operations may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method operations can also be performed by and apparatus of example embodiments may be implemented as special purpose logic circuitry e.g. an FPGA or an ASIC.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In embodiments deploying a programmable computing system it will be appreciated that both hardware and software architectures merit consideration. Specifically it will be appreciated that the choice of whether to implement certain functionality in permanently configured hardware e.g. an ASIC in temporarily configured hardware e.g. a combination of software and a programmable processor or in a combination of permanently and temporarily configured hardware may be a design choice. Below are set out hardware e.g. machine and software architectures that may be deployed in various example embodiments.

The example computer system includes a processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory and a static memory which communicate with each other via a bus . The computer system may further include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT . The computer system also includes an alphanumeric input device e.g. a keyboard or a touch sensitive display screen a user interface UI navigation or cursor control device e.g. a mouse a disk drive unit a signal generation device e.g. a speaker and a network interface device .

The disk drive unit includes a machine readable medium on which is stored one or more sets of data structures and instructions e.g. software embodying or utilized by any one or more of the methodologies or functions described herein. The instructions may also reside completely or at least partially within the main memory and or within the processor during execution thereof by the computer system with the main memory and the processor also constituting machine readable media .

While the machine readable medium is shown in an example embodiment to be a single medium the term machine readable medium may include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more instructions or data structures. The term machine readable medium shall also be taken to include any tangible medium that is capable of storing encoding or carrying instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure or that is capable of storing encoding or carrying data structures utilized by or associated with such instructions . The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories and optical and magnetic media. Specific examples of machine readable media include non volatile memory including by way of example semiconductor memory devices e.g. erasable programmable read only memory EPROM electrically erasable programmable read only memory EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks.

The instructions may further be transmitted or received over a communications network using a transmission medium. The instructions may be transmitted using the network interface device and any one of a number of well known transfer protocols e.g. HTTP . Examples of communication networks include a local area network LAN a wide area network WAN the Internet mobile telephone networks plain old telephone POTS networks and wireless data networks e.g. WiFi and WiMax networks . The term transmission medium shall be taken to include any intangible medium that is capable of storing encoding or carrying the instructions for execution by the machine and includes digital or analog communications signals or other intangible media to facilitate communication of such software.

Although an embodiment has been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the disclosure. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense. The accompanying drawings that form a part hereof show by way of illustration and not of limitation specific embodiments in which the subject matter may be practiced. The embodiments illustrated are described in sufficient detail to enable those skilled in the art to practice the teachings disclosed herein. Other embodiments may be utilized and derived therefrom such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure. This Detailed Description therefore is not to be taken in a limiting sense and the scope of various embodiments is defined only by the appended claims along with the full range of equivalents to which such claims are entitled.

Such embodiments of the inventive subject matter may be referred to herein individually and or collectively by the term invention merely for convenience and without intending to voluntarily limit the scope of this application to any single invention or inventive concept if more than one is in fact disclosed. Thus although specific embodiments have been illustrated and described herein it should be appreciated that any arrangement calculated to achieve the same purpose may be substituted for the specific embodiments shown. This disclosure is intended to cover any and all adaptations or variations of various embodiments. Combinations of the above embodiments and other embodiments not specifically described herein will be apparent to those of skill in the art upon reviewing the above description.

