---

title: Systems and methods for construction, maintenance, and improvement of knowledge representations
abstract: In one aspect, the present disclosure relates to a method which, in one example embodiment, can include reading text data corresponding to messages, creating semantic annotations to the text data to generate one or more annotated messages, and aggregating the annotated messages and storing information associated with the aggregated annotated messages in a message store. The method can further include performing, based on information from the message store and associated with the one or more messages, one or more global analytics functions that include: identifying an annotation error in the semantic annotations created using the trained statistical language model, updating the respective semantic annotation to correct the annotation error, and back-propagating corrected data corresponding to the updated semantic annotation into training data for further language model training.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09348815&OS=09348815&RS=09348815
owner: DIGITAL REASONING SYSTEMS, INC.
number: 09348815
owner_city: Franklin
owner_country: US
publication_date: 20150506
---
This Application is a continuation in part of and claims benefit under 35 U.S.C. 120 of U.S. patent application Ser. No. 14 320 566 filed Jun. 30 2014 which itself claims priority to and benefit under 35 U.S.C 119 e of U.S. Provisional Patent Application Ser. No. 61 841 071 filed Jun. 28 2013 U.S. Provisional Patent Application Ser. No. 61 841 054 filed Jun. 28 2013 and U.S. Provisional Patent Application Ser. No. 62 017 937 filed Jun. 27 2014. These above mentioned U.S. Patent Applications are hereby incorporated by reference herein in their entireties as if fully set forth below.

The growth of unstructured data can create a widening understanding gap within enterprises across all industries. Unstructured data may grow more rapidly than structured data which can create additional issues for enterprises because unstructured data can be human generated pervasive fluid and imprecise. Some difficulties in deriving critical actionable information from big data can be exemplified in recent financial scandals where although indications of problematic behavior were within the archived human communications data of the financial industry surrounding mountains of innocuous communications prevented timely action to address the growing risks. Similar difficulties can be faced by the intelligence community where thousands of reports are received daily but a limited ability to make meaningful connections between disparate sources of information may prevent timely responses to evolving situations. It is with respect to these and other considerations that aspects of the present disclosure are presented herein.

In one aspect the present disclosure relates to a computer implemented method. In one embodiment the method includes reading text data that corresponds to one or more messages and includes natural language content and or metadata. The method also includes creating one or more semantic annotations to the text data to generate one or more annotated messages. Creating the one or more semantic annotations can include generating at least in part by a trained statistical language model one or more predictive labels corresponding to part of speech syntactic role sentiment and or other language patterns associated with the text data. The method further includes aggregating the one or more annotated messages and storing information associated with the aggregated one or more annotated messages in a message store. The method further includes performing based on information from the message store and associated with the one or more messages one or more global analytics functions. The global analytics functions include identifying an annotation error in the semantic annotations created using the trained statistical language model updating the respective semantic annotation to correct the annotation error and back propagating corrected data corresponding to the updated semantic annotation into training data for further language model training.

In another aspect the present disclosure relates to a system. In one embodiment the system includes one or more processors and a memory that is operatively coupled to the one or processors. The memory stores computer executable instructions that when executed by the one or more processors cause the system to perform functions that include reading text data corresponding to one or more messages and creating one or more semantic annotations to the text data to generate one or more annotated messages. Creating the one or more semantic annotations comprises generating at least in part by a trained statistical language model one or more predictive labels corresponding to language patterns associated with the text data. The stored instructions when executed by the one or more processors causes the system to perform further functions that include aggregating the one or more annotated messages and storing information associated with the aggregated one or more annotated messages in a message store and performing based on information from the message store and associated with the one or more messages one or more global analytics functions. The global analytics functions include identifying an annotation error in the semantic annotations created using the trained statistical language model updating the respective semantic annotation to correct the annotation error and back propagating corrected data corresponding to the updated semantic annotation into training data for further language model training.

In another aspect the present disclosure relates to a non transitory computer readable medium. In one embodiment the computer readable medium stores instructions that when executed by one or more processors cause a computer to perform functions that include reading text data corresponding to one or more messages and creating one or more semantic annotations to the text data to generate one or more annotated messages. Creating the one or more semantic annotations comprises generating at least in part by a trained statistical language model one or more predictive labels corresponding to language patterns associated with the text data. The stored instructions when executed by the one or more processors causes the computer to perform further functions that include aggregating the one or more annotated messages and storing information associated with the aggregated one or more annotated messages in a message store and performing based on information from the message store and associated with the one or more messages one or more global analytics functions. The global analytics functions include identifying an annotation error in the semantic annotations created using the trained statistical language model updating the respective semantic annotation to correct the annotation error and back propagating corrected data corresponding to the updated semantic annotation into training data for further language model training.

The following detailed description is directed to computing systems and methods for performing functions that may include the construction maintenance and improvement of knowledge representations. Although example embodiments of the present disclosure are explained in detail it is to be understood that other embodiments are contemplated. Accordingly it is not intended that the present disclosure be limited in its scope to the details of construction and arrangement of components set forth in the following description or illustrated in the drawings. The present disclosure is capable of other embodiments and of being practiced or carried out in various ways.

It must also be noted that as used in the specification and the appended claims the singular forms a an and the include plural referents unless the context clearly dictates otherwise. Moreover titles or subtitles may be used in this specification for the convenience of a reader which shall have no influence on the scope of the present disclosure.

By comprising or containing or including is meant that at least the named compound element particle or method step is present in the composition or article or method but does not exclude the presence of other compounds materials particles method steps even if the other such compounds material particles method steps have the same function as what is named.

In describing example embodiments terminology will be resorted to for the sake of clarity. It is intended that each term contemplates its broadest meaning as understood by those skilled in the art and includes all technical equivalents that operate in a similar manner to accomplish a similar purpose.

The following provides non limiting definitions of some terms used herein in describing certain aspects of the present disclosure for convenience of the reader.

As used herein an agent may refer to an autonomous program module configured to perform specific tasks on behalf of a host and without requiring the interaction of a user.

As used herein an aggregate may refer to a collection defined by users or algorithms of pointers and values including sets of other primitives such as entities concepts relationships etc.

As used herein an application may refer to an assembly of reasoning APIs user experience and business objectives and constraints.

As used herein a contextual feature can be a feature that captures the context surrounding a mention. A contextual feature may comprise extractor tags and features at the word level in a sentence.

As used herein coreference resolution or entity resolution may refer to a process of determining whether two expressions or mentions in natural language refer to the same entity.

As used herein a coreference chain or coref chain may refer to one or more textual references to an entity.

As used herein an entity may refer to a set or aggregate of mentions that constitute an unambiguous identity of a person group thing or idea. An entity may be a group of coreferent sub entities which may also be referred to as a concept .

As used herein a feature may refer to a value derived from examination of the context of a concept relationships and messages. A feature can be explicitly in the message or inferred through analytics.

As used herein a feature vector may refer to an n dimensional vector of features such as numerical features that can represent an element or mention . Some machine learning processes described herein may utilize numerical representation of objects to facilitate processing and statistical analysis.

As used herein local entity may refer to a group of in document coreferent mentions which may also be referred to as a local coreference chain.

As used herein a mention may refer to a reference to a value in a position in a message that has been processed. Mention as used herein may additionally or alternatively refer to a data object that represents a chunk which can contain book keeping info token start token end etc. and features that aid in resolution.

As used herein a modifier may provide additional determination and specification of the entity predicate or relationship. A modifier may be necessarily bound in a relationship.

As used herein a non contextual feature may refer to features which are constant for a given word regardless of the context. Non contextual feature vectors may comprise tokenizer output and features at the character level for a given word.

As used herein a predicate may refer to the type of action or activity and reference to that activity independent of the subjects or objects of that activity.

As used herein reasoning may refer to the use or manipulation of concepts and relationships to answer end user questions. Reasoning may be primitive atomic or complex orchestrated to support a specific business use case .

As used herein a relationship may refer to an n tuple of concepts or relationships i.e. relationships can be recursive . A relationship can have a value as a label.

As used herein resolution may refer to the determination of a set or all references to create concepts or relationships.

As used herein space and time may refer to ranges that may constrain relationships. Inherently space and time may be of limited precision and can be implemented with different basic units of measure.

As used herein structured data may refer to attribute value pairs and relationships with pre defined meaning.

As used herein sub entity may refer to a group of coreferent local entities . A sub entity may also be the atomic unit of input for iterative global coreference processes as described in the present disclosure.

As used herein super entity may refer a coarse grained cluster. Person mentions can be part of a larger person super entity. As a further example all mentions belonging to a politics category can be part of one big super entity. Super entities can be used for minimizing a search space.

In the following detailed description references are made to the accompanying drawings that form a part hereof and that show by way of illustration specific embodiments or examples. In referring to the drawings like numerals represent like elements throughout the several figures.

Some fictitious names such as Roger Guta Raj Mojihan Gallot Company Proffett Gambrel William Schultz Kinsor Company John Smith Princetown University that are used throughout the present disclosure are intended for illustration purposes only and are not intended to refer to any specific real world persons or entities.

Some instances of specific terms used herein may be partially capitalized e.g. Resonate Knowledge Graph . Such partial capitalization is intended for the convenience of the reader and or to distinguish the features functions or concepts a particular term may represent in some specific embodiments of the present disclosure. Intentional mention of a specific proprietary or commercially used name may be signified herein by all capitalized characters e.g. JAVASCRIPT .

In some embodiments the present disclosure can provide for implementing analytics using both supervised and unsupervised machine learning techniques. Supervised mathematical models can encode a variety of different data features and associated weight information which can be stored in a data file and used to reconstruct a model at run time. The features utilized by these models may be determined by linguists and developers and may be fixed at model training time. Models can be retrained at any time but retraining may be done more infrequently once models reach certain levels of accuracy.

Such approaches can be used to capture linguistic phenomena by utilizing the models to label sequences of characters tokens elements with the correct linguistic information that a model was created to predict. According to some embodiments of the present disclosure a supervised approach can comprise two phases a training phase to identify those features that are significant for determining the correct label sequencing implemented by that model and a run time labeling phase that employs inference algorithms to assign attributes to the text being processed.

Training can be performed by passing annotated data to a machine learning training algorithm that creates an appropriate model. This data can be represented as vectors of features. Such machine learning training algorithms can learn the weights of features and persist them in a model so that inference algorithms can use the model to predict a correct label sequence to assign to the terms as they are being processed.

The use of statistical models can provide for a degree of language independence because the same underlying algorithms can be used to predict correct labeling sequences the process may slightly differ just in using a different set of models. For each language a new model can be created for each machine learning function using the language to identify significant features important to that model.

The present disclosure presented herein in accordance with some embodiments can provide for building a graph of global enterprise knowledge from data with integration of a set of knowledge services in the form of a rich Application Programming Interface API to access a Knowledge Graph abstracted from the data see e.g. . The present disclosure in accordance with some embodiments can provide an entity centric approach to data analytics focused on uncovering the interesting facts concepts events and relationships defined in the data rather than just filtering down and organizing a set of documents that may contain the information being sought by the user.

Now specifically referring to according to some embodiments of the present disclosure in order to assemble a rich Knowledge Graph from both unstructured data and structured data an analytical workflow can perform processes which may be generally described in terms of three main functional phases Read Resolve and Reason where each phase includes particular functional processes. In the Read phase see e.g. Read at block unstructured data e.g. web email instant messaging or social media data and structured data e.g. customer information orders or trades transactions or reference data can be ingested and natural language processing NLP entity extraction and fact extraction can be performed. As non limiting examples unstructured data may be accepted in a UTF 8 text format and structured data may be accepted in a specified XML format among other commonly used data formats.

In the Resolve phase see e.g. Resolve at block results from the Read phase can be assembled organized and related to perform global concept resolution and detect synonyms e.g. synonym generation and closely related concepts. Some aspects of the Resolve phase relate to Resonance as described in further detail below which it should be recognized is not equivalent to Resonate reasoning as described herein. In the Reason phase spatial and temporal reasoning may be applied and relationships uncovered that can allow resolved entities to be compared and correlated using various graph analysis techniques. The Reason phase can utilize reasoners of Global Analytics where functions of Resolve may be considered a type of reasoner. Reasoners can further include Resonate Similarity Associative Net Inference and Prediction . Various aspects of an analytical workflow that can utilize the Read Resolve and Reason phases may be performed in a distributed processing environment and the results can be stored into a unified entity storage architecture which may be referred to herein as a Knowledge Base .

As illustrated in systems and methods according to some embodiments of the present disclosure can utilize Local Analytics processes . Local Analytics can include processes in accordance with the Read phase and may refer to reading messages and enriching them with semantic annotations based on algorithms that utilize a priori models created from training and static background knowledge. Enrichment in Local Analytics may use structured prediction algorithms and classification algorithms. These algorithms may be supervised or semi supervised and can presume training of an a priori model which is evaluated at runtime. Output of Local Analytics processes can include a message with the annotations populated from the analytics which may be aggregated into an annotated message store of the Knowledge Base .

In conventional approaches a problem encountered when creating and managing entity centric information for large corpora of unstructured data is that many existing database architectures do not scale due to the large volume of entities generated and the multiple relationships that can exist between entities. To address such deficiencies the present disclosure in some embodiments can provide the Knowledge Base as a unified entity storage architecture that can perform at scale for both data insertion as well as data querying. In some embodiments the Knowledge Base can be a combination of persistent storage and intelligent data caching that can enable rapid storage and retrieval of entities concepts relationships text documents and related metadata. This can include the text content of messages the categorized individual tokens and semantic token groups comprising those messages and metadata such as properties relationships and events. Such combination of rich metadata and intelligent indexing can support among other benefits and advantages powerful search and rapid retrieval capabilities in addition to advanced analytical functions.

The Knowledge Base can provide storage and indexing for annotated messages where indexing may be passive and may not require active maintenance to support new analytics. An annotated message store can run locally e.g. in storage or can be distributed over many systems. The Knowledge Base may provide for searches see e.g. querying based on message ID strings any annotation value or composition of annotation values and or ranges of positions. The Knowledge Base may additionally or alternatively contain a Knowledge Graph representation of the system as described with reference to embodiments disclosed herein for example embodiments shown in . The Knowledge Graph may be derived through Global Analytics which may also be referred to as Advanced Analytics and may provide features to Global Analytics to enable the creation of the Knowledge Graph.

System level annotations may be added to every message such as a value hashing column that encrypts or disguises the values in the message allowing analysis to be anonymous and a second column can cover the visibility access rights on data that is either populated by the metadata on the message or by the system which may serve a dereferencing function based on a user s access privileges to the data. Annotated message storage may provide versioning for messages and updates to messages overwrites and may assume immutable representations of messages.

In some embodiments Global Analytics processes can take features from annotated message storage and run algorithms against aggregated or global metadata contained therein to produce maintain and enrich a unified representation of knowledge learned from the original data that may be stored in a Knowledge Graph. This may include the resolution of references yielding the creation of concepts categories and relationships through clustering similarity and ranking algorithms. At a functional level Resolve can be considered a reasoner of Global Analytics .

Kinds of analytic algorithms that may be used in Global Analytics at a formal level can include clustering including hierarchical nearest neighbor ranking maximum a posteri MAP inference and expectation maximization. The clustering nearest neighbor and ranking type algorithms have a family resemblance in that they can calculate relative similarity or dissimilarity between different classes or sets of objects based on features and then either return a ranked list or a partition of the objects into sets with some rationale. MAP and expectation maximization may share a family resemblance in predicting a best candidate or range of candidates given a set of condition of the Knowledge Graph at the time of evaluation.

As will be discussed in further detail below with respect to Global Analytics in accordance with some embodiments can use Resonate to back propagate from Global Analytics to Local Analytics . For example Resonate may identify categorization errors from an extractor or named entity resolution NER models and back propagate information to model training to fix the errors. Resonate can read from globally fixed data generate artificial training and teach to rebuild models thus acting as a type of agent to perform internal training.

 Associative Net which may also be referred to as associative network reasoning in accordance with some embodiments can be based in a distributional similarity hypothesis positing that words with similar meaning will be used in similar language constructs and contexts. Adapted to named entities this can mean that two entities e.g. people that perform similar functions e.g. occupation can be referred to in a similar manner. For instance it may be expected that American politicians will often appear in text near American political venues e.g. Washington or in similar situations e.g. stump speech . Associative Net functions according to example embodiments can leverage such an assumption to build for each word a compact signature that encodes all of the contexts in which it appears. The signatures for any two words can be efficiently compared to give a degree of similarity. Named entities that have a high degree of similarity according to Associative Net may often represent aliases for the same entity or other entities that serve a similar function.

Further aspects of Global Analytics in accordance with some embodiments of the present disclosure can utilize knowledge objects. Once it can be determined where i.e. in what places an entity has been mentioned information can be collected about the entity from all of the mention sites. Different pieces of information can be collected from which to construct entity knowledge objects including birth date death date political affiliation and nationality. These data points can be aggregated across all of the mentions for an entity and reported as attributes of the entity. Thus it may be observed for example that the entity named Barbara Streisand having alias Ms. Streisand was born in April 1942 as long as a birth date of April 1942 could be identified for one of the mentions e.g. She was born in April 1942 .

According to some embodiments a distributed map framework that can be used for Local Analytics can be instantiated using for example currently available HADOOP Map Reduce or STORM streaming technology. This can provide for a batch data ingestion process or a streaming ingestion process i.e. documents are fed in as they arrive in real time . According to some example embodiments Global Analytics processes can be instantiated as HADOOP Map Reduce jobs and this process may be executed periodically to incorporate new data being added to the Knowledge Base in corpus wide analytics processing. Global Analytics processes can read data generated by Local Analytics from the Knowledge Base using a customized API in accordance with the present disclosure to perform bulk reads of the required data. A particularized API task performing the data reads can be instantiated as HADOOP Map Reduce processes for example.

Global Analytics can support incremental updates such that rather than having to reprocess an entire data corpus whenever new documents are added to the Knowledge Base analytics performed on the new data can be incorporated with the analysis already contained in the Knowledge Base . In some embodiments systems and or methods can be deployed to an enterprise in a variety of configurations or can be accessed via a cloud service which can provide for a low cost of entry while supporting a comparable level of access to analytics as an installation behind an enterprise firewall.

Now also referring to the diagram of according to some embodiments of the present disclosure a number of ways can be provided for accessing the data and analysis contained in the Knowledge Base . A particularized API layer can enable organizations and developers to integrate aspects of the present disclosure with third party applications e.g. business intelligence tools search and discovery tools or create customized user interfaces that utilize results of the analysis. These can include a set of knowledge queries via a Knowledge Base query language in accordance with the present disclosure module commands that can provide access to specific functions performed by some modules in accordance with the present disclosure and through IMPALA.

Utilizing IMPALA users can query data using SQL like syntax including SELECT JOIN and aggregate functions in real time. This can use the same metadata SQL syntax ODBC driver and user interface as APACHE HIVE making the transition substantially seamless when moving from APACHE HIVE to IMPALA. A Knowledge Base query language according to some embodiments may be based on the MQL specification published as part of the FREEBASE project to serve as a JSON based JAVASCRIPT Object Notation query language. For web developers using JAVASCRIPT JSON is trivially transformed into JAVASCRIPT objects so it can be particularly convenient for work in which a browser based user interface is involved. Because the FREEBASE project s MQL usage is not mapped out as formal language there is no set schema to be designed against.

Standard Knowledge Base query requests according to the present disclosure may include operations for standard CRUD create read update delete operations. The Knowledge Base query engine can support create and read query types for the most common object types with some support for update queries for specific object types. Because Knowledge Base query commands can be built on a JSON based query language it may be intuitive to use JSON objects to specify the input parameters for this command form. The input parameters can be placed into a JSON object and then passed to the server in the request body. The result can be passed back to the client in a JSON object with the same format as the one passed in the request body.

In accordance with some embodiments a custom JAVA API in accordance with the present disclosure which may also be referred to as Reaper can support high performance bulk export operations on data tables in the Knowledge Base to support creation of custom analytics data views and data exports. It may be noted that this is not a run time API from a specific server. These tables can be accessed from specific backend storage technology being employed such as CASSANDRA HBASE or ACCUMULO. Reaper API can expose core data structures through documented business objects that conform to standard interfaces. Specifically an input formal JAVA class can be provided for each data type supported by the interface. This input format can tell HADOOP how to break the bulk read operation into separate tasks that utilize the HADOOP Map Reduce distributed execution environment allowing the bulk export function to scale to the amount of available hardware in the HADOOP cluster. The Global Analytics processes can also utilize the Reaper API to read the data from the Knowledge Graph that was generated by Local Analytics processes.

According to some embodiments in addition to searching for data certain systems and or methods can permit a user to get answers to questions they want to ask. Such functionality can be enabled in accordance with a high fidelity knowledge representation predicated on a graph abstraction that can be used by people and machines to understand human language in context which may be referred to as the Knowledge Graph. illustrates an example of the type of information a generated Knowledge Graph representation may contain and the diagram of illustrates organization and generation of a Knowledge Graph in accordance with some embodiments.

In some embodiments the Knowledge Graph can be built automatically from public and private data in near real time. A graph may be assembled with no prior knowledge of the data and can visually represent resolved entities in time and space. The entities can appear as nodes in the graph and contain aggregated knowledge about that entity e.g. when where they were born when where they went to school and or when where they worked . The Knowledge Graph according to some embodiments can provide for understanding entities and facts in relationships that can enable a user to quickly identify specific opportunities and risks to support crucial decision making.

As an example implementation of aspects of a Knowledge Graph according to some embodiments if building a compliance use case the analysis might have uncovered the following facts 

Other facts can also be made quickly visible such as the common connection to Princetown University between Roger and Raj or the connection between Roger Raj and William Schultz.

After reviewing these facts an analyst may then be able to infer information based on the Knowledge Graph representations for example 

Now also referring to the diagram of in accordance with some embodiments of the present disclosure the Knowledge Graph can graphically represent the information that has been extracted from a corpus for example information extracted via one or more functions in accordance with the Read phase. A Knowledge Graph can be viewed as two separate and related sub graphs the knowledge sub graph identifying the entities present in text and the relationships between them and the information sub graph which identifies the specific pieces of information that act as evidence support for the knowledge sub graph.

As illustrated in the example embodiment of the information sub graph can contain message nodes mention nodes assertion nodes and location nodes . Each message node can represent a single document from a corpus and can contain metadata information about the document in addition to its text and any document level analysis artifacts e.g. tokenization part of speech assignment name identification from the Read phase. These analytic outputs can be encoded within a separate graph on the dafGraph property of the message node.

The text of a message can refer to entities and describe various ways in which they interact. These entities can be represented in the information sub graph by mention nodes. Each mention node can represent a coreference chain one or more textual references to an entity from a single document identified from the local coreference output of Read processes.

Location nodes can represent geographic references within the text that can be disambiguated and geo coded to a specific coordinate. These location nodes can be linked to by message nodes and assertion nodes representing the geographic locations identified in a message and the geographic locations at which individual interactions took place. Assertion nodes can represent the interactions between entities that are identified during the Read phase. Within the information sub graph they can be encoded as subject verb object triples with time and location attributes. The verb and time information can be encoded within the properties of the assertion node and the subject object and location can be identified by edges from the assertion node to mention and location nodes see e.g. subject edge and object edge from assertion node to mention node and location edge from assertion node to location node . The location node can identify the geographic location at which the interaction is thought to have occurred.

The knowledge sub graph can aggregate individual pieces of information from messages into a corpus global view of the information that is organized around the entity. Prototype nodes entity nodes and concept nodes and the relationships between them can capture at a high level the individual pieces of information from the information sub graph. The prototype nodes can represent an initial high confidence clustering of mentions from a small portion of the corpus. A reason for this level of abstraction can be to address scale within the global coreference operation. Prototypes can aggregate mentions so that there are fewer prototypes to resolve than there are mentions. Prototypes can typically be constructed in parallel on smaller portions of a corpus. Prototypes can be linked to other prototypes by assertion edges which can abstract the assertion nodes from the information graph. Each assertion can specify a subject and object mention node and each of these mentions can contribute to a single prototype node. The prototype nodes corresponding to the subject and object mention nodes for an assertion can have an assertion edge between them.

Entity nodes can be considered as fundamental building blocks of the knowledge sub graph representing the global aggregation of information from prototype nodes. Each entity can have a link to its contributing prototypes as well as links to the other entities in which it was been observed to interact. The assertion edges to other entities can be inherited from its prototypes. An assertion edge confidence can be aggregated from the confidence of corresponding assertion edges on contributing prototype nodes. Entities themselves can be clustered into concept nodes representing a high level abstraction of a group of entities see e.g. super edge from entity node to concept node .

As discussed in some detail above an information sub graph in accordance with some embodiments can contain message nodes mention nodes assertion nodes and location nodes wherein each message node can represent a single document from a corpus and can contain metadata information about the document in addition to its text and any document level analysis artifacts from the Read phase. These analytic outputs can be encoded within a separate graph on the dafGraph property of the message node which relates to a graph which may relate to a document graph consisting of all nodes and edges that reference a common source document. A source document as referred to herein can be a single instance of text that is subject to analysis such as a single file within a file system.

The nodes in a document graph can represent analytic results features and properties. Features and properties can be key value pairs attached to nodes. Additionally these nodes may have relationships to other nodes in the graph see edges . For example a node may represent a single word of text or token . That node may then have a child relationship to a node representing the phrase of which the word is a part a chunk . The chunk node may have other children representing other words in the phrase. Each of these nodes may have additional properties describing the analytic component that generated the node a confidence associated with the node and so on.

In some embodiments information contained within a generated Knowledge Graph can help to answer a variety of questions relevant to specific use cases for instance who said what to whom and or what events are occurring when and where. Some embodiments can allow for a search that may not be otherwise easily expressed in a pre specified analytic format and some embodiments can provide for a user to browse the Knowledge Graph looking for a serendipitous connection a novel fact or to gain situational awareness related to an entity for instance.

Some embodiments can provide for easy browsing and searching of concepts in the Knowledge Graph by querying knowledge objects and visualizing captured information in a clean and intuitive graphical user interface which may be web based. In some embodiments a user can be presented with a list of the most active concepts in their database. The user can expand the time frame and filter results by concept category so that they are presented with for example a list of the people who have been the most active in the last 30 days.

When a user decides to investigate a given concept in some embodiments an entity profile can provided that may list key attributes such as aliases date of birth and death places of residence organization memberships titles spouses siblings and or children. The profile can also provide an interactive timeline that shows the number of times the concept is mentioned on any given date. A newsfeed can be tied to this timeline and sentences may be displayed where the concept appears as part of a subject predicate object triple during the selected period of time. Additionally the newsfeed can display how long ago the action took place the name of the document that reported the information and the total number of documents that made the same statement. This news can also be filtered by predicate category enabling the user to easily view specific types of interactions such as communication or travel.

In some embodiments aspects of the concept profiles can have an associated relationships tab in a graphical user interface. This visualization can identify other concepts in the Knowledge Base that are related with the current concept ordered by strength of relationship. These results can also be filtered by entity category. From the relationships tab the user can choose to navigate to the concept s profile or a profile that documents the relationship between the two concepts. This relationship profile may primarily consist of a timeline and newsfeed showing when and how the concepts interacted over time. The user is able to interact with the news and timeline in the same fashion as on the single concept profile. Implementing certain aspects of some embodiments of the present disclosure can remove the need for a user to write their own queries to explore their data and can provide a clean presentation of the most critical data and allow users to easily navigate the information in their systems which can empower users to understand not just what entities in their Knowledge Bases are doing but also how each is related to the other including relationships that would have otherwise been nearly impossible for a human to discover on their own. Such functionality can empower organizations and individuals with a more complete understanding of their data.

Various aspects of the Read Resolve and Reason workflow according to some embodiments of the present disclosure will now be discussed in further detail. As described in some detail above the Read Resolve and Reason phases can provide for building and exploring a graph of global enterprise knowledge. Mentions of entities can be identified during the Read phase and combined and organized into a graph of entities and relationships between them in the Resolve phase. In the Reason phase information inherent in the Knowledge Graph can be extracted and provided as actionable insights for a user.

In some aspects of the Read phase in accordance with some embodiments as data is read in text of the data can first be broken up into its foundational building blocks using a multi stage natural language processing NLP process. The NLP process can comprise determining sentence boundaries then breaking up the text into tokens. Each token can consist of a word punctuation mark or special character. Each token can then be analyzed and assigned a grammatical part of speech POS tag e.g. proper noun adjective adverb . The tokens can be further analyzed to determine if adjacent tokens should be cojoined together if they describe the same concept. For example if John and Smith were adjacent to each other they can be cojoined to form John Smith as a single concept. Other types of examples can include titles or company names. This process may be referred to as chunking which creates the elements or entities that can be used by downstream analytics.

A next step can be to analyze each chunk to determine if it belongs to a predefined category. Examples of categories can include people organizations businesses and vehicles. A library of predefined categories may be provided. However users may create their own custom categories using various training applications as described above.

In some embodiments upon completion of the NLP process the text has been broken down into its constituent parts forming a basic foundation of contextual meaning Using this foundation other analytic functions can then be performed such as identifying and cataloging significant activities or assertions between entities. In a grammatical sense these can be looked at as subject predicate object triples as they describe specific activities that occur between entities e.g. a person place or thing . These assertions can then be categorized to describe specific types of activities such as communications activities and or purchase acquisition activities.

Other analytics can include identifying and cataloging temporal and spatial references found in the text including indirect references to time and location. For example if the date of a document is known a temporal reference to next Thursday can be assigned the correct date based on the document date.

To illustrate analytics performed by Read processes according to some embodiments suppose that the following sentence is read in 

In some embodiments a second phase of the Read Resolve and Reason workflow is the Resolve phase. Analytics performed by Resolve processes can be more global in nature and span all documents processed by the Read phase. In some embodiments Resolve can be particularly privileged to make updates deletions and bootstrap the full structure of the Knowledge Graph.

Entity resolution can generally refer to a process of determining whether two expressions or mentions in natural language text refer to the same entity. Given a collection of mentions of entities extracted from a body of text mentions may be grouped such that two mentions belong to the same group cluster if they refer to the same entity. It may be recognized that an entity is coreferent with and refers to the same entity or that information associated with the entity is referring to multiple distinct real world individuals. Entity resolution according to some embodiments of the present disclosure can address an existing problem of identifying the correct entity named by each mention e.g. names pronoun and noun references .

Global cross document coreference resolution as disclosed herein can leverage the local in document coreference capabilities of Local Analytics. Within a single document an entity may be referred to one or more times in what may be called a coreference chain e.g. She her Barbara Ms. Streisand famous singer . The aggregate context nearby words for these mentions and other pertinent information features extracted from the text surrounding those mentions can form a signature for the chain. This chain signature can then be compared against chain signatures from other documents and when a similar chain e.g. Barbara Streisand singer Ms. Streisand has been identified they can be deemed coreferent and collapsed into a larger structure containing the mentions of both. This larger group of mentions and its signature can then participate further in the comparison and combination process.

Regarding global entity resolution across the data a specific entity may be referred to in a number of different ways. Returning to a previous example Roger Guta may be referred to in many different ways Roger Guta Rog Guta Mr. Guta Roger Kumir Guta etc. Although the specific string value may be different across all of these mentions they all refer to the same person. When doing analysis related to Roger Guta not capturing each mention of this person due to differences in how they are referenced could adversely impact the results. According to some embodiments contextual similarity of usage can be utilized as can properties associated with an entity and other algorithms to group all of these references into what can be referred to as a globally resolved concept. Without this capability an analysis of Roger Guta may miss some very important activities related to him as well as attribute activities to other people when in fact they all were related to the same person.

In some example embodiments in the Resolve phase similar concepts can be identified based on their usage in context e.g. synonym generation . A core premise of this analysis can be that language should be treated as a signal composed of symbols between agents. The encoding of meaning into the signal can be done through consistent selection of symbols that have stable histories of interactions e.g. co occurrences within short attention ranges over a longer global history of usage related to these symbols. The pattern of usage of a particular entity taken globally can form a signature. Entities that have similar usage patterns or signatures can be related semantically. Algorithms used to perform this analysis can provide a mathematical formalization and computation for that notion of similarity. This analysis can be useful for identifying both explicit and implicit relationships between people or other entities. For example the name of a world leader can be semantically related to other world leaders. Thus if searching for concepts similar to Barack Obama other people such as Vladimir Putin Angela Merkel and David Cameron may be returned because they all share the concept of being world leaders.

Continuing an illustration from discussions above the entity resolution analysis according to some embodiments of the present disclosure may uncover some additional facts about Roger Guta such as the fact that the Roger Guta mentioned in the announcement of the P G board appointment is the same Roger Guta who serves on the board of a leading investment bank. The similarity analysis may uncover that Roger Guta s former mentor is Raj Mojihan who also is the founder of Gallot Company. Other additional facts such as Roger Guta being born in 1956 and graduating from Princetown University would also be added to our understanding of the concept of Roger Guta .

In some embodiments a third phase of the Read Resolve and Reason workflow is Reason. Functions of the Reason phase of analysis can operate to understand and correlate all of the information discovered in the prior two phases to include important people places events and relationships uncovered in the data. According to some embodiments this can be accomplished by amplifying human intelligence through a variety of algorithms to manipulate the collection of concepts and relationships that ultimately help end users answer questions.

In accordance with some embodiments reasoning processes Reason phase may refer to the use or manipulation of concepts and relationships to answer end user questions. Reasoning may be primitive atomic or complex orchestrated to support a specific business use case . The following are some examples of types of reasoning sometimes expressed herein in terms of respective reasoners that can be used to amplify human intelligence in accordance with some embodiments.

 Connectivity reasoning can relate to given a set of features using an operator to test for linkages between concepts relationships or messages. Similarity reasoning see e.g. Similarity at block of can relate to given a set of features using an operator to compare concepts relationships or messages and generate a ranked order. A model thereby may relate to having a selection of features wherein a component of the system performs the weighting of features based on statistics associated with a global graph. A constraint of Similarity can be modular such that one kind of similarity algorithm can be chosen over others that may function together as a kind of composite function.

 Temporal and Spatial reasoning can relate to the assignment of space locale and time as a set of ranges used to constrain relationships and resolved entities. Frequency and Trending reasoning can relate to given a set of features using an operator to generate counts of concepts relationships or messages that satisfy constraints such as occurrence over time and optionally space.

 Pattern and Anomaly Detection reasoning can relate to given a set of features using an operator to test for the existence of or a change in the historical state or expectation of a concept relationship or message and detect and notify a user of matches. For example Pattern and Anomaly Detection reasoning can be used to analyze a past calendar week to determine what users are starting to interact as a group in the data that have never interacted with each other before. Also this type of reasoning can be used to look for new users that are starting to interact with together. The corresponding data may then be tagged for identification as an emerging group or emerging idea thus adding to the representation.

 Anomaly reasoning as used herein can generally be defined as a delta or deviation in an expectation of certain primitives in the Knowledge Graph. An Anomaly reasoner can be constantly calculating against a certain set of entities types of entities and looking for any deviation that is above the expectation beyond some constraint. In an exemplary implementation relating to communication between two parties one party may start communicating with a party outside of a company and potentially giving away in an unauthorized sense privileged information. If the one party is communicating with someone new that they previously did not communicate with this can be considered a deviation as can two parties discussing subjects that are normally not part of their ordinary conversations or where two parties that had a long term relationship in the past suddenly end communication.

 Grouping reasoning can relate to given a set of features using an operator to partition or separate a collection of concepts relationships or messages into sets. Anticipation and Prediction reasoning see e.g. Prediction at block in can relate to given a set of features using an operator to estimate future values of concepts relationships or messages. Inference reasoning see e.g. Inference at block in can relate to given a set of features using an operator to generate new non explicit connections between concepts relationships or messages through inductive deductive and abductive logic absolute or probabilistic . Influence reasoning can relate to a measurement of an effect of entities or objects to one another in the Knowledge Graph.

Now referring to aspects of Resonate according to some embodiments of the present disclosure will be described in further detail. Resonate in accordance with the embodiment shown in may perform the functions of Resonate shown in as well as further functions described below. As shown in the diagram of Resonate generally represented by block in accordance with some embodiments can provide a way of back propagating learning from Global Analytics to Local Analytics . It should be recognized that Global Analytics and Local Analytics as discussed herein can comprise some or all of the functionality of Global Analytics and Local Analytics as discussed above with respect to the embodiment shown in . In some embodiments Resonate can identify an error resulting from a statistical language model trained using training data and cause a model training process to correct the error and produce corrected updated training data . The error may be an error that occurred when predicatively annotating certain text data to have a particular value or label for example a categorization error from named entity recognition models. The annotations may be semantic annotations to text data for creating annotated messages by generating at least in part by a trained statistical language model predictive labels that correspond to part of speech syntactic role sentiment and or other language patterns associated with the text data.

In one embodiment Resonate can identify the errors at a global level in a Knowledge Graph through Global Analytics reasoners such as Resolve see e.g. Resolve at and corresponding description above . Resonate can update the respective data e.g. annotation label value to be accurate and consistent record the changes in a change log and then back propagate the corrected data into training information used by supervised model training at thereby improving the accuracy of future predictions.

Resonate can read from globally fixed data generate artificial training and teach to rebuild models. In some embodiments certain functions of Resonate can be implemented through the use of an autonomous trainer agent that performs internal training which is a different modality of training as compared to supervised training by a human analyst that annotates and corrects model results. In example embodiments end users can provide input user feedback to correct values and relationships in the Knowledge Graph via a user interface such as a graphical user interface to provide for user feedback driven correction of the Knowledge Graph . These corrections may be recorded in the change log . If multiple users make changes then different change logs may be reconciled through an administrative process whereby a user with particular permissions and authorities to make changes to the Knowledge Graph can determine and select the best and or most accurate updates and make them canonical. It should be appreciated that the changes are not limited to being made by human users and may be made using a reasoner or other automated statistical process as described herein with respect to certain embodiments. Having made the updates to the Knowledge Graph these changes can be recorded in the change log and back propagated into new training data for supervised model training processes at and yield more accurate prediction from the output of Local Analytics .

In some embodiments well vetted high confidence reference data from back end storage can be ingested e.g. via an ingestion engine not shown and treated with similar authority as end user feedback by overriding values that were derived from the Knowledge Graph and replacing those values with appropriate values from the reference data which may be at the discretion of a system administrator. The reference data may include customer lists ontologies lists of businesses and or census information about various people. The updated values from the Knowledge Graph can once again be back propagated into updated training data for supervised model training processes at and yield more accurate prediction from the output of Local Analytics .

In some embodiments with every update from any one of the above initial sources improvements in Local Analytics models can result. These Local Analytics models correspond to models created using the updated training data whereas the previous state of the models prior to the alteration and or improvements to the training data would be a model created using prior training data . A new model created using the updated training data can yield higher quality features and annotations on individual messages that are utilized by other Global Analytics reasoners to thereby improve the quality of their outputs which include aggregates and sets of concepts relationships and other key objects in the Knowledge Graph . These improvements mean that the outcomes of Global Analytics functions such as resolution functions performed by Resolve reasoners see e.g. Resolve in and corresponding description above will improve and therefore yield additional corrections for training information.

Therefore the implementation of Resonate functionality in accordance with some embodiments can provide for an ongoing loop that continually improves the quality of the Knowledge Graph. As such it effectively allows for a virtuous circle of improving data. This ongoing loop of Resonate can be performed indefinitely for a certain predefined number of iterations or until a certain predefined level of accuracy in annotations or other metrics is reached or exceeded for example a threshold level of accuracy and or based on a predetermined amount of error tolerance.

Certain user defined reasoners in accordance with some embodiments can perform functions such as social network identity resolution to outside structured data consumer data and or recommend in news stories based on interests of a user. User defined reasoners may also include reasoners for determining user influence on particular issues by mapping probability of an assertion to propagate in the Knowledge Graph from a target network based on a profile of the user and characterization of the assertion. User defined reasoners may also relate to changes in user opinion over time and assertion factorization of user opinion which is associated with messages assertions that may trace drive current makeup of popular assertions. Additionally user defined reasoners may also relate to user profile completion inferencing. As an illustrative example I know X Y about person A. I know they are most like persons B C who have property as True . . . with what confidence can I assume property Z is true of A . User defined reasoners can also identify emerging influencers change in influence over time relating to people issues and messages assertions. In some embodiments one or more reasoners described above that apply to social media data plus linked textual content may be used. A Knowledge Graph of properties and beliefs can be created from analyzing streams of conversation and metadata and projecting it over geography and over time.

Various types of reasoners can be system level reasoners residing in Resolve which can be system privileged. Types of reasoners may also relate to a taxonomy of categories that can be able to analyze activities for example world leaders that have meetings . As an example illustration inanimate objects like chairs or televisions do not have meetings but generally all people have meetings some world leaders occupy an office in the government and others such as corporate executives do not. Some types of reasoners may also relate to ontology of relationship clusters between induced categories.

Each of the above described Reasoning capabilities can be used in ensemble to enable complex reasoning capabilities such as social network analysis and sentiment analysis. As an example for social network analysis an application may use Connectivity Grouping and Frequency and Trending reasoning to show high level patterns and affinities among individuals and groups. As another example for sentiment analysis Connectivity and Grouping reasoning that leverage categorized modifiers as features can yield positive or negative sentiment detection and scoring about various concepts in the system.

As the Knowledge Graph becomes progressively richer earlier performed decisions for example as performed in the Read phase may be overridden by a reasoner. Whereas processes in the Read phase may be limited to one document at a time what knowledge was in the one document and what knowledge was in the model it was trained from a reasoner on the other hand may have knowledge of all the global data and can make corrections to errors. For example reasoners may have access to knowledge to correct an earlier mistake wherein three instances of the same person led to the entity being identified as an organization rather than a person. Accordingly reasoners can have the privilege and ability to override the mistake.

Aspects of Resonance according to some embodiments of the present disclosure can include streaming concept resolution. In streaming concept resolution when a state of a model has been built with an initial state through global co reference global concept resolution streaming resolution is enabled such that as data is coming in fast discrimination decisions may be performed as to where a given entity should be placed. Using stored conditional random field models decoding is performed which includes making a best judgment like a maximum a posteri probability judgment of what class a given stream belongs in such as a person or location. When a decision is made on the type of entity in the Read phase a feature vector can be created around that particular set of tokens to make that decision. Outputs from the streaming concept resolution from a sort of per message stage with each entity can put those into the right initial configuration after an initial configuration has been set up. As such a signature of an entity can be best matched to previously resolved entities as the data comes in. The system may run in a batch mode in the background. An example implementation can monitor a news feed in another country in real time as pivotal events unfold.

At operation a knowledge graph representation of the aggregated messages is constructed. At operation one or more semantic annotation errors are identified and at operation the one or more respective semantic annotations with the annotation errors are updated to correct the errors. Identifying the errors and updating the respective annotations operations and can include identifying the annotation errors from the knowledge graph representation and updating the respective annotations in the knowledge graph representation. Identifying the annotation errors from the knowledge graph representation and updating the respective annotations can additionally or alternatively include receiving an annotation correction from one or more users via a user interface.

Identifying the annotation errors operation can include identifying a categorization error from a named entity recognition NER model. Updating the respective annotations operation can include overriding values derived from the knowledge graph representation based at least in part on values from predetermined information in structured reference data. At operation the update to the annotations is recorded in a change log and operation corrected data corresponding to the corrected annotations is back propagated into training data for further training of the statistical language model. Updating the respective annotations operation and or back propagating the corrected data operation can be performed by an autonomous trainer agent. The process of identifying updating recording and back propagating can be performed repeatedly until a predetermined level of accuracy of the annotations has been reached and or a predetermined number of iterations have been performed.

Generated models such as enhanced models may be provided to other applications or components collectively represented by reference numeral for performing various natural language processing NLP functions at other locations in a larger system and or using resources provided across multiple devices of a distributed computing system. A user interface executing on the computer e.g. graphical user interface may be configured to receive user input related to for example text annotation functions associated with some embodiments described herein.

The improved annotation training prediction and predicted data operations may be managed via the model training client . Training prediction and storage of enhanced models can be implemented on another computer which may be locally or remotely coupled to and in communication with user computer via a communication link such as a wired or wireless network connection. The computer may include some or all of the components of the computer shown in . A base model may be improved by closing the feedback loop where the data may include tokenization part of speech POS tagging chunking and or name entity recognition NER annotation for example.

In some embodiments a base model may be used to predict annotations to a first segment of text. Users such as data analysts or linguists may then correct the annotation predictions. The resulting corrected data may then be used to train a new model based on just the corrections made to the predictions on the first segment of text. This new model may then be used to predict annotations on a second segment of text. The corrections made to predictions on the second segment of text may then be used to create a new model and predict annotations on a third segment of text and so on accordingly. This prediction annotation and training process may progressively improve a model as additional segments of text are processed.

As shown the computer includes a processing unit a system memory and a system bus that couples the memory to the processing unit . The computer further includes a mass storage device for storing program modules. The program modules may include modules executable to perform one or more functions associated with embodiments illustrated in one or more of . For example the program modules may be executable to perform functions for Local Analytics reasoning Global Analytics reasoning model training and or construction and maintenance of a Knowledge Graph as described above with reference to the embodiments shown in . The mass storage device further includes a data store which may be configured to function as for example the Knowledge Base and or message store described above with respect to the embodiments shown in .

The mass storage device is connected to the processing unit through a mass storage controller not shown connected to the bus . The mass storage device and its associated computer storage media provide non volatile storage for the computer . By way of example and not limitation computer readable storage media also referred to herein as computer readable storage medium may include volatile and non volatile removable and non removable media implemented in any method or technology for storage of information such as computer storage instructions data structures program modules or other data. For example computer readable storage media includes but is not limited to RAM ROM EPROM EEPROM flash memory or other solid state memory technology CD ROM digital versatile disks DVD HD DVD BLU RAY or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the computer . Computer readable storage media as described herein does not include transitory signals.

According to various embodiments the computer may operate in a networked environment using connections to other local or remote computers through a network via a network interface unit connected to the bus . The network interface unit may facilitate connection of the computing device inputs and outputs to one or more suitable networks and or connections such as a local area network LAN a wide area network WAN the Internet a cellular network a radio frequency network a Bluetooth enabled network a Wi Fi enabled network a satellite based network or other wired and or wireless networks for communication with external devices and or systems. The computer may also include an input output controller for receiving and processing input from a number of input devices. Input devices may include but are not limited to keyboards mice stylus touchscreens microphones audio capturing devices or image video capturing devices. An end user may utilize such input devices to interact with a user interface for example a graphical user interface for managing various functions performed by the computer .

The bus may enable the processing unit to read code and or data to from the mass storage device or other computer storage media. The computer storage media may represent apparatus in the form of storage elements that are implemented using any suitable technology including but not limited to semiconductors magnetic materials optics or the like. The program modules may include software instructions that when loaded into the processing unit and executed cause the computer to provide functions associated with embodiments illustrated in . The program modules may also provide various tools or techniques by which the computer may participate within the overall systems or operating environments using the components flows and data structures discussed throughout this description. In general the program module may when loaded into the processing unit and executed transform the processing unit and the overall computer from a general purpose computing system into a special purpose computing system.

The processing unit may be constructed from any number of transistors or other discrete circuit elements which may individually or collectively assume any number of states. More specifically the processing unit may operate as a finite state machine in response to executable instructions contained within the program modules . These computer executable instructions may transform the processing unit by specifying how the processing unit transitions between states thereby transforming the transistors or other discrete hardware elements constituting the processing unit . Encoding the program modules may also transform the physical structure of the computer readable storage media. The specific transformation of physical structure may depend on various factors in different implementations of this description. Examples of such factors may include but are not limited to the technology used to implement the computer readable storage media whether the computer readable storage media are characterized as primary or secondary storage and the like. For example if the computer readable storage media are implemented as semiconductor based memory the program modules may transform the physical state of the semiconductor memory when the software is encoded therein. For example the program modules may transform the state of transistors capacitors or other discrete circuit elements constituting the semiconductor memory.

As another example the computer storage media may be implemented using magnetic or optical technology. In such implementations the program modules may transform the physical state of magnetic or optical media when the software is encoded therein. These transformations may include altering the magnetic characteristics of particular locations within given magnetic media. These transformations may also include altering the physical features or characteristics of particular locations within given optical media to change the optical characteristics of those locations. Other transformations of physical media are possible without departing from the scope of the present disclosure.

Although some embodiments described herein have been described in language specific to computer structural features methodological acts and by computer readable media it is to be understood that the disclosure defined in the appended claims is not necessarily limited to the specific structures acts or media described. Therefore the specific structural features acts and mediums are disclosed as exemplary embodiments implementing the claimed disclosure.

It is to be understood that the mention of one or more steps of a method does not preclude the presence of additional method steps or intervening method steps between those steps expressly identified. Steps of a method may be performed in a different order than those described herein. Similarly it is also to be understood that the mention of one or more components in a device or system does not preclude the presence of additional components or intervening components between those components expressly identified.

The various embodiments described above are provided by way of illustration only and should not be construed to limit the scope of the present disclosure. Those skilled in the art will readily recognize various modifications and changes that may be made without following the example embodiments and applications illustrated and described herein and without departing from the true spirit and scope of the disclosure as set forth in the appended claims.

