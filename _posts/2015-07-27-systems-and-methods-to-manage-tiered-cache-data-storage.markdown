---

title: Systems and methods to manage tiered cache data storage
abstract: Systems and methods for managing records stored in a storage cache are provided. A cache index is created and maintained to track where records are stored in buckets in the storage cache. The cache index maps the memory locations of the cached records to the buckets in the cache storage and can be quickly traversed by a metadata manager to determine whether a requested record can be retrieved from the cache storage. Bucket addresses stored in the cache index include a generation number of the bucket that is used to determine whether the cached record is stale. The generation number allows a bucket manager to evict buckets in the cache without having to update the bucket addresses stored in the cache index. Further, the bucket manager is tiered thus allowing efficient use of differing filter functions and even different types of memories as may be desired in a given implementation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09489239&OS=09489239&RS=09489239
owner: PernixData, Inc.
number: 09489239
owner_city: San Jose
owner_country: US
publication_date: 20150727
---
This application is a continuation in part of and claims priority to U.S. application Ser. No. 14 455 090 filed on Aug. 8 2014 and entitled Systems and Methods to Manage Cache Data Storage and is also a continuation in part of and claims priority to U.S. application Ser. No. 14 609 085 filed on Jan. 29 2015 and entitled Systems and Methods to Manage Cache Data Storage in Working Memory of Computing System each of which is hereby incorporated by reference in its entirety.

This patent application relates generally to data caching and more specifically to managing cache data storage.

In computing systems a cache is a memory system or subsystem which transparently stores data so that future requests for that data can be served faster. As an example many modern microprocessors incorporate an instruction cache holding a number of instructions when the microprocessor executes a program loop where the same set of instructions are executed repeatedly these instructions are fetched from the instruction cache rather than from an external memory device at a performance penalty of an order of magnitude or more.

In other environments such as where a computing system hosts multiple virtual machines under the control of a hypervisor with each virtual machine running one or more applications caching of objects stored on a network attached storage system can provide significant performance improvements. In some instances records are cached and then written to the network attached storage system according to a write back algorithm. In the write back algorithm the received record is written to the cache before being written to the network attached storage system. The cache system can then direct the writing of the record to the network attached storage system.

When read commands are sent from the virtual machine to the network attached storage it may be more efficient to read the records from the cache rather than from the network attached storage. While other write through and write back caching algorithms exist caching and retrieving data quickly and accurately remains a challenge.

One common challenge in caching systems is that the read and write operations to the cache system are not optimized for the operational characteristics of the media used to store the contents of the cache system. Some examples of media used to store the contents of a cache system are random access memory RAM solid state disk SSD PCIe Flash Non volatile dual in line memory module NVDIMM etc. Organizing data on a cache device for a plurality of cache media types remains a challenge.

Finally storing data to and removing data from a cache system requires vigorous updates of metadata records of the cache system e.g. index entries that reference the data stored in the cache system at any given point in time . These updates impose a significant performance overhead to storing retrieving and removing data from the cache system. As cache system media becomes faster the overhead becomes a significant portion of the overall cache operation time and hampers efficient performance. More efficient metadata records for the cache system are required.

According to some embodiments a method of performing read commands and write commands to a tiered bucket manager comprising a master bucket manager a slave bucket manager and a migration thread the method comprising receiving a write command sent from a first virtual machine to a host operating system running on a computing system the write command instructing a storage system to store a first record at a first memory location and storing the first record in a first bucket of the master bucket manager if the master bucket manager is not full else evicting a second bucket of the master bucket manager that has been migrated to the slave bucket manager and storing the first record in the evicted second bucket of the master bucket manager else evicting a third bucket of the master bucket manager and storing the first record in the third bucket of the master bucket manager and receiving a read command sent from the first virtual machine to the host operating system running on the computing system the read command instructing the storage system to read a second record from a second memory location and determining that the second record is in the master bucket manager and reading the second record from the master bucket manager else determining that the second record has been evicted or migrated from the master bucket manager determining that the second record is in the slave bucket manager reading the second record from the slave bucket manager performing a reverse filter function on the second record read from the slave bucket manager using the migration thread writing the reverse filter functioned second record to the master bucket manager and reading the reverse filter functioned second record from the master bucket manager.

In a further embodiment the method further comprising migrating a third record from the master bucket manager to the slave bucket manager by determining that the master bucket manager is under pressure identifying a coldest bucket of the master bucket manager performing a filter function on contents of the coldest bucket of the master bucket manager using the migration thread writing the filter functioned contents of the coldest bucket of the master bucket manager to the slave bucket manager updating in a translation table of the tiered bucket manager a reference to the coldest bucket of the master bucket manager to further reference the slave bucket manager and marking as migrated the coldest bucket of the master bucket manager.

According to some embodiments a system comprising a master bucket manager configured to store cached records a slave bucket manager configured to store cached records migrated from the master bucket manager a migration thread configured to migrate cached records from the master bucket manager to the slave bucket manager using a filter function and a translation table configured to store references to cached records stored in the master bucket manager and cached records migrated from the master bucket manager to the slave bucket manager.

According to some embodiments a non transitory computer readable storage medium having instructions embodied thereon the instructions executable by one or more processors to perform read commands and write commands to a tiered bucket manager comprising a master bucket manager a slave bucket manager and a migration thread comprising receiving a write command sent from a first virtual machine to a host operating system running on a computing system the write command instructing a storage system to store a first record at a first memory location and storing the first record in a first bucket of the master bucket manager if the master bucket manager is not full else evicting a second bucket of the master bucket manager that has been migrated to the slave bucket manager and storing the first record in the evicted second bucket of the master bucket manager else evicting a third bucket of the master bucket manager and storing the first record in the third bucket of the master bucket manager and receiving a read command sent from the first virtual machine to the host operating system running on the computing system the read command instructing the storage system to read a second record from a second memory location and determining that the second record is in the master bucket manager and reading the second record from the master bucket manager else determining that the second record has been evicted or migrated from the master bucket manager determining that the second record is in the slave bucket manager reading the second record from the slave bucket manager performing a reverse filter function on the second record read from the slave bucket manager using the migration thread writing the reverse filter functioned second record to the master bucket manager and reading the reverse filter functioned second record from the master bucket manager.

Write back and write through caching techniques are used to reduce the amount of time required by a computing system to process read and write commands also referred to as IO commands by storing those commands in a faster short term memory such as a storage cache instead of relying solely on a slower long term memory such as a storage system. Records can be written to or read from the storage cache during operation.

A typical IO command identifies a record using a memory location of the storage system. However the caching system does not store the record at an address in the storage cache that is immediately recognizable from the memory location of the storage system. To read from the storage cache it is necessary to have a way to determine where the record is stored in the storage cache from the memory location of the storage system. According to various embodiments described herein a cache index is used to map a memory location of the storage system to a location in the storage cache when a record is written to the storage cache. The cache index may be extended to accommodate IO commands smaller than a predefined size. As described in the illustrative examples included herein the cache index can be in the form of a BTree also known as a Bayer Tree Bushy Tree or Boeing Tree .

The records are stored in buckets within the storage cache. A bucket is a predefined contiguous set of locations in the storage cache. Each bucket is allocated to one virtual machine at a time. The bucket has a bucket address that includes a bucket identifier a bucket index and a generation number. From the bucket identifier and the bucket index a location in the storage cache can be identified. From the generation number a determination can be made as to whether the record stored in the bucket is stale.

When a virtual machine generates a read command or a write command the application sends the generated command to the host operating system . The virtual machine includes in the generated command an instruction to read or write a record at a specified location in the storage system . The caching system receives the sent command and caches the record and the specified storage system memory location. In a write back system the generated write commands are subsequently sent to the storage system .

In some embodiments of the present approach and as is apparent to those skilled in the art in light of the teachings herein the environment of can be further simplified to being a computing system running an operating system running one or more applications that communicate directly or indirectly with the storage system .

A cache index is a logical data structure stored by the caching system . The cache index is configured to store for each memory location in the storage system that has a record written thereto a bucket address of a bucket in which a cached copy of the record is stored. In some embodiments the cache index is a BTree as discussed in greater detail in connection with .

When an IO command e.g. a read command or a write command is received the bucket manager is configured to determine the location in the storage cache containing the desired record from the bucket address in the cache index . The bucket manager then executes the command or causes the command to be executed by another component of the caching system . The functionalities of the bucket manager are explained in greater detail in connection with .

The metadata manager allocates those portions of the cache index that correspond to memory locations in the storage system e.g. SAN memory locations where records that have been cached in the cache storage are stored or will be stored. The metadata manager further traverses the cache index to determine whether a record is stored in the storage cache . The metadata manager can allocate or de allocate levels nodes or entries in the cache index depending on where records in the cache are stored in the storage system . As such the size of the cache index can be increased or decreased depending on the amount of records presently cached in the storage cache . The metadata manager can expand the cache index to include additional entries or levels. The functionalities of the metadata manager are explained in greater detail in connection with .

In an embodiment the cache index is organized into three levels and can be expanded to four levels as discussed elsewhere herein. Each level of the cache index contains one or more entries that are representative of a continuous range of memory locations in the storage system . For example in embodiments where the storage system is a SAN SAN memory locations expressed as SAN offset addresses are divided within the cache index so as to be contiguous with one another.

To illustrate is a diagram of a cache index in the form of a BTree according to various embodiments. The BTree has three levels depicted as levels zero one and two . Due to space limitations of the figures all of the entries and nodes in the BTree are not depicted. As explained in greater detail elsewhere herein level two includes bucket addresses that specify cache locations organized in terms of buckets in the storage cache . In the example embodiment of the storage system is a SAN and memory locations in the storage system are referred to as SAN memory locations .

Level zero comprises a single level zero node having a series of entries that in turn correspond to a range of SAN memory locations of the SAN. The entries within the level zero node at the level zero collectively correspond to all of the SAN memory locations. To illustrate level zero can contain 16 entries each corresponding to one sixteenth of the available SAN memory locations. The level zero entry can correspond to a first sixteenth of the SAN memory locations the adjacent entry can correspond to a second sixteenth of the SAN memory locations and so on for the third and fourth entries. In an embodiment the individual entries within the level zero comprise 16 bytes. The 16 bytes include a validity indicator and a pointer to a level one node of a plurality of level one nodes in a level one .

As is known in the art a SAN memory location can be expressed as an offset from SAN memory location zero 0 . Using the BTree and with the SAN having approximately 64 terabytes TB of storage the level zero entry corresponds to SAN memory locations at offsets of zero to four TB one sixteenth of 64 TB . The next entry of the level zero corresponds to SAN memory locations at offset of four TB to eight TB the third entry of the level zero corresponds to SAN memory locations at offset of eight TB to twelve TB and the fourth entry of the level zero corresponds to SAN memory locations at offset of twelve TB to sixteen TB and so on additional entries not depicted . Thus the entirety of the memory locations in SAN or other storage system can be represented within the level zero .

Below the level zero in the BTree the level one comprises a series of entries that each correspond to a narrower range of SAN memory locations than the entries at the level zero . Each entry within the level zero has a corresponding node at the level one e.g. level zero entry is the parent of level one node not all nodes and entries are shown . The individual entries within the level one include a validity indicator and a pointer to another entry in a level two . In some embodiments each entry e.g. level one entry comprises sixteen bytes. The depicted node within the level one comprises entries that collectively correspond to all of the SAN memory locations within level zero entry . Continuing the example above the level zero entry corresponds to SAN memory locations at offsets of zero to four TB. In one embodiment to represent the entirety of this portion in the SAN or other storage system each entry in the nodes of level one corresponds to 128 megabytes MB one thirty two thousandth of 4 TB and the level one comprises four nodes each potentially having 32 768 entries. Thus the level one entry corresponds to SAN offsets from zero to 128 MB the next offsets of 128 MB to 256 MB the next 256 MB to 384 MB and so on until the entirety of the four TB is represented in a node within level one .

Below the level one in the BTree the level two comprises a series of entries that each correspond to a narrower range of SAN memory locations than the entries at the level one . The entries within the shown level two node collectively correspond to all of the SAN memory locations within level one entry . Each entry within level one has a corresponding node at the level two not all nodes and entries are shown . Continuing the example above the level one entry can correspond to SAN memory locations at offsets of zero to 128 MB. In one embodiment to represent the entirety of this portion in the SAN each entry in the nodes of level two corresponds to four kilobytes kB one thirty two thousandth of 128 MB of SAN memory. Thus the level two entry corresponds to SAN offsets from zero to four kB the next offsets of 4 kB to 8 kB the next 8 kB to 12 kB and so on until the entirety of the 128 MB is represented in a node within level two .

The storage cache is organized in terms of buckets each representing for example 512 KB of cache memory. The exact size of the bucket can be chosen to be a value at which the underlying cache memory medium performs most efficiently. For example an embodiment that operates on NAND flash devices as the cache memory medium uses the erase block size of the underlying flash device as the bucket size. Each entry in the level two e.g. level two entry includes a bucket address that specifies a bucket of the plurality of buckets in the storage cache where the record stored at a SAN memory location is stored. Records stored at different SAN offsets can be stored in the same bucket . However each entry in the level two only includes one bucket address.

The second portion of the level two entry of the BTree comprises a bucket address . In the depicted embodiment the level two entry comprises only one bucket address. The bucket address is eight bytes and contains a bucket number a bucket index and a bucket generation number. The bucket number identifies a bucket of the buckets constructed within the storage cache where the record having that SAN memory address is stored. The bucket index identifies a location within the bucket where the record is stored. Because the buckets can be significantly larger than individual records multiple records at separate SAN offsets can be stored in the same bucket . In some instances a bucket is 512 KB of cache memory. A generation number included in the bucket address indicates the generation number of the bucket at the time the record was stored in the bucket . As will be discussed in connection with the bucket manager the bucket generation number is used when determining if the contents of bucket have been invalidated since the record was stored in the bucket .

When a read command is received the BTree is used to determine if the record of the read command specified by a SAN memory location is stored in the storage cache . If the record is stored in the storage cache the BTree identifies a bucket in the cache storage where the record is stored. is a flowchart of a method of executing a read command according to various embodiments. The method can be performed by the bucket manager in connection with the BTree and the storage cache . As will be explained the metadata manager is configured to traverse the BTree .

In an operation a read command sent from the virtual machine to the host operating system is received by the caching system . In embodiments where the storage system comprises a SAN the read command specifies the record to be read by a SAN memory location e.g. a SAN offset address and a length of data to be read. The read command also indicates a buffer where the record is to be written to.

In an operation a determination is made by for example the metadata manager whether the record has been cached for the SAN memory location. To determine whether the record stored at the SAN memory location is cached the cache index e.g. BTree is traversed by the metadata manager . The traversal of the cache index returns a cache miss or a bucket address of the cached record. is a flowchart of a method of traversing a BTree according to various embodiments.

In an operation the SAN offset address or memory address of the storage system included in the read command is used to identify a corresponding entry e.g. level zero entry in the level zero L0 of the BTree . The metadata manager in an operation determines whether the level zero entry is valid. The level zero entry is valid if at least one record has been stored in the range of SAN memory locations covered by the level zero entry . If no records have been stored in that range of SAN memory locations the offset is not cached in the BTree and the level zero entry is not valid.

If the level zero entry is valid the method continues to operation . In the operation the metadata manager reads the level one L1 entry e.g. the level one entry corresponding to the received SAN offset address. The metadata manager then determines in an operation whether the level one entry is valid. Like the determination in the operation the level one entry is valid if records have been stored in the corresponding portion of the SAN. If no records have been stored in that portion of the SAN the offset is not cached in the BTree and the level one entry is not valid. If the level one entry is valid the method returns a yes indicating that the SAN offset is cached in the BTree .

Returning to if the outcome of the determination in operation is that the SAN offset is not cached in the cache index a cache miss is returned in an operation . If however the outcome of the determination in the operation is that the offset is cached in the cache index the bucket manager reads the level two entry of the cache index corresponding to the SAN memory address in the operation . As part of the operation the bucket manager further determines the location in the storage cache where the record is stored from the bucket address . While not shown at the operation the method can return a cache miss operation if for example the validity bitmap indicates that the contents at the SAN memory location are not stored in the storage cache or if the level two entry does not contain a bucket address .

In an operation bucket generation numbers are compared to determine if there is a match. As explained with respect to the bucket address included in the cache index includes a bucket generation number indicating the generation of the bucket at the time the record was stored in the bucket . The bucket manager stores a current bucket generation number as part of the eviction process described elsewhere herein. If the bucket generation number stored in the cache index does not match the current bucket generation number stored by the bucket manager a cache miss is returned in operation . If the generation numbers do match in an operation the bucket manager reads the record identified in the read command from the storage cache .

In an operation a write command is received from the virtual machine by the caching system . In embodiments where the storage system comprises a SAN the write command comprises a SAN memory location where a record is to be stored a length of the record and the record to be stored.

In an operation a bucket address where the record is stored in the storage cache is obtained from the bucket manager . The operation is described in greater detail in connection with .

In an operation the metadata manager determines whether the level zero entry e.g. level zero entry corresponding to the SAN memory location is allocated i.e. valid in the cache index . If the L0 entry is not allocated the metadata manager allocates the L0 entry in an operation .

Once the L0 entry is allocated or validated the metadata manager determines whether the level one entry e.g. level one entry corresponding to the SAN memory location is allocated in the cache index in an operation . If the level one entry is not allocated the metadata manager allocates the level one entry in an operation .

In an operation the metadata manager determines whether the level two entry e.g. level two entry corresponding to the SAN memory location included in the write command is empty and thus available. If the level two entry is empty the metadata manager in an operation populates the obtained bucket address of the operation at the level two entry . In this operation the metadata manager further updates the validity bitmap of the level two entry to indicate the SAN memory location of the record.

If the outcome of the determination operation is that the level two entry is not empty in an operation the metadata manager determines whether the record included in the write command of the operation has completely overwritten the existing level two entry . If so the obtained bucket address is populated at the level two entry and the validity bitmap is updated in the operation .

If the outcome of the determination operation is that the record included in the write command of the operation did not completely overwrite the existing level two entry the received record can be an unaligned IO command having a size of less than four kB. In this case the metadata manager determines whether the level two entry contains a pointer to a level three entry in an operation .

If the outcome of the determination operation is that there is no pointer to a level three entry the metadata manager determines whether the level two entry is evicted in an operation . Eviction is discussed below at least in connection with . Similar to the operation the metadata manager determines whether the generation number in the bucket address obtained in the operation from the bucket manager matches a generation number in the bucket address stored in the cache index . If the generation numbers do not match the level two entry is evicted. The metadata manager populates the obtained bucket address of the operation at the level two entry and updates the validity bitmap in the operation .

In an operation if the outcome of the determination operation is that the level two entry is not evicted the metadata manager allocates a level three entry to accommodate the unaligned IO command in an operation . In this operation the metadata manager updates the level two entry to include a pointer to the level three entry .

In an operation the metadata manager merges the bucket address obtained in the operation and the existing bucket address in the level two entry to the allocated level three entry . Thus the level three entry can store two or more bucket addresses indicating where each unaligned IO command is stored.

Returning to the operation if the determination made is that there is an existing pointer to the level three entry in the level two entry in an operation the metadata manager determines if the level three entry has been evicted by comparing the generation numbers in the bucket addresses stored in the level three entry to the generation numbers in the bucket addresses maintained by the bucket manager . If the generation numbers do not match the buckets in the level three entry have been evicted and the operation is performed.

If however the determination made in the operation is that the level three entry has not been evicted the metadata manager performs operation where the bucket address obtained in the operation is merged with the existing bucket address into the level three entry .

In an operation the metadata manager receives an invalidate command from the virtual machine identifying a SAN memory location e.g. SAN offset address to be invalidated.

If the higher level entries are not allocated in the BTree for the SAN memory address included in the invalidate command the BTree does not store a bucket address for the SAN memory location. In an operation the metadata manager determines whether the level zero entry corresponding to the SAN memory address included in the invalidate command is allocated. If not the process ends in an operation . Otherwise in an operation the metadata manager determines whether the level one entry corresponding to the SAN memory address included in the invalidate command is allocated. If not the process ends in an operation .

Otherwise in an operation the metadata manager identifies the level two entry corresponding to the SAN memory location included in the invalidate command of the operation and clears the validation bitmap of the level two entry by setting all of the values to zero.

In an operation the metadata manager sends an eviction hint to the bucket manager . The eviction hint identifies the bucket address included in the level two entry and indicates to the bucket manager that the bucket manager can evict the bucket .

Eviction is the process by which buckets in the storage cache can be marked as free for subsequent reuse. is a flowchart of a method of evicting a bucket and returning a bucket address according to various embodiments. The method can be performed by the bucket manager and in an embodiment is initiated when a write command is received from the virtual machine .

In an operation the bucket manager determines whether there is a bucket allocated to the virtual machine from which the write command was received and having available space to store the record included in the received write command. If there is a bucket available in an operation the bucket manager writes the record included in the write command to the available bucket and returns the bucket address where the record was written to the metadata manager .

In an operation if there is no available bucket the bucket manager determines if an eviction hint has been received from the metadata manager as described in connection with . If an eviction hint has been received the method skips ahead to the operation discussed below.

In an operation if no eviction hint has been received the bucket manager identifies which virtual machine has the largest number of buckets allocated to it. The bucket manager determines a number of buckets allocated to each virtual machine in the environment . As discussed above by being allocated to a virtual machine the individual buckets contain records sent by only one virtual machine . A bucket descriptor array of the bucket identifies the virtual machine to which the bucket is allocated.

In an operation the buckets allocated to the identified virtual machine are evaluated so as to identify buckets having all of their stored records sent to the storage system . This is accomplished by the bucket manager checking a pin count of the bucket . The pin count is a value stored in a bucket descriptor array that indicates how many records stored in the bucket have not yet been written to the storage system . When a record is written to the bucket and before it is included in a write command sent to the storage system the pin count is incremented by the bucket manager . After the record in the bucket is retrieved and included in a write command sent to the storage system thus writing back the record the pin count is decremented by the bucket manager . When a bucket includes multiple records which can be at distinct memory locations in the storage system the pin count can be of a value up to the number of records in the bucket . As the records in the bucket are written back to the storage system the pin count is decremented by the bucket manager . A zero pin count indicates that the records stored in the bucket are stored in the storage system .

In an operation if more than one bucket allocated to the identified virtual machine has a zero pin count a least recently used LRU bucket is identified. An LRU bucket is a bucket that has been not been written to or read from more recently than other buckets allocated to the virtual machine . In an embodiment the LRU bucket is selected for eviction.

It is to be understood that by identifying a bucket to be evicted based on the determinations and buckets can be more evenly balanced among the virtual machines .

In an operation based on the eviction hint of the operation or the LRU bucket identified in the operation the bucket manager evicts the bucket . To evict the bucket the bucket manager increments a bucket generation number included in the bucket address maintained by the bucket manager . The bucket manager does not update or increment any bucket generation numbers in the bucket addresses stored in the cache index . In this way eviction is handled independently of the cache index . Thus when reading from or writing to the storage cache the bucket generation number in the bucket address stored in the cache index is compared to the bucket generation number stored by the bucket manager see e.g. operation operation and operation to ensure that the record is not stale and can be retrieved from the cache storage rather than the storage system .

In an operation the evicted bucket is allocated to the virtual machine that sent the write command by the bucket manager by writing a virtual machine identifier to the bucket descriptor array. In an operation the record is stored in the evicted bucket by the bucket manager . In an operation the bucket address with the incremented bucket generation number is returned by the bucket manager to the metadata manager .

Using the described systems and methods records sent from a virtual machine to a host operating system are cached. A cache index is used to determine a bucket in the storage cache where the record is cached based on a memory location of the storage system included in a read command. To write records to the storage cache the record is stored in a bucket and the cache index is updated to include the bucket address . Unaligned IO commands can be accommodated in the cache index by expanding the cache index to include a further level. Buckets can be evicted by the bucket manager independently of the cache index or the metadata manager resulting in more efficient eviction.

In a further embodiment the bucket manager operates in a tiered fashion as will now be explained. Referring now to a tiered bucket manager can be seen which from an external perspective e.g. application programming interface or API appears the same as non tiered bucket manager of thereby avoiding having to make any changes to components outside the bucket manager. To accomplish this tiered bucket manager includes additional components and logic to facilitate a tiered approach. In particular tiered bucket manager includes a translation table a master bucket manager a slave bucket manager and a migration thread .

In many respects master bucket manager of tiered bucket manager functions the same as non tiered bucket manager by transforming a bucket manager address again consisting of a bucket number and a generation number into a flash or memory address of the cache as well as performing write and eviction operations. However rather than evicting data to long term storage instead master bucket master migrates cached data to slave bucket manager hence the term tiered . Further such migrated cached data goes through a filter function performed by migration thread . In turn slave bucket manager also functions in much the same way as non tiered bucket manager by transforming a bucket manager address consisting of a bucket number a generation number and a length into a flash or memory address of the cache as well as performing eviction operations. Translation table keeps track of migrated cached data. Each of these functions and operations are explained further elsewhere herein.

Such a tiered approach has a number of potential advantages and use cases. One example is using data compression as the filter function which can be useful when master bucket manager is implemented in a faster likely more expensive form of memory than slave bucket manager . Such an approach keeps more actively accessed data referred to herein as hot in master bucket manager less actively accessed data referred to herein as warm in slave bucket manager and evicts least actively accessed data referred to herein cold . Another example is using data encryption as the filter function which can be useful when master bucket manager is implemented in a more secure likely more expensive hardware than slave bucket manager . It is to be understood that any combination of memory types can be used for master bucket manager and slave bucket manager and that any filter function including a filter function that makes no change to the cached data being migrated between master bucket manager and slave bucket manager can be used by migration thread to migrate cached data as desired in a given implementation.

It is to be further appreciated that because the tiered bucket manager approach provides the same interface as the previously described non tiered bucket manager approach the filter function e.g. compression does not affect and is hidden from other system components such as the cache index e.g. the BTree and therefore there is minimal impact related to updating the BTree caused by this tiered bucket manager approach. Stated differently the remapping of bucket addresses caused by data migration from master bucket manager to slave bucket manager and from slave bucket manager back to master bucket manager all happens within the tiered bucket manager and therefore at a lower level of data granularity so there is no I O needed to update the BTree which would be a more expensive process because that would require updating things in multiple places in the BTree.

The process of migration thread migrating cached data from master bucket manager to slave bucket manager will now be explained. Referring now to migration process begins with making a determination in step regarding whether master bucket manager is under pressure. In a preferred embodiment master bucket manager is deemed to be under pressure when it is full and those buckets that have been migrated to the slave bucket master is below a configurable threshold. The configurable threshold is set to a value e.g. 2 of the master bucket manager size in one embodiment that attempts to keep a small number of migrated buckets in the master bucket manager so that when the master bucket manager needs to evict buckets it can do so by evicting a migrated bucket. If master bucket manager is not under pressure then in step no data migration is needed and the process returns to step . However if master bucket manager is under pressure then in step the coldest bucket in master bucket manager is selected. In an embodiment this is performed by migration thread calling the eviction algorithm of master bucket manager to request that it identify the coldest bucket in master bucket manager using any known eviction approach e.g. FIFO LIFO LRU etc. . In the example of bucket 3 is the coldest bucket in master bucket manager .

Once the coldest bucket in master bucket manager is selected then in step the contents of that bucket are read and the filter function e.g. compression is performed on that read data. In the example of the contents of bucket 3 the coldest bucket in master bucket manager are read and compressed. The filtered e.g. compressed data is then written to slave bucket manager which may itself trigger an eviction within slave bucket manager as explained elsewhere herein in step and translation table is updated to reflect this data migration so that future read requests sent to tiered bucket manager will be able to locate and return the requested cached data as explained elsewhere herein in step and the coldest bucket in master bucket manager is marked as migrated in master bucket manager .

In the example of the compressed contents of bucket 3 have been written to bucket 0 of slave bucket manager . In this example as shown in where the bucket sizes of master bucket manager and slave bucket manager are each 512 kilobytes kB the data contents had filled bucket 3 of master slave manager yet do not fill bucket 0 of slave bucket manager due to the data being compressed from in this example 512 kB down to 100 kB according to the filter function before being stored in slave bucket manager . Further translation table is updated to reflect this data migration from master bucket manager to slave bucket manager by storing a translation from bucket 3 s address and generation number of master bucket manager to bucket 0 s address with a length of the stored compressed data content of slave bucket manager .

The process then returns to step to again make a determination regarding whether master bucket manager is under pressure.

Read operations using tiered bucket manager will now be explained. Referring now to read process begins with tiered bucket manager receiving a request from the cache index for cached data in this example a read of bucket address 10 with a generation number of 8. In step a determination is made regarding whether master bucket manager contains the requested data i.e. in this example does data stored in bucket address 10 contain data with a generation number of 8 . If it does then the read request is satisfied in step by returning the data from bucket address 10. Alternatively if the requested data has been evicted in which case the generation number does not match the read request or migrated in which case in this example bucket address 10 is marked in master bucket manager as being migrated from master bucket manager then the process continues with step where a lookup is performed on translation table . If this lookup results in not finding the requested data that is in this example there is no bucket address 10 in translation table then a read miss operation is performed in step . If this lookup results in finding the requested data in slave bucket manager that is in this example bucket address 10 with generation number 8 is found in translation table with a pointer to bucket 0 generation number 6 and with length 100 kB then the process continues with step where a read of slave bucket manager is performed using the slave bucket address and length. If the read of slave bucket manager cannot be performed because the data has been evicted from slave bucket manager e.g. the generation number stored in translation table for bucket 0 in slave bucket manager does not match the read request then a read miss operation is performed in step . Alternatively if the read of slave bucket manager can be performed then in step the read data is put through a reverse filter function e.g. decompression to return the filtered data back into its original unfiltered state. The original i.e. reverse filter function data is then written to the master bucket manager in step as explained elsewhere herein and the translation table is updated to reflect this reverse migration of data from slave bucket manager to master bucket manager e.g. by updating the translation table to now point to bucket 5 in master bucket manager . The data is then read in step from master bucket manager e.g. from bucket 5 of master bucket manager unless it has since been evicted as evidenced by a failure to match requested generation number in which case a read miss is returned in step and the read request is then satisfied in step . Returning to step if the lookup results in finding the data in master bucket manager because the requested data was previously reverse migrated from slave bucket manager to master bucket manager according to the process described herein then the process continues with step as has been explained.

As should now be clear read operations first attempt to read cached data from master bucket manager and failing that attempt to locate the requested data from slave bucket manager and if found there via translation table and due to an earlier migration operation the migrated data is reverse filtered and moved back to master bucket manager from slave bucket manager and an update is made to translation table to reflect this reverse migration so that the data can be located in and obtained from master bucket manager . Further as also explained generation numbers are likewise maintained to ensure the data is current.

Write operations using tiered bucket manager will now be explained. Referring now to write process begins with tiered bucket manager receiving a write from the cache index to write or cache data. In step a write operation is attempted by master bucket manager . If master bucket manager is not full then in step the write operation completes and the resulting bucket address is returned to the cache index. Alternatively if master bucket manager is full then in step a determination is made regarding whether there any buckets in master bucket manager that are marked as migrated. If there is one or more bucket in master bucket manager that is marked as migrated which means the data from that migrated bucket has been moved from master bucket manager to slave bucket manager making that data likely to be available for a future cache read operation and therefore a good candidate for eviction from master bucket manager then in step the coldest bucket marked as migrated in master bucket manager is evicted and the process continues with step by completing the write operation on the evicted bucket. If there is not one or more bucket in master bucket manager that is marked as migrated in master bucket manager then the coldest bucket of the buckets in master bucket manager is evicted and the process continues with step by completing the write operation on the evicted bucket. In this way write operations are performed in a similar fashion as with a non tiered bucket manager when there is room in master bucket manager and evictions can be performed based on migrated cold buckets as well as simply cold buckets in master bucket manager .

In an embodiment eviction operations on master bucket manager and slave bucket manager operate independently of each other. Further in light of the known art and teachings herein such eviction operations operate to provide fair share on both e.g. all virtual machines get to use X amount of storage in the master bucket manager and Y amount of storage in the slave bucket manager . Still further in an embodiment different eviction algorithms can be used in the master bucket manager than in the slave bucket manager to achieve best efficiency based on their respective requirements.

In a further embodiment the overall amount of storage or size of each of master bucket manager and slave bucket manager can be configured altered and adjusted or resized. In particular a configurable ratio can be set either by a user or by the system based on workload behavior between the amount of storage made available for use by master bucket manager and the amount of storage made available for use by slave bucket manager . Further should the overall storage be changed either by the system or due to some user input this configurable ratio can be maintained by resizing each of master bucket manager and slave bucket manager as needed. As a result the overall amount of storage and the respective usage of same by master bucket manager and slave bucket manager can be dynamically changed increased decreased and or ratio changed . Such size changes of either or both of master bucket manager and slave bucket manager can be accomplished as was described in U.S. application Ser. No. 14 609 085 incorporated by reference herein.

The disclosed method and apparatus has been explained above with reference to several embodiments. Other embodiments will be apparent to those skilled in the art in light of this disclosure. Certain aspects of the described method and apparatus may readily be implemented using configurations other than those described in the embodiments above or in conjunction with elements other than those described above. For example different algorithms and or logic circuits perhaps more complex than those described herein may be used.

Further it should also be appreciated that the described method and apparatus can be implemented in numerous ways including as a process an apparatus or a system. The methods described herein may be implemented by program instructions for instructing a processor to perform such methods and such instructions recorded on a non transitory computer readable storage medium such as a hard disk drive floppy disk optical disc such as a compact disc CD or digital versatile disc DVD flash memory etc. or communicated over a computer network wherein the program instructions are sent over optical or electronic communication links. It should be noted that the order of the steps of the methods described herein may be altered and still be within the scope of the disclosure.

It is to be understood that the examples given are for illustrative purposes only and may be extended to other implementations and embodiments with different conventions and techniques. For example cache indices other than BTrees and storage systems other than SANs can be used. While a number of embodiments are described there is no intent to limit the disclosure to the embodiment s disclosed herein. On the contrary the intent is to cover all alternatives modifications and equivalents apparent to those familiar with the art.

In the foregoing specification the invention is described with reference to specific embodiments thereof but those skilled in the art will recognize that the invention is not limited thereto. Various features and aspects of the above described invention may be used individually or jointly. Further the invention can be utilized in any number of environments and applications beyond those described herein without departing from the broader spirit and scope of the specification. The specification and drawings are accordingly to be regarded as illustrative rather than restrictive. It will be recognized that the terms comprising including and having as used herein are specifically intended to be read as open ended terms of art.

