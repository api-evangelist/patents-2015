---

title: Methods and systems for trouble shooting performance issues in networked storage systems
abstract: Methods and systems for managing resources in a storage system are provided. The methods include tracking performance of a plurality of resources used for reading and writing information at storage devices in a networked storage system, each resource represented by a logical object in a hierarchical structure and performance data associated with each logical object is maintained by a processor executing a management application out of a memory device; identifying a root object associated with a resource having a performance issue as indicated by a threshold violation for the resource; selecting a related object associated with a resource similar to the resource of the root object by the management application for comparing performance data of the root object with the related object; and using the comparison to verify that the root object is a root cause of the performance issue.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09542115&OS=09542115&RS=09542115
owner: NETAPP, INC.
number: 09542115
owner_city: Sunnyvale
owner_country: US
publication_date: 20150623
---
A portion of the disclosure herein contains material to which a claim for copyrights is made. The copyright owner the assignee of this patent application does not have any objection to the facsimile reproduction of any patent document as it appears in the USPTO patent files or records but reserves all other copyrights.

Various forms of storage systems are used today. These forms include direct attached storage DAS network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple clients with access to shared data backing up data and others.

A storage system typically includes at least a computing system executing a storage operating system for storing and retrieving data on behalf of one or more client computing systems may just be referred to as client or clients . The storage operating system stores and manages shared data containers in a set of mass storage devices.

Quality of Service QOS is a metric used in a storage environment to provide certain throughput for processing input output I O requests a response time goal within which I O requests are processed and a number of I O requests processed within a given time for example in a second IOPS . Throughput means an amount of data transferred within a given time in response to the I O requests for example in megabytes per second Mb s .

To process an I O request to read and or write data various resources are used within a storage system for example network resources processors storage devices and others. The different resources perform various functions in processing the I O requests.

As storage systems continue to expand in size complexity and operating speeds it is desirable to efficiently monitor resource usage identify performance issues determine the root cause of the performance issues and provide appropriate remediation options. Continuous efforts are being made to better manage networked storage environments.

In one aspect a machine implemented method is provided. The method includes tracking performance of a plurality of resources used for reading and writing information at storage devices in a networked storage system each resource represented by a logical object in a hierarchical structure and performance data associated with each logical object is maintained by a processor executing a management application out of a memory device identifying a root object associated with a resource having a performance issue as indicated by a threshold violation for the resource selecting a related object associated with a resource similar to the resource of the root object by the management application for comparing performance data of the root object with the related object and using the comparison to verify that the root object is a root cause of the performance issue.

In another aspect a non transitory machine readable storage medium having stored thereon instructions for performing a method comprising machine executable code which when executed by at least one machine causes the machine to track performance of a plurality of resources used for reading and writing information at storage devices in a networked storage system each resource represented by a logical object in a hierarchical structure and performance data associated with each logical object is maintained by a processor executing a management application out of a memory device identify a root object associated with a resource having a performance issue as indicated by a threshold violation for the resource select a related object associated with a resource similar to the resource of the root object by the management application for comparing performance data of the root object with the related object and use the comparison to verify that the root object is a root cause of the performance issue.

In yet another aspect a system having a memory with machine readable medium comprising machine executable code having stored thereon instructions is provided. A processor module coupled to the memory executes the machine executable code to track performance of a plurality of resources used for reading and writing information at storage devices in a networked storage system each resource represented by a logical object in a hierarchical structure and performance data associated with each logical object is maintained by a processor executing a management application out of a memory device identify a root object associated with a resource having a performance issue as indicated by a threshold violation for the resource select a related object associated with a resource similar to the resource of the root object by the management application for comparing performance data of the root object with the related object and use the comparison to verify that the root object is a root cause of the performance issue.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various thereof in connection with the attached drawings.

As a preliminary note the terms component module system and the like as used herein are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a hardware processor a hardware based processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example at non transitory computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one aspect a performance manager module is provided that interfaces with a storage operating system to collect quality of service QOS data or performance data for various resources. QOS provides a certain throughput i.e. amount of data that is transferred within a given time interval for example megabytes per seconds MBS latency and or a number of input output operations that can be processed within a time interval for example in a second referred to as IOPS . Latency means a delay in completing the processing of an I O request and may be measured using different metrics for example a response time in processing I O requests.

As mentioned above storage systems use various resources to process I O requests for writing and reading data to and from storage devices. The storage system maintains various counters and data measurement objects QOS data for providing QOS to clients. The QOS data may include throughput data a number of IOPS in a measurement period and an average response time within the measurement period a service time per visit to a resource a wait time per visit to the resource and a number of visits at the resource used for processing I O requests.

The performance manager uses historical QOS data obtained from the storage system to predict an expected range or threshold value for future QOS data. Future actual QOS data may be compared with the expected range to detect abnormal behavior. The abnormal behavior may be declared as an incident.

In one aspect of the present disclosure methods and systems for managing resources in a storage system are provided. The methods include tracking performance of a plurality of resources used for reading and writing information at storage devices in a networked storage system each resource represented by a logical object in a hierarchical structure and performance data associated with each logical object is maintained by a processor executing a management application or performance manager out of a memory device identifying a root object associated with a resource having a performance issue as indicated by a threshold violation for the resource selecting a related object associated with a resource similar to the resource of the root object via a graphical user interface GUI presented by the management application for comparing performance data of the root object with the related object and using the comparison to verify that the root object is a root cause of the performance issue.

The performance manager obtains the QOS data and stores it at a local data structure . In one aspect performance manager analyzes the QOS data for detecting performance issues that may be referred to as incidents and identifying resources and storage volumes affected by performance issues. Details regarding the various operations performed by the performance manager are provided below.

In one aspect storage system has access to a set of mass storage devices A N may be referred to as storage devices or simply as storage device within at least one storage subsystem . The storage devices may include writable storage device media such as magnetic disks video tape optical DVD magnetic tape non volatile memory devices for example solid state drives SSDs including self encrypting drives flash memory devices and any other similar media adapted to store information. The storage devices may be organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID . The aspects disclosed are not limited to any particular storage device type or storage device configuration.

In one aspect the storage system provides a set of logical storage volumes may be interchangeably referred to as volume or storage volume for providing physical storage space to clients A N or virtual machines VMs A N . A storage volume is a logical storage object and typically includes a file system in a NAS environment or a logical unit number LUN in a SAN environment. The various aspects described herein are not limited to any specific format in which physical storage is presented as logical storage volume LUNs and others 

Each storage volume may be configured to store data files or data containers or data objects scripts word processing documents executable programs and any other type of structured or unstructured data. From the perspective of one of the client systems each storage volume can appear to be a single drive. However each storage volume can represent storage space in at one storage device an aggregate of some or all of the storage space in multiple storage devices a RAID group or any other suitable set of storage space.

A storage volume is identified by a unique identifier Volume ID and is allocated certain storage space during a configuration process. When the storage volume is created a QOS policy may be associated with the storage volume such that requests associated with the storage volume can be managed appropriately. The QOS policy may be a part of a QOS policy group referred to as Policy Group that is used to manage QOS for several different storage volumes as a single unit. The QOS policy information may be stored at a QOS data structure maintained by a QOS module . QOS at the storage system level may be implemented by the QOS module . QOS module maintains various QOS data types that are monitored and analyzed by the performance manager as described below in detail.

The storage operating system organizes physical storage space at storage devices as one or more aggregate where each aggregate is a logical grouping of physical storage identified by a unique identifier and a location. The aggregate includes a certain amount of storage space that can be expanded. Within each aggregate one or more storage volumes are created whose size can be varied. A qtree sub volume unit may also be created within the storage volumes. For QOS management each aggregate and the storage devices within the aggregates are considered as resources that are used by storage volumes.

The storage system may be used to store and manage information at storage devices based on an I O request. The request may be based on file based access protocols for example the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP . Alternatively the request may use block based access protocols for example the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP .

In a typical mode of operation a client or a VM transmits one or more I O request such as a CFS or NFS read or write request over a connection system to the storage system . Storage operating system receives the request issues one or more I O commands to storage devices to read or write the data on behalf of the client system and issues a CIFS or NFS response containing the requested data over the network to the respective client system.

System may also include a virtual machine environment where a physical resource is time shared among a plurality of independently operating processor executable VMs. Each VM may function as a self contained platform running its own operating system OS and computer executable application software. The computer executable instructions running in a VM may be collectively referred to herein as guest software. In addition resources available within the VM may be referred to herein as guest resources. 

The guest software expects to operate as if it were running on a dedicated computer rather than in a VM. That is the guest software expects to control various events and have access to hardware resources on a physical computing system may also be referred to as a host platform or host system which maybe referred to herein as host hardware resources . The host hardware resource may include one or more processors resources resident on the processors e.g. control registers caches and others memory instructions residing in memory e.g. descriptor tables and other resources e.g. input output devices host attached storage network attached storage or other like storage that reside in a physical machine or are coupled to the host system.

In one aspect system may include a plurality of computing systems A N may also be referred to individually as host platform system or simply as server communicably coupled to the storage system executing via the connection system such as a local area network LAN wide area network WAN the Internet or any other interconnect type. As described herein the term communicably coupled may refer to a direct connection a network connection a wireless connection or other connections to enable communication between devices.

Host system A includes a processor executable virtual machine environment having a plurality of VMs A N that may be presented to client computing devices systems A N. VMs A N execute a plurality of guest OS A N may also be referred to as guest OS that share hardware resources . As described above hardware resources may include processors memory I O devices storage or any other hardware resource.

In one aspect host system interfaces with a virtual machine monitor VMM for example a processor executed Hyper V layer provided by Microsoft Corporation of Redmond Wash. a hypervisor layer provided by VMWare Inc. or any other type. VMM presents and manages the plurality of guest OS A N executed by the host system . The VMM may include or interface with a virtualization layer VIL that provides one or more virtualized hardware resource to each OS A N.

In one aspect VMM is executed by host system with VMs A N. In another aspect VMM may be executed by an independent stand alone computing system often referred to as a hypervisor server or VMM server and VMs A N are presented at one or more computing systems.

It is noteworthy that different vendors provide different virtualization environments for example VMware Corporation Microsoft Corporation and others. The generic virtualization environment described above with respect to may be customized to implement the aspects of the present disclosure. Furthermore VMM or VIL may execute other modules for example a storage driver network interface and others the details of which are not germane to the aspects described herein and hence have not been described in detail.

System may also include a management console that executes a processor executable management application for managing and configuring various elements of system . Application may be used to manage and configure VMs and clients as well as configure resources that are used by VMs clients according to one aspect. It is noteworthy that although a single management console is shown in system may include other management consoles performing certain functions for example managing storage systems managing network connections and other functions described below.

In one aspect application may be used to present storage space that is managed by storage system to clients A N or VMs . The clients may be grouped into different service levels where a client with a higher service level may be provided with more storage space than a client with a lower service level. A client at a higher level may also be provided with a certain QOS vis vis a client at a lower level.

Although storage system is shown as a stand alone system i.e. a non cluster based system in another aspect storage system may have a distributed architecture for example a cluster based system of . Before describing the various aspects of the performance manager the following provides a description of a cluster based storage system.

The clustered storage system includes a plurality of nodes . . a cluster switching fabric and a plurality of mass storage devices . . may be referred to as and similar to storage device .

Each of the plurality of nodes . . is configured to include a network module maybe referred to as N module a storage module maybe referred to as D module and a management module maybe referred to as M Module each of which can be implemented as a processor executable module. Specifically node . includes a network module . a storage module . and a management module . node . includes a network module . a storage module . and a management module . and node . includes a network module . a storage module . and a management module ..

The network modules . . include functionality that enable the respective nodes . . to connect to one or more of the client systems . .N over the computer network while the storage modules . . connect to one or more of the storage devices . .. Accordingly each of the plurality of nodes . . in the clustered storage server arrangement provides the functionality of a storage server.

The management modules . . provide management functions for the clustered storage system . The management modules . . collect storage information regarding storage devices .

Each node may execute or interface with a QOS module shown as . . that is similar to the QOS module . The QOS module may be executed for each node or a single QOS module may be used for the entire cluster. The aspects disclosed herein are not limited to the number of instances of QOS module that may be used in a cluster. Details regarding QOS module are provided below.

A switched virtualization layer including a plurality of virtual interfaces VIFs is provided to interface between the respective network modules . . and the client systems . .N allowing storage . . associated with the nodes . . to be presented to the client systems . .N as a single shared storage pool.

The clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers or storage virtual machines SVM in which each SVM represents a single storage system namespace with separate network access. Each SVM has a client domain and a security domain that are separate from the client and security domains of other SVMs. Moreover each SVM is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more SVMs. Client systems can access the data on a SVM from any node of the clustered system through the VIFs associated with that SVM. It is noteworthy that the aspects described herein are not limited to the use of SVMs.

Each of the nodes . . is defined as a computing system to provide application services to one or more of the client systems . .N. The nodes . . are interconnected by the switching fabric which for example may be embodied as a Gigabit Ethernet switch or any other type of switching connecting device.

Although depicts an equal number i.e. of the network modules . . the storage modules . . and the management modules . . any other suitable number of network modules storage modules and management modules may be provided. There may also be different numbers of network modules storage modules and or management modules within the clustered storage system . For example in alternative aspects the clustered storage system may include a plurality of network modules and a plurality of storage modules interconnected in a configuration that does not reflect a one to one correspondence between the network modules and storage modules.

Each client system . .N may request the services of one of the respective nodes . . . and that node may return the results of the services requested by the client system by exchanging packets over the computer network which may be wire based optical fiber wireless or any other suitable combination thereof.

Performance manager interfaces with the various nodes and obtains QOS data for QOS data structure . Details regarding the various modules of performance manager are now described with respect to .

Performance manager collects a certain minimal amount of data for example QOS data for 3 hours or 30 data samples of workload activity. After collecting the minimal QOS data performance manager generates an expected range or threshold values for future QOS data.

The expected range is a range of measured performance activity or QOS data of a workload over a period of time. For example a given twenty four hour period may be split into multiple time intervals. The expected range may be generated for each time interval. The expected range sets a baseline for what may be perceived to be typical activity for the workload. The upper boundary of the expected range establishes a dynamic performance threshold that changes over time. For example during 9.00 AM and 5.00 PM most employees of a business check their email between 9.00 AM 10.30 AM. The increased demand on email servers means an increase in the workload activity at the storage managed by the storage operating system. The demand on the storage may decrease during lunch time. The performance manager tracks this activity to determine the expected range or expected QOS data behavior for future activity.

Performance manager uses the expected range to represent and monitor I O response time and operations for a storage volume in a cluster. The performance manager tracks QOS data and in some cases identifies abnormal activity as incidents. An incident indicates that workload performance is outside a desirable level due to resource contention from other workloads i.e. workloads with higher usage of cluster resources may be causing the response time to increase. Incidents are considered as events that indicate I O performance issues at a storage volume caused by resource contention.

Performance manager compares historical QOS data with current QOS data to identify a victim workload whose performance may have decreased. Victim workloads may be identified based on response time deviation from an expected response time as described below. After identifying the victim the performance manager identifies the resource that may be in contention as well as the workloads or volumes that may be overusing the resources i.e. bully workloads .

Workloads are ranked to determine which bullies have the highest change in usage of the resource and which victims are most impacted. Based on the identification of victim and bully workloads a remediation plan may be recommended to correct the problems associated with the incident.

Referring now to System A shows two clusters A and B both similar to cluster described above. Each cluster includes the QOS module for implementing QOS policies that are established for different clients applications.

Cluster A may be accessible to clients . and . while cluster B is accessible to clients . .. Both clusters have access to storage subsystems and storage devices . .N.

Clusters A and B communicate with a collection module . The collection module may be a standalone computing device or integrated with performance manager . The aspects described herein are not limited to any particular configuration of collection module and performance manager .

Collection module includes one or more acquisition modules for collecting QOS data from the clusters. The data is pre processed by the pre processing module and stored as pre processed QOS data at a storage device not shown . Pre processing module formats the collected QOS data for the performance manager . Pre processed QOS data is provided to a collection module interface of the performance manager . QOS data received from collection module is stored as QOS data structure by performance manager at a storage device not shown .

Performance manager includes a plurality of modules for example a forecasting module a detection module and an incident analysis module that use the QOS data for detecting incidents and reporting the incidents to a client system via a GUI . Performance manager also recommends a corrective action plan to client .

Client may access the analysis results and recommendations using GUI . Details regarding the GUI module are provided below. Before describing the various processes involving performance manager and its components the following describes using the performance manager in a cloud based computing environment.

Storage system A and storage system B may be deployed by a cloud manager and or a cloud administrator configured to provision the host systems storage associated with one or more client devices e.g. client client and or services requested by the one or more client devices. As an example storage system A may be configured to be associated with SVM A and SVM . Storage system B may be configured to be associated with SVM B SVM D and SVM E.

In one or more aspects cloud manager may enable one or more client devices to self provision computing resources thereof. As an example cloud manager may manage cloud portion s e.g. cloud cloud associated with client and client . Client and or client may log into a console associated with cloud manager to access cloud and or cloud and the VMs A E therein through a public network e.g. Internet . The client devices and or VMs associated therewith provided in cloud computing environment may be analogous to the clients of A.

In order to address storage requirements requests associated with client and client cloud manager may be configured to appropriately provision SVM A SVM B SVM SVM D and SVM E and allocate to client and client . The aforementioned SVMs may be virtualized entities utilized by client and client to meet storage requirements thereof. Multi tenancy may allow for a storage system to have multiple SVMs associated therewith. A portion of the cloud e.g. cloud including SVM A SVM B and VMs e.g. VM A VM B associated therewith may be associated with client and a portion of the cloud e.g. cloud including SVM C SVM D and SVM E and VMs e.g. VM C VM D VM E associated therewith may be associated with client . In one or more aspects VMs may reside on storage exposed by SVM s .

The aforementioned cloud portions may be logical subsets of the cloud and may include VMs implemented with operating systems e.g. Linux Microsoft s Windows . Cloud as used herein may refer to the large pool of configurable computing resources e.g. virtualized computing resources that may be subjected to a pay per use model in which client s may enter into service agreement s with service provider s . The portion of the cloud therefore may refer to the pool of resources associated with a particular client. It is noteworthy that client and or client may be entities e.g. corporations departments and others and that there may be a number of computing devices associated with each of client and or client .

Cloud and or cloud may span across several geographic regions. In one or more aspects the aforementioned cloud portions may span multiple countries under differing jurisdictional guidelines. For example a jurisdictional guideline may deem that a SVM needs to be launched on hardware e.g. storage system located in the same jurisdiction as the corresponding client s .

In one or more aspects administrators of cloud computing environment may possess the authority to launch one or more SVMs on any of storage system A and storage system B irrespective of the location s thereof. Further in one or more aspects the aforementioned one or more SVMs may be associated with one or more versions of storage operating system . For example an administrator may modify the version of the storage operating system and or configuration settings on storage system A and or storage system B.

In one aspect cloud computing environment includes the performance manager and the collection module that have been described above. The various processes executed by the performance manager and the collection module are described below.

Before describing the various processes executed by the performance manager the following describes how QOS requests are handled at the cluster level with respect to .

As shown in the network module of a cluster includes a network interface A for receiving requests from clients. Network module executes a NFS module C for handling NFS requests a CIFS module D for handling CIFS requests a SCSI module for handling iSCSI requests and an others module F for handling other requests. A node interface G is used to communicate with QOS module storage module and or another network module . QOS management interface B is used to provide QOS data from the cluster to collection module for pre processing data.

QOS module includes a QOS controller A a QOS request classifier B and QOS policy data structure or Policy Group . The QOS policy data structure stores policy level details for implementing QOS for clients and storage volumes. The policy determines what latency and throughput rate is permitted for a client as well as for specific storage volumes. The policy determines how I O requests are processed for different volumes and clients.

The storage module executes a file system A a part of storage operating system described below and includes a storage layer B to interface with storage device . NVRAM C of the storage module may be used as cache for responding to I O requests.

A request arrives at network module from a client or from an internal process directly to file system A. Internal process in this context may include a de duplication module a replication engine module or any other entity that needs to perform a read and or write operation at the storage device . The request is sent to the QOS request classifier B to associate the request with a particular workload. The classifier B evaluates a request s attributes and looks for matches within QOS policy data structure . The request is assigned to a particular workload when there is a match. If there is no match then a default workload may be assigned.

Once the request is classified for a workload then the request processing can be controlled. QOS controller A determines if a rate limit i.e. a throughput rate for the request has been reached. If yes then the request is queued for later processing. If not then the request is sent to file system A for further processing with a completion deadline. The completion deadline is tagged with a message for the request.

File system A determines how queued requests should be processed based on completion deadlines. The last stage of QOS control for processing the request occurs at the physical storage device level. This could be based on latency with respect to storage device or for NVRAM C that may be used for any logged operation.

Various resources are used to process I O requests. As an example there are may be two types of resources a service center and a delay center resource. The service center is a resource category that can be represented by a queue with a wait time and a service time for example a processor that processes a request out of a queue . The delay center may be a logical representation for a control point where a request stalls waiting for a certain event to occur and hence the delay center represents the delay in request processing. The delay center may be represented by a queue that does not include service time and instead only represents wait time. The distinction between the two resource types is that for a service center the QOS data includes a number of visits wait time per visit and service time per visit for incident detection and analysis. For the delay center only the number of visits and the wait time per visit at the delay center are used as described below in detail.

Performance manager uses different flow types for incident detection and analysis. A flow type is a logical view for modeling request processing from a particular viewpoint. The flow types include two categories latency and utilization. A latency flow type is used for analyzing how long operations take at the service and delay centers. The latency flow type is used to identify a victim workload whose latency has increased beyond a certain level. A typical latency flow may involve writing data to a storage device based on a client request and there is latency involved in writing the data at the storage device. The utilization flow type is used to understand resource consumption of workloads and may be used to identify resource contention and a bully workload as described below in detail.

Referring now to delay center network is a resource queue that is used to track wait time due to external networks. Storage operating system often makes calls to external entities to wait on something before a request can proceed. Delay center tracks this wait time.

Network module CPU is another resource queue where I O requests wait for protocol processing by a network module processor. A separate queue for each node may be maintained.

NV RAM transfer delay center is used to track latency due bandwidth capacity of non volatile memory used by cluster nodes to store write data before the data is written to storage devices .

A storage aggregate or aggregate is a resource that may include more than one storage device for reading and writing information. Disk I O queue may be used to track utilization of storage devices . A storage module CPU represents a processor that is used to read and write data. The storage module CPU is a service center and a queue is used to track the wait time for any writes to storage devices by a storage module processor.

Nodes within a cluster communicate with each other. These may cause delays in processing I O requests. The cluster interconnect delay center is used to track the wait time for transfers using the cluster interconnect system. As an example a single queue maybe used to track delays due to cluster interconnects.

There may also be delay centers due to certain internal processes of storage operating system and various queues may be used to track those delays. For example a queue may be used to track the wait for I O requests that may be blocked for file system reasons. Another queue Delay Center Susp CP may be used to represent the wait time for Consistency Point CP related to the file system A. During a CP write requests are written in bulk at storage devices and this will typically cause other write requests to be blocked so that certain buffers are cleared.

Without limiting the various aspects of the present disclosure Table I below provides an example of the various service and delay centers that may be used by performance manager to track workload performance using different resources. Some of these resources are shown in . Table I also identifies the resource type i.e. utilization and or latency type .

A request may have a plurality of attributes for example a source a path a destination and I O properties. The source identifies the source from where a request originates for example an internal process a host or client address a user application and others.

The path defines the entry path into the storage system. For example a path may be a logical interface LIF or a protocol such as NFS CIFS iSCSI and Fibre Channel protocol.

A destination is the target of a request for example storage volumes LUNs data containers and others.

In one aspect streams may be grouped together based on client needs. For example if a group of clients make up a department on two different subnets then two different streams with the source restrictions can be defined and grouped within the same workload. Furthermore requests that fall into a workload are tracked together by performance for efficiency. Any requests that don t match a user or system defined workload may be assigned to a default workload.

In one aspect workload streams may be defined based on the I O attributes. The attributes may be defined by clients. Based on the stream definition performance manager tracks workloads as described below.

Referring back to a workload uses one or more resources for processing I O requests shown as A N as part of a resource object . The resources include service centers and delay centers that have been described above with respect to and Table I. For each resource a queue is maintained for tracking different statistics or QOS data . For example a response time and a number of visits a service time for service centers and a wait time are tracked. The term QOS data as used throughout this specification includes one or more of and according to one aspect.

Without limiting the various aspects of the present disclosure Table II below provides an example of a non exhaustive listing of the various objects that are used by the performance manager for incident detection and analysis where each object may have multiple instances 

Performance manager also uses a plurality of counter objects for incident detection and analysis. Without limiting the adaptive aspects an example of the various counter objects are shown and described in Table III below 

Without limiting the various aspects of the present disclosure Table IV below provides an example of the details associated with the object counters that are monitored by the performance manager for detecting incidents according to one aspect 

When a workload is responding slowly a user may want to analyze the workload to determine the root cause of any issues and then perform corrective action to solve the issues. Performance manager using QOS data collected from the different clusters and using the workload performance model detects such issues as incidents and then provides remedial actions.

Performance manager uses collected QOS data to predict dynamic threshold values for workloads. Using the dynamic threshold values and statically defined threshold values detection module detects one or more incidents. The incident analysis module then determines which resource may be in contention for a victim workload and identifies any bully workloads that may have caused the incident.

The QOS network object is used to monitor network resources for example network switches and associated bandwidth used by a clustered storage system.

The cluster node object stores information regarding a node for example a node identifier and other information. Each cluster node object is associated with a pluralities of child objects for example a cache object a QOS object for a storage module a QOS object for a network module a CPU object and an aggregate object . The cache object is used to track utilization of a cache for example NVRAM C . The QOS storage module tracks the QOS of a storage module defined by a QOS policy data structure described above in detail with respect to . The QOS network module object tracks the QOS for a network module. The CPU object is used to track CPU performance and utilization of a node.

The aggregate object tracks the utilization of a storage aggregate that is managed by a cluster node. The aggregate object may have various child objects for example a flash pool object that tracks usage of a plurality of flash based storage devices shown as flash pool . The flash pool object may have a SSD disk object that tracks the actual usage of specific SSD based storage devices. The RAID group is used to track the usage of storage devices configured as RAID devices. The RAID object includes a storage device object shown as a HDD hard disk drive that tracks the actual utilization of the storage devices.

Each cluster is provided a portset having a plurality of ports that may be used to access cluster resources. A port includes logic and circuitry for processing information that is used for communication between different resources of the storage system. The portset object tracks the various members of the portset using a port object and a LIF object . The LIF object includes a logical interface for example an IP address while the port object includes a port identifier for a port for example a world wide port number WWPN . It is noteworthy that the port object is also a child object of node that may use a port for network communication with clients.

A cluster may present one or more SVMs to client systems. The SVMs are tracked by the SVM object which is a child object of cluster . Each cluster is also associated with a policy group that is tracked by a policy group object . The policy group is associated with SVM object as well as storage volumes and LUNs. The storage volume is tracked by a volume object and the LUN is tracked by a LUN object . The volume object includes an identifier identifying a volume size of the volume clients associated with the volume volume type i.e. flexible or fixed size and other information. The LUN object includes information that identifies the LUN LUNID size of the LUN LUN type red write or read and write and other information.

As an example a cluster does not have any physical or logical parent since it is the first object in the hierarchy. Physical nodes are physical children of the cluster object while SVMs are logical children.

Node object has physical peers i.e. other nodes and physical children include aggregates on the same nodes as well as ports network interface cards. The workloads for a node include the sum of network module and storage module utilization. Node may also have another peer node configured as a HA partner.

Aggregate object has a node as a physical parent and another aggregate as a physical peer. The volumes on the aggregate are logical children. The aggregate utilization is tracked by a counter maintained by the performance manger based on data collected from the storage system.

Port object has a node as a physical parent while other ports in a same portset and on the same node are physical peers. The LIFs for the ports are logical children.

SVM have other SVMs as logical peers and a cluster is a physical parent. Volumes and LIFs are logical children.

In one aspect a GUI based tool is provided for evaluating resource performance and utilization. The GUI based tool shown as performance explorer below provides a grid where a user is able to view a root object that may be a root cause of a performance issue. The user is also able to select a category of objects and objects within the category for comparison with the root object to validate the performance issues associated with the root object before taking any remedial action.

It is noteworthy that although the examples shown herein are with respect to the GUI based tool but the functionality as captured by the various process flows may be implemented using one or more hardware processor executable application programming interface APIs .

The performance manager monitors the performance of various resources using the plurality of counters. Different metrics for example latency IOPS throughput utilization and others are monitored. The performance manager compares the resource performance with threshold values that are associated with individual resource. The monitoring may highlight resources that may be affected by performance issues i.e. undesirable latency utilization and others . If a threshold value is reached and the violation continues for a certain duration an incident or an event is generated for the user.

In block B a root object representing the resource is identified. The object may be identified based on when the performance of the resource has reached a threshold value. The threshold value may be based on latency and or utilization of the resource by one or more workloads.

In block B first a category of other objects is identified and then one or more of the other objects are selected for comparison with the root object. The other object also referred to as candidate object selection is based on the type of performance issue as well as the type of root object. The category of objects or object type is based on relevance to the root object type and the relationships that the objects have described above with respect to .

As an example the category of objects may be based on a child parent relationship for example if an aggregate is having performance issues then a volume that is a child object may be selected for comparison objects may of the same type as the root object may be referred to as sibling objects or peer objects for example peer volumes within a same aggregate may be peer volumes that are selected and objects of the same root object type but on a different high availability cluster node may be referred to as a node peer object and may be selected for comparison . Other remotely related but possibly relevant objects may be selected as well. These may include QOS workloads that use the object LIFs policy objects and others.

In block B the performance manager compares the performance of the other object with the root object to determine if the root object is the root cause of the performance issue highlighted in block B. The comparison is based on evaluating the performance of the other object and comparing that performance with the performance of root object. The comparison may be to determine when there were no issues with either objects determine if there has been a configuration change over time that may have causes the root object to behave differently and if a similar issue had been determined in the past with the root object and or the other object.

The comparison provides the user with a baseline to verify if the root object is the root cause of the performance issue. If the root object is the root cause and depending on the type of performance issue a remediation option is provided in block B.

In block B a performance explorer GUI is presented on a display device of a computer. An example of the performance explorer GUI is shown in as tab . In one aspect the performance explorer GUI is presented after an event is detected from a dashboard of the performance manager GUI by doing a search for a resource object from a URL that is mailed to a user with an incident report or from any other entry point.

In block B a grid is presented in the performance explorer GUI with at least one object that is related to the root object. As described above the related object is based on the root object type and the performance issue that may be highlighted. For example in the grid shows various objects under the column labelled volume. The time range for data comparison is shown as 72 hours under tab . Tab snows that volumes of a particular SVM are being listed in the grid region. In one aspect grid values change when the time period shown as time range is changed. This displays average behavior over a different duration for identifying candidate objects that may have similar symptoms or may be the root cause of a problem.

In block B an object is selected for comparison. For example the first entry for volume labelled as svm1 imror may be selected. Tab may be used to select the object. As an example the object is added to a shopping cart . In one aspect a plurality of objects maybe placed in the shopping cart . The performance explorer GUI enables hiding some of the objects without removing them. This enables the user to hide possible candidate objects but not discard them if the user is uncertain and wants to use them at a later time.

The shopping cart shows a comparing segment . The comparing segment shows a SVM labelled as svm2. By using tabs volumes svm1 and svn2 may be selected for comparison. One of the volumes may be the root object while the other volume may be the related object. A time series performance chart is then displayed in the chart display segment . The chart display segment shows the numbers of charts that are selected at any given time . The events are shown in segment . The latency graphs for the volume are shown as and the IOPS are provided in segment .

Based on the comparison the user can verify if the root object is the root cause of the performance issue. Thereafter a remediation plan is presented to the user in block B. The remediation may be based on the performance issue that is highlighted by the performance explorer. For example the remediation may be to move a workload from one resource to another change a policy setting or a threshold value put a limit on a workload upgrade equipment or any other action.

In one aspect an intuitive GUI is provided for root cause investigation and analysis. The performance manager provides a tool for the user to take action only after the root cause of the performance issue has been verified.

The process begins on block B when the various resources within a cluster or more than one cluster are operational. The different resources are managed as logical objects and performance data associated with the counters are obtained by the performance manager and stored at one or more data structures as described above.

In block the various resources are monitored. In block B an event or an incident associated with a cluster is identified. An event or incident may be highlighted if a threshold value for a resource has been reached.

In one aspect as an example a storage administrator is provided with a high level view of different clusters that the administrator may be managing. This is shown as segments and at the screen shot of . Segments show latency indicator for the SVMs volumes and LUNs. The number of IOPS for the various cluster nodes and SVMs the throughput rate for the nodes and the SVMs the disk utilization for the cluster aggregates and the node utilization.

Since each cluster is an independent entity the administrator is also provided with an overview of each cluster as shown in . The top portion of the screenshot of labelled as includes all the active events of cluster. Segment as an example shows that there are 25 active events. A graph illustrates five different cluster parameters latency IOPS throughput node utilization and disk utilization. Individual charts for IOPs and throughput are shown in segment .

Below segment is segment that is viewed by selecting the overview option . Segment shows a listing of various managed objects for example nodes SVMs aggregates volumes LUNs LIFs and ports that have been described above with respect to various figures includes C. The cluster level summary is further shown in segments and . Segment shows the CPU utilization for the top 10 node objects and segment shows the IOPS for top ten SVMs. The top 10 list is generated to highlight objects that are likely to have performance issues.

Referring back to in block B an object may be referred to as root object associated with an incident event is identified. As an example a user may identify the object by using the top ten listing of the screen shot of . In another aspect the user may select an object from screen shot of that provides an object inventory view of various objects that are being monitored by the performance manager at any given time. The object inventory lists the name of the objects the metrics that are being monitored for example latency IOPS throughout utilization and others any associated objects based on the relationships of C and any threshold policy that is associated with the objects. In one aspect the administrator may assign programmable threshold values to the objects using the screenshot of .

In block B the performance explorer GUI is initiated so that another resource object can be identified for comparison with the object identified in block B. An example of the performance explorer is shown in which is similar to that has been described above. The performance explorer is provided to confirm or deny that a resource object is the root cause of a performance issue. The performance explorer may also be used to determine if another resource object may be the root cause of the performance issue. The performance explorer grid may be used to select another object for comparison. The other object is selected based on its relationship with the root object. The various object relationships have been described above in detail.

In block B the performance of the other object is compared with the performance of the root object. The comparison is based on using performance data for a time period. The comparison may be initiated using the View and Compare option from the screen shot of . An example of the time series comparison is shown in where the latency of the two objects is shown over time. This enables the user to verify in block B if the root object or another object is the root cause of a performance issue. If the root object or another object is the root cause then an appropriate remediation plan can be presented to the user for fixing the performance issue.

Processors A B may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such hardware devices. The local storage comprises one or more storage devices utilized by the node to locally store configuration information for example in a configuration data structure . The configuration information may include information regarding storage volumes and the QOS associated with each storage volume.

The cluster access adapter comprises a plurality of ports adapted to couple node . to other nodes of cluster . In the illustrative aspect Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate aspects where the network modules and storage modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the network storage module for communicating with other network storage modules in the cluster .

Each node . is illustratively embodied as a dual processor storage system executing a storage operating system similar to that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories and files at storage .. However it will be apparent to those of ordinary skill in the art that the node . may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the network module on the node while the other processor B executes the functions of the storage module.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the disclosure described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node . by inter alia invoking storage operation in support of the storage service implemented by the node.

The network adapter comprises a plurality of ports adapted to couple the node . to one or more clients . .N over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Each client . .N may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node . to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored at storage device .. The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the storage devices over an I O interconnect arrangement such as a conventional high performance Fibre Channel link topology.

In one example storage operating system may include several modules or layers executed by one or both of network module and storage module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operation i.e. executes read write operation on storage in response to client . .N requests.

Storage operating system may also include a protocol layer and an associated network access layer to allow node . to communicate over a network with other systems such as clients . .N. Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices . . or are illustrated schematically as a path which illustrates the flow of data through storage operating system .

The storage operating system may also include a storage access layer and an associated storage driver layer to allow storage module to communicate with a storage device. The storage access layer may implement a higher level storage protocol such as RAID redundant array of inexpensive disks while the storage driver layer may implement a lower level storage device access protocol such as Fibre Channel or SCSI. The storage driver layer may maintain various data structures not shown for storing information regarding storage volume aggregate and various storage devices.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node . implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the disclosure described herein may apply to any tyke of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a storage device directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present disclosure may be utilized with any suitable file system including a write in place file system.

The processing system includes one or more processor s and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processor s are the central processing units CPUs of the processing system and thus control its overall operation. In certain aspects the processors accomplish this by executing software stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions implement the process steps described above may reside in and executed by processors from memory . For example instructions may be used to implement the forecasting module detection module and incident analysis module according to one aspect.

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a Fibre Channel adapter or the like.

The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

Thus methods and apparatus for collecting monitoring and trouble shooting performance issues have been described. Note that references throughout this specification to one aspect or an aspect mean that a particular feature structure or characteristic described in connection with the aspect is included in at least one aspect of the present disclosure. Therefore it is emphasized and should be appreciated that two or more references to an aspect or one aspect or an alternative aspect in various portions of this specification are not necessarily all referring to the same aspect. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more aspects of the disclosure as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred aspects it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

