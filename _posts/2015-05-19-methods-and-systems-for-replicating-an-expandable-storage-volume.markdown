---

title: Methods and systems for replicating an expandable storage volume
abstract: Machine implemented method and system for generating a disaster recovery copy of an expandable storage volume having a namespace for storing information for accessing data objects stored at a data constituent volume is provided. A transfer operation for transferring a point in time copy of the expandable storage volume from a first location to a second location is generated. Information regarding the expandable storage volume from the first location is retrieved and a destination expandable storage volume is resized to match components of the expandable storage volume at the first location. Thereafter, the point in time copy of the expandable storage volume from the first location to the second location is transferred and configuration information regarding the point in time copy is copied from the first location to the second location; a data structure for storing information regarding the transferred point in time copy of the expandable storage volume is updated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09171052&OS=09171052&RS=09171052
owner: NetApp, Inc.
number: 09171052
owner_city: Sunnyvale
owner_country: US
publication_date: 20150519
---
This patent application claims priority of and is a continuation of U.S. patent application Ser. No. 13 433 170 filed on Mar. 28 2012 and now U.S. Pat. No. 9 043 567 the disclosure of which is incorporated herein by reference in its entirety.

At least one embodiment of the present invention pertains to storage systems and more particularly to methods and systems using a unified namespace to access data objects in a storage system.

Network based storage or simply network storage is a common approach to backing up data making large amounts of data accessible to multiple users and other purposes. In a network storage environment a storage server or storage system makes data available to client also referred to as host systems by presenting or exporting to clients one or more logical data containers. There are various forms of network storage including network attached storage NAS and storage area network SAN . In a NAS context a storage server services file level requests from clients whereas in a SAN context a storage server services block level requests. Some storage servers are capable of servicing both file level requests and block level requests.

There are several trends that are relevant to network storage technology. The first is that the amount of data being stored within a typical enterprise is increasing from year to year. Second there are now multiple mechanisms or protocols by which a user may wish to access data stored in network storage system. For example consider a case where a user wishes to access a document stored at a particular location in a network storage system. The user may use a network file system NFS protocol to access the document over a local area network in a manner similar to how local storage is accessed. The user may also use an HTTP protocol to access a document over a wide area network such as an Internet network. Traditional storage systems use a different storage mechanism e.g. a different file system for presenting data over each such protocol. Accordingly traditional network storage systems do not allow the same stored data to be accessed concurrently over multiple different protocols at the same level of a protocol stack.

In addition network storage systems presently are constrained in the way they allow a user to store or navigate data. Consider for example a photo that is stored under a given path name such as home eng myname office.jpeg . In a traditional network storage system this path name maps to a specific volume and a specific file location e.g. an inode number . Thus a path name of a file e.g. a photo is closely tied to the file s storage location. In other words the physical storage location of the file is determined by the path name of the file. Accordingly in traditional storage systems the path name of the file needs to be updated every time the physical storage location of the file changes e.g. when the file is transferred to a different storage volume . This characteristic significantly limits the flexibility of the system.

Continuous efforts are being made to provide a flexible expandable storage system where data objects may be stored across storage volumes managed by different storage system nodes.

In one embodiment a machine implemented method and system for generating a disaster recovery copy of an expandable storage volume having a namespace for storing information for accessing data objects stored at a data constituent volume is provided. A transfer operation for transferring a point in time copy of the expandable storage volume from a first location to a second location is generated. Information regarding the expandable storage volume from the first location is retrieved and a destination expandable storage volume is resized to match components of the expandable storage volume at the first location. Thereafter the point in time copy of the expandable storage volume from the first location to the second location is transferred and configuration information regarding the point in time copy is copied from the first location to the second location and a data structure for storing information regarding the transferred point in time copy of the expandable storage volume is updated.

In another embodiment a machine implemented method for generating a disaster recovery copy of an expandable storage volume having a namespace for storing information for accessing data objects stored at a data constituent volume is provided. The method includes generating a transfer operation for transferring a snapshot of the expandable storage volume from a first location to a second location retrieving information regarding the expandable storage volume from the first location resizing a destination expandable storage volume at the second location to match components of the expandable storage volume at the first location transferring the snapshot of the expandable storage volume from the first location to the second location copying configuration information regarding the snapshot from the first location to the second location and updating a data structure for storing information regarding the transferred snapshot of the expandable storage volume.

In yet another embodiment a system for generating a disaster recovery copy of an expandable storage volume having a namespace for storing information for accessing data objects stored at a data constituent volume. The system includes a first storage system at a first location for managing the expandable storage volume a second storage system at a second location for managing a destination expandable storage volume a processor executable replication application configured to generate a transfer operation for transferring a snapshot of the expandable storage volume from the first location to the second location retrieve information regarding the expandable storage volume from the first location and resize the destination expandable storage volume at the second location to match components of the expandable storage volume at the first location wherein the snapshot of the expandable storage volume is transferred from the first location to the second location with configuration information regarding the snapshot.

In another embodiment a machine implemented method for managing a disaster recovery copy of an expandable storage volume at a first location having a namespace for storing information for accessing data objects stored at a data constituent volume is provided. The method includes resizing a destination expandable storage volume at a second location to match components of the expandable storage volume at the first location transferring a snapshot of the expandable storage volume from the first location to the second location to be stored as the disaster recovery copy presenting the disaster recovery copy to a computing system for reading and writing information at the second location resizing the expandable storage volume at the first location to match the destination expandable storage volume at the second location and re synchronizing the expandable storage volume with the destination storage volume after the first location becomes available.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

As a preliminary note the terms component module system and the like as used in this disclosure are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on non transitory computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick flash memory device or any other non volatile memory device or any other storage device in accordance with the claimed subject matter.

In one embodiment a machine implemented method and system for generating a disaster recovery copy of an expandable storage volume having a namespace for storing information for accessing data objects stored at a data constituent volume is provided. A transfer operation for transferring a point in time copy of the expandable storage volume from a first location to a second location is generated. Information regarding the expandable storage volume from the first location is retrieved and a destination expandable storage volume is resized to match components of the expandable storage volume at the first location. Thereafter the point in time copy of the expandable storage volume from the first location to the second location is transferred and configuration information regarding the point in time copy is copied from the first location to the second location a data structure for storing information regarding the transferred point in time copy of the expandable storage volume is updated.

Referring to a network data storage environment is shown. The storage environment includes a plurality of client systems . .N a storage server system and a network connecting the client systems . .N and the storage server system . As shown in the storage server system includes at least one storage server a switching fabric and a number of mass storage devices within a mass storage subsystem such as conventional magnetic disks optical disks such as CD ROM or DVD based storage magneto optical MO storage flash memory storage device or any other type of non volatile storage devices suitable for storing structured or unstructured data. The examples disclosed herein may reference a storage device as a disk but the adaptive embodiments disclosed herein are not limited to disks or any particular type of storage media device in the mass storage subsystem .

The storage server or servers may be for example one of the storage server products available from NetApp Inc. the assignee of the present application. The client systems . .N may access the storage server via network which can be a packet switched network for example a local area network LAN wide area network WAN or any other type of network.

The storage server maybe connected to the storage devices via the switching fabric which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable numbers of storage servers and or mass storage devices and or any other suitable network technologies may be employed. While implies in some embodiments a fully connected switching fabric where storage servers can see all storage devices it is understood that such a connected topology is not required. In some embodiments the storage devices can be directly connected to the storage servers such that no two storage servers see a given storage device.

The storage server can make some or all of the storage space on the storage devices available to the client systems . .N in a conventional manner. For example each storage device can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems . .N according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored at storage devices available to users and or application programs.

The storage server can present or export data stored at storage device as volumes also referred to herein as storage volumes to each of the client systems . .N. A volume is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object the volume and which is managed as a single administrative unit such as a single file system. A file system is a structured e.g. hierarchical set of stored logical containers of data e.g. volumes logical unit numbers LUNs directories files . Note that a file system does not have to include or be based on files per se as its units of data storage.

Various functions and configuration settings of the storage server and the mass storage subsystem can be controlled from a management console coupled to the network .

The storage environment includes a plurality of client systems . .M a clustered storage system and a network connecting the client systems and the clustered storage server system . As shown in the clustered storage server system includes a plurality of server nodes may also be referred to as nodes . .N a cluster switching fabric and a plurality of mass storage devices . .N similar to storage devices . Note that more than one mass storage device can be associated with each node .

Each of the nodes is configured to include several modules including an N module a D module and an M host each of which can be implemented by using a separate processor executable module and an instance of a replicated database RDB . Specifically node . includes an N module . a D module . and an M host . node .N includes an N module .N a D module .N and an M host .N and so forth. The N modules . .N include functionality that enables nodes . .N respectively to connect to one or more of the client systems over the network while the D modules . .N provide access to the data stored at storage devices . .N respectively. The M hosts provide management functions for the clustered storage server system including a system for replicating the Infinite Volume described below in detail. Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

In one embodiment RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage for information used by nodes including a volume location database VLDB not shown . The VLDB is a database that indicates the location within the cluster of each volume in the cluster i.e. the owning D module for each volume and is used by the N modules to identify the appropriate D module for any given volume to which access is requested.

A switched virtualization layer including a plurality of virtual interfaces VIFs is provided between the respective N modules . .N and the client systems . .M allowing the storage . .N associated with the nodes . .N to be presented to the client systems as a single shared storage pool.

The clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers in which each vserver represents a single storage system namespace with separate network access. Each vserver has a user domain and a security domain that are separate from the user and security domains of other vservers. Moreover each vserver is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more vservers. Client systems can access the data on a vserver from any node of the clustered system but only through the VIFs associated with that vserver. It is noteworthy that the embodiments described herein are not limited to the use of vservers.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The N modules and D modules cooperate to provide highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N modules and D modules in there may be differing numbers of N modules and or D modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the N modules and D modules. As such the description of a node comprising one N module and one D module should be understood to be illustrative only.

The storage controller further includes a memory a network adapter a cluster access adapter and a storage adapter all interconnected by an interconnect . Interconnect may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire or any other system.

The cluster access adapter includes a plurality of ports adapted to couple the node to other nodes of the cluster. In the illustrated embodiment Ethernet is used as the clustering protocol and interconnect media although other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternative embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N module and or D module for communicating with other N modules and or D modules of the cluster.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module such as a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks at the storage devices. Illustratively one processor can execute the functions of the N module on the node while another processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing processor executable code and data structures associated with the present disclosure. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processors s functionally organizes the storage controller by among other things configuring the processor s to invoke storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes a plurality of ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state disk SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on storage devices . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on storage devices can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . The storage devices can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes file systems.

The storage operating system facilitates clients access to data stored on the storage devices . In certain embodiments the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by storage devices . In certain embodiments a storage manager logically organizes the information as a hierarchical structure of named directories and files on the storage devices . Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the storage manager to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers LUNs .

In the illustrative embodiment the storage operating system is a version of the Data ONTAP operating system available from NetApp Inc. and the storage manager implements the Write Anywhere File Layout WAFL file system. However other storage operating systems are capable of being enhanced or created for use in accordance with the principles described herein.

In the illustrated embodiment the storage operating system includes multiple functional layers organized to form an integrated network protocol stack or more generally a multi protocol engine that provides data paths for clients to access information stored on the node using block and file access protocols. The multiprotocol engine in combination with underlying processing hardware also forms the N module . The multi protocol engine includes a network access layer which includes one or more network drivers that implement one or more lower level protocols to enable the processing system to communicate over the network such as Ethernet Internet Protocol IP Transport Control Protocol Internet Protocol TCP IP Fibre Channel Protocol FCP and or User Datagram Protocol Internet Protocol UDP IP . The multiprotocol engine also includes a protocol layer which implements various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP Internet small computer system interface iSCSI etc. Further the multiprotocol engine includes a cluster fabric CF interface module A which implements intra cluster communication with D modules and with other N modules.

In addition the storage operating system includes a set of layers organized to form a backend server that provides data paths for accessing information stored on the storage devices of the node . The backend server in combination with underlying processing hardware also forms the D module . To that end the backend server includes a storage manager module that manages any number of storage volumes a RAID system module and a storage driver system module .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the storage driver system implements a disk access protocol such as SCSI protocol or FCP.

The backend server also includes a CF interface module B to implement intra cluster communication with N modules and or other D modules. The CF interface modules A and B can cooperate to provide a single file system image across the D modules in the cluster. Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

The CF interface modules A B implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a D module exposing a CF application programming interface API to which an N module or another D module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface A on N module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the CF decoder of CF interface B on D module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client is forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager generates operations to load retrieve the requested data from storage device if it is not resident in memory . If the information is not in memory the storage manager indexes into a metadata file to access an appropriate entry and retrieve a logical virtual block number VBN . The storage manager then passes a message structure including the logical VBN to the RAID system the logical VBN is mapped to a disk identifier and disk block number DBN and sent to an appropriate driver e.g. SCSI of the storage driver system . The storage driver accesses the DBN from the specified storage device and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented at least partially in specially designed hardware. That is in an alternate embodiment of the invention some or all of the storage operating system is implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC for example.

The N module and D module each can be implemented as processing hardware configured by separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N module and a D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The techniques introduced here generally relate to a content repository implemented in a network storage server system such as described above. illustrates the overall architecture of the content repository according to one embodiment. The content repository includes a distributed object store an object location subsystem OLS a presentation layer and a management subsystem . Normally there will be a single instance of each of these components in the overall content repository and each of these components can be implemented in any one server node or distributed across two or more server nodes . The functional elements of each of these units i.e. the OLS presentation layer and management subsystem can be implemented by specially designed circuitry or by programmable circuitry programmed with software and or firmware or a combination thereof. The data storage elements of these units can be implemented using any known or convenient form or forms of data storage device.

The distributed object store provides the actual data storage for the data objects in the server system and includes multiple data constituent volumes may interchangeably be referred to as distinct single node object stores . A single node object store or data constituent volume is an object store that is implemented entirely within one node. Each data constituent volume is a logical non physical container of data such as a data constituent volume or a logical unit LUN . Some or all of the data constituent volumes that make up the distributed object store can be implemented in separate server nodes . Alternatively all of the data constituent volumes that make up the distributed object store can be implemented in the same server node. Any given server node can access multiple data constituent volumes and can include multiple data constituent volumes .

The distributed object store provides location independent addressing of data objects i.e. data objects can be moved among data constituent volumes without changing the data objects addressing with the ability to span the object address space across other similar systems spread over geographic distances. Note that the distributed object store has no namespace the namespace for the server system is provided by the presentation layer .

The term namespace as used herein refers to a virtual hierarchical collection of unique volume names or identifiers and directory paths to the volumes in which each volume represents a virtualized container storing a portion of the namespace descending from a single root directory. For example each volume associated with a namespace can be configured to store one or more data containers scripts word processing documents executable programs and others.

The presentation layer provides access to the distributed object store . It is generated by at least one presentation module i.e. it may be generated collectively by multiple presentation modules one in each multiple server nodes . The presentation module can be in the form of specially designed circuitry or programmable circuitry programmed with software and or firmware or a combination thereof.

The presentation layer receives client requests translates them into an internal protocol and sends them to the appropriate D module . The presentation layer provides two or more independent interfaces for accessing stored data e.g. a conventional NAS interface and a Web Service interface . The NAS interface allows access to the object store via one or more conventional NAS protocols such as NFS and or CIFS. Thus the NAS interface provides a file system like interface to the content repository.

The Web Service interface allows access to data stored in the object store via either named object access or raw object access also called flat object access . Named object access uses a namespace e.g. a file system like directory tree interface for accessing data objects as does NAS access whereas raw object access uses system generated global object IDs to access data objects as described further below. The Web Service interface allows access to the object store via Web Service as defined by the W3C using for example a protocol such as Simple Object Access Protocol SOAP or a RESTful REpresentational State Transfer ful protocol over HTTP.

The presentation layer further provides at least one namespace may also be referred to as namespace volume for accessing data via the NAS interface or the Web Service interface. In one embodiment this includes a Portable Operating System Interface POSIX namespace. The NAS interface allows access to data stored in the object store via the namespace s . The Web Service interface allows access to data stored in the object store via either the namespace s by using named object access or without using the namespace s by using raw object access . Thus the Web Service interface allows either named object access or raw object access and while named object access is accomplished using a namespace raw object access is not. Access by the presentation layer to the object store is via either a fast path or a slow path as discussed further below.

The function of the OLS is to store and provide valid location IDs and other information such as policy IDs of data objects based on their global object IDs these parameters are discussed further below . This is done for example when a client requests access to a data object by using only the global object ID instead of a complete object handle including the location ID or when the location ID within an object handle is no longer valid e.g. because the target data object has been moved . Note that the system thereby provides two distinct paths for accessing stored data namely the fast path and the slow path . The fast path provides data access when a valid location ID is provided by a client e.g. within an object handle . The slow path makes use of the OLS and is used in all other instances of data access. The fast path is so named because a target data object can be located directly from its valid location ID whereas the slow path is so named because it requires a number of additional steps relative to the fast path to determine the location of the target data object.

The management subsystem includes a content management component and an infrastructure management component . The infrastructure management component includes logic to allow an administrative user to manage the storage infrastructure e.g. configuration of nodes storage devices volumes LUNs etc. . The content management component is a policy based data management subsystem for managing the lifecycle of data objects and optionally the metadata stored in the content repository based on user specified policies. It can execute actions to enforce defined policies in response to system defined trigger events and or user defined trigger events e.g. attempted creation deletion access or migration of an object .

The specified policies may relate to for example system performance data protection and data security. Performance related policies may relate to for example which logical container a given data object should be placed in migrated from or to when the data object should be migrated or deleted etc. Data protection policies may relate to for example data backup and or data deletion. Data security policies may relate to for example when and how data should be encrypted who has access to particular data etc. The specified policies can also include polices for power management storage efficiency data retention and deletion criteria. The policies can be specified in any known convenient or desirable format and method. A policy in this context is not necessarily an explicit specification by a user of where to store what data when to move data etc. Rather a policy can be a set of specific rules regarding where to store what when to migrate data etc. derived by the system from the end user s SLOs i.e. a more general specification of the end user s expected performance data protection security etc. For example an administrative user might simply specify a range of performance that can be tolerated with respect to a particular parameter and in response the management subsystem would identify the appropriate data objects that need to be migrated where they should get migrated to and how quickly they need to be migrated.

In one embodiment the distributed object store is implemented by providing at least one data constituent volume in each of at least two D modules in the system any given D module can include zero or more single node object stores . Also implemented in each of at least two D modules in the system are an OLS store that contains mapping data structures used by the OLS including valid location IDs and policy IDs and a policy store e.g. a database that contains user specified policies relating to data objects note that at least some policies or policy information may also be cached in the N module to improve performance .

The presentation layer is implemented at least partially within each N module . In one embodiment the OLS is implemented partially by the N module and partially by the corresponding M host as illustrated in . More specifically in one embodiment the functions of the OLS are implemented by a special daemon in the M host and by the presentation layer in the N module .

In one embodiment the management subsystem is implemented at least partially within each M host . Nonetheless in some embodiments any of these subsystems may also be implemented at least partially within other modules. For example at least a portion of the content management component of the management subsystem can be implemented within one or more N modules to allow for example caching of policies in such N modules and or execution application of policies by such N module s . In that case the processing logic and state information for executing applying policies may be contained in one or more N modules while processing logic and state information for managing policies is stored in one or more M hosts . Administrative users can specify policies for use by the management subsystem via a user interface provided by the M host to access the management subsystem .

As noted above the distributed object store enables both path based access to data objects as well as direct access to data objects. For purposes of direct access the distributed object store uses a multilevel object handle as illustrated in . When a client creates a data object it receives an object handle as the response to creating the object. This is similar to a file handle that is returned when a file is created in a traditional storage system. The first level of the object handle is a system generated globally unique number called a global object ID that is permanently attached to the created data object. The second level of the object handle is a hint which includes the location ID of the data object and in the illustrated embodiment the policy ID of the data object. Clients can store this object handle containing the global object ID location ID and policy ID .

When a client attempts to read or write the data object using the direct access approach the client includes the object handle of the object in its read or write request to the server system . The server system first attempts to use the location ID within the object handle which is intended to be a pointer to the exact location within a volume where the data object is stored. In the common case this operation succeeds and the object is read written. This sequence is the fast path for I O see .

If however an object is moved from one location to another for example from one volume to another the server system creates a new location ID for the object. In that case the old location ID becomes stale invalid . The client may not be notified that the object has been moved or that the location ID is stale and may not receive the new location ID for the object at least until the client subsequently attempts to access that data object e.g. by providing an object handle with an invalid location ID . Or the client may be notified but may not be able or configured to accept or understand the notification.

The current mapping from global object ID to location ID is stored reliably in the OLS . If during fast path I O the server system discovers that the target data object no longer exists at the location pointed to by the provided location ID this means that the object must have been either deleted or moved. Therefore at that point the server system will invoke the OLS to determine the new valid location ID for the target object. The server system then uses the new location ID to read write the target object. At the same time the server system invalidates the old location ID and returns a new object handle to the client that contains the unchanged and unique global object ID as well as the new location ID. This process enables clients to transparently adapt to objects that move from one location to another for example in response to a change in policy .

An enhancement of this technique is for a client never to have to be concerned with refreshing the object handle when the location ID changes. In this case the server system is responsible for mapping the unchanging global object id to location ID. This can be done efficiently by compactly storing the mapping from global object ID to location ID in for example cache memory of one or more N modules .

As noted above the distributed object store enables path based access to data objects as well and such path based access is explained in further detail in the following sections.

In a traditional storage system a file is represented by a path such as u foo bar file.doc . In this example u is a directory under the root directory foo is a directory under u and so on. Therefore a file is uniquely identified by a single path. However since file handles and directory handles are tied to location in a traditional storage system an entire path name is tied to a specific location e.g. an inode of the file making it very difficult to move files around without having to rename them.

An inode is a data structure e.g. a 128 byte structure which is used to store information such as meta data about a data container for example a file. The meta data contained in an inode may include data information e.g. ownership of the file access permission for the file size of the file file type and location of the file on disk as described below. The file system uses a file handle i.e. an identifier that includes an inode number to retrieve an inode from a storage disk.

Now refer to which illustrates a mechanism that allows the server system to break the tight relationship between path names and location. As illustrated in the example of path names of data objects in the server system are stored in association with a namespace e.g. a directory namespace . The directory namespace maintains a separate directory entry e.g. for each data object stored in the distributed object store . A directory entry as indicated herein refers to an entry that describes a name of any type of data object e.g. directories files logical containers of data etc. . Each directory entry includes a path name e.g. NAME i.e. a logical address of the data object and a pointer e.g. REDIRECTOR POINTER shown as stub pointer for mapping the directory entry to the data object.

In a traditional storage system the pointer e.g. an inode number directly maps the path name to an inode associated with the data object. On the other hand in the illustrated embodiment shown in the pointer of each data object points to a stub file or a redirector file used interchangeably throughout this specification associated with the data object. A redirector file as indicated herein refers to a file that maintains an object locator of the data object. The object locator of the data object could either be the multilevel object handle or just the global object ID of the data object. In the illustrated embodiment the redirector file e.g. redirector file for data object is also stored within the directory namespace . In addition to the object locator data the redirector file may also contain other data such as metadata about the location of the redirector file etc.

As illustrated in for example the pointer included in the directory entry of data object points to a redirector file for data object instead of pointing to for example the inode of data object . The directory entry does not include any inode references to data object . The redirector file for data object includes an object locator i.e. the object handle or the global object ID of data object . As indicated above either the object handle or the global object ID of a data object is useful for identifying the specific location e.g. a physical address of the data object within the distributed object store . Accordingly the server system can map the directory entry of each data object to the specific location of the data object within the distributed object store . By using this mapping in conjunction with the OLS i.e. by mapping the path name to the global object ID and then mapping the global object ID to the location ID the server system can mimic a traditional file system hierarchy while providing the advantage of location independence of directory entries.

By having the directory entry pointer of a data object point to a redirector file containing the object locator information instead of pointing to an actual inode of the data object the server system introduces a layer of indirection between i.e. provides a logical separation of directory entries and storage locations of the stored data object. This separation facilitates transparent migration i.e. a data object can be moved without affecting its name and moreover it enables any particular data object to be represented by multiple path names thereby facilitating navigation. In particular this allows the implementation of a hierarchical protocol such as NFS on top of an object store while at the same time allowing access via a flat object address space wherein clients directly use the global object ID to access objects and maintaining the ability to do transparent migration.

In one embodiment instead of using a redirector file for maintaining the object locator i.e. the object handle or the global object ID of a data object the server system stores the global object ID of the data object directly within the directory entry of the data object. An example of such an embodiment is depicted in . In the illustrated example the directory entry for data object includes a path name and the global object ID of data object . In a traditional server system the directory entry would contain a path name and a reference to an inode e.g. the inode number of the data object. Instead of storing the inode reference the server system stores the global object ID of data object in conjunction with the path name within the directory entry of data object . As explained above the server system can use the global object ID of data object to identify the specific location of data object within the distributed object store . In this embodiment the directory entry includes an object locator i.e. a global object ID instead of directly pointing to the inode of the data object and therefore still maintains a layer of indirection between the directory entry and the physical storage location of the data object. As indicated above the global object ID is permanently attached to the data object and remains unchanged even if the data object is relocated within the distributed object store .

In one embodiment information regarding the various volumes of InfiniteVol for example identifiers for identifying the various volumes vserver identifiers and other information may be stored in a data structure for example or . This allows a user to expand or contract the size of the InfiniteVol based on user needs. When a new data constituent volume is added to InfiniteVol the data structure is updated to include information regarding the new volume. When a volume is removed from InfiniteVol then the information regarding the volume is removed from the data structure .

Data centers typically replicate storage volumes for example by taking snapshots such that a file system can be restored in case of a disaster. Snapshot without derogation to any trademark rights of NetApp Inc. means a point in time copy of a storage file system. A snapshot is a persistent point in time image of an active file system that enables quick recovery of data after data has been corrupted lost or altered. Snapshots can be created by copying the data at predetermined point in time to form a consistent image or virtually by using a pointer to form the image of the data.

Snapshots for multiple volumes typically managed by a single node are taken by using a consistency group. The consistency group is a logical structure that includes various storage volumes whose snapshots are taken at the same time. To take the snapshot of multiple volumes first the consistency group is started and the user is given an opportunity to add any number of storage volumes. Once the volumes are added a logical fence is generated by the storage operating system . The logical fence is a filtering mechanism that includes the information at the storage volumes in a snapshot at the time the fence is generated and excludes any read write requests that are received after the fence is created.

The logical fence may be enabled by the storage operating system by maintaining a data structure not shown at a storage device. The data structure tracks the I O requests that are generated after the fence is created so that the excluded I O requests can be handled after the snapshot is generated. Details of handling the excluded I O requests are not germane to the inventive embodiments disclosed herein.

After the snapshot is generated the storage volumes are unfenced and the consistency group is deleted. This approach operates well when the storage volumes are managed by a single node. The process however can be challenging when one has to take a snapshot for InfiniteVol that includes the namespace the OLS store and multiple data constituent volumes A N managed by a plurality of nodes in a cluster based environment. The embodiments described herein provide systems and methods for generating snapshots for InfiniteVol .

In one embodiment system includes a snapshot job manager that is configured to receive a snapshot request for generating a snapshot. The snapshot job manager creates a job for taking the snapshot. The snapshot job manager may be implemented by M host . The snapshot request is typically received from a client via user interface . The request may include an identifier may be referred to as repository identifier that uniquely identifies the InfiniteVol and a vserver identifier that uniquely identifies the vserver that interfaces with client systems for processing I O requests and owns the InfiniteVol . It is noteworthy that the processes and systems described herein are not limited to using a vserver or a similar entity. A stand alone storage server may be used to implement the various embodiments.

The snapshot request is passed by the snapshot job manager to a snapshot coordinator that may also be implemented by M host . The snapshot coordinator queries the volume data structure to obtain information regarding the various volumes of InfiniteVol . The volume data structure may be maintained by infrastructure management module of the management subsystem as described above at a storage location accessible to the infrastructure management module . In one embodiment the volume data structure is a searchable data structure with fields A C shown in .

Field A stores identification information for each vserver in the clustered environment for example Vserver . Field B identifies the InfiniteVols that are presented to each vserver identified by field A for example InfiniteVol. The various volumes within each InfiniteVol are identified by field C for example Namspace OLS and various data constituent volumes shown as DC DCN. As described above the volumes include namespace data constituent volumes A N and OLS store .

After the snapshot coordinator obtains information regarding the volumes of the InfiniteVol a consistency group is started to maintain consistency across a plurality of nodes . An example of a consistency group is shown in . The consistency group may include the namespace OLS store and data constituent volumes A N. When the consistency group is started at a given time a logical fence is created first for the namespace then for the OLS store followed by the data constituent volumes. The term fence as used herein means that the information within each volume when the consistency group is started would be included in the snapshot. This includes the completed write requests as well as updated redirector files namespace and OLS store information. Any read write operations after the consistency group is created are fenced off and are not included in the snapshot.

After a snapshot is taken the consistency group goes through a commit operation which indicates that the snapshot operation was a success and during the commit operation the volumes are unfenced. It is noteworthy that even if the snapshot operation is not a success the volumes are still unfenced. The order in which the volumes are unfenced is opposite to the order in which the volumes were fenced. For example the namespace is fenced first and unfenced last. The data constituent volume N is fenced last and unfenced first.

One reason the namespace is fenced first is because it includes the directory namespace for example and stub files for example . The stub files are used to access data objects that are stored by the data constituent volumes. If the namespace is not fenced and a data constituent volume is fenced off then there may be a mismatch between the namespace entries and the stored data objects at the data constituent volumes. By fencing the namespace first one can ensure that the data objects stored at that point in time will be consistent for a given snapshot across the plurality of volumes managed by a plurality of nodes.

The snapshot is taken at a given time across multiple nodes. Once the snapshot is taken the storage volumes are unfenced in the order described above. A snapshot data structure is then updated or generated if one does not exist. The snapshot data structure is stored at a memory location that is accessible to management subsystem . An example of the snapshot data structure is shown in having a plurality of fields for example fields A G that are now described in detail.

Field A identifies the vserver that is associated with the InfiniteVol for which a snapshot is taken at any given time. Field B identifies the InfiniteVol that is replicated. Field C provides a name for the overall snapshot and field D provides an identifier for the overall snapshot of InfiniteVol . Field E provides a snapshot identifier for each volume of InfiniteVol that is replicated and field F identifies each volume corresponding to the snapshot identifier of field E. Field G provides a timestamp for each snapshot providing a time when the snapshot was taken.

Snapshot data structure may be used to present snapshots of multiple volumes across multiple nodes to a user as a single logical entity. For example assume that an InfiniteVol having an identifier InfiniID includes 6 volumes including a namespace identified by Vn an OLS store identified by Vols and four data constituent volumes Vd Vd. The snapshot for the entire InfiniteVol may be identified as S and may be represented by an object S InfiniD . The snapshot of the namespace may be represented as Sn and the snapshot of the OLS store may be identified by Sols. The snapshots of the four data constituent volumes may be identified by Sd Sd Sd and Sd. The overall snapshot may be represented as 

S InfiniD may be used to manage the snapshots for various volumes of an InfiniteVol. S InfiniD may be presented to a storage administrator using management console for managing the various snapshots. A user for example client is only presented with a single object for example Sn Vn to access the various individual snapshots and is unaware of how the individual snapshots for various volumes are being handled by the underlying system. The user is able to access the entire snapshot using a single object as described below in more detail.

Once the snapshot data structure is updated that information is then uploaded to a cache of N Blade . In one embodiment cache may be used to respond to client requests to access snapshots via a snapshot access layer may also be called a data access layer as described below in detail.

In block S snapshot coordinator starts a consistency group for the volumes within InfiniteVol that are to be included in the snapshot. Namespace is fenced first followed by the OLS store and the data constituent volumes A N. Storage operating system or snapshot coordinator tracks the order in which the volumes are fenced across multiple volumes and multiple nodes. The tracking may be performed by maintaining a data structure that stores information regarding namespace OLS store and the data constituent volumes A N. The fence is applied in a serial manner such that the fence order mentioned above can be maintained. A time stamp for recording each fence may also be used for maintaining the order.

The namespace is fenced first because it is presented to users and includes the directory namespace with directory entries and stub files . The directory entries store pointers to the stub files that point to various objects stored at the data constituent volumes A N. By fencing the namespace first one can ensure that no changes to the stub files will be made after the fence is generated.

In block S the snapshots for the storage volumes are taken at the same time. Once the snapshots are taken the storage volumes are unfenced in block S. The order in which the volumes are unfenced is opposite to the order in which the volumes are fenced in block S i.e. the namespace is unfenced last while the data constituent volume that was fenced last is unfenced first. By unfencing the namespace last one can ensure that various stub files point to the appropriate unfenced data constituent volumes. If a data constituent volume is fenced and the namespace is unfenced then a user can submit a request to read or write an object but the request will not be serviced properly because the data constituent volume is still fenced when the request is received and therefore an error may be generated.

Storage operating system or snapshot coordinator maintains a record not shown of the order in which storage volumes are fenced. This information may be stored at any storage location that is accessible to snapshot coordinator or any other module that can provide this information to snapshot coordinator .

Thereafter in block S snapshot coordinator updates the snapshot data structure that has been described above in detail with respect to . The snapshot can then be presented as a single entity to a user. The snapshot has its own name and identifier that can be used to access the snapshots of the underlying storage volumes as described below in more detail. Thereafter the process ends in block S.

In block S the snapshot access layer retrieves a data constituent volume identifier and a snapshot identifier for the data constituent volume using the namespace identifier and the snapshot identifier from the snapshot data structure that has been described above.

In block S an active file system identifier as stored in the stub redirector file is replaced by the data constituent snapshot identifier and a handle is provided to the D module that manages the data constituent volume and its snapshot. Thereafter access to the snapshot of the data constituent volume is provided to the user.

An example of implementing process is shown in . The initial file handle or client request includes a snapshot identifier a namespace identifier and an inode identifier . The namespace identifier points to a stub file that includes a data constituent volume identifier with an inode identifier . The active file system value which may be 0 is replaced by 3 the snapshot identifier for data constituent volume . The snapshot identifier value of 3 is obtained by using the various fields of snapshot data structure described above.

Data handle with the snapshot identifier volume identifier and inode number is provided to the D module that manages the data constituent volume . The D module then returns the snapshot data that is provided to the client.

In one embodiment clients within a content repository are provided with an expandable storage volume having a plurality of volumes that may be managed by different storage server nodes. The plurality of volumes is replicated using the techniques described above. The clients can use a single object to access each replicated volume without having to spend any resources in managing the replicated volumes.

Due to proliferation of electronic information disaster recovery is a significant aspect of managing data storage centers. Typically information at a data center is stored at a primary storage location managed by a primary storage system. Information stored at the primary location is replicated and stored as a snapshot at the primary location. The snapshot from the primary location may be mirrored at another location may be referred to as remote location and stored as a base snapshot. When the information at the primary location changes the changed information is mirrored to the remote location. Thus the base snapshot is updated at the remote location as information at the primary location changes.

If the primary location becomes inoperable due to a disaster for example flooding earthquake or any other reason then a latest snapshot at the remote location consistent with the primary location is presented to the clients. The clients then use the remote location to store information. After the primary location becomes available again the information stored at the remote location is synchronized with the information stored at the primary location before the disaster. As part of the synchronization a snapshot of the remote location is taken so that the information stored at the remote location after the remote location became available can be synchronized with the information stored at the primary location prior to the disaster. NetApp Inc. provides SnapMirror technology without derogation of any trademark rights of NetApp Inc. for such a disaster recovery solution.

SnapMirror based disaster recovery solutions become complex in storage environments having InfiniteVols described above. Since an InfiniteVol includes a namespace an OLS store and a plurality of data constituent volumes that are managed by different nodes maintaining an accurate mirrored copy of the InfiniteVol at the remote location presenting an accurate view of the InfiniteVol from the remote location after a disaster and then re synchronizing the remote location with the primary location can be challenging. The adaptive embodiments described herein provide disaster recovery solutions for an environment using InfiniteVols.

The InfiniteVol includes a namespace an OLS store and data constituent volumes A N similar to namespace OLS store and the data constituent volumes A N described in detail above with respect to . The primary location stores at least a first snapshot A of InfiniteVol . The first snapshot A is transferred from the primary location to the remote location using a replication protocol . In one embodiment the replication protocol is a block replication protocol for example the Spin network protocol SpinNP protocol provided by NetApp Inc. the assignee of this application without derogation of any trademark rights that replicates information from the primary location to the remote location on a block by block basis.

SpinNP comprises a collection of methods functions constituting a SpinNP application programming interface API . The SpinNP API in this context is a set of software calls and routines that are made available exported by a process and that can be referenced by other processes. SpinNP protocol communication in a cluster for example occurs via connections. Communication is illustratively effected by a D blade See exposing the SpinNP API to which an N blade See or another D blade issues calls. To that end the CF interface modules A B described above in detail with respect to are organized as a CF encoder and a CF decoder. The CF encoder of e.g. CF interface A on N blade encapsulates a SpinNP message as i a local procedure call LPC when communicating a command to a D blade residing on the same node or ii a remote procedure call RPC when communicating the command to a D blade residing on a remote node of a cluster. In either case the CF decoder of CF interface B on the D blade de encapsulates the SpinNP message and processes the command. The adaptive embodiments described herein are not limited to SpinNP or any specific network protocol standard.

The remote InfiniteVol also includes namespace OLS store and a plurality of data constituent volumes A N. The remote location stores a base snapshot which is a copy of snapshot A. The namespace may be a replicated copy of namespace the OLS store may be a replicated copy of the OLS store and the plurality of data constituent volumes A N maybe a replication of data constituent volumes A N.

As InfiniteVol changes over time at the primary location more snapshots are taken of InfiniteVol . These snapshots are shown as N. The changes between snapshot A and N are frequently sent to the remote location so that the remote location is synchronized with the latest snapshot of InfiniteVol . The process of taking the snapshots and managing the snapshots is similar to the processes described above.

In one embodiment system or portions thereof are executed at the remote location by M module . System includes a mirroring coordinator that coordinates various tasks for generating the disaster recovery copy at the remote location . Mirroring coordinator has access to a destination volume data structure and a mirroring data structure that are described below in more detail. Mirroring coordinator communicates with the storage systems of the primary location via a source proxy module . The source proxy module is used to send and receive information to and from the primary location . In one embodiment the source proxy module is configured to format information that is transmitted to and received from primary location .

System also includes a snapshot coordinator similar to snapshot coordinator . Snapshot coordinator maintains a snapshot data structure similar to snapshot data structure described above with respect to . It is noteworthy that data structures and may be stored at a storage device at any location as long as the storage device is accessible directly or indirectly by N Module and the various components of system . Details regarding system operations are now provided with respect to the process flow diagrams of A B and .

In block S mirroring coordinator obtains a list of volumes for the source InfiniteVol . This information may be obtained by source proxy module from the M Module of primary cluster managing the source volume data structure for example .

In block S mirroring coordinator assigns the source volume identifiers for the volumes of Infinite Vol to each corresponding volume of the destination InfiniteVol . Mirroring coordinator maps the source volume identifiers for each volume of InfiniteVol to each volume of InfiniteVol . The mapped volume identifiers may be stored as part of destination volume data structure or otherwise.

In block S mirroring coordinator updates the destination volume data structure with the newly assigned source InfiniteVol identifiers. In one embodiment the original volume identifiers of each volume within destination InfiniteVol are mapped to volume identifiers of the source InfiniteVol . The mapping information may be maintained as a subset of destination volume data structure .

In block S a mirroring relationship is created between each volume of source InfiniteVol and each volume of the destination InfiniteVol . The relationship may be maintained at mirroring data structure having Fields A H that are described below in detail with respect to . It is noteworthy that instead of storing the mirroring relationships in mirroring data structure the relationships may be maintained at the destination volume data structure . The mirroring relationship enables transfer of information from each volume of the primary location to the remote location .

Referring to field A of data structure includes a destination cluster identifier that is used to uniquely identify the destination cluster at the remote location .

Field B identifies the destination InfiniteVol while field C identifies the destination storage repository that includes the InfiniteVol . The vserver managing the InfiniteVol may be identified by field D.

Field E includes a source cluster identifier that is used to uniquely identify the primary cluster at the primary location .

Field F identifies the source InfiniteVol while field G identifies the source storage repository that includes the source InfiniteVol . The vserver managing the InfiniteVol is identified by field H.

Field I identifies the relationship between the source InfiniteVol and destination InfiniteVol as a mirroring relationship. The snapshots of source InfiniteVol are replicated based on the configuration details J that may be customized based on user preferences. The configuration details J may specify how often the snapshots at the primary location are mirrored at the remote location access control information that limits access to the snapshots to authorized users and any other details. Once the mirroring relationships are generated a replicated copy of the InfiniteVol is transferred to remote location as described below in more detail.

In block S mirroring coordinator retrieves source volume information for example information provided by fields E H described above with respect to . The source InfiniteVol information may be obtained from the primary cluster managing InfiniteVol . The information may be received by source proxy module and then provided to mirroring coordinator .

In block S the destination InfiniteVol may be resized to match the source InfiniteVol . Based on the information collected in block S mirroring coordinator compares the source volume information with destination volume information stored at destination volume data structure . Based on the comparison mirroring coordinator determines if new volumes were added to the destination InfiniteVol prior to block S. If new volumes were added then volume identifiers for the newly created destination volumes are replaced by source volume identifiers as described above in block S of . Mirroring relationships are also generated for the new volumes.

In block S the mirroring relationship between the source and destination are confirmed by mirroring coordinator by checking data structure or . If a relationship does not exist for example for any new volumes then mirroring relationships are generated as described above with respect to .

In block S the source snapshot S A or an update is transferred to the remote location using the block replication protocol . Thereafter configuration information regarding Snapshot A is retrieved in block S. The configuration information may include a vserver identifier that identifies a vserver for example that presents InfiniteVol identifier that identifies the InfiniteVol a snapshot identifier that identifies the snapshot A a volume count that provides a count of volumes included within InfiniteVol a snapshot name identity of clients that are allowed to access the snapshot A an indicator indicating if the snapshot A is valid or invalid a list of all the volumes i.e. namespace OLS store and data constituent volumes that are included within the snapshot A a timestamp indicating when the snapshot A was generated and any other configuration information.

In block S the configuration information is replicated and the snapshot data structure is updated. The N module is also notified by the snapshot coordinator so that it can update cache and can redirect any requests for the transferred snapshots similar to the redirection described above with respect to . This enables the N module to redirect client requests from the active file system that may point to one snapshot on one volume that has finished a transfer and to another snapshot on another volume that is still in the middle of a transfer to a consistent snapshot on all volumes as described below with respect to .

The process starts in block S. In block S a common snapshot between the primary location and the remote location is first determined. This may be determined from the snapshot data structure that stores information regarding the various snapshots of InfiniteVol . In block S the source InfiniteVol is restored to the common snapshot.

In block S a transfer snapshot request is created. The destination InfiniteVol now becomes the source and the InfiniteVol at the primary location now becomes the destination for the snapshot transfer operation.

The process steps S S are similar to the process steps S S of except the InfiniteVol operates as the source and the InfiniteVol operates as a destination volume. The mirroring relationships are also reversed S so that correct volume pairs are used but in the opposite direction. For example before a disaster the snapshot A may point to snapshot A but in block S snapshot A points to snapshot A.

In block S the N module managing the source InfiniteVol is notified of the re synchronization by snapshot coordinator . This enables the N module to redirect client requests from the active file system that may point to one snapshot on one volume that has finished a transfer and to another snapshot on another volume that is still in the middle of a transfer to a consistent snapshot on all volumes as described below with respect to .

In block S the process first determines if the request is for the active file system. This is determined by namespace by evaluating the namespace handle. If a specific value identifying the active file system value is specified in the handle for example 0 See then the request is considered to be for the active file system. If the request is not for the active file system then the process moves to block S where access to the requested snapshot is processed as described above with respect to .

If the request is for the active file system then in block S the process determines if the active file system is redirected to a snapshot. This is determined by checking cache of N module that maintains an entry indicating if the active file system is redirected to a snapshot. If the active file system is not directed to a snapshot then in block S the user is provided access to the active file system of the destination InfiniteVol.

If the active file system is redirected to a snapshot then in block S the snapshot identifier of the request see is replaced by a snapshot identifier that was successfully transferred. The snapshot identifier is obtained from cache which is based on the snapshot data structure or described above with respect to . Thereafter in block S the client is provided access to the snapshot.

In one embodiment a recovery copy of the source InfiniteVol is generated at the remote location and made available to a user for example if disaster occurs. The user is able to access a consistent replicated copy with very little disruption.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or entirely by special purpose circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc. Software or firmware for implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

Thus a method and system for taking snapshots of an InfiniteVol and maintaining a disaster recovery copy have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the invention as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

