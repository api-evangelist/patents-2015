---

title: Scalable data storage architecture and methods of eliminating I/O traffic bottlenecks
abstract: A Storage Area Network (SAN) system has host computers, front-end SAN controllers (FE_SAN) connected via a bus or network interconnect to back-end SAN controllers (BE_SAN), and physical disk drives connected via network interconnect to the BE_SANs to provide distributed high performance centrally managed storage. Described are hardware and software architectural solutions designed to eliminate I/O traffic bottlenecks, improve scalability, and reduce the overall cost of SAN systems. In an embodiment, the BE_SAN has firmware to recognize when, in order to support a multidisc volume, such as a RAID volume, it is configured to support, it requires access to a physical disk attached to a second BE_SAN; when such a reference is recognized it passes assess commands to the second BE_SAN. Buffer memory of each FE_SAN is mapped into application memory space to increase access speed, where multiple hosts share an LBA the BE_SAN tracks writes and invalidates the unwritten buffers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09361038&OS=09361038&RS=09361038
owner: 
number: 09361038
owner_city: 
owner_country: 
publication_date: 20150730
---
This application is a continuation of U.S. patent application Ser. No. 14 252 268 filed Apr. 14 2014 which is a continuation in part of U.S. patent application Ser. No. 13 731 854 filed Dec. 31 2012 now. U.S. Pat. No. 8 725 906 which is a continuation in part of U.S. patent application Ser. No. 11 292 838 filed Dec. 2 2005 now U.S. Pat. No. 8 347 010 the disclosures of which are incorporated herein by reference.

This invention relates generally to the field of storage RAID controllers and Storage Area Network SAN systems. As ever increasing demand for more data hence for more data storage SAN and RAID technologies available today have failed to meet performance requirements and with prohibitively high cost made them out of reach for the majority of the small and some medium size businesses.

A majority of small and some medium size businesses and corporations recognizing the productivity gains resulting from high performance and high availability computing systems are often unable to acquire suchlike systems due to prohibitively high cost of data storage subsystems. To reduce the cost they often utilize large number of disjoint individual servers where each server is dedicated to one or more specific applications such as mail servers accounting application server etc. This environment clearly leads to underutilized available aggregate computing power of all servers since each server is dedicated to specific application and the workload cannot be shared. This approach introduces other issues such as system and network administration fault tolerance fragmented data storage data storage and backup management problems as well as system complexity and so forth. Data access and data sharing could be done at different levels such as at block level shared block storage multiple hosts accessing the same disk drives or Logical Unit Numbers LUNs or at the file level using file systems like Network File System Common Internet File System etc.

A Network File System NFS is a client server application that facilitates viewing and optionally storing and updating files stored on a remote computer often called a file server . The client system which may be a work station or cluster node has to run NFS client and the other computer file server needs the NFS server software. Both computers typically must have networking protocol software such as Transmission Control Protocol Internet Protocol TCP IP and networking hardware such as Ethernet InfiniBand Myrinet or other Network Interface Cards NICs installed since the NFS server and NFS client use network protocols to exchange the files data. This approach leads to a bandwidth bottleneck on both client and file server sides mainly due to NFS protocol overhead limited Peripheral Component Interconnect PCI bus data rate and possibly high latency associated with traditional architecture of the data storage subsystem attached to it.

A protocol standard developed by Microsoft Common Internet File System CIFS which allows programs to make requests for files and services located on a remote computer facilitates the same basic function as previously mentioned NFS. CIFS is typically utilized in Microsoft operating system environments to allow shared access from one computer to files stored on another remote computer within the same network. A CIFS client makes a request to a CIFS server usually in another computer for a file access or to pass a message. The server executes requested action and returns a response. CIFS is a public version of the Server Message Bock SMB protocol. The file server running CIFS suffers from the same problems as earlier mentioned NFS server because this is in essence the same or similar hardware and technology.

TCP IP protocol overhead together with network latency affects the performance of NFS CIFS subsystems by significantly increasing access delays for network attached disk when compared to locally attached disk. However locally attached disk performance usually is much lower compared to data storage subsystem implementations such as RAID or Storage Area Network SAN subsystem. Traditional SAN design and implementation even though in many cases superior to locally attached disk drives underutilize aggregate data rate potential of all attached disk drives by making use of time division multiplexing over typically small number of I O network links between servers and the SAN subsystem attached to it.

The present invention is an improvement over the existing data storage architectures by means of allowing parallel execution of the Front End code on the independent FE SAN controllers and employing locking mechanism in the Back End code executed on the BE SAN controllers to enforce data coherency and prevent data corruption.

In a preferred embodiment the FE SAN controllers accept all Small Computer System Interface SCSI commands messages and data for Front End processing. The resulting output is forwarded to their intended BE SAN controllers over the interconnecting network. Any network topology is allowed. From the BE SAN controllers the SCSI commands data and messages after Back End processing are sent to the anticipated SCSI targets disk drives or memory persistent devices or other storage systems. Neither SCSI target devices disk drives nor initiators host computers are aware of the underlying transport or FE SAN and BE SAN controllers. Hosts would perceive a virtual disk drive or drives attached to the FE SAN one or more . FE SAN controllers could be configured as a controller group to share the load command and message processing enhancing the available bandwidth and improving availability. In such cases the host would have a notion of a single multi channel controller attached to it.

A Storage Area Network SAN system has host computers front end SAN controllers FE SAN connected via a bus or network interconnect to back end SAN controllers BE SAN and physical disk drives connected via network interconnect to the BE SANs to provide distributed high performance centrally managed storage. Described are hardware and software architectural solutions designed to eliminate I O traffic bottlenecks improve scalability and reduce the overall cost of SAN systems. In an embodiment the BE SAN has firmware to recognize when in order to support a multidisc volume such as a RAID volume it is configured to support it requires access to a physical disk attached to a second BE SAN when such a reference is recognized it passes assess commands to the second BE SAN. Further the BE SAN has firmware to make use of the physical disk attached to the second BE SAN as a hot spare for RAID operations. Buffer memory of each FE SAN is mapped into application memory space to increase access speed where multiple hosts share an LBA the BE SAN tracks writes and invalidates the unwritten buffers.

This invention relates generally to the field of storage RAID controllers and Storage Area Network SAN systems. Principally the invention relates to improving data access speeds by facilitating massively parallel data access by separating Front End and Back End software or firmware functionality. Thus the invention executes appropriate code concurrently on separate Front End Storage Area Network FE SAN and Back End Storage Area Network BE SAN controllers connected via high speed network switched fabric such as InfiniBand Serial Attached SCSI SAS Fibre Channel FC Myrinet etc. .

A number of different storage interfaces may be supported at the BE SAN with this invention including standard devices such as SCSI SAS Advanced Technology Attachment ATA Serial ATA SATA FC and other similar disk storage as well as PCI Express PCIe Hyper Transport HT etc. interfaces.

The FE SAN interface card FE SAN controller may be provided in different physical formats PCI PCI X PCIe SBus or other interface board formats. Each FE SAN interface card has firmware software that provides SCSI device discovery error handling and recovery and some RAID functionality. The back end may be provided in a number of different physical formats such as in the standard disk drive enclosure format including 19 inch rack and standalone enclosures or an integrated circuit that is easily adaptable to the standard interior configuration of a SAN controller.

Each BE SAN controller has firmware software that provides SCSI device discovery fault management RAID functionality and disk locking methods for portions of the physical disk RDMA capability and error handling functionality. The firmware on each BE SAN controller interface provides all the functionality necessary to interface target disk storage interface such as SCSI SAS ATA SATA or FC disk drives or other data storage devices and systems.

In an embodiment the FE SAN has firmware that provides an Application Programming Interface API and an Application Binary Interface ABI to allow a host Operating System OS and applications to memory map a file or part of a file residing in the FE SAN controller memory to directly access the file data bypassing traditional OS stack and SCSI layers.

As ever increasing demand for more data hence for more data storage SAN and RAID technologies available today have failed to meet performance requirements. However this enabling scalable technology is specifically designed to extend system capabilities and eliminate I O bottlenecks.

To achieve the best performance every server or cluster node should be connected directly or through a rearrangeably non blocking switched fabric to the SAN subsystem. The SAN subsystem should be able to support the sum of data rates on all links between the servers or cluster nodes and the SAN subsystem without significantly increasing the command queue size the number of outstanding requests in the queue waiting to be executed . However the architectural designs hardware and software of today s SAN subsystems create bottlenecks in the SAN controllers due to insufficient aggregate bandwidth and limited scalability. Even though Fibre Channel FC is designed with the intent to allow similar architectures the protocol complexity and prohibitively high cost of the FC technology HBAs and FC switches effectively prevented significant progress in the field of data storage.

Preferably the present invention is implemented in firmware running over a multitasking pre emptive Real Time Operating System RTOS on hardware platform comprised of one or more embedded Central Processing Units CPUs possibly Application Specific Integrated Circuits ASIC or Field Programmable Gate Arrays FPGA a Random Access Memory RAM and programmable input output I O interfaces. It is to be appreciated that the various processes and functions described herein may be either part of the hardware embedded microinstructions running on the hardware or firmware embedded software executed by the RTOS. However it should be further understood that the present invention might be implemented in various other forms of hardware software firmware or a combination thereof.

The SCSI standard defines the device model and SCSI command set for all SCSI devices please see . The SCSI command set is designed to provide efficient peer to peer operation and control of SCSI devices disk drives tapes printers etc. by an operating system. The SCSI command set provides multiple operating systems concurrent access and control over one or more SCSI devices. However proper coordination of these activities between the multiple operating systems is critical to avoid data corruption. Commands that assist with coordination between multiple operating systems are described in the SCSI standard.

Atomic execution for some commands in the system from is an essential part of the invention since it helps protect data integrity. Atomic execution requires that a certain atomic command be effectively executed in their entirety without interruption by of other commands or messages regardless of whether those other commands precede or follow the atomic command. Front End code generally performs command processing and translates Virtual Block Addresses VBA to Logical Block Addresses LBA and sends them to the appropriate BE SAN controllers. The Back End code then checks whether the command could be executed immediately or not. The command may have to be queued for later execution or a BUSY signal could be returned to the Front End code because the SCSI standard typically does not require a guarantee when and in what order commands are executed particularly when components of a system are busy. Since the Front End and Back End codes execute asynchronously until a command that requires atomic execution is to be started. For successful execution all the requirements for successful completion of the atomic command have to be met before the atomic command can begin. In an embodiment writing to device media is an atomic command.

For example to ensure correct execution of a WRITE command before writing data to intended LBAs those LBAs have to be locked and off limit to other WRITE or READ commands. That means that WRITE command has to be executed atomically. The Front End code examines the command and determines that a WRITE command has been received. Then it sends a LOCK request to the appropriate BE SAN controllers to acquire a distributed lock. Back End code responds acknowledging successful locking of the requested LBAs. At this point it is safe to write to the locked LBAs LBAs usually spread across multiple disk drives. Upon the successful completion of the WRITE command the LOCK is removed by the Front End code. There are some other commands that might require atomic execution however the same locking mechanism could be used for those commands.

The previously described method does not permit concurrent writes to occur to the same LBAs and thereby limits write performance to a single file. Indeed it is not possible to actually write data to a disk media simultaneously from two different and independent sources without having a risk of actually corrupting the data. However if sufficient buffer space is available at BE SAN controllers the data that have to be written from number of different hosts initiators could be concurrently spooled into pre allocated independent memory buffers without risking data corruption. To achieve this kind of parallelism on a WRITE request FE SAN controller requests a LOCK for all LBAs that must be written to. If currently there are no other writers the LOCK will be granted. However if there is another writer holding a LOCK every BE SAN controller that has to be written to checks if there is sufficient buffer space to satisfy the request. If the request could be satisfied a tagged LOCK will be issued and the data transfer from the host to the allocated buffer would be initiated. The actual writes to the disks would be done in the order that tagged LOCKs are granted. If the Front End code for some reason was unable to acquire a LOCK for all the LBAs that it needs it will release all already locked LBAs and start the process all over again. In case that default number of attempts is exceeded upper management software layers will resolve the issue by verifying the reason for the unsuccessful locking.

An additional advantage is that the LOCKs are associated with LUNs using specific ID identifiers . Therefore Back End code would not have to search through long LOCK tables to discover whether a command request could be executed or not. Another advantage is that only LBAs that are locked out might be inaccessible during that time. All other LBAs remain accessible allowing greater level for parallelism.

The actual implementation may vary due to use of some specific hardware or of the shelf software. However the principal architecture would not change. It would be relatively straight forward to use of the shelf inexpensive but not limited to x86 hardware platforms with embedded RTOS or embedded Linux OS on it for the BE SAN controllers since great deal of functionality is already available. For the FE SAN controllers the same platform could be used as for the BE SAN controllers. However if InfiniBand or Myrinet network is the interconnect of choice between FE SAN and BE SAN controllers it would be natural to use of the shelf or modified PCIe single board computer or Myrinet interface cards to execute Front End code on them. In alternative embodiments such as those where communications between FE SAN and host are over a PCI or other bus a PCI or other bus single board computer may be used. For entry level systems where the cost is more important than performance the Front End code could be embedded in the device driver or in a software module running on the host computer.

In an embodiment individual storage units such as hard disk drives are coupled to back end storage controller BE SAN . Further individual storage units such as hard disk drives are coupled to at least a second back end storage controller BE SAN . Both BE SANs are coupled to be accessed through network from at least one or more FE SAN controller . Further each FE SAN is coupled to at least one host computer .

In an embodiment virtual disk operating with drives on the second BE SAN is also configurable as a RAID in an embodiment a RAID 5 mode virtual disk and in an alternative embodiment a RAID 6 mode virtual disk.

Some existing RAID controllers including some operable in RAID 5 or RAID 6 modes have the ability to reconstruct data onto a replacement hard disk from remaining disks used by a RAID dataset or virtual disk in these systems once a failed disk has been replaced with a spare disk data is reconstructed onto the spare disk. Once data is reconstructed onto the spare disk which becomes a formerly spare disk redundancy is restored and should a second disk used by the RAID virtual disk fail in turn data can once again be reconstructed from still functioning disks which may include the formerly spare disk and provided to host systems.

A hot spare is a disk that is attached to a RAID controller and unused by existing configured virtual disks. When a disk containing part of data of a RAID virtual disk fails the hot spare may be substituted for the failed disk without requiring that a technician physically replace the failed disk. In some systems this replacement may require operator intervention and in other systems a hot spare may be substituted for a failed disk automatically. Once a hot spare is substituted into a RAID dataset or virtual disk data may be reconstructed onto the hot spare which becomes a formerly spare disk and redundancy may be restored. Typically however the hot spare must be attached to the same RAID controller as the failed drive.

In an embodiment a storage system such as the one depicted on the provides access to the stored data on the storage media via FE SAN controller s device driver and SCSI interface. However the same storage system provides an API and ABI for mapping portion of the FE SAN controller memory into application address space. UNIX Linux and other Operating Systems provide methods such as POSIX compliant mmap system call see PRIOR ART that allows an application to map a file into its memory . Therefore there is one to one correspondence between a memory address and a word in the file. For example Linux kernel documentation describes in details how mmap method is used in Linux and how the PCI memory is mapped above the highest system memory.

In an embodiment on it is shown how memory buffers B and B corresponding to a file LBAs may be allocated mapped and distributed across a number of FE SAN controllers and and BE SAN controllers and . Front End firmware FW and FW running on FE SAN controllers and allocates and manages memory buffers B and B upon receiving a request from an application A or A or host OS via provided API and ABI to map a file or portion of a file into application address space. In addition the memory buffers may have one or more MEMORY BUFFERs ATTRIBUTEs MEM BUFF ATTR that define the system behavior upon a command or control message is received.

Applications A and A running on computers and may send a request to map a file or portion of a file into their respective address spaces Front End firmware FW or FW running on FE SAN and controllers sends a request as needed to the corresponding Back End firmware FW FW or FW running on BE SAN and controllers to process the request. The memory buffers B B B B and B are dynamically allocated and managed upon application accesses a specific memory location that corresponds to a specific LBA . When applications A and A write data to the memory mapped file a copy on write method is used to automatically copy data from FE SAN mmapped memory mapped buffers B and B to corresponding buffers B B and B allocated on the BE SAN controllers and . It is presumed that the application s that were granted direct memory access to the FE SAN and controllers memory buffers assume complete control and responsibility for synchronizing and maintaining the file data coherency. For example a number of different flags can be used with mmap method. Linux mmap man page gives an example of how mmap is implemented in Linux. However these memory mapped buffers physically reside in the FE SAN controller s memory and are mapped into the address space used by the kernel or application at what ordinarily be a software buffer location.

If an application running on host computer writes data to mmapped memory buffers of a file in the FE SAN controller the copy on write method copies data automatically to the corresponding buffers B B and B on BE SAN controllers and . Because Back End firmware FW FW or FW maintains information about all LBAs that are cached in mmapped memory buffers B and B for all FE SAN controllers and if data is modified on one FE SAN controller say based on the MEM BUFF ATTR the corresponding memory addresses may be automatically invalidated on all other FE SAN controllers in this example this would be FE SAN . As described above on msync system call a LOCK is placed on the LBAs and corresponding memory buffers B B and B that have to be flushed to the media to prevent any data modification until the transaction is complete. Because of the latency related to copy on write method between FE SAN and BE SAN controllers memory buffers the data coherency should be guaranteed and maintained by the applications for instance an explicit request such as msync or msync buff could be used to accomplish data synchronization. Typically msync call is associated with synchronizing data to the media. However an msync buff call synchronizes the data in the memory of the FE SAN and BE SAN controllers without immediately writing the data to the media.

In an embodiment on it is shown how memory buffers B and C may be allocated mapped and distributed across a number of FE SAN and and BE SAN and controllers. However the main difference between the apparatus depicted on and is that all memory buffers B and C on the FE SAN controllers and have their corresponding independent buffers B B B and C C C on the BE SAN controllers and . Thus the method to create corresponding memory buffers on the BE SAN controllers is the same as described above. However when synchronizing the data on msync request all the buffers containing the file data that need to be synchronized are specified in the request. Also which buffers are invalidated and which are not on the FE SAN and BE SAN controllers depends on whether they contain a newly written data and on the attributes found in the MEM BUFF ATTR. This behavior is explained in details in an example in the following flow chart . The key benefit to maintaining independent memory buffers on the BE SAN controllers and is that concurrent copy on write even to the same LBAs does not alter the coherent view of the file data. When an application wants to commit the newly written data to the memory mapped file for example the new data written by application A to the memory buffers B on the FE SAN controller and mirrored to corresponding independent buffers B B and B on the BE SAN controllers and the file data synchronization is done upon explicit request. Thus the applications on different host computers and may see different data if the buffers B B B C C and C are not synchronized. This atomic synchronization is done upon receiving an explicit request for example using msync or msync buff calls from an application A or host OS via provided API and ABI. In addition copy on write data mirroring protects the data in case that either FE SAN or or BE SAN controller or fails.

Similarly to the apparatus and method described with reference to Back End firmware FW maintains information about all mmapped memory buffers B and C on all FE SAN controllers and . For example an application A on the computer issues msync or msync buff synchronization request via Front End firmware FW running on the EF SAN controller . Upon receiving msync request the Back End firmware FW synchronizes the data written by the application A. Thus subsequent read commands return the data with the latest and up to date changes. However if the data in the memory buffers C is modified the MEM BUFF ATTRs for the buffers of the file define whether the modified data should be invalidated or not. Thus the actual behavior of the system may be altered by privileged applications and users.

The flow diagram of an embodiment on the shows a method of the present invention for data synchronization which is done upon an application A request to commit the written data into FE SAN controller s memory which is mapped into the application s address space. However the file or portion of the file is mapped via corresponding memory buffers B and C to the applications A and A address spaces via API and ABI primitives.

The method is entered at step after application A sends a request to the Front End firmware FW to synchronize the data that is placed into specified LBAs in the memory buffers B. The Front End firmware FW calculates on which BE SAN controllers or the mirrored LBAs reside. Then at step the Front End firmware FW sends synchronize LBAs requests to all BE SAN controllers or that own the modified LBAs.

At step Back End firmware FW executes lock LBAs method placing LOCKs on all memory buffers B and C containing the modified LBAs. At step the Back End firmware FW executes sync lbas LBAs which either modifies the pointers to point to the memory locations with up to date data or copies to the memory locations that maintain the up to date LBAs.

At step the firmware FW creates a list of all the FE SAN controllers that keep cached version of the LBAs that are to be synchronized and at step sends a multicast message to those FE SAN controllers in this example requesting that those LBAs or the data in the corresponding memory buffers to be invalidated.

In this example FE SAN controller is the only controller that keeps those LBA in the cache thus at the step the Front End firmware receives and starts the execution of invalidate msg LBAs . At the next step the Front End firmware FW determines if those LBAs contain modified data since the last update from the BE SAN controller. If the answer is No it proceeds to the step and invalidates specified LBAs and sends the done invalid msg LBAs to the requesting BS SAN controller .

At the step Back End firmware on the BE SAN controller invalidates the corresponding buffers from the previous step updates pointers and proceeds to step .

At step requesting Back End firmware FW releases the locks for the successfully invalidated LBAs on the FE SAN controllers and the method ends.

However if the answer at step is Yes then the firmware determines from the MEM BUFF ATTR whether it should or should not invalidate those LBAs or pages . If the answer is Yes proceeds to the step however if the answer is No then it proceeds to the step and informs the Back End firmware about the action taken.

At step the Back End firmware FW releases the locks for the LBAs or pages specified in the message from the FE SAN controllers and the method ends. Obviously the actual implementation based on the actual hardware properties may be somewhat different than the case presented in the example here but the main idea would stay the same.

Back End firmware FW FW and FW running on BE SAN controllers and maintain read and write pointers for each memory buffer on every FE SAN controller and and BE SAN controller and . Thus on an application msync request the read pointers for the memory pages specified in msync request for all FE SAN controllers will point to the memory buffers with the up to date data or the latest written data as described above. Thus all the memory buffers specified in msync request on all other FE SAN controllers based on the MEM BUFF ATTR attributes may be automatically invalidated. The granularity of the memory buffers is one page or block which is typically 512 bytes or 4 KB in size obviously the actual size depends on specific hardware and software implementation. When an application writes into a memory buffer on the FE SAN controller then a dirty data flag is set automatically. When application commits the data the Back End firmware looks into a list to find all FE SAN controllers which cached that particular page s and sends a multicast message to invalidate that those page s on all other FE SAN controllers in this case FE SAN controller .

Thus in general if in the meantime another instance of an application has altered the same page s LBAs on another FE SAN controller then based on the attributes set in the MEM BUFF ATTR it is defined what the firmware on that controller does either invalidates that page s and rereads it them from the BE SAN controller s on the next attempt to read from the page or ignores the invalidate message. In either case the parallel application has to be aware that the data may have been altered on a remote FE SAN controller.

However if for some reason an application does not coordinate the I O requests but still requires strict data coherency then the data coherency may be maintained by executing atomic write or read requests which acquire locks and execute atomically as described herein for a standard write command.

In addition as described above periodically or on mflush system call a LOCK is placed on the LBAs and memory buffers that have to be flushed to the media to prevent any data changes while the transaction is in progress. Thus upon completion of the current flush data to the media command the data on the media is up to date.

If an explicit atomic read or write operation is required an application executes an atomic read a read or atomic write a write call. An atomic read or write request always invokes synchronization primitives using the same method as described above. Thus a read request issued by any application running on a or computer after a write always returns the latest data. In this case MEM BUFF ATTR cannot alter this atomic execution. The behavior of a read and a write commands is the same as if a SCSI read or write command is issued with a Force Unit Access FUA or FUA NV Force Unit Access Non Volatile bit set.

In addition if an application needs wants to create a persistent shared memory it creates a memory mapped shared file while the coherent view of the data enforcement may be done by simply executing atomic a read and a write commands calls. However the techniques described above may be used to accomplish the same goal if strict data coherency is not required even write back caching without mirroring technique may be used.

A file system module or residing on the FE SAN controller or communicates directly with the FE SAN controller Front End firmware FW and FW. In addition the Front End firmware FW and FW provides automatic synchronization as described above for all fread fwrite fopen fclose and other calls made to the file system modules and .

However if the system cost is the most important concern then the entire or part of the file system modules and could be executed in host computers and memory as separate software modules communicating with FE SAN controller s firmware via provided API and ABI.

In another embodiment of the present invention illustrated in in addition to providing support for storage devices such as HDDs and SSDs and attached at BE SAN controller s and or it may also support and integrate legacy storage RAID and SAN systems and as well as other storage devices that can be accessed via directly connected links or via SAN Interconnect s . Thus the data stored on the back end storage devices and as well as on legacy storage systems and may be accessed by an application via FE SAN and and BE SAN controllers and . The benefit of using such approach is to leverage the existing storage infrastructure while caching technique and method described in the present invention and supported by FE SAN and and BE SAN controllers and hardware and embedded firmware and software to aggregate reshape and cache the data to speed up an application access to the cached data. Because of the FE SAN and BE SAN controllers ability to aggregate the cached data and reshape the back end I O traffic the utilization of the legacy RAID and SAN systems and should be also improved due to receiving mostly optimal I O workload. The LUNs presented by the legacy storage systems and may be re exported via BE SAN and FE SAN controllers and or aggregated into new LUNs and presented to the host systems and . Any network topology of interconnecting networks or fabrics and is allowed. All data access and locking methods as well as FE SAN controller s memory mapping methods disclosed in this document may be used with the system architecture presented in the . In addition the system may be used with embedded file systems as described above.

The term firmware as used herein with reference to FE SAN and BE SAN firmware includes machine readable instructions executable by a processor and present in a memory associated with that processor whether the memory is a read only memory an erasable whether ultraviolet or electrically and rewritable read only memory or a random access memory loaded when the system is started. As such the term firmware may include portions traditionally defined as software as well as portions traditionally defined as firmware.

The foregoing description should not be taken as limiting the scope of the disclosure. It should thus be noted that the matter contained in the above description or shown in the accompanying drawings should be interpreted as illustrative and not in a limiting sense. The following claims are intended to cover generic and specific features described herein as well as all statements of the scope of the present method and system.

