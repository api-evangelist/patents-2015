---

title: Automated video logging methods and systems
abstract: Exemplary embodiments of systems and methods are provided for automatically creating time-based video metadata for a video source and a video playback mechanism. An automated logging process can be provided for receiving a digital video stream, analyzing one or more frames of the digital video stream, extracting a time from each of the one or more frames analyzed, and creating a clock index file associating a time with each of the one or more analyzed frames. The process can further provide for parsing one or more received data files, extracting time-based metadata from the one or more parsed data files, and determining a frame of the digital video stream that correlates to the extracted time based metadata.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09583149&OS=09583149&RS=09583149
owner: 
number: 09583149
owner_city: 
owner_country: 
publication_date: 20150423
---
This application relates to and claims priority from U.S. Provisional Patent Application Ser. No. 61 983 225 filed Apr. 23 2014 the entire disclosure of which is hereby incorporated herein by reference.

The present disclosure relates to exemplary embodiments of methods and systems for providing metadata and more particularly to exemplary embodiments of methods and systems for automatically creating time based metadata associated with a video clip.

In video management and processing systems a video clip can be stored along with a database containing time based metadata e.g. data associated with instants of time relative to the start of a video clip . Time based metadata can be specified in relative terms and is generally by a frame number relative to the start of the video clip. In some systems a time code value can be used which can specify hours minutes seconds and a frame number relative to the last second.

These data can be referred to as the metadata of the media that is if the video clip is considered the primary data the metadata can be a data set that relates to the video clip directly but may not be a part of the primary data. An example of such metadata is a data triple comprised of the values 100 11 45 and two pointer where the triple indicates respectively the frame number relative to the start of the video clip game time for the current game period e.g. in a basketball game and the latter describes what is being portrayed in the video clip. The metadata can be any information from text to images to audio and or video data.

The database of time based metadata can be used to quickly recall the video clip and to find specific elements the user is interested in within a video clip. Additionally interactive television applications can rely on time based metadata to provide timely relevant content to users who are watching the video media either on a primary display or a secondary display such as a tablet laptop or smartphone.

Many sources of data exist that can be used to create time based metadata. For example a scorecard for a sporting event where each event in the game is associated with the time remaining in that segment of the game or a data stream including a social media e.g. Twitter or Facebook data stream wherein users post about players or actions and those posts contain temporal information about the players or actions e.g. a server generated time stamp on the post and or descriptive text about the time in the game written by the author of the post . Similarly third party data providers exist that supply suitable data feeds for certain events for example sporting events as a service that can be leveraged to create time based metadata.

Acquisition of metadata which can be referred to as logging can generally be achieved by having users manually associate data with a particular time index in a stream of a video clip. For example a user can watch a video clip on a display device and input tags with time based metadata into a database via a computer keyboard and mouse controls or other type of electronic device. The metadata may be typed as text or the user may depress or click a button containing a tag to perform the metadata association.

In e.g. sports broadcasting the timely association of metadata to video is important to the development of consumer products and the industry as a whole. Video metadata enables the rapid search and retrieval of video clips used during live sports broadcasts scripted commentary shows and on social media. The two primary concerns of content owners are i the quality of the metadata and ii minimizing the total production time of the clips.

For example in order to promote a live game on television a broadcaster may publish clips of the game on social media. The clips must be published with enough time to allow the viewer to watch the game on television. Additionally for discussion during breaks of the game on air radio or television commentators or fans may wish to review clips that may have occurred seconds before and retrieve previous occurrences of similar clips to add value to the discussion.

Currently video metadata is acquired using human input which inhibits the ability of content owners or publishers to process all of the required data and accurately acquire video metadata in a timely fashion.

At least some of the above described problems can be addressed by exemplary embodiments of the methods and systems according to the present disclosure. Exemplary embodiments of systems and methods are provided for automatically creating time based video metadata for a video source and a video playback mechanism. The systems and methods can provide for identification of a region or multiple regions in a frame of a video that contains metadata information extracting and processing those regions to conform to input to a computer vision system using computer vision techniques to transform that image data into machine readable data with an associated video time index and extracting data from an accompanying data stream having similar metadata that can be matched to the data extracted from the video.

In some exemplary embodiments an automated video logging system can be provided comprising a logging client for receiving a digital video stream wherein the logging client analyzes one or more frames of the digital video stream extracts a time from each of the one or more frames analyzed and creates a clock index file associating a time with each of the one or more analyzed frames. The logging client can further provide the clock index file for storage to a database. The digital video stream can be a live stream. The logging client can automatically detect coordinates of a clock on each of the one or more frames analyzed to extract the time from each of the one or more frames using one or more computer vision techniques.

The automated video logging system can further comprise a server for providing the digital video stream to the logging client. The logging client and the server can be connected over a network and the digital video stream can be provided from the server to the logging client through the network. The server can comprise one or more servers.

The logging client can assign a sequential frame number to each of the one or more frames analyzed of the digital video stream. The logging client can parse one or more data files and extract time based metadata from the one or more parsed data files. The logging client can determine a frame of the digital video stream that correlates to the extracted time based metadata. The logging client can query the clock index file to associate a time with the extracted time based metadata to determine the frame of the digital video stream that correlates to the extracted time based metadata. The one or more data files can be received from a real time server. The one or more data files can be stored in a database.

In some exemplary embodiments an automated logging process can be provided comprising receiving a digital video stream analyzing one or more frames of the digital video stream extracting a time from each of the one or more frames analyzed and creating a clock index file associating a time with each of the one or more analyzed frames.

The automated logging process can further comprise parsing one or more received data files and extracting time based metadata from the one or more parsed data files. The one or more data files can be received from a real time server. The automated logging process can further comprise determining a frame of the digital video stream that correlates to the extracted time based metadata. The automated logging process can further comprise querying the clock index file to associate a time with the extracted time based metadata to determine the frame of the digital video stream that correlates to the extracted time based metadata.

The automated logging process can further comprise creating a log entry of the extracted time based metadata with the correlated frame and the time associated with each of the frames and storing the log entry in a database. The automated logging process can further comprise automatically detecting coordinates of a clock on each of the one or more frames analyzed to extract the time from each of the one or more frames using one or more computer vision techniques.

Throughout the figures the same reference numerals and characters unless otherwise stated are used to denote like features elements components or portions of the illustrated embodiments. Moreover while the subject disclosure will now be described in detail with reference to the figures it is done so in connection with the illustrative embodiments. It is intended that changes and modifications can be made to the described embodiments without departing from the true scope and spirit of the subject disclosure.

Exemplary embodiments of the methods and systems of the present disclosure will now be described with reference to the figures. The following description of the various embodiments is merely exemplary in nature and is in no way intended to limit the scope of the disclosure its application or uses.

In some exemplary embodiments the logging client can be implemented as a single server system and in some embodiments it can be implemented as a distributed system of multiple servers. In some embodiments the data server can be implemented as a single server system and in some embodiments it can be implemented as a distributed system of multiple servers. In some embodiments the video server can be implemented as a single server system and in some embodiments it can be implemented as a distributed system of multiple servers.

In some embodiments the data server can crawl the Internet and perform live searches or otherwise discover time based information relative to the video stream being processed by the logging client . For live or recorded events the time based information can include a user s public social media e.g. Twitter Facebook or similar public posts on the Internet or any other time stamped information publicly available on the Internet. In some embodiments the data server can be provided by a third party commercial data service or similar distributed information system or service. In some embodiments the data server may be part of a manual logging system where users have entered data using computer keyboard and mouse equipment or any other type of computer equipment.

In some embodiments the data server can push data through the network to the logging client . In some embodiments the data server can provide an application programming interface API to the logging client so that the logging client may query the data server for relevant data. In operation the data server can provide data files to the logging client through the network . In some embodiments the data server can provide data files to the logging client on a shared disk or other storage medium connected to the systems. Data files can be a set of data that the system will match to internal metadata extracted from the video clip using optical character recognition techniques or other computer vision techniques such as but not limited to face detection face recognition motion analysis on the video clip. Data files can contain external metadata that can be associated with a real time value portrayed in the video clip for example 10 seconds to go in the first quarter of a sporting event such as a basketball game.

In some embodiments the video server can stream data live through network to the logging client using standard video streaming protocols for example RTMP Real Time Messaging Protocol or HLS HTTP Live Streaming In some embodiments the video server can provide a complete video file that has previously been recorded or digitized through the network to the logging client . In operation the digital video stream can be provided to the logging client through the network . In some embodiments the digital video stream can be stored as a video file on a shared disk or other storage medium connected to the systems.

The logging client can be a computer system comprised of a processor memory and file system that runs the executable software code described herein. A database can be provided having a set of files and a structure in which data can be stored retrieved and otherwise managed. In some embodiments a third party vendor can provide the database and in some embodiments it can have solely text files on a file system. In operation a combination of databases can be used in concert where databases of single and distributed systems can work together.

In some exemplary embodiments at a start when a video frame is received from a stream or loaded from a file at the logging client can run a video process that can analyze the digital video stream frame by frame at . When the video process first starts analyzing the digital video stream it assigns a frame number offset of zero 0 to that frame. In operation the video process may begin for a digital video stream or file where processing has already occurred for previous frames and some clock index data may already exist. In some embodiments the video process may assign a frame number offset based on the last frame number offset present in the clock index file . At the video process digitally extracts into computer memory and processes the region identified as containing for example an on screen clock e.g. as illustrated in providing an exemplary illustration of a video frame illustrating on screen text that can be turned into time based metadata and performs computer image processing to prepare it for computer vision processing. Once transformed computer vision processing e.g. optical character recognition can be performed on the extracted image region which can extract machine readable information from the extracted image region. At the video process can write the resulting recognized time string to the clock index file along with the corresponding frame number offset from the start of the video clip. The frame number offset can be incremented by one after each frame is processed at so that the correct frame number offset is associated with each video frame subsequently received by the algorithm at .

In some exemplary embodiments of the present disclosure video time code information can be present in internal metadata found in the digital video stream . The video time code information can be extracted at during the processing of the frame received at by the video process . At the video process digitally extracts into computer memory and processes the region identified as containing for example an on screen clock e.g. as illustrated in providing an exemplary illustration of a video frame illustrating on screen text that can be turned into time based metadata and performs computer image processing to prepare it for computer vision processing. Once transformed computer vision processing e.g. optical character recognition can be performed on the extracted image region which can extract machine readable information from the extracted image region. At the video process can write the resulting recognized time string to the clock index file along with the corresponding video time code information extracted from the digital video stream .

In some exemplary embodiments of the present disclosure at a start when a video frame is received at the logging client can automatically discover the region on the screen that contains the on screen clock e.g. as illustrated in providing an exemplary illustration of a video frame illustrating on screen text that can be turned into time based metadata using e.g. computer vision optical character recognition and computer logic to find the region that contains the graphical representation of a clock value. Once this region is identified automatically machine readable information can be extracted from the image region. At the video process can write the resulting recognized time string to the clock index file along with the corresponding frame number offset from the start of the video clip or video time code information extracted from the digital video stream .

An exemplary output of the video process is illustrated in which provides an exemplary illustration of a table showing an output of a video process and an auto logging process as a combined table with time based metadata . Various different outputs of the video process and methods of data representation extracted from a computer database are contemplated by the present disclosure and are not limited to the exemplary embodiments illustrated in . The video process can populate the data columns illustrated in with the computer vision recognized clock times and the frame number offset from the start of the video clip. When the end of the video stream is reached at then the video process can end at . In addition to clock times in some embodiments of the present disclosure the video process can perform computer vision processing on additional regions of the frame at e.g. a score of an away team e.g. a score of a home team and e.g. the quarter or period in the game in order to extract additional metadata in order to better determine the time portrayed in the media. In some embodiments of the present disclosure certain time code data may be present in the video stream and such time code data can be extracted during the video process and associated with the time based metadata along with the frame number e.g. the video time code in the form of hours minutes seconds and frames or any other used format .

In some embodiments the video process and the auto logging process can be implemented as a same software process running on a CPU while in other embodiments the video process and the auto logging process can be separate software processes running on either a single system or multiple distributed systems.

In some embodiments time based metadata can interface with a system such as a media asset management system where video clips and time based metadata can be stored together. The media asset management system can present a user interface to operators of the system to view the automatically generated time aligned metadata and the corresponding video clip. In some embodiments the auto logging process can run continuously as the event unfolds and when the time based metadata becomes available the media asset management system can present the auto logged clip to a user. In some embodiments where the event being recorded has occurred and the auto logging process has been completed all auto logged clips and time based metadata can be available for review in the media asset management system. The media asset management system can provide an interface through which a user can search the automatically generated time aligned metadata and retrieve specific clips based on the metadata attached to each entry present clock index also referenced in data files .

The data files may contain inaccurate information due to data acquisition user error or other problems. Additionally certain events may occur in the video that are of interest to the user but may not be logged automatically by the auto logging process . In some embodiments a correction workflow can be provided to allow the user to review edit and correct errors present in the data files errors introduced by the auto logging process and or omissions in the data files . The correction workflow can be embodied by a user interface that can present video clips to the user as they are automatically logged requesting visual confirmation that the content of the video clips match the automatically logged time aligned metadata. If the user confirms that the data is correct the time based metadata can be updated to reflect the confirmed data. In some embodiments the user can adjust the frame number or time code of the clip and or the metadata itself and submit the corrected time aligned metadata to the database .

Various advantages can be provided for in the exemplary embodiments of the present disclosure. For example instead of manually inputting metadata using computer keyboard and mouse equipment while watching a video clip the present disclosure describes exemplary embodiments of methods and systems that can provide a video logging system that can automatically create time based metadata by e.g. processing the video clip with a computer vision system. The exemplary embodiments of the present disclosure can further provide a system and method for applying algorithms to convert data acquired by third parties into video metadata using computer vision techniques. The converted metadata can consist of a higher density of information than can be acquired by a human in real time and can be acquired from high quality sources to ensure accuracy. This system and method can enable timely and accurate association of video metadata to live or recorded video thereby facilitating new and higher quality products and services for consumers.

The automated video logging system can comprise a server that can receive a digitized video clip or digitized video stream over a computer network and a corresponding data file or stream containing real time values and data representing times and events portrayed in the video clip. The automated video logging system can also contain a computer software process that can analyze the video extract time data from a clock displayed on the video frame and or analyzes the image using e.g. optical character recognition OCR .

In some exemplary embodiments of the present disclosure the automated video logging system can receive a data file from a third party provider e.g. the official data collector for a sports league containing real time values portrayed in the video clip e.g. a play by play report for a sporting event with game period and time remaining data and automatically assign a frame number or a video time code from the video to each event specified in the data file.

In some exemplary embodiments of the present disclosure the automated video logging system can scan social media sites e.g. Twitter or Facebook sports web sites or other similar public Internet data stream for relevant events extract time based metadata from those events using common data extraction and processing methods natural language processing methods or similar computer recognition techniques and automatically assign a frame number or video time code from the video to each event specified in the data file.

In some exemplary embodiments of the present disclosure the automated video logging system can receive a data file from a manual logging system such as the Hapn.in system developed by Stainless Code where users can enter data using computer keyboard and mouse equipment and the automated video logging system can automatically assign a frame number or a video time code from the video to each event specified in the data file.

Various other considerations can also be addressed in the exemplary applications described according to the exemplary embodiments of the present disclosure. The exemplary embodiments of the present disclosure can be used in various configurations and in different systems. Various computing arrangements can be provided having a processor s configured or programmed to perform the exemplary steps and or procedures of the exemplary embodiments of the present disclosure described above. Various data described above can be stored in various storage arrangements e.g. hard drive memory device such as RAM ROM memory stick floppy drive and or other tangible computer accessible medium . The processor s can access the storage arrangement s to execute a computer program or a set of instructions stored on or in the storage arrangement which can perform the procedures according to the exemplary embodiments of the methods and systems of the present disclosure.

The foregoing merely illustrates the principles of the disclosure. Various modifications and alterations to the described embodiments will be apparent to those skilled in the art in view of the teachings herein. It will thus be appreciated that those skilled in the art will be able to devise numerous systems arrangements manufacture and methods which although not explicitly shown or described herein embody the principles of the disclosure and are thus within the spirit and scope of the disclosure. The disclosures of all systems documents and publications cited herein are hereby incorporated herein by reference in their entireties.

