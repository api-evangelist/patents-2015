---

title: Reducing network congestion by preferentially dropping packets sent by high bandwidth sources
abstract: Some embodiments provide a method for reducing congestion in a network stack that includes a series of components that send data packets through the network stack to a network. At a first component of the network stack, the method receives a data packet from a second component of the network stack. The method identifies a usage indicator value for a flow to which the data packet belongs. The usage indicator value is based on a comparison of a size of the flow to a size of a queue for a third component of the network stack. The method determines whether to send the data packet based on a comparison of the usage indicator value to a threshold usage value. The method sends the data packet to a next component of the network stack only when the usage indicator value is less than the threshold usage value.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09544238&OS=09544238&RS=09544238
owner: NICIRA, INC.
number: 09544238
owner_city: Palo Alto
owner_country: US
publication_date: 20150311
---
Some current data centers and private enterprise networks run server virtualization software on compute nodes. These compute nodes generate large amounts of network traffic that includes traffic originating from the virtual machines as well as infrastructure traffic. Infrastructure traffic is traffic that originates from the host machine layer rather than a particular virtual machine implemented on the host machine.

Currently some networks send traffic as individual packets of data. A data item larger than an individual packet is broken down into multiple packets each packet is then sent over a network to a destination system e.g. a computer or virtual machine . When the packets reach their destination the data in the packets is reassembled to recreate the original data item. In current systems a packet is not guaranteed to reach its destination. Therefore for each packet successfully received the destination system sends an acknowledgement message back to the source address of the packet. The acknowledgement message alerts the original sender that that packet has been received. When a source system sends a packet that is lost in transmission e.g. the packet is sent to a malfunctioning or busy intermediate system the destination system does not send an acknowledgement message for that packet. The sending system is set up under the assumption that an unacknowledged packet was lost in transmission. Accordingly when a threshold amount of time passes after a packet is sent without the sending system receiving an acknowledgement message the sending system re sends the packet. In some network systems the threshold time is based on the round trip time between the sending and receiving systems. That is in some cases the allowable threshold is the time for the packet to travel from the source system to the destination system plus the time for the acknowledgement message to be generated and travel back to the source system plus some buffer time to account for reasonable delays.

When a source system and destination system are geographically distant the round trip time could be hundreds or thousands of milliseconds. The round trip time is great enough that it would be very inefficient to send one packet and then wait for acknowledgement of that packet before sending the next packet. Accordingly many packets are sent while waiting for the acknowledgement message for the first packet to arrive. The sending of many packets while waiting for an acknowledgement message to arrive causes problems when part of the transmission path between the systems is congested. Various networking links between systems have a limited memory capacity and serve as part of the path for multiple source and destination systems. When the memory capacity of an intermediary system is full or too close to full the intermediate system will start to drop packets or refuse new packets in some cases causing other intermediate systems to drop packets. In some cases an intermediary system refusing packets causes a great enough delay that a source system re sends the packets. The re sent packets can further increase congestion making the original problem worse.

In some networking systems when a threshold number of acknowledgement messages are missed within a particular amount of time the source system determines that there is congestion on the path the packets are taking The source system then slows down the rate of packet transmission in order to allow the congestion to clear. However when the round trip time for packet and acknowledgement message is long many packets can be sent out before the source system recognizes that congestion is an issue. This causes inefficient retransmission of packets that will be stopped by congestion and or make the congestion worse. The inefficiency is particularly great when the point of congestion is within the same host machine as the source system e.g. the congestion is at one or more layers of a set of network transmission layers on a kernel of the host machine and the destination machine is far away. That is the traffic congestion is at the beginning of the transmission path but the round trip time is long and therefore the threshold for determining that packets are being lost is correspondingly long. In such cases it takes a long time to identify that there is congestion and many packets are sent at a high rate during that time.

Some embodiments provide a congestion notification system for a computer virtualization networking stack. The computer virtualization networking stack of some embodiments is a series of software and or hardware processes and or components that send data in packets through the series of processes ultimately to a network. Although the term component is used herein one of ordinary skill in the art will understand that in some embodiments processes are used instead of components. In some embodiments some or all of the components include queues for storing data packets until the data packet can be sent to the next component in the series. These queues have finite capacity for data packets. When a queue of a component is full any additional data packets arriving at that component will be dropped. The packet sender will then have to resend the packets in order for the data in them to be received by the destination computer machine or virtual machine.

The congestion notification system of some embodiments sends messages from lower layer e.g. closer to the network in the series of components components to higher layer e.g. closer to the packet sender components. When the higher layer components receive the congestion notification messages the higher layer components reduce the sending rate of packets in some cases the rate is reduced to zero to allow the lower layer components to lower congestion i.e. create more space in their queues by sending more data packets through the series of components . In some embodiments the higher layer components resume full speed sending of packets after a threshold time elapses without further notification of congestion. In other embodiments the higher layer components resume full speed sending of packets after receiving a message indicating reduced congestion in the lower layer components.

In some embodiments a congestion notification message is sent from a lower layer component e.g. a physical network interface card PNIC to a middle layer component e.g. a virtual network interface card VNIC . When the middle layer component receives the congestion message the middle layer component begins dropping packets received from a higher layer component e.g. a TCP IP stack module . In some embodiments the higher layer component provides usage indicators to the middle layer component with each packet. The usage indicators of some embodiments identify what portion of a queue of the lower layer component is occupied by unacknowledged data from the same data flow as the accompanying packet. The middle layer component of some embodiments uses the usage indicator for each packet to determine whether to send the packet to the next component or drop the packet. In some embodiments the middle layer component compares the indicated usage level to a threshold level sends packets with indicated usage below that threshold and drops packets with indicated usage above that threshold. In some embodiments the threshold is a pre set value e.g. 0.3 40 etc. . In other embodiments the threshold value is randomly generated for each packet. In such embodiments some packets with a higher indicated usage than some blocked packets will be sent to the next component and some packets with a lower indicated usage than some sent packets will be dropped. However in such embodiments packets with higher indicated usage will be more likely to be dropped.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

Some embodiments of the invention provide early notification of congestion to a packet sender in order to allow the packet sender to reduce the rate at which it sends packets. By reducing the rate of sending packets earlier than would be possible in prior art systems the present invention reduces wasted time and further congestion delays caused by additional packets and re sent packets from the packet sender.

The congestion notification system of some embodiments sends congestion notification messages from lower layer components of a computer virtualization network stack to higher layer components of the stack. In response to the congestion messages the higher layer components reduce the rate of sending packets to allow the congestion to clear. The congestion notification system of some embodiments is also described in U.S. patent application Ser. No. 14 320 416 filed Jun. 30 2014 now published as U.S. Patent Publication 2015 0381505 which is incorporated herein by reference.

The packet sender is a source of data to be sent over the network . The network stack layers represent different processes on a host machine that each receive packets from the packet sender which may be one of many packet senders that generates and sends packets into the network stack or a previous network stack layer process the packets and send the packets to the next network stack layer or the PNIC . As indicated by the ellipsis between layers and in some embodiments additional network stack layers are provided. In some embodiments the network stack layer is provided by the PNIC and provides a queue for data segments e.g. segments of 1500 bytes generated by splitting up larger data packets e.g. data packets of 65 kB . In the illustrated embodiment the PNIC splits the packets into segments. However in other embodiments a module of a previous network stack layer e.g. a virtual network interface card VNIC splits the packets into segments. In the illustrated embodiment of only network stack layer includes a queue to store segments before sending the segments to the network through the PNIC . However in other embodiments other network stack layers also include queues to store packets and or segments. If a new packet comes in to a particular network stack layer while the queue of that network stack layer is full the network stack layer must drop the packet segments or drop another packet to make room for the new packet segments.

The congestion notification system is implemented by some embodiments in order to reduce the number of additional packets sent by the higher layer components components of layers farther from the network to the lower layer components components of layers closer to the network while the queues of the lower layer components are full or nearly full . In stage the queues of network stack layer is nearly full as indicated by congestion indicator . The congestion indicator occupies a portion of network stack layer proportional to the percentage of the queue that is occupied by segments waiting to be sent out. Additionally data packet arrives at network stack layer further increasing the fullness of the queue.

Even though the queue is not entirely full if the network stack layer receives too many more packets before the congestion of the queues can be reduced e.g. by sending segments to the network through PNIC the queue will fill completely. When the queue fills completely network stack layers will start dropping packets. Additionally in some cases a large queue in layer will cause unacceptable delays for applications that rely on small amounts of data to be sent with minimal delays such as voice over Internet protocol VOIP applications even when no segments are actually dropped.

To alert the process that manages network stack layer e.g. a module that implements a VNIC to the status of the queue of layer as fuller than a threshold percentage in stage the process that manages network stack layers or an intermediate layer between layers and in some embodiments sends a CN message to the process that manages network stack e.g. a module that implements a TCP IP stack . The CN message indicates a need to reduce the rate of sending packets.

Also in stage the packet sender sends out data in two flows i.e. two different sets of IP source IP destination source port destination port and protocol values . In stage the data in the flows are shown as data packets and respectively. However in some embodiments the data in the flows are sent as streams of data from one or more packet senders and is not apportioned into packets until the data reaches a particular network stack layer such as layer e.g. a TCP IP module .

In stage the network stack layer in some embodiments a TCP IP module determines what portion of the queue of PNIC is represented by the flow of each packet and . The network stack layer sends the data of packets and on as packets and respectively. The network stack sends out packets and accompanied by usage indicators and respectively. The usage indicators and indicate how high a portion of the PNIC queue is represented by unacknowledged packets previously sent in their respective flows. In some embodiments unacknowledged packets as described in greater detail below are those packets which have been sent but for which an acknowledgment message has not been received and which have not been identified by the sender as lost. In stage the PNIC sends segments out over the network which reduces the queue in layer below what it would have been with the addition of the data of packet . The PNIC similarly sends out segments in stage and in stage .

In response to the CN message sent in stage the network stack layer in stage reduces the rate of sending packets in some embodiments the rate is reduced to zero in some circumstances . The layer of some embodiments preferentially drops packets with a high PNIC queue usage e.g. packets from flows that already occupy a large portion of the PNIC queue in layer . In the illustrated case the unacknowledged data of the flow of packet represents a high percentage of the PNIC queue size as conceptually indicated by the high percentage of the packet s corresponding usage indicator that is dark . In contrast the unacknowledged data of the flow of packet represents a low percentage of the PNIC queue size as conceptually indicated by the low percentage of the packet s corresponding usage indicator that is dark . Accordingly in stage the network stack layer e.g. a VNIC drops packet while sending packet on toward the PNIC . The layer also sends an error message to network stack layer e.g. a TCP IP module to indicate that the packet was dropped.

The congestion notification system of some embodiments is implemented to solve problems caused by sending data over long distances on a conventional IP network. When sending data over a long distance there is a long delay between a point on the network becoming congested and a sender of data packets determining that there is congestion on the network. During this long delay the data packet sender sends out many more packets worsening the congestion. In systems without congestion notification the long delay occurs even when the congestion in the network is within the same host machine as the data packet sender.

The hosts and implement virtual machines not shown . The packet sources produce data to be sent from one host to another. The data is transferred as multiple packets sent over a network. When the host machines are distant from each other the round trip time for a packet and an acknowledgement message can be hundreds of milliseconds. Because of the long round trip time the expected time between sending a packet and receiving an acknowledgement message is correspondingly long. Therefore the amount of time it takes to determine from the absence of acknowledgement messages that the connection is congested is also correspondingly long. In that time many additional packets can be sent out causing further congestion and being blocked. Furthermore in some network systems the higher the delay between the sender and the receiver the higher the number of packets that the sender has to send without receiving acknowledgements in order for the sender and or other network components to determine that the connection is congested.

Some embodiments provide a congestion notification system that works within a particular host to allow network stack layers to identify and respond to local congestion quickly resulting in less packet loss and less overall congestion. For example in some embodiments if there is congestion on a host between a virtual switch and a PNIC the congestion notification system alerts the packet source a module implementing the TCP IP stack or some other packet generating or transmitting element about the congestion. The element that receives the notification then reduces the transmission rate of the packets e.g. by storing packets in its own buffer dropping packets etc. in the case of packet transmitters or not generating packets in the case of packet generators . The reduced transmission rate of packets allows the lower layer component to deal with the congestion before receiving too many additional packets.

This specification often refers to packets and packet headers as with the packets . The term packet is used here as well as throughout this application to refer to a collection of bits in a particular format sent across a network. It should be understood that the term packet may be used herein to refer to various formatted collections of bits that may be sent across a network such as Ethernet frames TCP segments UDP datagrams IP packets etc.

The congestion notification system of some embodiments is implemented on host machines of a data center or private enterprise networking system. illustrates a host computer with multiple network stack layers. The figure shows a system in which the congestion notification system of some embodiments is implemented. The figure includes a host machine that implements a user space and a kernel . In the user space the host implements virtual machines with virtual network interface cards VNICs . In the kernel the host implements multiple packet sources e.g. various network control processes TCP IP stack with VNICs virtual switch and uplink manager . The host machine includes a physical network interface card PNIC . The kernel is conceptually divided into multiple layers representing different processes or sets of processes that implement network stack layers. The virtual machines of some embodiments also implement packet sources e.g. applications that send and receive data from machines accessible through a network .

Host machine could be a host machine on a multi tenant datacenter or a host machine on a single tenant enterprise network. The user space and kernel are divisions of the computing capabilities of the host machine and may be implemented using different sets of application programming interfaces APIs . Accordingly processes running in the user space may have different restrictions on them and or have access to different resources than processes running in the kernel . The virtual machines simulate separate computers. The virtual machines can be virtual machines controlled by a single entity e.g. a single tenant or can be controlled by multiple entities e.g. multiple tenants . The virtual network interface cards VNICs are software constructs that the virtual machines use to connect to a virtual switch in the kernel of the host .

TCP IP stack is a software construct that manipulates data received from various network processes converting the data into IP packets that can be sent through the VNICs to virtual switch through the virtual and then out to a network e.g. a public datacenter an enterprise network the Internet etc. . Virtual switch is a software construct that receives IP packets from within the host and routes them toward their destinations inside or outside the host . The virtual switch also receives packets from outside the host and routes them to their destinations in the host . The uplink manager processes packets going from the virtual switch to the PNIC . The uplink manager stores the packets in an uplink queue until the PNIC is available to send the packets out. The PNIC is a hardware element that receives packets from within the host that have destinations outside the host and forwards those packets toward their destinations over a network. The PNIC also receives packets from outside the host e.g. from a local network or an external network such as the Internet and forwards those packets to the virtual switch for distribution within the host .

The TCP IP stack is a stack of protocols that together translate data from the various processes into IP packets that can be sent out on an IP network e.g. the Internet . The TCP IP stack does not send the packets directly to their destinations. Instead the TCP IP stack sends the IP packets through the VNICs . The VNICs store the packets in a virtual adapter dispatch queue until the virtual switch is ready to send the packets further. The virtual switch is a next hop in the direction of the ultimate destination of the IP packets. The virtual switch examines each IP packet individually to determine whether the destination of the packet is to a process running on the host or to a process or machine outside of the host . When an IP packet is addressed to a destination on the host the virtual switch sends the IP packet to the destination process on the host . When an IP packet is addressed to a destination not on the host the virtual switch forwards the IP packet to the uplink manager to be queued for PNIC . The PNIC sends the IP packet to a network not shown for further forwarding to its destination.

The kernel is shown as conceptually divided into multiple layers . The layers are not physical layers on the host but are shown to represent the order in which data is sent from network process packet source to the PNIC . Layer is a TCP IP layer including the TCP IP stack that converts data from network process packet source into TCP IP packets. In some embodiments the TCP IP stack includes a TCP buffer for storing packets. Layer is a virtual interface layer including the VNICs which provide a virtual adapter dispatch queue. Switch port layer includes virtual switch . In some embodiments virtual switch sends packets from the VNICs to the uplink manager but does not include a packet queue. In other embodiments the virtual switch does include a packet queue. Uplink layer includes uplink manager . Uplink manager in some embodiments includes an uplink queue. The PNIC layer includes PNIC . In some embodiments the PNIC does not include a packet queue. In other embodiments the PNIC does include a packet queue.

In some embodiments any layer that includes a queue can be a potential choke point where packets can be dropped when the queue is full. Accordingly in some embodiments processes included in any or all layers with a queue provide congestion notification messages. Although the layers are shown with a single component each in the network stack in some embodiments a particular layer could include multiple components. For example in some embodiments the switch packet layer between the VNIC and the virtual switch could include multiple components such as firewalls virus scanners queue shaping applications and or any other applications associated with the networking software or by third parties etc. Any or all of these components could include queues that can become congested. Such queues create multiple chokepoints between the VNIC and the virtual switch. Therefore these components in some embodiments send congestion notification messages. Furthermore in some embodiments any or all of the additional components mentioned above are implemented between any other two identified layers e.g. between the virtual switch and the uplink manager instead of or in addition to being implemented between the VNIC and the virtual switch.

The congestion notification system of some embodiments is implemented to allow congestion notification between multiple components in a series of components in a computer virtualization networking stack. In some embodiments some or all of the components of the computer virtualization networking stack are controlled by separate entities e.g. separate computer processes components and or hardware on a host . illustrates a set of packets passing through a computer virtualization networking stack of some embodiments. In addition to various packet sending and transmitting elements from includes network and packets in stages . The figure shows the ideal progression of packets under ideal conditions with no congestion from a data sending process packet source to a network . In the bandwidth of each component and and network is represented by the height of the component. Under ideal conditions each component has the same bandwidth so none of the components becomes a bottleneck when packets are being sent. Because illustrates a computer virtualization networking stack under ideal conditions each component has the same bandwidth in this figure. However in subsequent figures with less ideal conditions the bandwidths of the components and the illustrated heights of the components in those figures vary.

The packets start from the process packet source in stage . In some embodiments the process generates the packets in other embodiments the process provides raw data to another process e.g. the module implementing a TCP IP stack which divides the raw data into packets. The packets or raw data are then transmitted to TCP IP stack in stage . The TCP IP stack translates the data into TCP IP packets. The packets are then transmitted to VNIC in stage . The packets are then transmitted to virtual switch in stage . The packets are then transmitted to uplink manager in stage . The packets are transmitted to PNIC in stage and then sent out onto network in stage .

In stage packets are being sent from packet source to TCP IP stack . Packets are being sent from VNIC to virtual switch . Packets are being sent from virtual switch to uplink manager . The uplink manager already has packets in its uplink queue as indicated by congestion indicator but in this stage PNIC is busy so no packets can be sent from the uplink queue to the PNIC by the uplink manager . In stage the uplink queue of the uplink manager is almost full as indicated by congestion indicator . In this figure congestion indicators occupy a portion of their corresponding component in a given stage that is proportional to how full the queue is during that stage.

In stage packets arrive at uplink manager . The packets finish filling up the uplink queue of the uplink manager as indicated by congestion indicator which covers the entire uplink manager . Meanwhile packets arrive at TCP IP stack ready to be sent to VNIC Packets arrive at the virtual switch ready to be sent to uplink manager with its full uplink queue .

In stage PNIC is no longer busy so uplink manager sends an individual packet from the uplink queue to PNIC . In the illustrated example the uplink manager has only enough bandwidth to send one packet per stage to the PNIC . In the same stage the packets arrive at the uplink manager . Since only one packet has left the uplink queue of the uplink manager there is only enough room in the uplink queue for one of the 4 packets . Accordingly one of the packets is stored in the uplink queue leaving the uplink queue full again and the uplink manager drops the other three packets represented as dropped packets . Additionally packets are transferred to VNIC ready for transfer to virtual switch .

In stage PNIC is also not busy so uplink manager sends an individual packet from the uplink queue to PNIC . Similarly PNIC sends individual packet out to the network. As mentioned above in the illustrated example the uplink manager has only enough bandwidth to send one packet per stage to the PNIC . In the same stage no packets arrive at the uplink manager leaving the uplink queue with one packet s worth of space as indicated by congestion indicator which covers most of uplink manager . Additionally packets are transferred from VNIC to virtual switch . In the next stage not shown the packets will reach the uplink manager overflowing the uplink queue again and forcing the uplink manager to again drop packets. With no congestion notification system in place packets will be dropped from two groups of packets.

In stage packets are being sent from process packet source to TCP IP stack . Packets are about to be sent from VNIC to virtual switch . Packets are being sent from virtual switch to uplink manager . The uplink manager already has packets in its uplink queue as indicated by congestion indicator but in this stage PNIC is busy so no packets can be sent from the uplink queue to the PNIC by the uplink manager . In stage the uplink queue of the uplink manager is almost full as indicated by congestion indicator . In this figure congestion indicators occupy a portion of their corresponding component in a given stage that is proportional to how full the queue is during that stage.

In contrast to the computer virtualization networking stack of the computer virtualization networking stack of implements a congestion notification system. As the uplink queue of uplink manager is more than a threshold amount full the uplink manager sends a congestion notification message to the VNIC . This message alerts the VNIC to the congested status of the uplink queue. Although the uplink queue is not completely full when the congestion notification message is sent the message is sent in stage because packets that are already past VNIC e.g. packets could use up the rest of the space in the uplink queue of uplink manager .

In stage the packets arrive at uplink manager . The packets finish filling up the uplink queue of the uplink manager as indicated by congestion indicator which covers the entire uplink manager . Meanwhile packets arrive at TCP IP stack ready to be sent to VNIC . The VNIC in response to the congestion notification message has stored packets rather than sending the packets to virtual switch as happened to packets in . The storage of these packets in the VNIC queue of VNIC is shown by congestion indicator . The results of storing these packets rather than sending them to the virtual switch are shown in stage . While the embodiment of stores all the packets of packets in some embodiments some fraction of the packets e.g. based on the bandwidth of the uplink manager are sent while the rest are queued. For example in some embodiments the VNIC would send as many packets per stage as the uplink manager can send to the PNIC here one packet per stage when the PNIC is not busy.

In stage PNIC is no longer busy so uplink manager sends an individual packet from the uplink queue to PNIC . In the illustrated example the uplink manager has only enough bandwidth to send one packet per stage to the PNIC . In the same stage the packets having been stored in the VNIC queue of VNIC in stage do not arrive at the uplink manager . As the packets have not reached the uplink manager the packets do not overflow the uplink queue of the uplink manager as was the case with packets in stage of . Accordingly no packets are dropped. Furthermore the uplink queue is left with space for a packet as indicated by congestion indicator which covers most but not all of uplink manager .

Additionally packets are transferred to VNIC ready for transfer to virtual switch . However because of the previously received congestion notification message the VNIC will store the packets in the VNIC queue rather than sending them to virtual switch in stage . In stage PNIC is also not busy so uplink manager sends an individual packet from the uplink queue to PNIC . Similarly PNIC sends individual packet out to the network. As mentioned above in the illustrated example the uplink manager has only enough bandwidth to send one packet per stage to the PNIC . In the same stage no packets arrive at the uplink manager leaving the uplink queue with two packet s worth of space as indicated by congestion indicator which covers most of uplink manager . As mentioned above packets are not transferred from VNIC to virtual switch . Therefore in the next stage not shown no packets will reach the uplink manager. The uplink queue will not overflow and the uplink manager will not have to drop any packets. With the congestion notification system in place no packets are dropped from packets and .

The congestion notification system of includes a single component the uplink manager that notifies a higher component about congestion and a single component the VNIC that receives notifications about congestion. However other embodiments provide multiple components that notify higher components about congestion and or multiple components that receive notification about congestion. illustrates a congestion notification system of some embodiments with multiple components that notify higher components about congestion. The figure includes kernel packet source other network management application TCP IP stack with VNIC virtual switch uplink manager PNIC chokepoints and congestion notification system message bus congestion notification messages and threshold reached notification messages .

The packet source and other network management application are network management processes running on kernel . These and other network management processes not shown produce data that is sent as packets through various computer virtualization networking stack processes such as the TCP IP stack VNIC virtual switch and uplink manager . The data packets are then sent on to the PNIC for transmission to a network not shown . The computer virtualization networking stack processes of some embodiments include multiple chokepoints. In the embodiment of there are three chokepoints and . Each chokepoint represents a packet queue administered by a component of the computer virtualization networking stack processes. The packet queues are used to store packets until the packets can be sent to the next component in the stack. For example chokepoint conceptually illustrates an uplink queue implemented by uplink manager . In some embodiments each component in the computer virtualization networking stack has a queue. In other embodiments some components have a queue and other components do not.

In the illustrated embodiment the VNIC is the sole component that receives notification of congestion further down the computer virtualization networking stack. In some embodiments the component responsible for a queue notifies the VNIC when congestion reaches a threshold level. That is when the queue is a threshold percentage full. In some embodiments each queue has an independent threshold setting. For example in some embodiments a first threshold level could be set for the queue represented by chokepoint e.g. a threshold at 90 a second threshold could be set for the queue represented by chokepoint e.g. a threshold at 70 etc. In some embodiments there is a default threshold setting for each chokepoint. In some such embodiments the default thresholds for the chokepoints can be overridden by configuration of the component managing the queue.

When the queue of a particular component reaches the threshold fullness the component sends a congestion notification message to the higher layer component here VNIC through a congestion notification system message bus . The congestion notification message of some embodiments includes more data than the fact that the threshold has been reached. For example the congestion notification message of some embodiments includes the actual percentage of congestion e.g. if the threshold level is 70 but the queue is actually 85 full the congestion notification message will indicate that the congestion level is 85 .

The congestion notification message of some embodiments includes the source of the packets e.g. the source IP address and or MAC address of the packet that are congesting the queue. In some embodiments the identified source is the source of the last packet to enter the queue. In some embodiments the identified source is source of the last packet received before the congestion message is sent e.g. the top packet in the queue or the last dropped packet . In other embodiments the identified source is based on a most common source of a sampling of packets e.g. a fraction or all of the packets in the queue. In some embodiments by the congestion notification system identifying the source of the congestion the higher component can selectively reduce the rate of sending packets only of those slow down packets from the source of the congestion without reducing the rates of sending packets from other sources. In some embodiments the congestion notification messages include other data relating to one or more packets in the queue e.g. the destination size a hash of the contents etc. . In some embodiments when the packets are TCP packets the congestion notification messages include TCP flow information e.g. source address destination address source port and destination port protocol in use a hash of the data in the message etc. .

The congestion notification message is then sent to the higher component e.g. VNIC as a threshold reached notification message . In some embodiments the threshold reached notification message is a copy of the congestion notification message . In other embodiments the threshold reached notification message is a summary or an aggregate of congestion notification messages. In some embodiments the threshold reached notifications for a particular chokepoint are sent only to components that subscribe to the message bus to receive congestion notifications about that particular chokepoint. In some embodiments a component subscribes by sending a subscription message for a chokepoint to a congestion notification system message bus. In other embodiments a component is subscribed by its configuration e.g. the subscription or non subscription for each chokepoint is set in a configuration setting of the subscribing component .

This figure also illustrates that in some embodiments that provide congestion notification options not all chokepoints have to be monitored. Furthermore different chokepoints can be monitored to send congestion notifications to different higher components. Here packet source is subscribed to receive congestion notifications about chokepoint e.g. from the uplink manager through the message bus . Because the packet source is not subscribed to receive notification about any other chokepoints the only threshold reached notification messages that the packet source will receive identify congestion at the queue of the uplink manager i.e. chokepoint . The VNIC is subscribed to receive congestion notifications about chokepoint . Because the VNIC is not subscribed to receive notification about any other chokepoint the only threshold reached notification messages that the VNIC will receive identify congestion at a queue of the virtual switch .

In addition to the chokepoints monitored for congestion in this figure the PNIC sends congestion notification messages when its queue represented by chokepoint is above its threshold value. However the congestion notification messages are not sent from the congestion notification system message bus to any higher layer components because no higher layer components are subscribed to monitor congestion of the chokepoint . In some such embodiments the congestion notification system message bus then discards the congestion messages from that chokepoint. In alternate embodiments the component managing the queue of the chokepoint does not send congestion messages to a congestion notification system message bus when no higher layer component is monitoring the congestion at that chokepoint. In some embodiments the congestion notification message includes an indicator of which higher component should receive the congestion notification message.

In some embodiments each queue has multiple independent threshold settings e.g. one for each higher component receiving congestion notifications . In the embodiment of the packet source and VNIC subscribe to receive configuration notification messages about separate queues. However in some embodiments each queue can be independently monitored by each higher component. For example the packet source could subscribe to be notified when the uplink queue represented by chokepoint is above its threshold while the VNIC could also subscribe to be notified when the uplink queue represented by chokepoint is above its threshold.

As described with respect to when the queue of a particular component reaches the threshold fullness the component sends a congestion notification message to the higher layer component here VNIC and packet source through a congestion notification system message bus . The congestion notification message of some embodiments includes more data than the fact that the threshold has been reached. For example the congestion notification message of some embodiments includes the actual percentage of congestion e.g. if the threshold level is 70 but the queue is actually 85 full the congestion notification message will indicate that the congestion level is 85 . The congestion notification message of some embodiments includes the source of the packets e.g. the source IP address and or MAC address of the packet that are congesting the queue. In some embodiments the identified source is the last packet to enter the queue. In other embodiments the identified source is based on a most common source of a sampling of packets e.g. a fraction or all of the packets in the queue. In some embodiments by the congestion notification system identifying the source of the congestion the higher component can selectively reduce the rate of sending packets only of those slow down packets from the source of the congestion without reducing the rates of sending packets from other sources. In some embodiments the congestion notification messages include other data relating to one or more packets in the queue e.g. the destination size a hash of the contents etc. .

Some embodiments provide a separate congestion notification system message bus for each higher component to be notified of congestion. For example some embodiments provide a separate congestion notification system message bus for each of multiple VNICs on a TCP IP stack of a kernel. illustrates a congestion notification system of some embodiments in which multiple VNICs are notified of congestion through multiple congestion notification system message buses. The figure includes the same components and processes as with the addition of a second VNIC chokepoint and congestion notification system message bus . The congestion notification system message bus of this embodiment sends threshold reached notifications to the second VNIC . In both the VNICs and receive notifications of congestion.

In this figure VNIC is subscribed to receive congestion notifications about chokepoints actually to the virtual switch that manages the queue represented by chokepoint and . Accordingly the VNIC receives threshold reached notification messages to identify congestion at the queues of the uplink manager the PNIC and the port of the virtual switch represented by chokepoint . The threshold reached notification messages for VNIC are sent from congestion notification system message bus .

The VNIC is subscribed to receive congestion notifications about chokepoints actually to the process e.g. the virtual switch that manages the queue represented by chokepoint and . Accordingly the VNIC receives threshold reached notification messages to identify congestion at the queues of the uplink manager the PNIC and the port of the virtual switch represented by chokepoint . The threshold reached notification messages for VNIC are sent from congestion notification system message bus .

As described with respect to when the queue of a particular component reaches the threshold fullness the component sends a congestion notification message to the higher layer component here VNICs through a congestion notification system message bus and the VNIC through a congestion notification system message bus . The congestion notification message of some embodiments includes more data than the fact that the threshold has been reached. For example the congestion notification message of some embodiments includes the actual percentage of congestion e.g. if the threshold level is 70 but the queue is actually 85 full the congestion notification message will indicate that the congestion level is 85 . The congestion notification message of some embodiments includes the source of the packets e.g. the source IP address and or MAC address of the packet that are congesting the queue. In some embodiments the identified source is the last packet to enter the queue. In other embodiments the identified source is based on a most common source of a sampling of packets e.g. a fraction or all of the packets in the queue. In some embodiments by the congestion notification system identifying the source of the congestion the higher component can selectively reduce the rate of sending packets only of those slow down packets from the source of the congestion without reducing the rates of sending packets from other sources. In some embodiments the congestion notification messages include other data relating to one or more packets in the queue e.g. the destination size a hash of the contents etc. .

In some embodiments various processes are implemented by components of the computer virtualization networking stack in order to set thresholds at which lower layer components should provide congestion notification messages and in order to properly respond to congestion by notifying higher layer components. In some embodiments a particular component both receives congestion notifications from relatively lower layer components and sends congestion notifications to relatively higher layer components.

In order to receive congestion notifications from a lower network layer the higher layer component subscribes to receive congestion notifications about a lower layer queue managing component. A threshold congestion setting of the chokepoint e.g. a default setting or a setting set in the configuration of the component managing the queue determines what level of congestion i.e. fullness of the queue managed by the queue managing component will result in a congestion notification message being sent to the congestion notification system message bus.

The higher layer component continues to send packets at the set rate until it receives a notification of new or increased congestion e.g. a threshold reached notification message or a congestion notification message from a lower layer component either directly or through a message bus . When the higher layer component receives such a notification the state transitions to state which reduces the packet rate assuming the packet rate is not already sufficiently low as a result of previous congestion notification messages . If the rate is above zero the higher layer component then transitions back to state and continues to send packets at the new reduced rate. In some embodiments the reduced packet rate could be zero in which case the higher layer component transitions to state and stops sending packets until a threshold time has passed since the last congestion notification.

When a threshold time has passed since the last received congestion notification the higher layer component transitions from state or to state . In state the higher layer component increases the packet sending rate and returns to state .

In some embodiments the packet rate is lowered by a particular amount each time a congestion notification is received. In some embodiments the rate of sending packets from the higher layer component is lowered to zero when the congestion notification is received and the packets are stored in a queue of the higher layer component see e.g. . In other embodiments the higher layer component rejects requests from even higher layer components to generate data packets e.g. a VNIC or TCP IP stack will slow down generation or not generate packets for a packet source while there is congestion in the lower layer components . In some embodiments the packet sending rate is lowered by an amount based on the level of congestion. For example the packet sending rate could be half of a maximum rate when the congestion in a queue is 70 full a quarter of maximum rate when the congestion in the queue is 85 full and zero when the congestion in the queue is 100 full.

After operation the process determines at whether the queue is above the threshold congestion setting for sending congestion notification messages. When the queue is not above the threshold the process proceeds to operation to send out packets. When the queue is above the threshold the process reports the queue congestion level i.e. how full the queue is to the higher layer component through a congestion notification system message bus. The process then sends at packets to the next lower layer component at the best possible rate. In some embodiments the best possible rate is the highest rate at which the next lower component will accept packets. In other embodiments the best possible rate is lowered in response to congestion notification messages from even lower layer components e.g. in some embodiments a PNIC sends congestion notification messages to an uplink manager either instead of or in addition to the uplink manager sending congestion notification messages to a VNIC . In some embodiments the best possible rate under some circumstances e.g. when there is congestion below or when the next component not accepting packets may be zero in which case the process does not send out packets in operation .

The process then determines at whether the system has been shut down e.g. after receiving a shutdown command . When the system has not been shut down the process returns to operation to receive the next packet. When the system has been shut down the process ends.

In the above described embodiments the process e.g. the process that implements a VNIC that receives the congestion notification message reduces the rate of sending packets to lower layer components. In some embodiments reducing the rate of sending packets includes dropping some packets e.g. when there is no VNIC queue or the VNIC queue is too small to store all the incoming packets until the congestion in the lower layer components is relieved . When multiple data flows e.g. data packets with matching source IP destination IP source port destination port and communications protocol are being sent out of the host through a particular PNIC it is possible for one data flow to consume a larger fraction of the PNIC queue than other data flows e.g. by sending more data in a particular period of time . Data flows that consume relatively more of the PNIC queue are referred to herein as larger flows while data flows that consume relatively less of the PNIC queue are referred to herein as smaller flows . In some embodiments the higher layer components e.g. VNIC processes that drop the packets selectively drop a higher percentage of the packets from larger flows than from the smaller flows. However in some embodiments the process that drops the packets does not store a record of how much data has been produced by each flow sent by each flow or is taking up space in the PNIC queue.

The PNIC of some embodiments sends out data in relatively small data segments e.g. about 1500 bytes per segment in some embodiments . Space in the PNIC queue of some embodiments is limited to storing a particular number of data segments e.g. 500 data segments . The source of a packet of data may send packets of data much larger than the PNIC s segment size. For example in some embodiments a TCP IP stack produces data in packets of about 65 kB. These large packets are then broken down by processes that operate between the TCP IP stack and the PNIC or at the PNIC into data segments for transmission from the PNIC to a network outside the host. In order to estimate which data flows are consuming the most queue space in the PNIC some embodiments provide a source of data packets e.g. a process implementing a TCP IP stack that sends a usage indicator with each packet to identify how many unacknowledged packets are outstanding for the data flow associated with that packet. The dropping process e.g. the VNIC process uses these usage indicators to determine which packets to drop.

The PNIC has a limited capacity to store packets e.g. in a PNIC queue . In some embodiments the PNIC can store up to a set number of segments e.g. 500 . When the PNIC reaches its full capacity it stops accepting more packets until enough segments have been sent to leave space in the queue for segments derived from a new packet. In some embodiments when the queue of PNIC is full beyond a particular threshold level the PNIC sends a congestion notification message to any VNICs that send data to that PNIC e.g. as described with respect to . In the previous sections the VNIC was described as dropping packets after receiving a congestion notification but not as preferentially dropping particular packets. However in some embodiments the VNIC preferentially drops packets coming from the flows that are taxing the resources of the PNIC e.g. flows are using up the most space in the PNIC queue .

The VNIC of some embodiments does not directly measure which flows are using the most queue space of the PNIC . In some such embodiments the module implementing the TCP IP stack keeps track of how much data has been sent out for each flow e.g. a particular flow being all data with the same source IP address destination IP address source port destination port and protocol that have not yet received acknowledgement messages e.g. messages from the destination computer or device acknowledging delivery of the data at the destination IP and port address of the packet . A packet for which 1 an acknowledgement message has not been received and 2 the TCP IP module has not identified as lost is described herein as an unacknowledged packet . Data from such a packet may be described herein as unacknowledged data or as in flight. In some embodiments a packet is identified as lost when either 1 the TCP IP stack receives a notification that the packet has been dropped e.g. from the VNIC or 2 when a threshold time has elapsed since the TCP IP module sent the packet without receiving an acknowledgement message for the packet.

In some embodiments the module implementing the TCP IP stack sends a usage indicator to the VNIC with each packet to indicate what proportion of the PNIC queue is represented by the unacknowledged packets. For example in packet is part of Flow A. As indicated by usage indicator Flow A has enough unacknowledged data to account for 60 of the queue of PNIC . Similarly packet is part of Flow B. As indicated by usage indicator Flow B has enough unacknowledged data to account for 3 of the queue of PNIC . In the first example if the PNIC has a queue size of 500 segments of 1500 bytes each then Flow A represents enough data for 60 of the 500 segments i.e. 300 segments of 1500 bytes each or a total of 450 000 bytes of data . In the second example if the PNIC has a queue size of 500 segments of 1500 bytes each then Flow B represents enough data for 3 of the 500 segments i.e. 15 segments of 1500 bytes each or a total of 22 500 bytes of data .

The TCP IP stack is provided with the PNIC queue size as configured constant in some embodiments. Specifically when an uplink is created by joining a PNIC to a virtual switch some embodiments establish the queue size for the uplink at this time by reading the queue size from the system configuration. If the queue size is modified some embodiments pass this information to the TCP IP stack so it can use the new value to calculate the usage indicator.

In some embodiments the value is based on the amount of unacknowledged data even though not all of the unacknowledged data is necessarily occupying space in the queue of the PNIC. In some cases data may be unacknowledged because either the segments containing the data are stalled somewhere on the network or because the acknowledgement messages are stalled e.g. either on the network or in the host . For example if the PNIC has sent data from a particular flow then that data is no longer in the PNIC s queue but the data may remain unacknowledged due to external conditions e.g. a slow system at the destination long round trip to the destination etc. . In some circumstances enough unacknowledged data can be sent to the network by the PNIC that the usage indicator indicates greater than 100 usage of the PNIC queue.

The VNIC of some embodiments determines whether or not to drop a particular packet sent from the TCP IP stack . Various embodiments use various calculations to determine whether to drop a particular packet during periods of high congestions. In some embodiments the usage indicator corresponding to a particular packet is compared to a threshold level. The VNIC of such embodiments drops packets whose corresponding usage indicator is higher than that threshold while sending packets whose corresponding usage indicator is lower than the threshold on toward the PNIC . For example the VNIC compares usage indicator 60 to a threshold value e.g. 40 . The usage indicator value 60 is greater than a 40 threshold value so the VNIC makes a decision to drop the corresponding packet . The VNIC also compares usage indicator 3 to a threshold value e.g. 40 . The usage indicator is less than the 40 threshold value so the VNIC makes a decision to send the corresponding packet of on toward the PNIC as shown in . In some embodiments when the VNIC drops a packet the VNIC also sends an error message to the TCP IP stack to indicate that the packet has been dropped. Although the above example uses the same threshold value 40 when comparing packets from both flows in some embodiments the threshold values are not the same for each flow. For example some flows may be designated as having higher priorities than others and such flows may have higher threshold values than the lower priority flows. In some embodiments the threshold value for a particular flow is not constant e.g. threshold values in various embodiments are based on one or more of the congestion level of the PNIC queue how many packets from a particular flow have been dropped recently the type of application producing the packets or any other reason for having non constant threshold values .

Rather than a one to one correspondence between a VNIC and a PNIC in some embodiments the PNICs operate in NIC teams with outgoing packets sent through the teaming layer prior to the uplink and PNIC. In this case the VNIC may be notified of congestion when one of the PNIC queues is congested and will begin dropping packets heading towards that PNIC. However the VNIC will not be aware as to which flows are sent to the congested PNIC and which flows are sent to the other PNICs in the team and therefore packets for all flows sent to the NIC team will be subject to potentially being dropped by the VNIC. In this case the queue size used for calculating the usage indicator will be that of the congested PNIC queue in some embodiments known to the TCP IP stack as described above. Some embodiments though may include state sharing between the teaming layer and the VNIC to indicate which flows are being sent to the PNIC with the congested queue and which flows are being sent to the other PNICs in the team.

The VNIC of the embodiment illustrated in drops packets based on threshold levels. In such an embodiment packets from a flow with a higher than threshold usage of the PNIC queue will be dropped until the proportion of unacknowledged packets drops below the threshold level or the threshold level rises to exceed the proportion of unacknowledged packets. The percentage of unacknowledged packets drops in some embodiments either when the TCP IP stack receives an acknowledgement message for the outstanding packets or when the TCP IP stack determines that those packets are permanently lost such as when the acknowledgement is not received within a threshold period such packets are sometimes referred to as timed out .

In some embodiments the VNIC uses a probabilistic determination to select packets to be dropped. In some such embodiments the VNIC drops packets with a probability in proportion to how large a percentage of the PNIC queue the flow corresponding to the packet occupies. For example in some such embodiments the VNIC will drop X of the packets on average of a flow whose usage indicator is X . illustrate a system in which the VNIC drops packets on a probabilistic basis. illustrates a system of some embodiments for determining which packets to drop. The figure includes host machine virtual machine applications and TCP IP stack VNIC PNIC and data packets and with respective usage indicators and . The host machine is a physical computer that implements the virtual machines including virtual machine and other virtual machines not shown . The virtual machine is a simulation of an independent computer running applications and . The applications and send data to other virtual or actual computers through a TCP IP stack of some embodiments that translates the data into large packets e.g. 65 kilobyte kB packets such as packets and and sends the packets to VNIC . The VNIC of some embodiments sends the packets of data to PNIC . In some embodiments the packets of data are routed and or adjusted by other modules between the VNIC and the PNIC . In some embodiments these modules are similar to or identical to the virtual switch and uplink manager of . In some embodiments the uplink manager or some other final network stack layer is implemented as part of the PNIC . In some embodiments when a packet of data reaches the PNIC the packet is divided into smaller segments of data e.g. 1500 bytes per segment . The segments are then sent out of the host machine through the PNIC .

In the previous section the VNIC was described as dropping any packets from a flow with a proportion of unacknowledged data corresponding to a threshold percentage of the PNIC queue. In some embodiments the VNIC uses a random threshold for each packet and therefore drops a random set of packets coming from the flows that are taxing the resources of the PNIC in proportion to how large the unacknowledged data of those flows are compared to the PNIC queue size.

As with VNIC of the VNIC of does not directly measure which flows are using the most queue space of the PNIC . In some such embodiments the module implementing the TCP IP stack keeps track of how much data has been sent out for each flow that have not yet received acknowledgement messages. In some embodiments the module implementing the TCP IP stack sends a usage indicator to the VNIC with each packet to indicate what proportion of the PNIC queue is represented by the unacknowledged packets.

In packet belongs to Flow A. As indicated by usage indicator Flow A has enough unacknowledged data to account for 60 of the queue of PNIC . Similarly packet belongs to Flow B. As indicated by usage indicator Flow B has enough unacknowledged data to account for 3 of the queue of PNIC . In the first example if the PNIC has a queue size of 500 segments of 1500 bytes each then Flow A represents enough data for 60 of the 500 segments. In the second example if the PNIC has a queue size of 500 segments of 1500 bytes each then Flow B represents enough data for 3 of the 500 segments.

The VNIC of some embodiments determines whether or not to drop a particular packet sent from the TCP IP stack . In the embodiments of the usage indicator corresponding to a particular packet is compared to a randomly generated threshold level e.g. using a different random threshold level for each packet . The VNIC of such embodiments drops packets whose corresponding usage indicator is higher than the random threshold for the corresponding packet while sending packets whose corresponding usage indicator is lower than the random threshold for the corresponding packet on toward the PNIC . conceptually illustrates such a calculation by VNIC . In the VNIC compares usage indicator 60 to a randomly generated threshold value 72 . The usage indicator is less than the randomly generated threshold value so the VNIC makes decision to send the corresponding packet of on toward the PNIC . The VNIC also compares usage indicator 3 to the different random threshold value 1 . The usage indicator is greater than the threshold value so the VNIC makes decision to drop the corresponding packet of . One of ordinary skill in the art will understand that although the illustrated example shows a scenario in which the VNIC sends on a packet of a larger flow and drops a packet of a smaller flow due to the random threshold levels the VNIC is nonetheless more likely to send on a particular packet from a smaller flow than send on a packet from a larger flow. Furthermore one of ordinary skill in the art will understand that the numbers indicated as random will be generated in some embodiments by a pseudo random number generator. In a similar manner to VNIC of in some embodiments when the VNIC drops a packet the VNIC also sends an error message to the TCP IP stack to indicate that the packet has been dropped.

Various embodiments of the packet dropping system use various different processes for sending usage indicators between the TCP IP module and the VNIC module. conceptually illustrates a process of some embodiments for sending a packet and a corresponding usage indicator from a higher layer component e.g. a TCP IP module to a lower layer component e.g. a VNIC that determines whether to drop data packets. The process receives at a data packet to send to the VNIC. In some embodiments the higher layer and lower layer components are both processes implemented as part of a virtual machine and the data comes from an application running on that virtual machine. In some embodiments the higher and lower layer components are on a kernel of a host machine and the data packet comes from processes that also run on the kernel of the host machine. Although the process is described as receiving a packet in some embodiments the data packet received by the higher layer component is in any of several formats including but not limited to a stream of data multiple smaller packets of data or a packet of data larger than the higher level is capable of sending which will be broken up by the higher layer component into smaller packets for sending to the lower layer component.

The process identifies at a size of the queue of a PNIC of the host and the size of segments in the PNIC queue. In some embodiments the host PNIC size is provided from configuration data in terms of the number of segments the PNIC queue can hold. The TCP IP stack is provided with the PNIC queue size as configured constant in some embodiments. Specifically when an uplink is created by joining a PNIC to a virtual switch some embodiments establish the queue size for the uplink at this time by reading the queue size from the system configuration. If the queue size is modified some embodiments pass this information to the TCP IP stack so it can use the new value to calculate the usage indicator.

The maximum size of the segments in the PNIC queue is fixed in some embodiments e.g. in some embodiments the maximum size of each segment is 1500 bytes . Although the process is described as identifying the PNIC queue size and segment size after receiving the data packets in some embodiments these values are fixed or only change infrequently. Accordingly in such embodiments the higher layer component e.g. the TCP IP module determines these values once or infrequently rather than identifying them separately for each packet.

The process then identifies at how many PNIC queue segments are represented by unacknowledged data for a flow of a packet that the TCP IP module is about to send to a VNIC. In some embodiments the process identifies this number of segments by keeping track of the total size of the data in unacknowledged packets outstanding for each flow e.g. updating unacknowledged data sizes in operation below . Therefore the TCP IP module has both the size of the unacknowledged data and the segment size for PNIC segments. Accordingly such embodiments identify the number of outstanding segments by dividing the amount of unacknowledged data by the previously identified segment size of the PNIC segments.

The process determines at the ratio of unacknowledged segments in a particular flow i.e. a flow of a packet that the TCP IP module is about to send to the VNIC to the size of the PNIC queue. In some embodiments this ratio is calculated as a fraction of the total queue e.g. 0.1 0.6 0.99 etc. . In other embodiments this ratio is calculated as a percentage of the total queue e.g. 10 60 99 etc. . In some embodiments this ratio is calculated based on the amount of data previously sent in a particular flow and not on the amount of data in the packet that the TCP IP module is about to send. In other embodiments the amount of data in the packet about to be sent is included in the ratio.

The process sends at a packet to the VNIC i.e. the next packet in the flow for which the TCP IP module calculated the ratio . In some embodiments each packet is a fixed size e.g. 65 kB . In other embodiments the packets are a variable size e.g. a size dependent on how much data needs to be sent up to a maximum size in each packet . Along with the packet the process sends also at an indicator of the ratio of unacknowledged segments to host PNIC queue size. In various embodiments this indicator is a percentage a fraction or some other number based on the calculated ratio. In various different embodiments the indicator might be sent before the packet after the packet or prepended appended or otherwise sent as part of the packet itself. Furthermore in other embodiments the TCP IP module might update a table with a usage indicator value for the flow which the VNIC reads when determining whether to drop the packet.

In addition some embodiments may mark a packet as non droppable. Specifically the TCP IP module may mark certain important types of packets as non droppable to prevent the VNIC from dropping the packet even if the packet belongs to a large flow e.g. packets that are part of a handshake protocol for establishing a connection or keeping a connection alive .

The process then updates at the size of the unacknowledged data flow for the packet. In some embodiments the updated value is based on adding the size of the outgoing packet data to a previously identified size of unacknowledged data. In some embodiments in addition to adding outgoing packets to the unacknowledged data size the process also reduces the unacknowledged data size based on error messages received from the VNIC indicating that a packet has been dropped and or based on previously sent packets timing out i.e. remaining unacknowledged for more than a threshold amount of time . The process then ends with respect to that packet. One of ordinary skill in the art will understand that the process will be performed multiple times in the course of sending multiple packets.

In some embodiments a lower layer component e.g. a VNIC receives packets and identifying data from a higher layer component e.g. a TCP IP module and determines whether to drop the packets. conceptually illustrates a process of some embodiments for determining whether to drop packets. The process receives at an outgoing packet from the TCP IP module. As mentioned above the packet may be of a fixed size e.g. 65 kB or a variable size e.g. between a minimum size and 65 kB . Along with the packet the process receives at an indicator of the ratio of unacknowledged data for the flow of the received packet to the size of the queue of the PNIC. As mentioned above in various embodiments the indicator may be in various mathematical formats e.g. decimalized fraction percentage or other mathematical value based on the calculated ratio . The indicator may be received before after or as part of the packet in various embodiments.

The process determines at whether there is congestion ahead. When there is no congestion ahead no need exists to drop packets. Accordingly in that case the process sends at the packet on toward the PNIC e.g. through the layers of components identified in . In some embodiments the VNIC divides the packet into segments before sending the data in the packet toward the PNIC. In other embodiments the VNIC sends the entire packet which will be divided before or at the PNIC.

When there is congestion ahead e.g. as indicated by a congestion notification message as described in the process identifies at a threshold number representing a threshold level for the indicator of the ratio. In some embodiments the threshold number is a fixed value. In other embodiments however the threshold number is determined by generating a random threshold number e.g. as described with respect to .

The process then determines at whether the ratio of unacknowledged segments indicated by the TCP IP module for the flow of the packet is greater than the threshold. When the ratio is greater than the threshold the process drops at the packet and sends at an error message to the TCP IP module to indicate that the packet was dropped. The process then ends for the packet received in operation . When the ratio is less than the threshold at the process sends at the packet toward the PNIC. The process then ends for the packet received in operation . One of ordinary skill in the art will understand that the process is performed multiple times for multiple packets.

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives RAM chips hard drives EPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself

As used in this specification the terms computer host machine server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

This specification refers throughout to computational and network environments that include virtual machines VMs . However virtual machines are merely one example of data compute nodes DCNs or data compute end nodes also referred to as addressable nodes. DCNs may include non virtualized physical hosts virtual machines containers that run on top of a host operating system without the need for a hypervisor or separate operating system and hypervisor kernel network interface modules.

VMs in some embodiments operate with their own guest operating systems on a host using resources of the host virtualized by virtualization software e.g. a hypervisor virtual machine monitor etc. . The tenant i.e. the owner of the VM can choose which applications to operate on top of the guest operating system. Some containers on the other hand are constructs that run on top of a host operating system without the need for a hypervisor or separate guest operating system. In some embodiments the host operating system uses name spaces to isolate the containers from each other and therefore provides operating system level segregation of the different groups of applications that operate within different containers. This segregation is akin to the VM segregation that is offered in hypervisor virtualized environments that virtualize system hardware and thus can be viewed as a form of virtualization that isolates different groups of applications that operate in different containers. Such containers are more lightweight than VMs.

Hypervisor kernel network interface modules in some embodiments is a non VM DCN that includes a network stack with a hypervisor kernel network interface and receive transmit threads. One example of a hypervisor kernel network interface module is the vmknic module that is part of the ESXi hypervisor of VMware Inc.

It should be understood that while the specification refers to VMs the examples given could be any type of DCNs including physical hosts VMs non VM containers and hypervisor kernel network interface modules. In fact the example networks could include combinations of different types of DCNs in some embodiments.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition each conceptually illustrates a process. The specific operations of the processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. For example updating the size of the unacknowledged data for a flow is shown as operation in . However such and update could be performed at one or more other stages of the process . Furthermore any of the described processes could be implemented using several sub processes or as part of larger macro processes.

