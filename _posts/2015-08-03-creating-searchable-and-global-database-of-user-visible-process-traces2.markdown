---

title: Creating searchable and global database of user visible process traces
abstract: In one example, a controller device includes one or more network interfaces communicatively coupled to one or more devices of a virtual network, and a processor configured to determine, for the virtual network, a set of two or more related processes executed by respective devices in the virtual network, receive via the network interfaces data for the set of two or more related processes, and aggregate the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09606896&OS=09606896&RS=09606896
owner: Juniper Networks, Inc.
number: 09606896
owner_city: Sunnyvale
owner_country: US
publication_date: 20150803
---
This application is a continuation of U.S. application Ser. No. 13 835 483 filed Mar. 15 2013 which claims the benefit of U.S. Provisional Application No. 61 729 474 filed Nov. 23 2012 U.S. Provisional Application No. 61 723 684 filed Nov. 7 2012 U.S. Provisional Application No. 61 723 685 filed Nov. 7 2012 U.S. Provisional Application No. 61 721 979 filed Nov. 2 2012 U.S. Provisional Application No. 61 721 994 filed Nov. 2 2012 the entire contents of each of which are hereby incorporated by reference.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of data center. The data center may for example host all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general this disclosure describes techniques for automatically tracing back from a central location e.g. by using a structurally queryable SQL able central database where the trace back occurs long after a failure occurred for thereby identifying likely faulty processes in massively distributed complex systems such as software defined network SDN systems.

In one example a method includes determining by a controller device for a virtual network a set of two or more related processes executed by respective devices in the virtual network receiving by the controller device data for the set of two or more related processes and aggregating by the controller device the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.

In another example a controller device includes one or more network interfaces communicatively coupled to one or more devices of a virtual network and a processor configured to determine for the virtual network a set of two or more related processes executed by respective devices in the virtual network receive via the network interfaces data for the set of two or more related processes and aggregate the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.

In another example a computer readable storage medium having stored thereon instructions that when executed cause a processor to determine by a controller device for a virtual network a set of two or more related processes executed by respective devices in the virtual network receive by the controller device data for the set of two or more related processes and aggregate by the controller device the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.

The details of one or more examples are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

Faults and failures in the context of software systems are not one and the same thing. When a failure happens it is not always easy to determine who or what was at fault blame worthy especially if the failure is discovered long after the fault occurred and especially if the point of fault can be anywhere in a massively distributed system such as in a software defined network SDN system.

A simple example of a fault might be an attempted division by zero in a math processing part of an executing software process. Ideally the CPU or other data processing hardware component will issue an exception flag when such a violation of basic math rules is attempted and a corresponding error log will be generated locally for the hardware component and or software component in which the violation was attempted. Later when a problem debugging analyst reviews the local log he she will spot the exception flag s and recognize that a simple math rule violation such as division by zero was attempted.

A slightly more complex example of a fault might be a generation of an out of range result value in a math processing part of an executing software process. For example the allowed range for an accounting procurement account might be not less than 5.00 but not more than 1000.00 where for some reason an executed math operation produces a result value such as 4.99 or 1000.01 and the violation is not caught by hardware means. Ideally the executing software will include a results validation thread and the latter will issue one or more exception flags when such a violation of non basic math and application specific rules is attempted. Thereafter a corresponding error log may be generated locally for the results validation thread of the local process in which the violation was attempted. Later when a problem debugging analyst reviews the local log he she will spot the exception flag s and recognize that one or more application specific rule violations were attempted. The problem debugging analyst may then formulate corrective code for avoiding recurrence of the violation s .

These simple examples do not address the question of what happens when a rules violating or other fault causing procedure takes place in a multi encapsulated computing and or telecommunications environment such as that where many virtual machines are respectively executing many distributed processes across a massively distributed system such as a software defined network SDN system. In that case even if an exception log exists the problem debugging analyst often does not know where in the massively distributed system to look because there are too many possibilities and too many spaced apart locations e.g. physical servers that are miles apart in which the fault indicating log or logs might reside. Moreover because it may take a long time to realize that a problem occurred by the time the problem debugging analyst retrospectively begins the query the local exception logs some of them may have already been overwritten by more recent logs due to memory capacity constraints at given local facilities.

This problem may become particularly acute in systems that are very complex massive in size in terms of number of unique components and or in terms of spatial geographic extent and where such systems are expected to be up and running at full capacity as much as possible. An example of such a highly complex massively sized and full time running system is a software defined networking SDN system.

Examples of SDN systems include so called data centers e.g. cloud computing centers that are used to support operations of the Internet including data storage search and retrieval. Additional details for one SDN example are given below. In brief and sufficient for the present introduction an SDN system may be comprised of many thousands of complex server computers a.k.a. servers programmed to run plural virtual machines and encapsulated processes and sub processes thereof many thousands of network channels and routers switches distributed over many thousands of miles where the expectation of users is that both the complex software and hardware components of such a system will remain failure free and operational on a highly reliable and scalable basis.

It is to be understood that the term virtual as used herein does not mean abstract. Instead it refers to physical means by way of which details of an underlying hardware and or software system are hidden from a user encapsulated and or by way of which details of an underlying other virtual system are hidden from a user. It is also to be understood that the term software as used herein does not mean software in an abstract sense but rather means a physically real and not ephemerally transient thing which non abstract non ephemerally transient thing is usable for digitally controlling how a configurable digital data processing system and or configurable analog signal processing system operates.

In view of the above introductory description as to the difference between simple fault failures in simple systems and in view of the above introductory description as to the difference between small simple systems and massively complex and distributed systems that execute hundreds of thousands if not more of processes and sub processes encapsulated in respectively large numbers of virtual machines and cross communicated over a vast telecommunications system it would advantageous to have a method and system for automatically tracing back from a central location and long after a failure occurred the exception logs that were generated for thereby identifying likely faulty processes in such massively distributed complex systems.

The techniques of this disclosure are generally directed to providing trace back from a central location in a massively distributed complex system such as a software defined network SDN system.

In one example a method of identifying likely faulty processes in a massively distributed complex system includes subdividing the system into a plurality of tiers each characterized by having alike components and alike kinds of processes normally executing therein subdividing system executions as belong to respective ones of a plurality of user accessible entities such as user viewable virtual networks a.k.a. user viewable VNets or more generally User Viewable Entities UVE s and assigning a unique identifying key UVEKey for each respective UVE of each respective system tier TRx of each respective virtual and or physical execution machine VOPEM and of each respective process instance PIN tagging corresponding trace logs with two or more of such UVE TRx and PIN identifying keys e.g. UVEKey TRxKey PINKey VMKey PMKey when the trace is locally generated transmitting the tagged traces to and storing them in a centralized database that can be structurally queried with use of one or more of these identifying keys e.g. UVEKey TRxKey PINKey VMKey PMKey .

For each respective UVE and tier the method may include identifying respective process reports that cross correlate with a corresponding UVE Key and a corresponding Tier key where the reports may include quantitative failure or fault parameters such as memory failures telecommunications failures processor failures packet resends and or drops etc. and relaying the UVE and Tier tagged reports to a centralized and query able database. For each respective process report that is locally generated automatically tagging the report with one or more linking keys including a UVEKey. For each respective tier the method may include automatically determining what part of its resources are used by each of respective UVE s and automatically determining if the allocated resources of any UVE are insufficient due to repeated component failures e.g. lost packets . For each respective UVE and its detected component failures the method may include logically associating the detected component failures with one or more of the respective captured parameter snapshots that immediately preceded the respective component failures for that UVE.

The method may further include investigating those of the UVE associated reports that were correlated to failure as being likely to point to the at fault components and or tiers of that UVE. The method may also include taking preemptive corrective and or work around measures for those of the respective tier components and UVEs that were determined to be more highly likely to enter a failure mode based on the investigation.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some examples data center may be individual network servers network peers or otherwise.

In this example data center includes set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN TOR switches coupled to a distribution layer of chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregates traffic flows and provides high speed connectivity between TOR switches . TOR switches A and B may be network devices that provide layer 2 MAC address and or layer 3 IP address routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers using service provider network .

Virtual network controller VNC provides a logically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more examples of this disclosure. In some examples virtual network controller may operate in response to configuration input received from network administrator .

In accordance with the techniques of this disclosure virtual network controller may be configured to aggregate data for a set of two or more related processes to form aggregated data for the set of two or more related processes. In particular virtual network controller may determine the set of two or more related processes executed by respective devices e.g. servers in a virtual network of data center and receive data for the set of two or more related processes. In general the set of processes may correspond to a common tier e.g. a common network plane and each of the processes in a particular set may be substantially similar. By executing substantially similar processes on different devices data center may provide high availability and reduce risk of failure.

More particularly in accordance with the techniques of this disclosure computing devices of data center and processes executed by the computing devices may be divided into various tiers. Within each tier there may be a set of related e.g. substantially similar processes. Furthermore virtual network controller may define User Visible Entities UVEs for the various tiers. The UVEs may define various data for monitoring processes of the various tiers. For example the UVEs may define attributes of processes to retrieve. Virtual network controller may receive data output during execution of the processes and in accordance with the UVEs extract values for the attributes defined by the UVEs. Virtual network controller may further aggregate this data. For example the UVE may define a manner in which to aggregate certain types of data corresponding to the attributes such as addition union over sets concatenation list generation or the like.

Virtual network controller may then generate one or more reports that are indicative of a tier and aggregated values for one or more attributes corresponding to the tier as defined by a corresponding UVE. This aggregation can be performed transparently to the devices executing the processes. That is the devices executing the processes need not take any part in the aggregation. An administrator may use the generated report to diagnose various aspects of the virtual network of data center . For example the report may include data indicative of one or more of a quantitative failure a fault parameter a memory failure a telecommunications failure a processor failure a packet resend and or a dropped communication session. The administrator may determine using the report whether any or all of these conditions apply and act accordingly e.g. by reprogramming a device of data center replacing a device of data center adding replacing or removing links between devices adding or upgrading software for one or more devices of data center or the like based on the contents of the report.

In some examples virtual network controller includes an analytics layer that is an intermediate layer that acts on generic rules. The UVEs may define rules in accordance with the analytics layer. Thus virtual network controller may operate substantially automatically that is without user interference to perform the techniques of this disclosure. The analytics tier may use definitions of the UVEs to extract information from communications output by the devices executing the corresponding processes and aggregate values for certain attributes as defined by the UVEs of the communications.

In this manner virtual network controller represents an example of a controller device configured to determine for a virtual network a set of two or more related processes executed by respective devices in the virtual network receive data for the set of two or more related processes and aggregate the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.

Each virtual switch may execute within a hypervisor a host operating system or other component of each of servers . In the example of virtual switch executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on one of host servers . In the example of virtual switch A manages virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual subnets VN VN managed by the hypervisor .

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual switch A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the physical server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system i.e. server A in the example of .

In one implementation each of servers includes a virtual network agent VN agent A X VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general each VN agent communicates with virtual network controller which generates commands to control routing of packets through data center . VN agents may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent A and VN agent A may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent A and the VN agent A may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual switches e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Other example tunneling protocols may be used including IP over GRE VxLAN MPLS over GRE etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. on or more routing tables that store routing information for the physical network as well as the overlay network of data center . Similarly switches and virtual switches maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual switch A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with one or more outer IP addresses.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual switches and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicasting replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details of an example router that utilizes a radix tree for route resolution the contents of U.S. Pat. No. 7 184 437 being incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual switch includes a default network forwarding table NFTand provides a default route that allows packet to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric .

Moreover virtual network controller and virtual switches may communicate using virtual subnet VN in accordance with default network forwarding table NFTduring discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network. Once connectivity with the virtual network controller is established the virtual network controller updates its local routing table to take into account new information about any failed links and directs virtual switches to update their local network forwarding tables . For example virtual network controller may output commands to virtual network agents to update one or more NFTs to direct virtual switches to change the tunneling encapsulation so as to re route communications within the overlay network for example to avoid a failed link.

When link failure is detected a virtual network agent local to the failed link e.g. VN Agent A may immediately change the encapsulation of network packet to redirect traffic within the overlay network and notifies virtual network controller of the routing change. In turn virtual network controller updates its routing information and may issue messages to other virtual network agents to update local routing information stored by the virtual network agents within network forwarding tables .

In this example chassis switch CH which may be any of chassis switches of is coupled to Top of Rack TOR switches A B TORs by chassis link A and chassis link B respectively chassis links . TORs may in some examples be any of TORs of . In the example of TORs are also coupled to servers A B servers by TOR links A D TOR links . Servers may be any of servers . Here servers communicate with both TORs and can physically reside in either associated rack. TORs each communicate with a number of network switches including chassis switch A.

Chassis switch A has a processor A in communication with an interface for communication with a network as shown as well as a bus that connects a memory not shown to processor A. The memory may store a number of software modules. These modules include software that controls network routing such as an Open Shortest Path First OSPF module not shown containing instructions for operating the chassis switch A in compliance with the OSPF protocol. Chassis switch A maintains routing table RT table A containing routing information for packets which describes a topology of a network. Routing table A may be for example a table of packet destination Internet protocol IP addresses and the corresponding next hop e.g. expressed as a link to a network component.

TORs each have a respective processor B C an interface in communication with chassis switch A and a memory not shown . Each memory contains software modules including an OSPF module and routing table B C as described above.

TORs and chassis switch A may exchange routing information specifying available routes such as by using a link state routing protocol such as OSPF or IS IS. TORs may be configured as owners of different routing subnets. For example TOR A is configured as the owner of Subnet 1 which is the subnet 10.10.10.0 24 in the example of and TOR A is configured as the owner of Subnet 2 which is the subnet 10.10.11.0 24 in the example of . As owners of their respective Subnets TORs locally store the individual routes for their subnets and need not broadcast all route advertisements up to chassis switch A. Instead in general TORs will only advertise their subnet addresses to chassis switch A.

Chassis switch A maintains a routing table RT table A which includes routes expressed as subnets reachable by TORs based on route advertisements received from TORs . In the example of RT table A stores routes indicating that traffic destined for addresses within the subnet 10.10.11.0 24 can be forwarded on link B to TOR B and traffic destined for addresses within the subnet 10.10.10.0 24 can be forwarded on link A to TOR A.

In typical operation chassis switch A receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on routing table A to determine the corresponding destination component and forwards the packets accordingly. For example if the destination IP address of a received packet is 10.10.0.0 i.e. the address of the subnet of TOR A the routing table of chassis switch A indicates that the packet is to be sent to TOR A via link A and chassis switch A transmits the packet accordingly ultimately for forwarding to a specific one of the servers .

Similarly each of TORs receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on its routing table to determine the corresponding destination component and forwards the packets according to the result of the lookup.

Virtual network controller VNC of illustrates a distributed implementation of a VNC that includes multiple VNC nodes A N collectively VNC nodes to execute the functionality of a data center VNC including managing the operation of virtual switches for one or more virtual networks implemented within the data center. Each of VNC nodes may represent a different server of the data center e.g. any of servers of or alternatively on a server or controller coupled to the IP fabric by e.g. an edge router of a service provider network or a customer edge device of the data center network. In some instances some of VNC nodes may execute as separate virtual machines on the same server.

Each of VNC nodes may control a different non overlapping set of data center elements such as servers individual virtual switches executing within servers individual interfaces associated with virtual switches chassis switches TOR switches and or communication links. VNC nodes peer with one another using peering links to exchange information for distributed databases including distributed databases A K collectively distributed databases and routing information e.g. routes for routing information bases A N collectively RIBs . Peering links may represent peering links for a routing protocol such as a Border Gateway Protocol BGP implementation or another peering protocol by which VNC nodes may coordinate to share information according to a peering relationship.

VNC nodes of VNC include respective RIBs each having e.g. one or more routing tables that store routing information for the physical network and or one or more overlay networks of the data center controlled by VNC . In some instances one of RIBs e.g. RIB A may store the complete routing table for any of the virtual networks operating within the data center and controlled by the corresponding VNC node e.g. VNC node A .

In general distributed databases define the configuration or describe the operation of virtual networks by the data center controlled by distributed VNC . For instance distributes databases may include databases that describe a configuration of one or more virtual networks the hardware software configurations and capabilities of data center servers performance or diagnostic information for one or more virtual networks and or the underlying physical network the topology of the underlying physical network including server chassis switch TOR switch interfaces and interconnecting links and so on. Distributed databases may each be implemented using e.g. a distributed hash table DHT to provide a lookup service for key value pairs of the distributed database stored by different VNC nodes .

As illustrated in the example of distributed virtual network controller VNC includes one or more virtual network controller VNC nodes A N collectively VNC nodes . Each of VNC nodes may represent any of VNC nodes of virtual network controller of . VNC nodes that peer with one another according to a peering protocol operating over network . Network may represent an example instance of switch fabric and or IP fabric of . In the illustrated example VNC nodes peer with one another using a Border Gateway Protocol BGP implementation an example of a peering protocol. VNC nodes provide to one another using the peering protocol information related to respective elements of the virtual network managed at least in part by the VNC nodes . For example VNC node A may manage a first set of one or more servers operating as virtual network switches for the virtual network. VNC node A may send information relating to the management or operation of the first set of servers to VNC node N by BGP A. Other elements managed by VNC nodes may include network controllers and or appliances network infrastructure devices e.g. L2 or L3 switches communication links firewalls and VNC nodes for example. Because VNC nodes have a peer relationship rather than a master slave relationship information may be sufficiently easily shared between the VNC nodes . In addition hardware and or software of VNC nodes may be sufficiently easily replaced providing satisfactory resource fungibility.

Each of VNC nodes may include substantially similar components for performing substantially similar functionality said functionality being described hereinafter primarily with respect to VNC node A. VNC node A may include an analytics database A for storing diagnostic information related to a first set of elements managed by VNC node A. VNC node A may share at least some diagnostic information related to one or more of the first set of elements managed by VNC node A and stored in analytics database as well as to receive at least some diagnostic information related to any of the elements managed by others of VNC nodes . Analytics database A may represent a distributed hash table DHT for instance or any suitable data structure for storing diagnostic information for network elements in a distributed manner in cooperation with others of VNC nodes . Analytics databases A N collectively analytics databases may represent at least in part one of distributed databases of distributed virtual network controller of .

VNC node A may include a configuration database A for storing configuration information related to a first set of elements managed by VNC node A. Control plane components of VNC node A may store configuration information to configuration database A using interface A which may represent an Interface for Metadata Access Points IF MAP protocol implementation. VNC node A may share at least some configuration information related to one or more of the first set of elements managed by VNC node A and stored in configuration database A as well as to receive at least some configuration information related to any of the elements managed by others of VNC nodes . Configuration database A may represent a distributed hash table DHT for instance or any suitable data structure for storing configuration information for network elements in a distributed manner in cooperation with others of VNC nodes . Configuration databases A N collectively configuration databases may represent at least in part one of distributed databases of distributed virtual network controller of .

Virtual network controller may perform any one or more of the illustrated virtual network controller operations represented by modules which may include orchestration user interface VNC global load balancing and one or more applications . VNC executes orchestration module to facilitate the operation of one or more virtual networks in response to a dynamic demand environment by e.g. spawning removing virtual machines in data center servers adjusting computing capabilities allocating network storage resources and modifying a virtual topology connecting virtual switches of a virtual network. VNC global load balancing executed by VNC supports load balancing of analytics configuration communication tasks e.g. among VNC nodes . Applications may represent one or more network applications executed by VNC nodes to e.g. change topology of physical and or virtual networks add services or affect packet forwarding.

User interface includes an interface usable to an administrator or software agent to control the operation of VNC nodes . For instance user interface may include methods by which an administrator may modify e.g. configuration database A of VNC node A. Administration of the one or more virtual networks operated by VNC may proceed by uniform user interface that provides a single point of administration which may reduce an administration cost of the one or more virtual networks.

VNC node A may include a control plane virtual machine VM A that executes control plane protocols to facilitate the distributed VNC techniques described herein. Control plane VM A may in some instances represent a native process. In the illustrated example control VM A executes BGP A to provide information related to the first set of elements managed by VNC node A to e.g. control plane virtual machine N of VNC node N. Control plane VM A may use an open standards based protocol e.g. BGP based L3VPN to distribute information about its virtual network s with other control plane instances and or other third party networking equipment s . Given the peering based model according to one or more aspects described herein different control plane instances e.g. different instances of control plane VMs A N may execute different software versions. In one or more aspects e.g. control plane VM A may include a type of software of a particular version and the control plane VM N may include a different version of the same type of software. The peering configuration of the control node devices may enable use of different software versions for the control plane VMs A N. The execution of multiple control plane VMs by respective VNC nodes may prevent the emergence of a single point of failure.

Control plane VM A communicates with virtual network switches e.g. illustrated VM switch executed by server using a communication protocol operating over network . Virtual network switches facilitate overlay networks in the one or more virtual networks. In the illustrated example control plane VM A uses Extensible Messaging and Presence Protocol XMPP A to communicate with at least virtual network switch by XMPP interface A. Virtual network route data statistics collection logs and configuration information may in accordance with XMPP A be sent as XML documents for communication between control plane VM A and the virtual network switches. Control plane VM A may in turn route data to other XMPP servers such as an analytics collector or may retrieve configuration information on behalf of one or more virtual network switches. Control plane VM A may further execute a communication interface A for communicating with configuration virtual machine VM A associated with configuration database A. Communication interface A may represent an IF MAP interface.

VNC node A may further include configuration VM A to store configuration information for the first set of element to and manage configuration database A. Configuration VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A. Configuration VM A and control plane VM A may communicate using IF MAP by communication interface A and using XMPP by communication interface A. In some aspects configuration VM A may include a horizontally scalable multi tenant IF MAP server and a distributed hash table DHT based IF MAP database that represents configuration database A. In some aspects configuration VM A may include a configuration translator which may translate a user friendly higher level virtual network configuration to a standards based protocol configuration e.g. a BGP L3VPN configuration which may be stored using configuration database A. Communication interface may include an IF MAP interface for communicating with other network elements. The use of the IF MAP may make the storage and management of virtual network configurations very flexible and extensible given that the IF MAP schema can be dynamically updated. Advantageously aspects of virtual network controller may be flexible for new applications .

VNC node A may further include an analytics virtual machine VM A to store diagnostic information and or visibility information related to at least the first set of elements managed by VNC node A. Control plane VM and analytics VM may communicate using an XMPP implementation by communication interface A. Analytics VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A.

Analytics VM A may include analytics database A which may represent an instance of a distributed database that stores visibility data for virtual networks such as one of distributed database of distributed virtual network controller of . Visibility information may describe visibility of both distributed VNC itself and of customer networks. The distributed database may include an XMPP interface on a first side and a REST JASON XMPP interface on a second side.

Virtual network switch may implement the layer 3 forwarding and policy enforcement point for one or more end points and or one or more hosts. The one or more end points or one and or one or more hosts may be classified into a virtual network due to configuration from control plane VM A. Control plane VM A may also distribute virtual to physical mapping for each end point to all other end points as routes. These routes may give the next hop mapping virtual IP to physical IP and encapsulation technique used e.g. one of IPinIP NVGRE VXLAN etc. . Virtual network switch may be agnostic to actual tunneling encapsulation used. Virtual network switch may also trap interesting layer 2 L2 packets broadcast packets and or implement proxy for the packets e.g. using one of Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP Domain Name Service DNS etc.

In some cases different VNC nodes may be provided by different suppliers. However the peering configuration of VNC nodes may enable use of different hardware and or software provided by different suppliers for implementing the VNC nodes of distributed VNC . A system operating according to the techniques described above may provide logical view of network topology to end host irrespective of physical network topology access type and or location. Distributed VNC provides programmatic ways for network operators and or applications to change topology to affect packet forwarding and or to add services as well as horizontal scaling of network services e.g. firewall without changing the end host view of the network.

In accordance with the techniques of this disclosure virtual network controller may be configured to aggregate data for a set of two or more related processes to form aggregated data for the set of two or more related processes. In particular virtual network controller may determine the set of two or more related processes executed by respective devices in a virtual network of network and receive data for the set of two or more related processes. In general the set of processes may correspond to a common tier e.g. a common network plane and each of the processes in a particular set may be substantially similar. By executing substantially similar processes on different devices network may provide high availability and reduce risk of failure.

More particularly in accordance with the techniques of this disclosure computing devices of network and processes executed by the computing devices may be divided into various tiers. Within each tier there may be a set of related e.g. substantially similar processes. Furthermore virtual network controller may define User Visible Entities UVEs for the various tiers. The UVEs may define various data for monitoring processes of the various tiers. For example the UVEs may define attributes of processes to retrieve. Virtual network controller may receive data output during execution of the processes and in accordance with the UVEs extract values for the attributes defined by the UVEs. Virtual network controller may further aggregate this data. For example the UVE may define a manner in which to aggregate certain types of data corresponding to the attributes such as addition union over sets concatenation list generation or the like.

Virtual network controller may then generate one or more reports that are indicative of a tier and aggregated values for one or more attributes corresponding to the tier as defined by a corresponding UVE. This aggregation can be performed transparently to the devices executing the processes. That is the devices executing the processes need not take any part in the aggregation. An administrator may use the generated report to diagnose various aspects of the virtual network of network . For example the report may include data indicative of one or more of a quantitative failure a fault parameter a memory failure a telecommunications failure a processor failure a packet resend and or a dropped communication session. The administrator may determine using the report whether any or all of these conditions apply and act accordingly e.g. by reprogramming a device of network replacing a device of network adding replacing or removing links between devices adding or upgrading software for one or more devices of network or the like based on the contents of the report.

In some examples virtual network controller includes an analytics layer that is an intermediate layer that acts on generic rules. The UVEs may define rules in accordance with the analytics layer. Thus virtual network controller may operate substantially automatically that is without user interference to perform the techniques of this disclosure. The analytics tier may use definitions of the UVEs to extract information from communications output by the devices executing the corresponding processes and aggregate values for certain attributes as defined by the UVEs of the communications.

In this manner virtual network controller represents an example of a controller device configured to determine for a virtual network a set of two or more related processes executed by respective devices in the virtual network receive data for the set of two or more related processes and aggregate the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.

Such an environment tends to be very dynamic from an applications point of view. It may be desirable to have a level of automation that insulates users from the infrastructure details and that can avoid the need for manual intervention to interconnect the physical servers to provide the computation storage and or telecommunications capacities required to enable the various applications to execute to one level of sufficiency or another.

In order to enable automation and agility of the infrastructure e.g. a physical interconnect fabric as well as a scalable processes of physical and or virtual machines there is a growing trend to deploy either an overlay networking solution or a virtualized networking system on top of physical computer clusters where the overlay and or virtualizing subsystem encapsulates and automatically manages the details of keeping the many physical data processing resources e.g. resources of servers the many physical network switches and routers e.g. switches which may correspond to devices of IP fabric in such as chassis switches and or TOR switches of and channels e.g. channel up and running at desired bandwidths BW and desired qualities of service QOS represented in by element .

In such an environment each of many servers e.g. servers may be running one or more application processes e.g. process and or guest operating systems internals of which are not explicitly shown . In order to enable many guest operating systems also called virtual machines VMs on a single server the system may utilize a virtual machines monitoring system commonly known as hypervisor such as ESX Hyper V KVM Xen etc. . Hypervisor of represents an example of such a hypervisor. A single application e.g. which includes a process or thread UVP executing inside a processes support means may require many instances of computer and storage resources that may be provided by the infrastructure as multiple individual servers and or multiple virtual machines running on one or more servers. In order for the application to share information amongst its distributed computer and storage instances and with the outside world a telecommunications network is generally used to enable movement of this information as for example packet conveyed data signals . Every time a new application is instantiated and or changed on the infrastructure a respective virtual network e.g. VNet may be created and or changed to support the new changed application and to allow all its computer and storage instances to share information with one another and or the outside world.

The term user viewable as used herein is to be understood as referring to a user defined partitioning of at least an inter processes communications layer of a system into mutually exclusive sectors where all the inter processes communications of a set of processes under investigation by the user are presumed by the user to be limited to taking place through one of the partition sectors but not any of the others and thus investigation of process interactions e.g. for purpose of debugging a problem of such interactions can be limited to investigating inter process communications only occurring within the associated one of the plural sectors. The term user viewable as used herein may be understood more broadly as referring not only to the one inter processes communications sector but also to the processes themselves and to the other system resources in addition to the associated and dedicated sector of the communications layer that the processes under investigation are presumed to use on an exclusive basis e.g. dedicated virtual machines that support the processes under investigation . Thus all the inter process activities of the set of processes under investigation by the user can be presumed by the user to be limited to taking place only inside the user defined User Visible Entity UVE and thus investigation of process actions and interactions e.g. for purpose of debugging a problem of such interactions can be limited to investigating operations taking place only inside the associated UVE.

An example of what could be deemed to be a UVE is all the processes of an identified user application program including inter processes communications resources dedicated to that application program and including computer and storage resources dedicated to that application program. However the definition is not limited to a single user and or a single application program. A single UVE can encompass resources dedicated to multiple unrelated applications of one or more identified users subject to appropriate authentication that the resources e.g. a partitioned part of the system communications layer are indeed dedicated to the identified multiple applications. In other words more generally a UVE may be thought of as representing some dedicated partitioned for investigation aspects of operational states of a system or of its resources where investigation can then be limited to the dedicated aspects for purpose of trying to operate provision troubleshoot or analyze a corresponding part of the system in some manner. The UVE in many instances may exclude those lower level aspects of system states and system resources that are for internal system use only are hidden encapsulated away from the user processes that are under investigation and are thus not intended to represent part of the system application interface to an outside world of user applications. Making all system state and resources including lower level ones visible to external applications may be confusing and overwhelming to investigatory applications rather than helpful. The dedicated subset of system states and resources that are deemed to be User Visible may be re evaluated from time to time and changed as respective users see fit.

Each UVE may be associated with a definition file. The definition file may specify a type of aggregation for each attribute associated with the UVE e.g. summation concatenation list formation union over sets or the like . An aggregator may use the aggregation information of the definition file. That is processes corresponding to the UVE need not use the aggregation information. Each of the UVEs may be specified in an interface definition language file. An analytics tier may use extra attributes defined in the interface definition language file to perform aggregation. When UVEs are sent to the analytics tier messages may mark attributes of the UVEs with aggregation tier. Thus devices or processes executing at the analytics tier may receive both values for attributes associated with a UVE as well as aggregation information associated with the UVE. In this manner these devices or processes at the analytics tier may use the aggregation information to determine how to aggregate values for the attributes. In this manner the aggregation of information can be performed without the processes that generate the information actively participating in the aggregation. That is an aggregator may receive individual streams of information from the various processes and aggregate the information without requiring a change on the part of the processes that generate the information to be aggregated.

Additionally the system states and resources that are deemed to be part of a specific UVE do not each have to be directly visible to a corresponding one or more non administrative users of the application s which fit inside the specific UVE. Rather the corresponding compute storage and telecommunication resources need only be visible to each other for allowing corresponding user processes to interact with one another as required by the corresponding application and or set of applications that are under investigation. At the same time the corresponding compute storage and telecommunication resources should not be visible to and capable of being interfered by resources of external other user applications that are not under investigation. If something goes wrong e.g. a failure within a given application or set of applications it is expected that only the visible compute storage and telecommunication resources that are dedicated to that given application will be involved and thus those are the ones that should be under investigation. Stated otherwise and referring briefly to a specific User Visible Entity e.g. UVE 2 may be conceptualized as a dedicated partition that cuts orthogonally across a plurality of system planes strata including one or more system telecommunication planes e.g. a virtual forwarding plane .

In a virtualized or overlay network environment the edge of the network is extended from the physical network elements e.g. switches or the like such as routers to software switches e.g. a VRouter like of running inside the corresponding hypervisor also in or inside the host operating system on the physical server e.g. . The so virtualized and or overlayed network that is used by the interacting applications to communicate with their respective process instances is created dynamically and managed by software switch controlling means e.g. having its own addressing and security scheme where the latter is orthogonal from the physical network and its addressing scheme. There are many different methods that can be employed to transport packets e.g. within and across the virtual network s and over the physical network.

Network IP and or Ethernet packets e.g. generated or consumed by the instances of each isolated application in the virtual networking domain are encapsulated in further IP and or Ethernet packets that are transported by the physical network. Herein the virtual network packet will be referred to as inner packet and the physical network packet will be referred to as outer packet. The function of encapsulation and or de capsulation of the virtual network packet within physical network packet is done in the hypervisor or the host O S not shown running on the server . In addition the encapsulation and de capsulation function can also be performed at the edge of the network in a first hop physical network switch e.g. one of switches a network router or the like. This functionality is commonly called tunneling and is actively used in networking to create overlay networks. There are many different tunneling protocols used in the industry wherein different protocols are carried within another protocol for example IP over GRE VxLAN MPLS over GRE etc.

Cloud data center networks can constitute an example of a massively distributed complex system because the number of interconnected servers can be very large with each server presenting one or more links each having a respective 1 Gbps or 10 Gbps or greater bandwidth link. In order to construct a network that can interconnect all such links operators generally use a number of switches or routers each with N input ingress links M output egress links. Each of these individual switches can act as an IP router with its own IP address es . Plural routers may be operatively cross coupled to define CLOS networks of routers or similar multi stage routing arrangements.

Referring to some of the specifics shown in there can be a plurality of different kinds of components in respective tiers or service planes of a virtualized overlay system.

One of these planes is the virtual to physical forwarding plane shown in . It includes the so called virtual network routers VNRouters or more simply VRouters . These components can reside in the respective hypervisors of the respective physical servers e.g. or they can reside on a so called Top of Rack switch not shown which is typically included in the virtual to physical forwarding plane . When the VRouter is disposed in the hypervisor it acts as a software switch having both respective virtual ports connected to the virtual machines VMs and physical ports corresponding to the physical I O ports of the respective server . Each VNRouter selectively routes switches packets between its virtual ports and the physical ports and or between its virtual ports. The VNRouters may be considered as Data Forwarding Plane components of the Virtual Network System. In order to support scalable flexibility some of the VRouters e.g. in a given hypervisor may be dedicated to servicing a first virtual network UV Vnet 1 another subset of the VRouters e.g. may be dedicated to servicing a second virtual network UV Vnet 2 and yet another subset of the VRouters e.g. may be held in reserve for dynamic assignment to one of the first and second user viewable networks e.g. UV Vnet 1 UV Vnet 2 or to a dynamically instantiated other virtual network Vnet .

Another of the plural tiers or planes within the SDN system is referred to as the Control Plane and it may contain a plurality of virtual machines VM implementing respective Controllers or Controller Processes. These are typically configured as horizontally scalable components just as the VRouters are typically configured as horizontally scalable components that provide dynamically scalable control functions within the Virtual Network System. The Controllers each operatively couples to a respective set of VNRouters and each distributes respective routing information signals to its dynamically scalable set of VNRouters. In one example the relative scale of the Virtual Network System is on the order of 100 s of 1000 s of VNRouters e.g. and 100 s of corresponding Controllers e.g. VNcp1 .

Another of the plural tiers or planes within the SDN system is referred to as the Configurations Plane and it may contain a plurality of virtual machines VM implementing respective Configuration Processes. These are typically configured as horizontally scalable components just as the VRouters and the Controllers are typically configured as horizontally scalable components that provide control functions with respect to interconnect and or other configurations within the Virtual Network System. The Configuration controllers each operatively couples to a respective parts of the physical network and or to respective parts of the Control Plane and each distributes respective configuration information signals to its controlled counterparts.

Yet another of the plural tiers or planes within the SDN system is referred to as the Analytics plane . Components e.g. VMn1 within the Analytics plane are typically charged with automatically monitoring and or automatically collecting reported states of other parts of the Virtual Network System. In other words the Analytics component are typically tasked with gathering information from all other components in the system so as to develop a bird s eye or big picture view of what is occurring in the system as a whole. This Big Data information is generally stored in a persistent database. This information can then be used to show the current state of the system to help debug problems to do historical or real time analysis of the system and so on.

Because of the highly scalable and variable nature of the SDN System it is prone to many fault and failure modes. In other words because of the scale and numbers of components involved it is likely that one or more are in failure mode. On the other hand it is desired that the SDN System provide its users e.g. with continuously robust reliable wide bandwidth and high quality services. So the in failure mode components need to be worked around for example by drawing on the spare components that are typically held in reserve in each horizontally scalable tier. In other words the SDN System may be resilient and continue to operate at near peak capability despite isolated failures in various ones of its components. The various components that desirably avoid failures and or are configured to work around known or expected failure modes include the different kinds of components in the respective and different tiers or planes including the forwarding plane the control plane the configuration plane and even the global analytics plane .

Sometimes a complex and hard to trace and debug failure mode develops in such a complex and massively distributed system. Sometimes it is necessary for system administrators e.g. to trace back to individual processes e.g. UVP UVP . . . UVPn within individual user viewable domains e.g. user viewable virtual networks Vnet s or more generally UVE s in order to determine what went wrong.

However a commonly shared search key that ties together the individual and local trace logs of respective processes in respective virtual and or physical machines a commonly shared search key that ties together the processes of specific component tiers and a commonly shared search key that ties together the processes of specific process instances may not be available.

In accordance with one aspect of the present disclosure at the time of trace log generation each such trace log or other report is automatically tagged with at least one of a a unique User Viewable Entity UVE identifying key UVEKey that identifies the corresponding dedicated virtual network Vnet and or other UVE to which the process of the respective report e.g. trace log belongs b a respective system tier identifying key TRxKey that identifies the corresponding system tier TRx e.g. Virtual to physical Forwarding Plane to which the process of the respective report belongs c a respective virtual and or physical execution machine identifying key VMKey PMKey that identifies the corresponding virtual machine VM and or physical machine PM to which the process of the respective report belongs d a respective process instance identifying key PINKey that identifies the corresponding instance PIN of a plurally instantiated and also identified process to which the respective report belongs and e a respective current operational state indication Op State that indicates a current operational state of a respective UVE Tier VM PM or other such stratifying attribute with which the respective report is associated.

The so tagged process reports e.g. trace logs exception reports etc. are then automatically relayed at or soon after the time of generation to a centralized database e.g. in a Global Analytics Plane so that they will not be lost due to local memory failures or faults and so that they can then be centrally queried by a system administrator e.g. who uses one or more of the added on tags e.g. UVEKey TRxKey VMKey PMKey PINKey Tier Op State other strata Op State etc. as part of structured queries into the centralized database e.g. maintained in a Global Analytics Plane of the massively distributed system for determining the more likely causes of later in time failures. The query able database may also be used even before a failure occurs to identify likely fault conditions that may lead to a failure where the likely fault conditions are determined based on past historical records in the database that indicate which of various strata Op States in combination with which of various exception reports are likely to lead to process failures.

More specifically the here disclosed teachings may be applied to a Distributed System e.g. a software defined network SDN system made up of multiple tiers each having a respective different functionality. Each tier is meant to be independently horizontally scalable consisting of multiple processes running on different machines with each process carrying out the same basic operations but on different instance of respective data. No one process within a tier may manage the entire tier. In other words the processes are peers and the tier is designed not to have a single point of failure that brings down the whole tier.

For sake of operations reliability and system maintenance the System Administrator Admin is empowered by the present disclosure to see the system as a collection of unique User Visible Entities UVEs . Each UVE has per tier attributes which help the Admin to confirm that the tiers are working correctly together or not. But the UVE alone does not have any per process representations. When normal operations are taking place the Admin does not need to be aware how many processes exist in a given tier or what their individual statuses are.

However the actual operations of the system are happening on a per tier per instance basis. Investigation of some types of complex problems may need examination of execution traces from the individual processes. The present disclosure empowers the Admin to efficiently query process execution traces by use of various UVE tier and or other tagging data contained in the traces. The present disclosure empowers the Admin to easily correlate process execution traces to respective UVEs. The present disclosure empowers the Admin to programmatically access UVE state changes and process execution traces and run analysis algorithms that will allow the Admin to catch and or predict problems automatically and or trigger the collection of more information.

In accordance with one aspect of the present disclosure a special Analytics Tier is provided which the user can connect to for looking at the dedicated UVEs and the massively distributed processes that belong to the respective UVE s. More specifically the following Data Structure and method may be used 

In the special Analytics Tier there is automatically defined one Op State object per UVE that represents the aggregated operational state of the object and which contains the following 

Individual Processes in various tiers are not allowed to generate free form text messages for their process execution trace in some examples. Instead in such examples they are forced to automatically generate objects containing the following information and send them up to the Analytics Tier 

The schema of Tier Op State can be different on a per UVE per Tier basis. When a respective process sends its process execution trace it may choose to fill in only a subset of the attributes rather than all. The reason is that place in the code where the process execution trace is being generated from may not have easy access to all attributes.

Once the respective processes reports are relayed to the centralized Analytics Tier Analytics Processing may proceed as follows. Different processes in the Analytics Tier may be made responsible for tracking the Operational States of different UVEs so that the Analytics layer is horizontally scalable. When an analytics process receives a process execution trace object it updates its view of the aggregated state of the given UVE. This process execution trace object can arrive as an event or batches of trace objects can be periodically read from a log.

In one example a Virtual Network System has at least 3 tiers in addition to the Analytics Tier . Tier 1 is the Config Tier configurators plane in with a respective one or more virtual machines therein and for sake of this example 3 processes A A and A executing in that tier. Tier 2 is ControlPlane Tier with a corresponding 5 processes B B executing in that tier for the sake of this example. Tier 3 is the SDN Forwarding Tier with 20 processes C C executing in that tier for the sake of this example.

Consider now a specific UVE say one representing a Virtual Network having the identification of being UV VNet 1 among a plurality of otherwise identified UVE s see of .

The corresponding Virtual Network exists on processes C C C and C of the SDN Forwarding Tier and each sends their versions of these attributes as part of their respective execution traces. For example some traces report a change in bytes received . Other traces report that an element has been added to or deleted from virtual machine names . In response the analytics process responsible for the identified Virtual Network e.g. gets these traces. It maintains and updates 4 different Tier 3 snapshots of the respective Virtual Network e.g. . Then when the analytics process gets a bytes received trace for process C it just needs to replace the bytes received attribute for corresponding snapshot C. But when it gets a virtual machine names item addition or deletion for process C it needs to add or delete from its virtual machine names attribute for snapshot C. Thus workload on the Analytics Tier may be minimized for normal process operations.

During process failures or error conditions it is possible that some process execution traces are lost . To mitigate this the method uses sequence numbers on a per UVE basis for traces sent from a process up to the Analytics layer. If there is a gap in sequence numbers the analytics layer can ask the process to replay the entire current state of that UVE on that process. Also in one example processes explicitly send a trace when a UVE disappears from the process.

Interactions between Admins and or other users and the Analytics layer may include the following When the system user asks for the Tier 3 state of this UVE the request is forwarded to the analytics process responsible for the respective Virtual Network e.g. . This analytics process may present the users with one single view by aggregating the 4 pieces of Op State snapshots of process C C C and C for example. This aggregation method may involve simple addition across these process views e.g. for bytes received or a union of sets e.g. for virtual machine names or other operations as well. The Analytics tier can do this on demand when a user or another program asks for this UVE or it can do this proactively and periodically.

In view of the above it is seen that a method is provided for realizing the goal of tying together with one or more commonly shared search keys such as a UVEKey a TRxKey a PINKey a VMKey a PMKey and or a strata Op State indicator individual trace logs and or other automatically generated reports of processes spread across a massively distributed system e.g. a software defined network SDN system so that those of the reports that are associated with one another by virtue of commonality to a specific UVE and or Tier and or virtual machine and or strata Op State etc. may be logically linked together for analysis purposes. The centralized and cross correlated reports that are automatically created by this method may be used to analyze complex failure modes and even to predict likely failures of particular components before the failures actually happen and to then responsively replace and or restart the likely to fail components and or to reconfigure resource capacities e.g. number of VM s number of PM s amount of physical memory etc. to reconfigure interconnects for getting around the likely to fail components before the latter actually fail. For instance this prediction ability may allow system operators to systematically bring down corresponding parts of the system during off peak hours and to replace and or fix the likely to fail components before actual failure thus minimizing the impact of likely failures on the overall system.

In accordance with the present disclosure a method is provided for globally analyzing down to the processes level the components of a massively distributed system and identifying likely at fault components in such a massively distributed complex system. The method includes one or more of the following steps 

 a subdividing the system into a plurality of tiers e.g. each characterized by having alike components e.g. VRouters within that tier 

 b subdividing system operations into a plurality of User Visible Entities UVE s e.g. virtual networks or Vnets each characterized by having respective processes and other resources dedicated to serving needs of the respective UVE s 

 c for each respective UVE and tier identifying respective process reports that cross correlate with a corresponding UVE Key and a corresponding Tier key where the reports may include quantitative failure or fault parameters such as memory failures telecommunications failures processor failures packet resends and or drops etc. and relaying the UVE and Tier tagged reports to a centralized and query able database 

 d for each respective process report that is locally generated automatically tagging the report with one or more linking keys including a UVEKey 

 e for each respective tier automatically determining what part of its resources are used by each of respective UVE s and automatically determining if the allocated resources of any UVE are insufficient due to repeated component failures e.g. lost packets 

 f for each respective UVE and its detected component failures logically associating the detected component failures with one or more of the respective captured parameter snapshots that immediately preceded the respective component failures for that UVE 

 g investigating those of the UVE associated reports that were correlated to failure as being likely to point to the at fault components and or tiers of that UVE and

 h taking preemptive corrective and or work around measures for those of the respective tier components and UVEs that were determined to be more highly likely to enter a failure mode based on the investigation.

Referring to shown here is a block diagram of an SDN system that includes for a respective one of its tiers e.g. the VRouters tier a corresponding reports classifier that is coupled to automatically repeatedly e.g. periodically receive parameter snapshots indicative of corresponding operating modes of the components e.g. virtual processes not shown in respective ones of the VRouters and to automatically tag them. More specifically during tagging mode each of the parameters snapshots is accompanied has added to it one or more of a respective UVE Key Tier Key VM Key PM key and or one or more Strata Op State indicators e.g. Tier Op State indicating whether the sample belongs to a failure Op State or a non failure Op State for example. The tagged reports from the various resources e.g. servers of the massively distributed system are then transmitted to a more centralized Analytics engine for structured aggregation according to respective UVE identifications and strata identifications. The aggregated reports may include data that classifies them as belonging to either a normal operations good class or as a distressed of failed bad e.g. as measured up from a 0 likely to be bad plane to a 100 likely to be bad plane along the Z axis . These aggregated outputs are stored in and used by the corresponding analytics engine to determine what is happening for each UVE and or tier on a big picture basis. In one example the corresponding analytics engine is coupled to a re configuration engine that in the case where a subsequently received and analyzed parameter snapshots indicate likelihood of failure re configures the system so as to preemptively try to avoid failure.

In one example the Analytics plane collects respective snapshot data relevant to likelihood of failure from various components within the respective UVE s tiers planes physical resources of the system. Respective snapshot data may include for example parameters like CPU utilization levels memory utilization levels alarm levels in the various system parts and so on. These collected respective and likely to be relevant snapshots could be early indicators of growing faults and or upcoming failures in respective tiers and or for respective UVE s e.g. ones that have greater demand for system bandwidths .

While shows by way of example the collecting of snapshots from the VRouters tier 232 239 of a respective one server it is understood that similar collections of respectively relevant and tagged parameter snapshots may be taking place for other tiers and or system planes and or servers across the massively distributed system and aggregated into the Analytics engine . The XYZ frame work shown in is for sake of simple illustration of aggregated and classified parameters and it is within the contemplation of the disclosure to have N dimensional mappings with each axis e.g. U V X Y etc. representing a respective one of the monitored parameters as distributed relative to UVE relative to tier relative to physical machine PM and so on. Part of the analytic investigation may include that of determining for each tier and UVE what parameters are best indicators of growing faults and or predictable failures. Trained classification algorithms may afterwards be used to predict the likelihood of failure of the respective components on a continuous basis as the data is being collected by the Analytics for newly instantiated UVE s and virtual processes.

Initially analytics engine may receive object trace data for a User Visible Entity UVE with e.g. key X from instance Y in tier Z . That is the UVE may specify an identifier of X for a key an identifier of Y for an instance and an identifier of Z for a tier where the identifiers X Y and Z may comprise respective numeric values e.g. integer values. Thus there may be a plurality of tiers and Z represents the Ztier Y represents a Yinstance within tier Z and X represents a key in the Yinstance of tier Z for a particular trace. Analytics engine may then store an object trace in a database using keys X Y and Z . In this manner keys X Y and Z can act as index values for uniquely identifying data for the trace in the database e.g. for updating querying retrieving or otherwise accessing data for the trace.

Analytics engine may then determine whether key X belongs to a current analysis instance . When analytics engine determines that key X for the trace does not belong to the current analysis instance NO branch of analytics engine may forward the created object trace to an analytics instance that owns key X . On the other hand assuming that the current analysis instance owns key X YES branch of analytics engine may determine whether the object trace is a full snapshot or deletion of a UVE on Y after receiving an object trace for UVE with key X of instance Y .

When the object trace is a full snapshot or deletion of the UVE on Y YES branch of analytics engine may update snapshot Y with data of received object X . That is for a full snapshot analytics engine may replace snapshot Y with the new contents from the object trace. Alternatively for deletion analytics engine may remove snapshot Y. In either case analytics engine may update a sequence number for snapshot Y of Object X. Furthermore analytics engine may build an aggregated state of object X for tier Z . This may involve iterating over all snapshots for Object X and doing appropriate calculations e.g. taking a sum of values a union of sets or the like to generate aggregate data for Object X. Analytics engine may then display the aggregated state for Object X.

On the other hand when the object trace is not a full snapshot or deletion NO branch of analytics engine may determine whether there was a gap in sequence numbers for data of the object with key X in instance Y . If analytics engine determines that there is such a gap YES branch of analytics engine may request for instance Y in tier Z a full snapshot of UVE X . In this manner in response to detecting a gap in the sequence numbers for a process of a tier analytics engine may send instructions to the device that is executing the process to replay a current state for the UVE.

Alternatively when there is not such a gap NO branch of analytics engine may update snapshot Y of object X . This may include for atomic types of attributes associated with object X replacing values of the atomic types with new values of the new snapshot. For add or delete notification container types analytics engine may add delete or modify a local copy of the container. For all cases analytics engine may update the sequence number for snapshot Y of object X. Furthermore analytics engine may build an aggregated state of object X for tier Z . This may involve iterating over all snapshots for Object X and doing appropriate calculations e.g. taking a sum of values a union of sets or the like to generate aggregate data for Object X. Analytics engine may then display the aggregated state for Object X.

As shown in the specific example of computing device includes one or more processors one or more communication units one or more input devices one or more output devices and one or more storage devices . Computing device in the specific example of further includes operating system virtualization module and one or more applications A N collectively applications . Each of components and may be interconnected physically communicatively and or operatively for inter component communications. As one example in components and may be coupled by one or more communication channels . In some examples communication channels may include a system bus network connection interprocess communication data structure or any other channel for communicating data. Virtualization module and applications as well as operating system may also communicate information with one another as well as with other components in computing device .

Processors in one example are configured to implement functionality and or process instructions for execution within computing device . For example processors may be capable of processing instructions stored in storage devices . Examples of processors may include any one or more of a microprocessor a controller a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within computing device during operation. Storage devices in some examples are described as a computer readable storage medium. In some examples storage devices are a temporary memory meaning that a primary purpose of storage devices is not long term storage. Storage devices in some examples are described as a volatile memory meaning that storage devices do not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in one example are used by software or applications running on computing device e.g. operating system virtualization module and the like to temporarily store information during program execution.

Storage devices in some examples also include one or more computer readable storage media. Storage devices may be configured to store larger amounts of information than volatile memory. Storage devices may further be configured for long term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs tape cartridges or cassettes optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM .

Computing device in some examples also includes one or more communication units . Communication units represent examples of network interfaces for communicating with external devices e.g. devices of an SDN that execute various processes e.g. processes conforming to various tiers as discussed above. Computing device in one example utilizes communication units to communicate with external devices. Communication units may communicate in some examples by sending data packets over one or more networks such as one or more wireless networks via inbound and outbound links. Communication units may include one or more network interface cards IFCs such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and WiFi radio components. In some examples computing device utilizes communication units to receive data regarding processes executed by external devices which processors may aggregate in accordance with the techniques of this disclosure.

Computing device in one example also includes one or more input devices . Input devices in some examples are configured to receive input from a user through tactile audio or video feedback. Examples of input devices include a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive screen.

One or more output devices may also be included in computing device . Output devices in some examples are configured to provide output to a user using tactile audio or video stimuli. Output devices in one example include a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output devices include a speaker a cathode ray tube CRT monitor a liquid crystal display LCD or any other type of device that can generate intelligible output to a user.

Computing device may include operating system . Operating system in some examples controls the operation of components of computing device . For example operating system in one example facilitates the communication of modules applications with processors communication units input devices output devices and storage devices . Applications may each include program instructions and or data that are executable by computing device . As one example application A may include instructions that cause computing device to perform one or more of the operations and actions described in the present disclosure.

In accordance with techniques of the present disclosure computing device may be configured to aggregate data for a set of two or more related processes to form aggregated data for the set of two or more related processes. In particular computing device may determine the set of two or more related processes executed by respective devices in a virtual network and receive data for the set of two or more related processes. In general the set of processes may correspond to a common tier e.g. a common network plane and each of the processes in a particular set may be substantially similar.

More particularly in accordance with the techniques of this disclosure computing devices communicatively coupled to computing device via communication units and processes executed by the computing devices may be divided into various tiers. Within each tier there may be a set of related e.g. substantially similar processes. Furthermore computing device may define User Visible Entities UVEs for the various tiers. The UVEs may define various data for monitoring processes of the various tiers. For example the UVEs may define attributes of processes to retrieve. Computing device may receive data output during execution of the processes and in accordance with the UVEs extract values for the attributes defined by the UVEs. Computing device may further aggregate this data. For example the UVE may define a manner in which to aggregate certain types of data corresponding to the attributes such as addition union over sets concatenation list generation or the like.

Computing device may then generate one or more reports that are indicative of a tier and aggregated values for one or more attributes corresponding to the tier as defined by a corresponding UVE. This aggregation can be performed transparently to the devices executing the processes. That is the devices executing the processes need not take any part in the aggregation. An administrator may use the generated report to diagnose various aspects of the virtual network. For example the report may include data indicative of one or more of a quantitative failure a fault parameter a memory failure a telecommunications failure a processor failure a packet resend and or a dropped communication session. The administrator may determine using the report whether any or all of these conditions apply and act accordingly e.g. by reprogramming a device communicatively coupled to computing device via communication units adding replacing or removing links between devices adding or upgrading software for one or more devices or the like based on the contents of the report.

In some examples computing device includes an analytics layer that is an intermediate layer that acts on generic rules. The UVEs may define rules in accordance with the analytics layer. Thus computing device may operate substantially automatically that is without user interference to perform the techniques of this disclosure. The analytics tier may use definitions of the UVEs to extract information from communications output by the devices executing the corresponding processes and aggregate values for certain attributes as defined by the UVEs of the communications.

In this manner computing device represents an example of a controller device configured to determine for a virtual network a set of two or more related processes executed by respective devices in the virtual network receive data for the set of two or more related processes and aggregate the data for the set of two or more related processes to form aggregated data for the set of two or more related processes.

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit including hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various techniques described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware firmware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware firmware or software components or integrated within common or separate hardware firmware or software components.

The techniques described in this disclosure may also be embodied or encoded in an article of manufacture including a computer readable storage medium encoded with instructions. Instructions embedded or encoded in an article of manufacture including a computer readable storage medium encoded may cause one or more programmable processors or other processors to implement one or more of the techniques described herein such as when instructions included or encoded in the computer readable storage medium are executed by the one or more processors. Computer readable storage media may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a compact disc ROM CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. In some examples an article of manufacture may include one or more computer readable storage media.

A computer readable storage medium comprises a non transitory medium. The term non transitory indicates that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

The techniques of this disclosure may also be embodied in transitory storage media such as signals and carrier waves. Such media are generally referred to as communications media. Thus computer readable media generally may correspond to 1 tangible computer readable storage media that is non transitory or 2 a communication medium such as a signal or carrier wave. Communication media such as signals and carrier waves are considered transitory and hence not considered non transitory.

Various examples have been described. These and other examples are within the scope of the following claims.

