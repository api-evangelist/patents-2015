---

title: Systems and methods for automatically aggregating write requests
abstract: Described herein are various systems and methods to automatically decide to aggregate data write requests in a distributed data store. A system initiates outgoing data write requests in synchronization with incoming data store commands, thereby facilitating low-latency read-back of the data. In response to an absence of data read requests, the system automatically changes such that each request includes two or more data sets, thereby breaking synchronization but consequently reducing traffic load on a switching network within the system. If the system later detects data read requests for previously stored data, the system will automatically change back to the original synchronized state, thereby decreasing the latency of accessing stored data. The system alternates between the modes of operation to achieve balance between low latency of data access and reduced traffic load on the switching network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477412&OS=09477412&RS=09477412
owner: Parallel Machines Ltd.
number: 09477412
owner_city: Tel Aviv
owner_country: IL
publication_date: 20150723
---
The present application is related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 089 453 filed on Dec. 9 2014 which is hereby incorporated by reference.

The present application is also related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 109 663 filed on Jan. 30 2015 which is hereby incorporated by reference.

The present application is also related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 121 523 filed on Feb. 27 2015 which is hereby incorporated by reference.

The present application is also related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 129 876 filed on Mar. 8 2015 which is hereby incorporated by reference.

The present application is also related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 141 904 filed on Apr. 2 2015 which is hereby incorporated by reference.

The present application is also related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 149 606 filed on Apr. 19 2015 which is hereby incorporated by reference.

The present application is also related to and claims priority under 35 USC 120 to U.S. Provisional Application No. 62 180 269 filed on Jun. 16 2015 which is hereby incorporated by reference.

In data systems an application programming interface APIs may be defined and created in which data write requests are generated and sent in synchronization with received data store commands such that each data write request is conveyed and stored in the system as a result of a respective data store command. This approach permits synchronization and low latency read back of the data but also increases communication load in the system. Conversely an API may be defined and created in which two or more data write requests are aggregated and conveyed together for storage in the system. This approach reduces communication load but can adversely affect latency if there is a read request for data previously included in a store command but which has not been actually stored yet due to aggregation of write requests. Needed are systems and methods that can reduce communication load without significantly increasing system latency.

Described herein are systems and methods that can reduce communication load associated with write requests without significantly increasing system latency. One embodiment is a system configured to automatically decide to aggregate data write requests in a distributed data store such that the system can alternate between individual data write requests conveyed and stored seriatim and multiple data write requests that are aggregated to reduce the load on a switching network. In one particular embodiment of such a system the system includes a plurality of memory modules constituting a distributed data store in which the plurality of memory modules are associated respectively with a plurality of data interfaces. In this embodiment the system includes also a switching network. In this embodiment the system includes also a first compute element configured to receive sequentially a sequence of store commands associated respectively with a sequence of data sets in which each of the store commands instructs the first compute element to store the respective data set in the distributed data store. In addition the first compute element is further configured to initiate as a result of the sequence a series of data write requests respectively with some of the commands and in synchronization therewith in which i each of the data write requests comprises one of the data sets to be conveyed and ii each of the data write requests is sent via the switching network to one of the data interfaces that consequently stores in the respective memory module the data set conveyed. In addition each of the data interfaces is configured to detect a condition in which i at least a certain number of the data write requests have arrived at the data interface while ii there has been essentially no attempt by any part of the system to read any of the data sets conveyed to the data interface thereby signifying that maintaining low latency through said synchronization is not critical. Consequently the first compute element as a response to one of the data interfaces signaling that the condition has been detected is further configured to send at least one aggregated data write request to the data interface in which the aggregated data write request conveys at least two of the data sets that have not yet been conveyed. The change from individual data sets to aggregated data sets breaks the synchronization and hence increases a latency associated with storage of the data sets but the change also achieves a reduction of the load on the switching network.

One embodiment is a method for signaling a compute element to start aggregating data write requests. In one particular form of such embodiment a data interface receives from a compute element a series of data write requests conveying respectively a series of data sets in which each of the data sets conveyed is to be stored by the data interface. Further the data interface stores each of the data sets conveyed as soon as the data set is conveyed such that the storing is performed in synchronization with said reception of the data write requests thereby facilitating low latency read back of the data sets already conveyed. Further the data interface detects a condition in which i at least a certain number of the data write requests have arrived at the data interface while ii there has been essentially no attempt by any element of the system to read back from the data interface any of the data sets conveyed to the data interface. Further as a result of the detection of this condition the data interface signals to the compute element that the condition has been detected thereby implying to the compute element that i it is not critical to maintain a low latency in conjunction with said storage of the data and therefore ii aggregation of data write requests is now possible. Further as a result of the signaling the data interface receives from a compute element at least one aggregated data write request in which the aggregated data write request conveys at least two additional data sets to be stored by the data interface.

In this description cache related memory transaction or a direct cache related memory transaction is a transfer of one or more data packets to or from a cache memory. A latency critical cache transaction is a cache transaction in which delay of a data packet to or from the cache memory is likely to delay execution of the task being implemented by the system.

In this description general communication transaction is a transfer of one or more data packets from one part of a communication system to another part where neither part is a cache memory.

In this description a communication transaction is a transfer of one or more data packets from one part of a communication system to another part. This term includes both cache related memory transaction and general communication transaction .

In this description a shared input output medium is part of a system that receives or sends both a data packet in a cache related memory transaction and a data packet in a general communication transaction. Non limiting examples of shared input output medium include a PCIE computer extension bus an Ethernet connection and an InfiniBand interconnect.

In this description an external I O element is a structural element outside of the system. Non limiting examples include a hard disc a graphic card and a network adapter.

In this description an external memory element is a structure outside the system that holds data which may be accessed by the system in order to complete a cache related memory transaction or other memory transactions.

In this description cache coherency is the outcome of a process by which consistency is achieved between a cache memory and one or more additional cache memory locations inside or external to the system. Generally data will be copied from one source to the other such that coherency is achieved and maintained. There may be a separate protocol called a cache coherency protocol in order to implement cache coherency.

In this description an electro optical interface is a structure that allows conversion of an electrical signal into an optical signal or vice versa.

In this description a prolonged synchronous random access read cycle is a synchronous RAM read cycle that has been lengthened in time to permit access from an external memory element.

In this description shared memory pool is a plurality of memory modules that are accessible to at least two separate data consumers in order to facilitate memory disaggregation in a system.

In this description simultaneously means essentially simultaneously . In other words two or more operations occur within a single time period. This does not mean necessarily that each operation consumes the same amount of time that is one possibility but in other embodiments simultaneously occurring operations consume different amounts of time. This also does not mean necessarily that the two operations are occurring continuously that is one possibility but in other embodiments an operation may occur in discrete steps within the single time period. In this description simultaneity is the action of two or more operations occurring simultaneously .

In this description efficiently is a characterization of an operation whose intention and or effect is to increase the utilization rate of one or more structural elements of a system. Hence to efficiently use a compute element is an operation that is structured and timed such that the utilization rate of the compute element is increased. Hence efficiently mixing and timing at least two key value transactions is an operation by which two or more needed data values are identified requested received and processed in such a manner that the utilization rate of the compute element in increased.

In this description utilization rate is the percentage of time that a structural element of a system is engaged in useful activity. The opposite of utilization rate is idle rate .

In this description a needed data value is a data element that is held by a server and needed by a compute element to complete a compute operation being conducted by the compute element. The phrase data value and the word value are the same as needed data value since it is understand that in all cases a value is a data value and in all cases a data value is needed by a compute element for the purpose just described.

In this description derive is the operation by which a compute element determines that a needed data value is held by one or more specific servers. The phrase derive sometimes appears as identify since the objective and end of this operation is to identify the specific server or servers holding the needed data value. If a needed data value is held in two or more servers in some embodiments the compute element will identify the specific server that will be asked to send the needed data value.

In this description request is the operation by which a compute element asks to receive a needed set of data or data value from a server holding that set of data or data value. The request may be sent from the compute element to either a NIC and then to a switched network or directly to the switched network. The request is then sent from the switched network to the server holding the needed data value. The request may be sent over a data bus.

In this description propagation of a request for a needed data value is the period of time that passes from the moment a compute element first sends a request to the moment that that the request is received by a server holding the needed data value.

In this description get is the operation by which a compute element receives a needed data value from a server. The needed data value is sent from the server to a switching network optionally to a NIC and then optionally to a DMA controller or directly to the DMA controller and from the DMA controller or the NIC or the switching network either directly to the compute element or to a cache memory from which the compute element will receive the needed data value.

In this description process is the operation by which a compute element performs computations on a needed data value that it has received. In other words the compute element fulfills the need by performing computations on the needed data element. If for example the social security number of a person is required the needed data value may be the person s name and number and the process may by the operation by which the compute element strips off the number and then applies it in another computation or operation.

In this description compute element is that part of the system which performs traditional computational operations. In this description it may be the part of the system that performs the derive request and process operations. In some embodiments the compute element also receives the needed data value from a server via a switching network a DMA and optionally a NIC. In other embodiments the requested data value is not received directly by the compute element but is received rather by the cache memory in which case the compute element obtains the needed value from the cache memory. A compute element may or may not be part of a CPU that includes multiple compute elements.

In this description executing the request is the operation during which a server that has received a request for a needed data value identifies the location of the needed data value and prepares to send the needed data value to a switching network.

In this description key value transaction is the set of all the operations in which a location of a needed data value is derived from a key the data value is requested optionally with the key sent by a compute element through a communication network to a server holding the data value the request received by the server executed by the server the data value sent by the server through the communication network gotten by the compute element and processed by the compute element.

In this description latency critical means that a delay of processing a certain request for a value may cause a delay in system operation thereby introducing an inefficiency into the system and degrading system performance. In some embodiments the period of time for a latency critical operation is predefined which means that exceeding that predefined time will or at least may degrade system performance whereas completing the operation within that period of time will not degrade system performance. In other embodiments the time period that is latency critical is predefined but is also flexible depending on circumstances at the particular moment of performing the latency critical operation.

In this description determining whether a compute element is authorized to access a particular data set in a shared memory pool is the process that determines whether a particular compute element in a system has been authorized by some reliable source to access a particular data set that is stored in a shared memory pool.

In this description accessing a data set encompasses any or all of entering an original value in a data set requesting to receive an existing data set receiving an existing data set and modifying one or more values in an existing data set.

In this description preventing delivery of a data set to a compute element is the process by which an access controller or other part of a system prevents such data set from being delivered to the compute element even though specifically requested by the compute element. In some cases denial of access is total such that the compute element may not access any part of the data set. In some cases denial access is partial such that the compute element may access part but not all of a data set. In some cases denial is conditional such that the compute element may not access the data set in its current form but the system may modify the data set such that the compute element may access the modified data set. The prevention of delivery may be achieved using various techniques such as blocking of communication interfering with electronic processes interfering with software processes altering addresses altering data or any other way resulting in such prevention.

In this description data set is a data structure that a compute element might access in order for the compute element to process a certain function. A data set may be a single data item or may be multiple data items of any number or length.

In this description a server may be a computer of any kind a motherboard MB or any other holder of structures for either or both of data memory and data processing.

In this description random access memory may include RAM DRAM flash memory or any other type of memory element that allows random access to the memory element or at least a random access read cycle in conjunction with the memory element. The term does not include any type of storage element that must be accessed sequentially such as a sequentially accessed hard disk drive HDD or a sequentially accessed optical disc.

In this description data interface is a unit or sub system that controls the flow of data between two or more parts of a system. A data interface may alter the data flowing through it. A data interface may handle communication aspects related to the flow of data such as networking. A data interface may access memory modules storing the data. A data interface may handle messages in conjunction with the two or more parts of the system. A data interface may handle signaling aspects related to controlling any of the parts of the system. Some possible non limiting examples of a data interface include an ASIC an FPGA a CPU a microcontroller a communication controller a memory buffer glue logic and combinations thereof.

In this description data corpus is the entire amount of data included in related data sets which together make up a complete file or other complete unit of information that may be accessed and processed by multiple compute elements. As one example the data corpus may be a copy of all the pages in the Internet and each data set would be a single page.

In this description a memory module is a physical entity in a system that stores data and that may be accessed independently of any other memory module in the system and in parallel to any other memory module in the system. Possible examples include a DIMM card or other physical entity that may be attached or removed from the system or a memory chip that is part of the system but that is not necessarily removed or re attached at will.

In this description data resiliency means the ability of a system to reconstruct a data set even if the system does not have all of the data that makes up that data set. Any number of problems may arise in that require data resiliency including without limitation i the destruction of data ii the corruption of data iii the destruction of any part of the operating application or other software in the system iv the corruption of any part of operating application or other software in the system v the destruction of a compute element erasure coding interface data interface memory module server or other physical element of the system and vi the malfunction whether temporary or permanent of a compute element erasure coding interface data interface memory module server or other physical element of the system. In all such cases the system is designed and functions to provide data resiliency to overcome the problem and thus provide correct and whole data sets.

In this description an external destination is a destination that is outside a system wherein such system may include a switching network compute elements and memory modules storing data sets. An external destination may be a data center a computer a server or any other component or group of components that are capable of receiving an electronic communication message.

In this description a respective block means the specific memory block within a flash memory that is associated with a particular linked list such that aggregation of all the changes indicated in the linked list will result in execution of all such changes when the memory block is rewritten.

In this description a portion of a row means several elements of a row from a matrix but not all of the elements of the row that are received and processed by one of the compute elements in the system. Also in this description a portion of a column means several elements of a column from a matrix but not all of the elements of the column that are received and processed by one of the compute elements in the system.

One embodiment is a system configured to mix cache related memory transactions together with general communication transactions over a shared input output medium. Various embodiments include a shared input output medium associated with a medium controller mc a cache agent ca and a first cache memory associated with said cache agent ca. Further in some embodiments the cache agent ca is configured to initiate init direct cache related memory transactions tran between the first cache memory and an external memory element via said shared input output medium . Further in some embodiments the medium controller mc is configured to block general communication transactions tran via said shared input output medium during the direct cache related memory transactions tran thereby achieving the mix of transactions without delaying the direct cache related memory transactions tran.

In one alternative embodiment to the system just described the medium controller mc includes a direct memory access DMA controller dma configured to perform the direct cache related memory transactions tran by executing a direct copy operation copy between the first cache memory and the external memory element via the shared input output medium .

In one possible variation of the alternative embodiment just described the direct memory access DMA controller dma is further configured to perform the general communication transactions tran by executing another direct copy operation copy in conjunction with an external input output element via the shared input output medium .

In a second alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions further the direct cache related memory transactions tran are latency critical cache transactions. Further the medium controller mc is configured to interrupt any of the general communication transactions tran and immediately commence the direct cache related memory transactions tran thereby facilitating the latency criticality.

In one possible variation of the second alternative embodiment just described further both said direct cache related memory transactions tran and general communication transactions tran are packet based transactions tran P and tran P is performed via the medium controller mc in conjunction with the shared input output medium . Further the medium controller mc is configured to stop stop on going communication of a first packet tran first P belonging to the general communication transactions tran via the shared input output medium and substantially immediately commence communication of a second packet tran second P belonging to the direct cache related memory transactions tran via the shared input output medium instead thereby achieving the interruption at the packet level.

In one possible configuration of the possible variation just described further the medium controller mc is configured to resume resume communication of the first packet tran first P after the second packet tran second P has finished communicating thereby facilitating packet fragmentation.

In a third alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions the shared input output medium is based on an interconnect element selected from a group consisting of i peripheral component interconnect express PCIE computer expansion bus pcie ii Ethernet eth and iii InfiniBand inf.

In one embodiment associated with the PCIE computer expansion bus pcie the medium controller mc may be implemented as part of a root complex root associated with said PCIE computer expansion bus pcie.

In one embodiment associated with the Ethernet eth the medium controller mc may be implemented as part of a media access controller MAC mac associated with said Ethernet eth.

In a fourth alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions further the direct cache related memory transactions tran and general communication transactions tran are packet based transactions tran P and tran P is performed via the medium controller mc in conjunction with said the shared input output medium . Further the medium controller mc is configured to deny access to the shared input output medium from a first packet tran first P belonging to the general communication transactions tran and instead to grant access to the shared input output medium to a second packet tran second P belonging to the direct cache related memory transactions tran thereby giving higher priority to the direct cache related memory transactions tran over the general communication transactions tran.

In a fifth alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions further there is at least a first compute element c associated with the cache memory and there is a memory controller associated with an external dynamic random access memory DRAM dram. Further the system is integrated inside a central processing unit CPU integrated circuit cpu and at least some of the general communication transactions tran are associated with the memory controller and DRAM dram.

In a sixth alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions further the system achieves the mix without delaying the direct cache related memory transactions tran which allows the system to execute cache coherency protocols in conjunction with the cache memory and the external memory element .

In a seventh alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions the shared input output medium includes an electro optical interface a and an optical fiber fiber ab operative to transport the direct cache related memory transactions tran and the general communication transactions tran.

In an eighth alternative embodiment to the system of mixing cache related memory transactions together with general communication transactions further including a first c and a second d electro optical interface both of which are associated with a first optical fiber fiber cd and are operative to transport the direct cache related memory transactions tran in conjunction with the medium controller and the external memory element .

In a possible variation of the eighth alternative embodiment just described further including a third e and a fourth f electro optical interface both of which are associated with a second optical fiber fiber ef and are operative to transport the general communication transactions tran in conjunction with the medium controller and an external input output element .

In a first alternative embodiment to the method just described further the cache performance is associated with a performance parameter selected from a group consisting of i latency and ii bandwidth.

In a second alternative embodiment to the method just described for mixing cache related memory transactions together with general communication transactions over a shared input output medium without adversely affecting cache performance further the general communication transactions tran are packet based transactions tran P performed via the medium controller mc in conjunction with the shared input output medium . Also the cache performance is associated with latency and this latency is lower than a time required to transmit a shortest packet belonging to said packet based transaction tran P.

In a first alternative embodiment to the method just described the cache performance is associated with a performance parameter selected from a group consisting of i latency and ii bandwidth.

In a second alternative embodiment to the method just described for mixing cache related memory transactions together with general communication transactions over a shared input output medium without adversely affecting cache performance further the general communication transactions tran are packet based transactions tran P performed via the medium controller mc in conjunction with the shared input output medium . Also the cache performance is associated with latency and said latency is lower than a time required to transmit a shortest packet belonging to said packet based transaction tran P.

One embodiment is a system configured to cache automatically an external memory element as a result of a random access read cycle tr R. In one embodiment the system includes a first random access memory RAM R a first interface i configured to connect the system with a first compute element c using synchronous random access transactions tr and a second interface i configured to connect connect the system with an external memory element . In some embodiments the system is configured to prolong tr prolong a synchronous random access read cycle tr R initiated by the first compute element c in conjunction with the first interface i when the synchronous random access read cycle tr R is detected to be addressed to a first memory location L of the external memory element currently not cached by the first random access memory R fetch L fetch via the second interface i from the external memory element at least one data element D associated with the first memory location L serve D serve to the first compute element c as part of said synchronous random access read cycle tr R prolonged via the first interface i the at least one data element D that was previously fetched thereby concluding successfully said synchronous random access read cycle tr R and optionally write D write the at least one data element D to the first random access memory R thereby caching automatically the first memory location L for faster future access by the first compute element c.

In one alternative embodiment to the system just described to cache automatically an external memory element further the first compute element is placed on a first motherboard MB the system is implemented on a first printed circuit board PCB having a form factor of a dual in line memory module DIMM DIMM such that the system is connected to the first motherboard MB like a dual in line memory module and such that said first compute element c perceives the system as essentially a dual in line memory module the external memory element is not placed on the first motherboard MB and the second interface i is an electrical optical interface i EO connected to said external memory element via an optical fiber fiber together operative to facilitate the connection connect.

In a second alternative embodiment to the system described above to cache automatically an external memory element further the synchronous random access read cycle tr R is performed using a signal configuration selected from a group consisting of i single data rate SDR ii double data rate DDR and iii quad data rate QDR .

In a third alternative embodiment to the system described above to cache automatically an external memory element further the prolonging tr R prolong of the synchronous random access read cycle tr R is done in order to allow enough time for the system to perform the fetch L fetch and further the synchronous random access read cycle tr R is allowed to conclude at such time that said serving D serve is possible thereby ending said prolonging tr R prolong.

In one possible variation of the third alternative embodiment just described further the synchronous random access read cycle tr R is performed over a double data rate DDR bus configuration and the prolonging tr R prolong is done using a procedure selected from a group consisting of i manipulating a data strobe signal belonging to said DDR bus configuration ii manipulating an error signal belonging to said DDR bus configuration iii reducing dynamically a clock frame of the DDR bus configuration iv adjusting dynamically a latency configuration associated with said DDR bus configuration and v any general procedure operative to affect timing of said synchronous random access read cycle tr R.

In a fourth alternative embodiment to the system described above to cache automatically an external memory element further a system controller cont is included and configured to fetch Li fetch add additional data elements Dn respectively from additional memory locations Ln of the external memory element where the additional memory locations are estimated based at least in part on the first memory location L and the memory locations are to be accessed in the future by said compute element c and write Dn write the additional data elements Dn fetched to the first random access memory R thereby caching automatically the additional memory locations Ln for faster future access by the first compute element.

In one possible variation of the fourth alternative embodiment just described further the writing Dn write of the additional data elements Dn is operated concurrently with additional tr R W add synchronous random access read cycles or synchronous random access write cycles made by the first compute element c in conjunction with the first interface i and the first random access memory R.

In one possible configuration of the possible variation just described further the concurrent operation is made possible at least in part by the first random access memory R being a dual ported random access memory.

One embodiment is a system configured to cache a shared memory pool using at least two memory modules including a first compute element c and a second computer element cn which are associated with respectively a first memory module m and a second memory module mn memory module where each of the compute elements is configured to communicate with its respective memory module using synchronous random access transactions tr. Also a shared memory pool connected with the first m and second mn memory modules via a first data link DL and a second data link DLn respectively. In some embodiments the system is configured to use the first m and second mn memory modules as a cache to the shared memory pool such that sets of data D cached on the first m or second mn memory modules are read tr R by the respective compute element using the synchronous random access transactions tr and other sets of data D that are not cached on the first m or second mn memory modules are fetched DL fetch from the shared memory pool into the first m or the second mn memory module upon demand from the memory module s respective compute element.

In one alternative embodiment to the system just described to cache a shared memory pool using at least two memory modules further the first m memory module is a first dual in line memory module DIMM DIMM .

In one possible variation of the alternative embodiment just described further the first compute element c is placed on a first motherboard MB the first dual in line memory module DIMM is connected to the first motherboard MB via a first dual in line memory module slot DIMM slot and the first data link DL includes a first optical fiber fiber .

In one possible configuration of the possible variation just described further the second mn memory module is a second dual in line memory module DIMM n the second compute element cn is placed on a second motherboard MB n the second dual in line memory module DIMM n is connected to the second motherboard MB n via a second dual in line memory module slot DIMM n slot the second data link DLn includes a second optical fiber fiber n the first MB and second MB n motherboard are placed in a first S and a second S n server respectively and the shared memory pool is placed in a third server server thereby facilitating distributed operation and memory disaggregation.

In a second alternative embodiment to the system described above to cache a shared memory pool using at least two memory modules further the first memory module m includes a first random access memory R operative to cache the sets of data D a first interface i configured to communicate with the first compute element c using the synchronous random access transactions tr and a second interface i configured to transact with the external shared memory pool via the first data link DL.

In a third alternative embodiment to the system described above to cache a shared memory pool using at least two memory modules further the sets of data D and other sets of data D are arranged in a page format P and Pn respectively. In some embodiments the system is further configured to conclude that at least some of the other sets of data D are currently not cached on said first memory module m to issue in the first compute element c a page fault condition to fetch DL fetch by the first compute element c at least one page Pn from said shared memory pool where the at least one page Pn contains at least some of the other sets of data D and cache the at least one page Pn in said first memory module m for further use.

In a fourth alternative embodiment to the system described above to cache a shared memory pool using at least two memory modules further the first memory module m is configured to facilitate the reading tr R of the sets of data D concurrently with the fetching DL fetch of the other sets of data D such that the fetching DL fetch of the other sets of data D does not reduce data throughput associated with the readings tr R.

In one possible variation of the fourth alternative embodiment just described further the first memory module m comprises a first random access memory R including a first D and a second D bank of dynamic random access memory DRAM . In some embodiments the concurrency is facilitated by the reading tr R in made from the first bank D in when the fetching DL fetch in is done with the second bank D in and by the reading tr R made from the second bank D in when the fetching DL fetch in is done with the first bank D in effectively facilitating operation of the first random access memory R as a dual ported random access memory.

One embodiment is a system configured to propagate data among a plurality of compute elements via a shared memory pool including a plurality of compute elements c cn associated with respectively a plurality of memory modules m mn where each compute element is configured to exchange ex data D with its respective memory module using synchronous random access memory transactions tr. In this embodiment further a shared memory pool is connected with the plurality of memory modules m mn via a plurality of data links DL DLn respectively. In some embodiments the system is configured to use the plurality of data links DL DLn to further exchange ext the data D between the plurality of memory modules m mn and the shared memory pool such that at least some of the data D propagates from one c of the plurality of compute elements to the shared memory pool and from the shared memory pool to another one cn of the plurality of compute elements.

One embodiment is a system configured to allow a plurality of compute elements concurrent access to a shared memory pool including a switching network operative to transport concurrently sets of data D D Dn associated with a plurality of memory transactions D TR D TR Dn TR. In this embodiment further a first plurality of data interfaces n configured to connect respectively a plurality of compute elements c c cn with the switching network . In this embodiment further a shared memory pool including a plurality of memory modules m m mk connected to the switching network via a second plurality of data interfaces k respectively wherein the shared memory pool is configured to store or serve the sets of data D D Dn concurrently by utilizing the plurality of memory modules concurrently thereby facilitating a parallel memory access by the plurality of compute elements c c cn in conjunction with the plurality of memory transactions D TR D TR Dn TR via the switching network.

One alternative embodiment to the system just described to allow a plurality of compute elements concurrent access to a shared memory pool further including a plurality of servers S S S n housing respectively the plurality of compute elements c c cn and a memory server S memory housing the switching network and the second plurality of data interfaces k. In some embodiments the first plurality of data interfaces n includes respectively a plurality of optical fibers fiber fiber fiber n configured to transport the plurality of memory transactions D TR D TR Dn TR between the plurality of servers S S S n and the memory server S memory. In some embodiments the at least one of the first plurality of data interfaces n is a shared input output medium. In some embodiments at least one of the plurality of memory transactions D TR D TR Dn TR is done in conjunction with at least one of the plurality of compute elements c c cn using synchronous random access transactions.

In a second alternative embodiment to the system described above to allow a plurality of compute elements concurrent access to a shared memory pool further the first plurality of data interfaces n include at least 8 eight data interfaces the plurality of memory modules m m mk include at least 8 eight memory modules and the plurality of memory transactions D TR D TR Dn TR has an aggregated bandwidth of at least 400 Giga bits per second.

In a third alternative embodiment to the system described above to allow a plurality of compute elements concurrent access to a shared memory pool further each of the plurality of memory modules m m mk is a dynamic random access memory accessed by the respective one of the second plurality of data interfaces k using synchronous random access memory transactions and the latency achieved with each of the plurality of memory transactions D TR D TR Dn TR is lower than 2 two microseconds.

In a fourth alternative embodiment to the system described above to allow a plurality of compute elements concurrent access to a shared memory pool further the switching network is a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network iii a cross bar switching network and iv an integrated circuit IC configured to multiplex said sets of data D D Dn in conjunction with said plurality of memory modules m m mk thereby facilitating said transporting concurrently of said sets of data D D Dn.

In a fifth alternative embodiment to the system described above to allow a plurality of compute elements concurrent access to a shared memory pool further including a second plurality of serves S S S k housing respectively the plurality of memory modules m m mk. In some embodiments the second plurality of data interfaces k includes respectively a plurality of optical fibers fiber fiber fiber k configured to transport the plurality of memory transactions D TR D TR Dn TR between the second plurality of servers S S S k and the switching network .

In some embodiments of the KVS is a shared memory pool from which includes multiple memory modules m m mk where each memory module is associated with a particular server. In as shown memory module m would be associated with first server a memory module m would be associated with second server b and memory module mk would be associated with third server c. However many different configurations are possible and a single server may include two or more memory modules provided that the entire system includes a multiplicity of memory modules and a multiplicity of servers and that all of the memory modules are included in at least two servers. In a configuration with memory modules the data values are stored in the memory modules for example data value v in memory module m data value v in memory module m and data value v in memory module mk but this is only one of multiple possible configurations provided that all of the data values are stored in two or more memory modules that are located in two or more servers. In some embodiments one or more of the multiple memory modules m m mk are based on random access memory RAM which may be a dynamic RAM DRAM or a flash memory in two non limiting examples and at least as far as read cycles are concerned thereby facilitating the execution of data value requests from the first compute element c. In some embodiments a memory module can execute a data value request in a period between 200 and 2 500 nanoseconds.

In in the bottom transfer of information a first compute element c uses a key here k to identify the server location of a needed data value here second data value v. The first compute element c then sends a request req to receive this data value v where such request req is sent to the switching network and then to the server holding the data value v here second server b.

In the specific embodiment shown in time flows from the top to the bottom actions of the first compute element c are illustrated on the left actions of the second server b are illustrated on the right and interactions between the first compute element c and the second server b are illustrated by lines pointing between these two structures in which information transfers are via the switched network . The server location e.g. the address of the server associated with a second needed data value is derived c der s by the first compute element c after which the first compute element c receives get a first needed data value that was previously requested and the first compute element c sends a new request for a second needed data value req to the second server b after which the first compute element c processes the first data value c pro v and the first compute element derives the server location of a third needed data value c der s after which the first compute element c receives get the second needed data value and the first compute element sends a future request req for the third needed data value after which the first compute element processes the second needed data value c pro v.

After the second server b receives from the switching network the new request for a second needed data value req the second server b executes this request req exe by locating optionally using the second key which is included in the new request req the needed data value within the server b and preparing to send it to the switching network . The period of time from which the first compute element c sends a new request for a second needed data value req until that request is received by the second server b is a request propagation time req prop. During the propagation period req prop the period during which the second server b executes the data request req exe and the time period get during which the second needed data value is transferred from the second server b to the first compute element c the first compute element c processes the first needed data value c pro v and in a first period derives the server location of the third needed data value c der s. This interleaving of activity between the various structural elements of the system increases the utilization rate of the first compute element c and thereby enhances the efficient usage of the first compute element c.

In the embodiment illustrated in processing of the first needed data value c pro v occurs before the derivation of server location for the third needed data value c der s. This is only one of multiple embodiments. In some alternative embodiments the derivation of server location for the third needed data value c der s occurs before the processing of the first needed data value c pro v. In other alternative embodiments the processing of the first needed data value c pro v occurs in parallel with the derivation of the server location for the third needed data value c der s. All of these embodiments are possible because in all of them the first compute element c continues to be utilized which means that the first compute element s c utilization rate is relatively high and therefore its usage is relatively efficient.

In one embodiment at least part of cache memory is dedicated for usage by only the first compute element c in conjunction with execution of the general tasks illustrated and described in thus ensuring performance and timing in accordance with some embodiments.

It will be understood that the particular embodiment illustrated in is only one of multiple possible embodiments. In some alternative embodiments there is only a single compute element but some of its sub structures are dedicated to the general tasks illustrated and described in whereas other of its sub structures executed unrelated processes. In some alternative embodiments there are two compute elements in which some sub structures of a first compute element c are dedicated to general tasks while others execute unrelated tasks and similarly some sub structures of a second compute element c are dedicated to general tasks while others execute unrelated tasks. In some alternative embodiments different sub structures within a compute element are either dedicated to general tasks or execute unrelated processes but the status of a particular sub structure will change over time depending on system characteristics processing demands and other factors provided that every instant of time there are some sub structures that perform only general tasks while other sub structures execute only unrelated processes.

One embodiment is a system operative to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys including a first compute element c associated with a first cache memory and a distributed key value store KVS including a plurality of servers a c configured to store a plurality of values v v v associated with a plurality of keys k k k in which the plurality of servers is communicatively connected with said first cache memory via a switching network . Further the system is configured to send from the first compute element c to a second b of the plurality of servers identified c der s using a second k plurality of keys via said switching network a new request req to receive a second v of the plurality of values associated with the second key k. Further the system is configured to receive get via said switching network from a first a of said plurality of servers into said first cache memory a first v of said plurality of values previously requested. Further after completion of the operations just described the system is further configured to process c pro v in the first compute element c in conjunction with the first cache memory the first value v received simultaneously with the second server b and switching network handling the new request req. The system is further configured to derive c der s in the first compute element c from a third k plurality of keys during a first period prior to receiving get and processing c pro v the second value v an identity of a third c of the plurality of servers into which to send a future request req for a third v of said plurality of values thereby facilitating said efficient usage.

In one alternative embodiment to the system just described to efficiently use a compute element the handling includes i propagation req prop of the new request req via the switching network and ii executing req exe the new request req by the second server b.

In one possible configuration of the alternative embodiment just described i the propagation req prop takes between 150 to 2 000 nanoseconds ii the executing req exe of the new request req takes between 200 and 2 500 nanoseconds and iii the processing c pro v takes between 500 and 5 000 nanoseconds. In this way the processing c pro v may extends over a period that is similar in magnitude to the handling thereby making said simultaneity possibly more critical for achieving the efficient usage. In one possible embodiment of the possible configuration described herein the distributed key value store is a shared memory pool that includes a plurality of memory modules m m mk wherein each of the plurality of servers a b c is associated with at least one of said plurality of memory modules m m mk and wherein the plurality of values v v v are stored in the plurality of memory modules m m mk.

In possible variation of the possible configuration described above the plurality of memory modules m m mk are based on random access memory thereby facilitating the executing req exe of the new request req taking between 200 and 2 500 nanoseconds. This possible variation may be implemented whether or not the distributed key value store is a shared memory pool .

In a second alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys the system is further configured to dedicate the first compute element c for i sending any one of the requests req req to receive respectively any one of the plurality of values v v ii processing c pro v c pro v any one of the plurality of values v v and iii deriving c der s c der s identities of any one of the plurality of servers b c using respectively any one of the plurality of keys k k. In this way there are minimized at least i a second period between the receiving get and the processing c pro v and ii a third period between the processing c pro v and the deriving c der s. This minimization of i and ii facilitates the efficient usage of a compute element c.

In a first variation to the second alternative embodiment described above The system further includes a second compute element c together with the first compute element c belonging to a first central processing unit CPU CPU and an operating system OS OS configured to control and manage the first c and second c compute element wherein the operating system OS is further configured to manage a plurality of processes comprising i said sending req receiving get processing c pro v and deriving c der s and ii other unrelated processes pr. Also the operating system OS is further configured to achieve the dedication by blocking the other unrelated processes pr from running on said first compute element c and by causing the other unrelated processes pr to run on the second compute element c.

In a second variation to the second alternative embodiment described above as a result of the dedication the simultaneity and the first cache memory the derivation c der s and the processing c pro v together account for at least 50 fifty per cent of time spent by the first compute element c over a period extending from a beginning of said sending req to an end of said deriving c der s. This utilization rate thereby achieves a high computational duty cycle which thereby allows the first compute element c to process the plurality of keys k k k and values v v v at an increased rate.

In a first configuration to the second variation to the second alternative embodiment described above further the period extending from the beginning of the sending to the end of the deriving is less than 10 ten microseconds.

In a second configuration to the second variation to the second alternative embodiment described above further the increased rate facilitates a sustained transaction rate of at least 100 000 one hundred thousand of the plurality of keys and values per second.

In a third alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys further the derivation is done by applying on the third key k a technique selected from a group consisting of i hashing ii table based mapping and iii any mapping technique either analytical or using look up tables.

In a fourth alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys further the first compute element c and the first cache memory belong to a first central processing unit CPU CPU such that the first compute element c has a high bandwidth access to the first cache memory thereby allowing the processing c pro v to conclude in less than 5 five microseconds.

In one possible configuration of the fourth alternative embodiment just described the high bandwidth is more than 100 one hundred Giga bits per second.

In a fifth alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys the system further comprises a direct memory access DMA controller configured to receive get the first value v via the switching network directly into the first cache memory .

In one a variation of the fifth alternative embodiment just described further the direct memory access controller frees the first compute element c to perform the identification c der s of the second server b simultaneously with the receiving get of the first value v. In this way the efficient usage is facilitated.

In a sixth alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys the system is further configured to send to the third c of the plurality of servers identified via said switching network the future request req to receive the third value v and to receive get via the switching network from the second server b into the first cache memory the second value v. The system is also configured after completion of the send and receive operations just described to process c pro v the second value v received simultaneously with the third server c and switching network handling of the future request req.

In a seventh alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys system further comprises a network interface card NIC configured to associate the first compute element c and the first cache memory to the said switching network . Also the network interface card is further configured to block or delay any communication currently preventing the network interface card from immediately performing the sending req thereby preventing the first compute element c from waiting before performing said sending thereby facilitating the efficient usage of the first compute element c.

In an eighth alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys further the deriving c der s is done simultaneously with the second server b and the switching network handling of the new request req.

In a ninth alternative embodiment to the system described above to efficiently use a compute element to process a plurality of values distributed over a plurality of servers using a plurality of keys the system further comprises a direct memory access DMA controller configured to receive get the second value v via the switching network directly into the first cache memory wherein the direct memory access controller frees the first compute element c to perform the processing c pro v simultaneously with the receiving get of the second value v. The operation described in this ninth alternative embodiment thereby facilitates efficient usage of the first compute element c.

In the various system embodiment described above the processing c pro v is depicted as occurring before the deriving c der s. However this particular order of events is not required. In various alternative embodiments the deriving c der s occurs before the processing c pro v. Also in different alternative embodiments the deriving c der s occurs in parallel with the processing c pro v.

In a first alternative embodiment to the method just described further the first compute element c derives c der s from a third of the plurality of keys k during a first period prior to receiving get and processing c pro v the second value v an identity of a third c of the plurality of servers into which to send a future request req for a third v of the plurality values.

The first compute element c and the distributed KVS are in communicative contact through a shared input output medium and a medium controller mc which together handle requests for data values from the first compute element c to the KVS and which handle also data values sent from the KVS to either the first compute element c or to the cache memory . In some embodiments the system includes also a direct memory access DMA controller which receives data values from the shared input output medium and medium controller mc and which may pass such data values directly to the cache memory rather than to the first compute element c thereby at least temporarily freeing the first compute element c.

In some embodiments illustrated in the KVS is a shared memory pool from which includes multiple memory modules m m mk and wherein one of the memory modules is configured to store the first value v. In some embodiments the multiple memory modules m m mk are based on random access memory thereby facilitating fast extraction of at least the desired value v. In some embodiments fast extraction can be executed in less than 3 three microseconds. In some embodiments the blocking of lower priority transactions tran enables sending of the new request req from in less than 3 three microseconds thereby matching timing of the extraction and consequently thereby facilitating overall fast key value transactions kv tran each such fast transaction taking less than 10 ten microseconds.

One embodiment is a system configured to interleave high priority key value transactions kv tran together with lower priority transactions tran over a shared input output medium including a shared input output medium associated with a medium controller mc a central processing unit CPU CPU including a first compute element c and a first cache memory and a key value store KVS communicatively connected with the central processing unit CPU via the shared input output medium . Further the central processing unit CPU is configured to initiate high priority key value transactions kv tran in conjunction with the key value store KVS said shared input output medium and the medium controller mc is configured to block lower priority transactions tran via the shared input output medium during at least parts of the high priority key value transactions kv tran thereby achieving the interleaving without delaying the high priority key value transactions kv tran.

In one alternative to the system to interleave transactions further the key value store KVS is configured to store a first value v associated with a first key k. Further the high priority key value transactions kv tran include at least a new request req from for the first value v wherein the new request req is sent from the first compute element c to the key value store via the shared input output medium and the new request req conveys the first key k to the key value store .

In some embodiments the key value store KVS is a distributed key value store including a plurality of servers a b c. In some forms of these embodiments the distributed key value store is a shared memory pool including a plurality of memory modules m m mk wherein one of the plurality of memory modules is configured to store the first value v. In some forms of these embodiments the plurality of memory modules m m mk are based on random access memory thereby facilitating fast extraction of at least the first value v. In some forms of these embodiments fast extraction is done in less than 3 three microseconds. In some forms of these embodiments the blocking of lower priority transactions tran enables sending of the new request in less than 3 three microseconds thereby matching timing of the extraction thereby consequently facilitating overall fast key value transactions each transaction taking less than 10 ten micro second.

In a second alternative to the system to interleave transactions further the high priority key value transactions kv tran are latency critical key value transactions and the medium controller mc is configured to interrupt any of the lower priority transactions tran and immediately commence at least one of the high priority key value transactions kv tran thereby facilitating said latency criticality.

In one possible configuration of the second alternative embodiment just described further both the high priority key value transaction kv tran and the lower priority transactions tran are packet based transactions performed via the medium controller mc in conjunction with the shared input output medium . Further the medium controller mc is configured to stop stop on going communication of a first packet tran first P belonging to the lower priority transactions tran via the shared input output medium and immediately to commence communication of a second packet kv tran second P belonging to the high priority key value transaction kv tran via the shared input output medium instead thereby achieving the communication interruption at the packet level.

In one possible variation of the configuration just described the medium controller mc is configured to resume resume communication of the first packet tran first P after the second packet kv tran second P has finished communicating thereby facilitating packet fragmentation.

In a third alternative to the system to interleave transactions further the shared input output medium is based on an interconnect element selected from a group consisting of i peripheral component interconnect express PCIE computer expansion bus pcie from ii Ethernet eth from and iii a network interface card NIC NIC.

In some embodiments associated with the PCIE computer expansion bus pcie from the medium controller mc may be implemented as part of a root complex root from associated with the PCIE computer expansion bus pcie.

In some embodiments associated with the Ethernet eth from the medium controller mc may be implemented as part of a media access controller MAC mac from associated with the Ethernet eth.

In some embodiments associated with the NIC NIC the medium controller mc may be implemented as part of a media access controller MAC mac associated with the NIC NIC. In some forms of these embodiments the NIC NIC is in compliance with Ethernet.

In a fourth alternative to the system to interleave transactions further both the high priority key value transactions kv tran and the lower priority transactions tran are packet based transactions performed via the medium controller mc in conjunction with the shared input output medium . Further the medium controller mc is configured to deny access to the shared input output medium from a first packet tran first P belonging to the lower priority transactions tran and instead grant access to the shared input output medium to a second packet kv tran second P belonging to the high priority key value transactions kv tran thereby giving higher priority to the high priority key value transactions kv tran over the lower priority transactions tran.

In a fifth alternative to the system to interleave transactions further the key value store is configured to store a first value v associated with a first key k. Further the high priority key value transactions kv tran include at least sending of the first value v from the key value store KVS to the central processing unit CPU via the shared input output medium .

In one possible configuration of the fifth alternative just described the system includes further a direct memory access DMA controller configured to receive the first value v via the shared input output medium directly into the first cache memory .

In a sixth alternative embodiment to the system to interleave transactions further the shared input output medium includes an electro optical interface a from and an optical fiber fiber ab from which are operative to transport the high priority key value transactions kv tran and the lower priority transactions tran.

In step as a result of the detection the medium controller mc stops handling of a first packet tran first P associated with a lower priority transactions tran via the shared input output medium . In step the medium controller mc commences transmission of the second packet kv tran second P via said shared input output medium thereby preventing the lower priority transactions tran from delaying the high priority key value transaction kv tran.

In a first alternative to the method just described for mixing high priority key value transactions kv tran together with lower priority transactions tran further the prevention leads to a preservation of timing performance of the high priority key value transactions kv tran wherein such timing performance is selected from a group consisting of i latency of the high priority key value transactions kv tran and ii bandwidth of the high priority key value transactions kv tran.

In a second alternative to the method described for mixing high priority key value transactions kv tran together with lower priority transactions tran further the prevention leads to a preservation of latency of the high priority key value transactions kv tran and as a result such latency of the high priority key value transactions kv tran is shorter than a time required to transmit a shortest packet belonging to said lower priority transactions tran.

In a first alternative to the method just described for mixing high priority key value transactions kv tran together with lower priority transactions tran further the prevention leads to a preservation of timing performance of the high priority key value transactions kv tran wherein such timing performance is selected from a group consisting of i latency of the high priority key value transactions kv tran and ii bandwidth of the high priority key value transactions kv tran.

In a second alternative to the method described for mixing high priority key value transactions kv tran together with lower priority transactions tran further the prevention leads to a preservation of latency of the high priority key value transactions kv tran and as a result such latency of the high priority key value transactions kv tran is shorter than a time required to transmit a shortest packet belonging to lower priority transactions tran.

In one embodiment said delaying comprises prioritizing the new request req ahead of the lower priority transaction tran or other network related activity such that lower priority transaction tran or other network related activity starts only after the communicating of the first key k.

One embodiment is a system configured to facilitate low latency key value transactions including a shared input output medium associated with a medium controller mc a central processing unit CPU CPU and a key value store comprising a first data interface and a first memory module m said first data interface is configured to find a first value v in said first memory module and extract said first value from said first memory module using random access read cycles and said key value store is communicatively connected with said central processing unit CPU via said shared input output medium . In one embodiment the central processing unit CPU is configured to initiate a high priority key value transaction kv tran in conjunction with said key value store by sending to said key value store via said shared input output medium a new request req for said first value v said new request comprising a first key k associated with said first value and operative to facilitate said finding and the medium controller mc is configured to block lower priority transactions tran via said shared input output medium thereby preventing said lower priority transactions from delaying said new request req thereby allowing the system to minimize a time between said sending of the new request to said extraction of the first value v. In one embodiment said prevention of delay and said random access read cycles together result in said minimization such that said time between said sending of the new request req to said extraction of the first value v is kept below 5 five microseconds. In one embodiment as a result from said minimization said high priority key value transaction kv tran results in the delivery of said first value v to said central processing unit CPU in less than 10 ten microseconds from said initiation.

In an alternative embodiment illustrated in a second compute element c is co located on the first server a with the first data interface and the first memory module m. The second compute element c is in communicative contact with the first data interface via a local data bus which could be for example a PCIE bus or Infiniband. The second compute element c requests req a second data set D from the first memory module m. The processing of the second request req is similar to the processing of the request req from the first compute element c. This second request req is sent to the first data interface . Simultaneously i the access controller determines if the second compute element c is authorized to access the second data set D while ii the first data interface in conjunction with the first memory module m perform a second random access read cycle RD D resulting in the retrieval of the second data set D. If the access controller determines that the second compute element c is authorized to access the second data set D then the second data set D is sent to the second compute element c over the local data bus . If the second compute element c is not authorized to access the second data set D then the access controller prevents delivery of the second data set D to the second compute element c.

In an alternative embodiment illustrated in a system is configured to allow or not allow a compute element to write a data set into the shared memory pool. In one embodiment a first compute element c requests to write a third data set into a third address located within the first memory module m. This third request is sent from the first compute element c over the communication network to the first data interface and the third data set is then temporarily stored in buffer TB. After the first compute element c sends this third request the first compute element c can continue doing other work without waiting for an immediate response to the third request. If the access controller determines that the first compute element c is authorized to write the third data set into the third address then the first data interface may copy the third data set into the third address within the first memory module m. If the first compute element is not authorized to write into the third address then the access controller will prevent the copying of the third data set into the third address within the first memory module m.

In an alternative to the alternative embodiment just described the requesting compute element is not the first compute element c but rather the second compute element c in which case the third request is conveyed by the local data bus and the rest of the process is essentially as described above all with the second compute element c rather than the first compute element c.

In the various embodiments illustrated in different permutations are possible. For example if a particular compute element be it the first c or the second c or another compute element makes multiple requests all of which are rejected by the access controller due to lack of authorization that compute element may be barred from accessing a particular memory module or barred even from accessing any data set in the system.

The communicative connection between the reliable source source and the secured configuration sec is any kind of communication link while encryption and or authentication techniques are employed in order to facilitate said secure configuration.

One embodiment is a system operative to control random memory access in a shared memory pool including a first data interface associated with a first memory module m belonging to a shared memory pool an access controller associated with the first data interface and with the first memory module m and a first compute element c connected with the first data interface via a communication network whereas the first memory module m is an external memory element relative to the first compute element c. That is to say there is not a direct connection between the first compute element c and the first memory module m e.g. the two are placed on different servers . Further the first data interface is configured to receive via the communication network a new request req from the first compute element c to access a first set of data D currently stored in the first memory module m. Further the first data interface is further configured to retrieve the first set of data D as a response to the new request req by performing at least a first random access read cycle RD D in conjunction with the first memory module m. Further the access controller is configured to prevent delivery of said first set of data D to said first compute element c when determining that said first compute element is not authorized to access the first set of data but such that the retrieval is allowed to start anyway thereby preventing the determination from delaying the retrieval when the first compute element is authorized to access the first set of data.

In one embodiment said retrieval is relatively a low latency process due to the read cycle RD D being a random access read cycle that does not require sequential access. In one embodiment the retrieval which is a relatively low latency process comprises the random access read cycle RD D and the retrieval is therefore executed entirely over a period of between 10 nanoseconds and 1000 nanoseconds thereby making said retrieval highly sensitive to even relatively short delays of between 10 nanoseconds and 1000 nanoseconds associated with said determination thereby requiring said retrieval to start regardless of said determination process.

In one alternative embodiment to the system operative to control random memory access in a shared memory pool the system includes further a second compute element c associated with the first memory module m whereas the first memory module is a local memory element relative to the second compute element. The system includes further a local data bus operative to communicatively connect the second compute element c with the first data interface . Further the first data interface is configured to receive via the local data bus a second request req from the second compute element c to access a second set of data D currently stored in the first memory module m. Further the first data interface is configured to retrieve the second set of data D as a response to said second request req by performing at least a second random access read cycle RD D in conjunction with the first memory module m. Further the access controller is configured to prevent delivery of the second set of data D to the second compute element c after determining that the second compute element in not authorized to access the second set of data.

In one possible configuration of the alternative embodiment described above further the access controller is implemented as a hardware element having a secured configuration function sec operative to set the access controller into a state in which the second compute element c is not authorized to access the second data set D. Further the secured configuration function sec is controllable only by a reliable source source that is not related to the second compute element c thereby preventing the second compute element c from altering the state thereby assuring that the second compute element does not gain access to the second data set D.

In a second possible configuration of the alternative embodiment described above further the second compute element c the first data interface the access controller and the first memory module m are placed inside a first server a. Further the first compute element c is placed inside a second server b which is communicatively connected with the first server a via the communication network .

In one variation of the second possible configuration described above further the first data interface the access controller and the first memory module m are packed as a first module module inside the first server a

In one option of the variation described above further the second compute element c is placed on a first motherboard MB. Further the first module module has a form factor of a card and is connected to the first motherboard MB via a first slot SL in the first motherboard.

In a second alternative embodiment to the system operative to control random memory access in a shared memory pool further the retrieval is performed prior to the prevention such that the retrieval is performed simultaneously with the determination thereby avoiding delays in the retrieval. Further the prevention is achieved by blocking the first set of data D retrieved from reaching the first compute element c.

In a third alternative embodiment to the system operative to control random memory access in a shared memory pool further the prevention is achieved by interfering with the retrieval after the determination thereby causing the retrieval to fail.

In a fourth alternative embodiment to the system operative to control random memory access in a shared memory pool further the shared memory pool is a key value store the first data set D is a first value v associated with a first key k the first key k is conveyed by said new request req and the retrieval comprises finding the first value v in the first memory module m using the first key k conveyed prior to the performing of the first random access read cycle RD D.

In one possible configuration of the fourth alternative embodiment described above further the authorization is managed by a reliable source source at the key value store level such that the first compute element c is authorized to access a first plurality of values associated respectively with a first plurality of keys and such that the first compute element is not authorized to access a second plurality of values associated respectively with a second plurality of keys wherein the first value v belongs to said second plurality of values.

In a fifth alternative embodiment to the system operative to control random memory access in a shared memory pool further the first memory module m is based on a random access memory RAM the first data set D is located in a first address associated with the random access memory and the first address is conveyed by the new request req.

In one possible configuration of the fifth alternative embodiment described above further the authorization is managed by a reliable source source at the random access memory address level such that the first compute element c is authorized to access a first range of addresses and such that the first compute element is not authorized to access a second range of addresses wherein the first data set D has an address that is within the second range of addresses. In some embodiments the random access memory RAM is DRAM. In some embodiments random access memory RAM is Flash memory.

One embodiment is a system operative to control random memory access in a shared memory pool including a first data interface associated with a first memory module m belonging to a shared memory pool an access controller and a temporary write buffer TB associated with the first data interface and the first memory module m and a first compute element c connected with the first data interface via a communication network whereas the first memory module m is a memory element that is external relative to the first compute element. Further the first data interface is configured to receive via the communication network a third request from the first compute element c to perform a random write cycle for a third set of data into a third address within the first memory module m. Further the first data interface is configured to temporarily store the third set of data and third address in the temporary write buffer TB as a response to the third request thereby allowing the first compute element c to assume that the third set of data is now successfully stored in the first memory module m. Further the first data interface is configured to copy the third set of data from the temporary write buffer TB into the third address within the first memory module m using at least one random access write cycle but only after said access controller determining that the first compute element c is authorized to write into the third address.

One embodiment is a system module operative to control data access in a shared memory pool including a first memory module m belonging to a shared memory pool configured to store a first D and a second D set of data. The system includes also a first data interface associated with the first memory module m and having access to i a first connection con with a local data bus of a second system MB and to ii a second connection con with a communication network . The system includes also an access controller associated with the first data interface and the first memory module m. Further the first data interface is configured to facilitate a first memory transaction associated with the first set of data D via the communication network between a first compute element c and the first memory module m. Further the first data interface is configured to facilitate a second memory transaction associated with the second set of data D via the local data bus between a second compute element c belonging to the second system MB and the first memory module m. Further the access controller is configured to prevent the second compute element c from performing a third memory transaction via the local data bus in conjunction with the first set of data D by causing the first data interface to not facilitate the third memory transaction.

In an alternative embodiment to the system module operative to control data access in a shared memory pool further the second system MB is a motherboard having a first slot SL and the first connection con is a connector operative to connect with said first slot.

In one possible configuration of the alternative embodiment just described further the first local bus is selected from a group of interconnects consisting of i peripheral component interconnect express PCIE computer expansion bus ii Ethernet and iii Infiniband.

In a second alternative embodiment to the system module operative to control data access in a shared memory pool further the communication network is based on Ethernet and the second connection con in an Ethernet connector. In one embodiment system module is a network interface card NIC .

In an alternative embodiment to the method just described for determining authorization to retrieve a first value v in a key value store while preserving low latency associated with random access retrieval further when the determination process results in a conclusion that the first compute element c is authorized to access said value v the access controller allows delivery of the retrieved value v to the first compute element c.

In an embodiment alternative to the embodiment shown in the internal registries R R and Rk are not part of data interfaces. Rather there is a separate module between the data interfaces k and the memory modules m m mk. This separate module includes one or more internal registries and the functions of the internal registries as described above are implemented in this separate module rather than in the data interfaces illustrated in .

One embodiment is a system that is operative to distributively process a plurality of data sets stored on a plurality of memory modules. One particular form of such embodiment includes a plurality of compute elements c c cn a shared memory pool with a plurality of memory modules m m mk configured to distributively store a plurality of data sets D D D D D D and a plurality of data interfaces k associated respectively with the plurality of memory modules m m mk. Further each of the data interfaces is configured to 

 i receive data requests DR DR from any one of the plurality of compute elements such as DR from c or DR from c 

 ii identify from the data sets D D of the memory module m the data sets D D that were not served yet 

 iii serve SR SR as replies to the data requests DR DR respectively the data sets identified D D respectively and

 iv keep track of the data sets already served such that as an example after responding with D to data request DR data interface keeps a record of the fact that D was just served and therefore data interface knows not to respond again with D to another data request such as DR but rather to respond with D to data request DR since D has not yet been served. Further each of the plurality of compute elements is configured to i send some of the data requests DR DR to at least some of the plurality of data interfaces respectively ii receive respectively some of the replies SR SR comprising some of the data sets D D respectively and iii process the data sets received Further the compute elements continue to send data requests receive replies and process data until a first condition is met. For example one condition might be that all of the data sets that are part of the data corpus are served and processed.

In one alternative embodiment to the system just described further the data requests DR DR DR do not specify certain which of the plurality of data sets D D D D D D should be served to the compute elements c c. Rather the identification and the keeping track constitute the only way by which the plurality of data interfaces k know which one of the plurality of data sets is to be specifically served to the specific compute element making the data request and thereby identification and keeping track constitute the only way by which the system insures that none of the data sets is served more than once. As a non limiting example when sending data request DR compute element c does not specify in the request that data set D is to be served as a response. The decision to send data set D as a response to data request DR is made independently by data interface based on records kept indicating that data set D was not yet served. The records may be kept within the internal register R of data interface .

In one possible configuration of the alternative embodiment just descried further the plurality of compute elements c c cn comprises a first compute element c and a second compute element c the plurality of data interfaces k comprises a first data interface including a first internal registry R that is configured to facilitate the identification and the keeping track and the plurality of memory modules m m mk comprises a first memory module m associated with the first data interface and configured to store a first data set D and a second data set D. Further the first compute element c is configured to send a first data request DR to the first data interface and the first data interface is configured to i conclude according to the first internal registry R that the first data set D is next for processing from the ones of the data sets D D stored in the first memory module m ii extract the first data set D from the first memory module m iii serve SR the first data set D extracted to the first compute element c and iv update the first internal registry R to reflect said serving of the first data set. Further the second compute element c is configured to send a second data request DR to the first data interface and the first data interface is configured to i conclude according to the first internal registry R that the second data set D is next for processing from the ones of the data sets D D stored in the first memory module m ii extract the second data set D from the first memory module m iii serve the second data set D extracted to the second compute element c and iv update the first internal registry R to reflect said serving of the second data set.

In one possible variation of the configuration just described further the plurality of data interfaces k comprises a second data interface including a second internal registry R that is configured to facilitate the identification and the keeping track and the plurality of memory modules m m mk comprises a second memory module m associated with said second data interface and configured to store a third data set D and a fourth data set D. Further the first compute element c is configured to send a third data request RD to the second data interface and the second data interface is configured to i conclude according to the second internal registry R that the third data set D is next for processing from the ones of the data sets D D stored in the second memory module m ii extract the third data set D from the second memory module m iii serve the third data set D extracted to the first compute element c and iv update the second internal registry R to reflect said serving of the third data set. Further the second compute element c is configured to send a fourth of said data requests to the second data interface and the second data interface is configured to i conclude according to the second internal registry R that the fourth data set D is next for processing from the ones of the data sets D D stored in the second memory module m iii extract the fourth data set D from the second memory module m iii serve the fourth data set D extracted to the second compute element c and iv update the second internal registry R to reflect said serving of the fourth data set.

In a second alternative embodiment to the system described to be operative to distributively process a plurality of data sets stored on a plurality of memory modules further the plurality of compute elements c c cn are configured to execute distributively a first task associated with the plurality of data sets D D D D D D by performing the processing of the data sets received.

In one possible configuration of the second alternative embodiment just described further the execution of the first task can be done in any order of the processing of plurality of data sets such that any one of the plurality of data sets can be processed before or after any other of the plurality of data sets. In other words there is flexibility in the order in which data sets may be processed.

In one possible variation of the configuration just described further the plurality of data sets D D D D D D constitute a first data corpus and the first task is selected from a group consisting of i counting number of occurrences of specific items in the first data corpus ii determining size of the data corpus iii calculating a mathematical property for each of the data sets and iv running a mathematical filtering process on each of the data sets.

In a third alternative embodiment to the system described to be operative to distributively process a plurality of data sets stored on a plurality of memory modules further each of the compute elements c c cn is configured per each of the sending of one of the data requests made by such compute element to select one of the plurality of data interfaces as a target of receiving such data request wherein the selection is done using a first technique. As a non limiting example compute element c chooses to send data request DR to data interface and then chooses to send data request DR to data interface but compute element c could have instead chosen to send data request DR to data interface k and in that event compute element c would have received a different data set such as data set D as a response to data request DR.

In one possible configuration of the third alternative embodiment just described further the first technique is round robin selection.

In one possible configuration of the third alternative embodiment just described further the first technique is pseudo random selection.

In one possible configuration of the third alternative embodiment just described further the selection is unrelated and independent of the identification and the keeping track.

In a fourth alternative embodiment to the system described to be operative to distributively process a plurality of data sets stored on a plurality of memory modules further the keeping track of the data sets already served facilitates a result in which none of the data sets is served more than once.

In a fifth alternative embodiment to the system described to be operative to distributively process a plurality of data sets stored on a plurality of memory modules further the first condition is a condition in which the plurality of data sets is served and processed in its entirety.

In one alternative embodiment to the method just described further the plurality of data sets is a plurality of values associated with a respective plurality of keys and the data requests are requests for the values associated with the keys. For example a plurality of values v v v all from may be associated respectively with a plurality of keys e.g. k k k all from and the data requests are requests for the values associated with the keys.

In one possible configuration of the alternative embodiment just described the plurality of compute elements c c cn do not need to keep track of which values have already been served because a record of served values is already kept by each data interface. Therefore the requests do not need to specify specific keys or values because the data interfaces already know which keys and values can still be served to the plurality of compute elements.

As it becomes available or is about to become available to process additional data sets the first compute element c sends a third data request DR to the first data interface G. The first data interface G concludes based on information in the internal registry RG that the first and second data sets have already been served to the compute elements but a third data set is next for processing so the first data interface G extracts f the third data set D from the shared memory serves SR the third data set to the first compute element c and updates the internal registry RG to reflect the serving of the third data set. The first compute element c continues to perform processing p of data sets related to the task here by processing the third data set received in response SR.

As it becomes available or is about to become available to process additional data sets the first compute element c sends a fourth data request DR to the first data interface G. The first data interface G concludes based on information in the internal registry RG that the first second and third data sets have already been served to the compute elements but a fourth data set is next for processing so the first data interface G extracts f the fourth data set D from the shared memory serves SR the fourth data set to the first compute element c and updates the internal registry RG to reflect the serving of the fourth data set. The first compute element c continues to perform processing p of data sets related to the task here by processing the third data set received in response SR.

It is understood that in all of the steps described above the compute elements c c can process data sets only after they have received such data sets from the first data interface G. The first data interface G however has at least two alternative modes for fetching and sending data sets to the compute elements c c. In one mode the first data interface G fetches a data set only after it has received a data request from one of the compute elements. This mode is reflected in element f in which the first data interface G first receives a data request DR from the first compute element c the first data interface G then fetches f the third data set and the first data interface G then serves SR third data set to the first compute element c. In a second mode the first data interface G first fetches the next available data set before the first data interface G has received any data request from any of the compute elements so the first data interface G is ready to serve the next data set immediately upon receiving the next data request from one of the compute elements c c. This mode is illustrated in f in which the first data interface G fetches a first data set prior to receiving the first data request DR from the first compute element c in f in which the first data interface G fetches a second data set prior to receiving the second data request DR from the second compute element c and in f in which the first data interface G fetches a fourth data set prior to receiving the fourth data request DR from the first compute element c. By this second mode there is no loss of time that might have resulted if the first data interface G were fetching a data set while the requesting compute element was waiting for data.

In one embodiment over the same first period per the second compute element c issues only one data request DR because the corresponding processing p of the corresponding second data set d requires long time and further processing by the second compute element c will not fit within the time period of per. In this way the second compute element c issues data requests at a rate that is associated to the processing capabilities or availability of the second compute element c.

As explained above each of the first compute element c and the first compute element c issues data requests in accordance with its processing capabilities or availability within a given time period. It is to be understood that data requests receiving of data sets and processing of data sets by the compute elements c and c are not synchronized and therefore are unpredictably interleaved. Further the compute elements c c are not aware of exactly which data set is received per each data request but the compute elements c c do not request specific data sets do not make the selection of which data sets they will receive and do not know which data sets have been received from the first data interface G. It is the first data interface G that decides which data sets to serve based on the records kept in the internal registry RG the data sets selected have never yet been served to the compute element c c and the data sets are served by the first data interface G in response to specific data requests from the compute elements c c. The keeping of records in the internal registry RG and the selection of data sets to be served based on those records allows the achievement of load balancing among the various compute elements c c and this is true whether or not the various compute elements have the same processing capabilities or processing availabilities.

One embodiment is a system operative to achieve load balancing among a plurality of compute elements c c cn accessing a shared memory pool . One particular form of such embodiment includes a shared memory pool configured to store and serve a plurality of data sets D D D D D D comprising at least a first data set D and a second data set D a first data interface G configured to extract and serve any of the plurality of data sets D D D D D D from the shared memory pool and comprising an internal registry RG configured to keep track of the data sets extracted and served and a plurality of compute elements c c cn comprising at least a first compute element c and a second compute element c wherein the plurality of compute elements c c are communicatively connected with the first data interface G and the plurality of compute elements c c are configured to execute distributively a first task associated with the plurality of data sets D D D D D D. Further the first compute element c is configured to send a first data request DR to the first data interface G after deciding that the first compute element is currently available or will be soon available to start or continue contributing to execution of the task i.e. processing one of the data sets and the first data interface G is configured to i conclude according to the records kept in the internal registry RG that the first data set D is next for processing ii extract f the first data set D from the shared memory pool iii serve SR the first data set extracted to the first compute element c for performing said contribution p i.e. processing data set D and iv update the internal registry RG to reflect the serving of the first data set D to the first compute element c. Further the second compute element c is configured to send a second data request DR to the first data interface G after deciding that the second compute element c is currently available or will be soon available to start or continue contributing to execution of the task and the first data interface G is configured to i conclude according to the internal registry RG reflecting that the first data set D has already been served that the second data set D is next for processing ii extract f the second data set from the shared memory pool iii serve SR the second data set extracted to the second compute element c for performing the contribution p i.e. processing data set D and iv update the internal registry RG to reflect the serving of the second data set D to the second server c. As herein described the decisions regarding the availabilities facilitate the load balancing in conjunction with the executing distributively of the first task all without the plurality of compute elements c c cn being aware of the order in which the plurality of data sets are extracted and served by the first data interface G.

In one alternative embodiment to the system just described further the plurality of data sets further comprises at least a third data set D. Also the first compute element c is further configured to send a next data request DR to the first data interface G after deciding that the first compute element c is currently available or will be soon available to continue contributing to the execution of the task and the first data interface G is configured to i conclude according to the internal registry RG that the third data set D is next for processing ii extract f the third data set from the shared memory pool iii serve SR the third data set extracted to the first compute element c for performing the contribution p i.e. processing data set D and iv update the internal registry RG to reflect the serving of the third data set D.

In one possible configuration of the first alternative embodiment just described further the next data request DR is sent only after the first compute element c finishes the processing p of the first data set D thereby further facilitating said load balancing.

In a second possible configuration of the first alternative embodiment just described further the first data request DR and next data request DR are sent by the first compute element c at a rate that corresponds to a rate at which the first compute element c is capable of processing p p the first data set D and the third data set D thereby further facilitating said load balancing.

In a second alternative embodiment to the above described system operative to achieve load balancing among a plurality of compute elements c c cn accessing a shared memory pool further the concluding and the updating guarantee that no data set is served more than once in conjunction with the first task.

In a third alternative embodiment to the above described system operative to achieve load balancing among a plurality of compute elements c c cn accessing a shared memory pool further the conclusion by said first data interface G regarding the second data set D is made after the second data request DR has been sent and as a consequence of the second data request DR being sent.

In a fourth alternative embodiment to the above described system operative to achieve load balancing among a plurality of compute elements c c cn accessing a shared memory pool further the conclusion by the first data interface G regarding the second data set D is made as a result of the first data set D being served SR and before the second data request DR has been sent such that by the time the second data request DR has been sent the conclusion by the first data interface G regarding the second data set D has already been made.

In a fifth alternative embodiment to the above described system operative to achieve load balancing among a plurality of compute elements c c cn accessing a shared memory pool further the extraction f of the second data set D from the shared memory pool is done after the second data request DR has been sent and as a consequence of the second data request DR being sent.

In a sixth alternative embodiment to the above described system operative to achieve load balancing among a plurality of compute elements c c cn accessing a shared memory pool further the extraction f of the second data set D from the shared memory pool is done as a result of the first data set D being served SR and before the second data request DR has been sent such that by the time the second data request DR has been sent the second data set D is already present in the first data interface G and ready to be served by the first data interface G to a compute element.

In one alternative embodiment to the method just described further the initial state is associated with a first task to be performed by the plurality of compute elements c c cn in conjunction with the first data corpus and the initial state is set among the first data interface G and the plurality of compute elements c c cn in conjunction with the first task thereby allowing the keeping record receiving and serving to commence.

In one possible configuration of the alternative embodiment just described said record keeping receiving and serving allow the plurality of compute elements c c cn to distributively perform the first task such that each of the plurality of compute elements c c cn performs a portion of the first task that is determined by the compute element itself according to the rate at which that compete element is making data requests to the first data interface G.

In one possible variation of the configuration just described the rate at which each compute element makes data requests is determined by the compute element itself according to the present load on the compute element or the availability of computational capability of the compute element.

In one option of the variation just described the data requests DR DR DR DR do not specify specific identities of the data sets D D D D D D to be served such that the specific identities of the data sets served are determined solely by the first data interface G according to the records kept by the internal registry RG thereby allowing the plurality of compute elements c c cn to perform the first task asynchronously thereby allowing the plurality of compute elements c c cn to achieve load balancing efficiently.

In a second possible configuration of the alternative embodiment described above the receiving of data requests and the serving of data sets in response to the data requests end when the entire first data corpus has been served to the plurality of compute element c c cn.

In a possible variation of the second configuration just described the execution of the first task is achieved after the entire data corpus has been served to the plurality of compute elements c c cn and after each of the compute elements has processed all of the data sets that were served to that compute element by the first data interface G.

In a third possible configuration of the alternative embodiment described above further the first data interface G performs on the plurality of data sets D D D D D D a pre processing activity associated with the first task after the extracting f f f f of the data sets and prior to the serving SR SR SR SR of the data sets.

It should be understood that there may be any number of servers or other pieces of physical hardware in the system and such servers or hardware may include any combination of the physical elements in the system provided that the entire system includes all of the compute elements c c ck all of the erasure coding interfaces k all of the data interfaces k and all of the memory modules m m mk plus whatever other hardware elements have been added to the system . For example one system might have a server including all of the memory modules and all of the data interfaces a separate server including all of the erasure coding interfaces and a separate server including all of the compute elements. Or alternatively there may be two more servers for the compute elements and or two or more servers for the erasure coding interfaces and or two or more servers for the data interfaces and memory modules. In alternative embodiments one or more compute elements may be co located on a server with one or more erasure coding interfaces and or one or more data interfaces and memory modules provided that all of the compute elements erasure coding interfaces data interfaces and memory modules are located on some server or other physical hardware.

One embodiment is a system operative to achieve data resiliency in a shared memory pool . One particular form of such embodiment includes a plurality of memory modules m m mk belonging to a shared memory pool and associated respectively with a plurality of data interfaces k a first erasure coding interface communicatively connected with the plurality of data interfaces k and a first compute element c communicatively connected with the first erasure coding interface . Further the plurality of memory modules m m mk are configured to distributively store a plurality of data sets D D such that each data set is distributively stored among at least two of the memory modules in a form of a plurality of data fragments coded using a first erasure coding scheme and each data fragment is stored on a different one of the at least two memory modules. As an example a first data set D may include first data fragment D stored in first memory module m second data fragment D stored in second memory module m and third data segment D k stored in third memory module mk. As another example as either a substitute for the first data set D or in addition to the first data set D there may be a second data set D including a first data fragment D stored in first memory module m a second data fragment D stored in second memory module m and a third data segment D k stored in third memory module mk. Further the first compute element c is configured to send to the first erasure coding interface a request DR for one of the data sets. For example the first erasure encoding interface may request a first data set D. Further the first erasure coding interface is configured to i convert the request into a first plurality of secondary data requests DR a DR b DR k ii send the first plurality of secondary data requests respectively into at least a first sub set of the plurality of data interfaces k iii receive as responses SR a SR b SR k at least a sub set of the plurality of data fragments D D D k associated with the one of the data sets D iv reconstruct rec the one of the data sets D using the first erasure coding scheme from the data fragments received D D D k and v send the reconstruction to the first compute element c as a response SR to the request DR made. Further each of the plurality of data interfaces that is each of k is configured to i receive from the first erasure coding interface one of the plurality of secondary data requests such as for example secondary data request DR a received at first date interface ii extract from the respective memory module such as for example from first memory module m associated with first data interface using a random access read cycle RA a one of the data fragments D associated with the one secondary data request and iii send SR a the data fragment D extracted to the first erasure coding interface as part of the responses received by the first erasure coding interface .

In a first alternative embodiment to the system just described further one of the plurality of memory modules m and its associated data interface are located in a first server S . Further the first erasure coding interface the first compute element c others of the plurality of memory modules m mk and others of the associated data interfaces k are all located outside the first server S . The ultimate result is that due to the uses of the first erasure coding interface and the first erasure coding scheme the system is a distributed system that is configured to endure any failure in the first server S and further that the reconstruction rec is unaffected by the possible failure in the first server S .

In one possible configuration of the first alternative embodiment just described the system includes also additional erasure coding interfaces m each configured to perform all tasks associated with the first erasure coding interface such that any failure of the first erasure coding interface still allows the system to perform the reconstruction rec using at least one of the additional erasure coding interfaces such as the second erasure coding interface instead of the failed first erasure coding interface .

In one possible variation of the configuration just described further the first erasure coding interface is located in a second server S while the additional erasure coding interfaces m the first compute element c the others of the plurality of memory modules m mk and the associated data interfaces k are all located outside said second server S . The result is that the system is further distributed and is configured to endure any failure in the second server S such that the reconstruction rec would still be possible even after a failure in the second server S .

In a second alternative embodiment to the above described system operative to achieve data resiliency in a shared memory pool the system further includes additional erasure coding interfaces m each of which is configured to perform all tasks associated with the first erasure coding interface . Further the system also includes additional compute elements c cn each of which is configured to associate with at least one of the erasure coding interfaces for example compute element c with erasure coding interface and compute element cn with erasure coding interface m in conjunction with erasure coding transactions such as rec and alike associated with the plurality of memory modules m m mk and the plurality of data fragments D D D k D D D k. As a result of the additions set forth in this second possible alternative each of the plurality of compute elements including the first compute element is configured to receive one of the data sets D reconstructed rec using at least one of the additional erasure coding interfaces and also the shared memory pool is configured to serve the plurality of data sets D D to the plurality of compute elements regardless of any failure in one of the memory modules m m mk.

In one possible option for the second alternative embodiment just described each erasure coding interface m is associated with one of the compute elements c c cn.

In another possible option for the second alternative embodiment just described each of the compute elements c c cn can use any one or any combination of the erasure encoding interfaces m thereby creating a resilient matrix of both data and erasure coding resources capable of enduring any single failure scenario in the system. In one possible option of this embodiment the different elements in the resilient matrix are interconnected using a switching network or an interconnect fabric .

In one possible configuration of the second alternative embodiment further the plurality of memory modules m m mk are based on dynamic random access memory DRAM at least 64 sixty four memory modules are included in the plurality of memory modules and the first erasure coding interface together with the additional erasure coding interfaces m are communicatively connected with the plurality of data interfaces k using a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network and iii a cross bar switching network. One result of this possible configuration is that a rate at which the data sets D D are being reconstructed rec is at least 400 Giga bits per second.

In a third alternative embodiment to the above described system operative to achieve data resiliency in a shared memory pool further the plurality of memory modules m m mk are based on random access memory RAM and therefore the random access read cycles RA a RA b RA k allow the extraction to proceed at data rates that support the first compute element c in receiving said data sets D D after said reconstruction rec at data rates that are limited only by the ability of the first compute element c to communicate.

In one possible configuration of the third alternative embodiment further the random access memory in memory modules m m mk is a dynamic random access memory DRAM and the first erasure coding interface is communicatively connected with the plurality of data interfaces k using a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network and iii a cross bar switching network. One result of this possible configuration is that a first period beginning in the sending of the request DR and ending in the receiving of the response SR to the request is bounded by five microseconds. In one embodiment said random access read cycles RA a RA b RA k are done simultaneously as facilitated by the plurality of data interfaces k acting together thereby facilitating said bound of five microseconds.

In a second possible configuration of the third alternative embodiment further the random access memory in memory modules m m mk is a dynamic random access memory DRAM and the first erasure coding interface is communicatively connected with the plurality of data interfaces k using a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network and iii a cross bar switching network. One result of this possible configuration is that a rate at which the data sets D D are being reconstructed is at least 100 Giga bits per second.

In a fourth alternative embodiment to the above described system operative to achieve data resiliency in a shared memory pool further the one of the data sets D is a first value v illustrated in associated with a first key k illustrated in and. A and the first value v is stored as one of the pluralities of data fragments D D D k in the plurality of memory modules m m mk. Further the request DR for one of the data sets D is a request for the first value v in which the request DR conveys the first key k. Further the first plurality of secondary data requests DR a DR b DR k are requests for the one of the pluralities of data fragments D D D k in which each of the requests for the one of the pluralities of data fragments conveys the first key k or a derivative of the first key k to the respective data interface k. Further the respective data interface k is configured to use the first key k or a derivative of the first key to determine an address from which to perform said random access read cycles RA a RA b RA k.

One embodiment is a system operative to stream data resiliently into a shared memory pool . One particular form of such embodiment includes a plurality of memory modules m m mk belonging to a shared memory pool and associated respectively with a plurality of data interfaces k a first erasure coding interface communicatively connected with the plurality of data interfaces k and a first compute element c communicatively connected with the first erasure coding interface . Further the first compute element c is configured to stream STR a plurality of data sets D D into the first erasure coding interface . Further the first erasure coding interface is configured to i receive the stream ii convert in real time each of the plurality of data sets D D in the stream into a plurality of data fragments for example first plurality D D D k and second plurality D D D k using a first erasure coding scheme and stream each of the pluralities of data fragments respectively into the plurality of data interfaces for example D D and D k into and k respectively such that a plurality of sub streams STR STR STRk of data fragments are created in conjunction with the plurality of data interfaces. Further each of the data interfaces k is configured to i receive one of said sub streams of data fragments for example receiving sub stream STR containing fragments D and D and ii write in real time each of the data fragments in the sub stream into the respective memory module for example into memory module m associated with data interface using a random access write cycle WR. One result of this embodiment is a real time erasure coding of the stream STR of data sets into the shared memory pool as facilitated by the first erasure coding interface and multiple random access write cycles WR WR WRk each of which is associated with a data interface k.

In an alternative embodiment to the system just described to stream data resiliently into a shared memory pool further the plurality of memory modules m m mk are based on random access memory RAM and therefore the random access write cycles WR WR WRk allow the writing to proceed at data rates that support the first compute element c in writing the data sets D D after said conversion at data rates that are limited only by the ability of the first compute element c to communicate.

In one possible configuration of the alternative embodiment just described further the random access memory m m mk is a dynamic random access memory DRAM and the first erasure coding interface is communicatively connected with the plurality of data interfaces k using a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network and iii a cross bar switching network. One result of this possible configuration is that any one of the data sets D D is written in the plurality of memory modules m m mk no later than 5 five microseconds from being put in said stream STR. In one embodiment said random access write cycles WR WR WRk are done simultaneously as facilitated by the plurality of data interfaces k acting together thereby facilitating said bound of five microseconds.

In a second possible configuration of the alternative embodiment described above to the system operative to stream data resiliently into a shared memory pool further the random access memory m m mk is a dynamic random access memory DRAM and the first erasure coding interface is communicatively connected with the plurality of data interfaces k using a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network and iii a cross bar switching network. One result of this possible configuration is that a rate at which the data sets D D are being written is at least 100 Giga bits per second.

The shared memory pool includes the memory modules m m mk including the data sets Dn D D respectively. Data interfaces are associated with the memory modules m m mk respectively and are communicatively connected with the switching network . shows the first memory module m and first data interface included in a separate server S whereas the other memory modules m mk and their respective data interfaces k are not included in separate server S or in any other separate server. However it is understood that any combination of separate servers is possible including no servers for any of the memory modules or data interfaces a single separate server for all of the memory modules and data interfaces each pair of a memory module and its associated data interface in a separate module or some pairs of memory modules and data interfaces in separate servers while other pairs are not in separate servers. The system also includes the gateway compute node gate which as shown in is in a separate server S but which in alternative embodiments may be part of another server with other element of the system and in additional alternative embodiments is not placed in a separate server.

The system achieves communication with the destinations DST via the memory network mem net while simultaneously achieving using the memory network mem net the access D TR D TR by the plurality of compute elements c c to the plurality of data sets D D Dn in conjunction with the first latency performance associated with such access. One result is that the low latency between the compute elements c c and the data sets D D Dn is preserved with no negative impact by communications between the compute element c c and the plurality of external destinations DST. The forwarded communication transmission TR with the external destinations DST that is from the gateway compute node gate to the external destinations DST uses a second communication protocol that may or may not be low latency since the latency of communication between the compute elements c c and the external destinations DST is generally less critical for system performance than latency between the compute elements c c and the data sets Dn D D.

One embodiment is a system operative to communicate with destinations DST external to the system via a memory network mem net. In a particular embodiment the system includes a gateway compute node gate a plurality of compute elements c c and a memory network mem net. In a particular embodiment the memory network mem net includes a shared memory pool configured to store a plurality of data sets D D Dn and a switching network . Further the plurality of compute elements c c are configured to access D TR D TR the plurality of data sets D D Dn via the switching network using a first communication protocol adapted for low latency transmissions thereby resulting in the memory network mem net having a first latency performance in conjunction with the access by the compute elements c c. Further the gateway compute node gate is configured to obtain obt from the plurality of compute nodes c c via the memory network mem net using the first communication protocol or another communication protocol adapted for low latency transmissions a plurality of general communication messages mes intended for a plurality of destinations DST external to the system . The gateway compute node gate is further configured to transmit TR the plurality of general communication messages mes to the plurality of destinations DST external to the system via a general communication network gn using a second communication protocol adapted for the general communication network gn. One result is that the system achieves the communication with the destinations DST via the memory network mem net while simultaneously achieving using the memory network the access D TR D TR to the plurality of data sets D D Dn in conjunction with said first latency performance.

In a first alternative embodiment to the system just described further the switching network is a switching network selected from a group consisting of i a non blocking switching network ii a fat tree packet switching network and iii a cross bar switching network thereby facilitating the access D TR D TR being simultaneous in conjunction with at least some of the plurality of data sets D D Dn such that at least one D of the data sets is accessed simultaneously with at least another D of the data sets thereby preventing delays associated with the access thereby further facilitating the first latency performance in conjunction with the first communication protocol.

In a second alternative embodiment to the system described above further the shared memory pool includes a plurality of memory modules m m mk associated respectively with a plurality of data interfaces k communicatively connected with the switching network in which the plurality of data sets D D Dn are distributed among the plurality of memory modules such that each data interface e.g. is configured to extract from its respective memory module e.g. m the respective data set e.g. D simultaneously with another of the data interfaces e.g. k extracting from its respective memory module e.g. mk the respective data set e.g. D and such that as a result at least one of the data sets e.g. D is transported to one of the compute elements e.g. c in conjunction with the access D TR simultaneously with at least another of the data sets e.g. D being transported to another of the compute elements e.g. c in conjunction with the access D TR thereby preventing delays associated with said access thereby further facilitating the first latency performance in conjunction with the first communication protocol.

In a first possible configuration of the second alternative embodiment just described further the memory modules m m mk are based on random access memory RAM in which the extraction of the data sets Dn D D is performed using random access read cycles thereby further facilitating the first latency performance in conjunction with said first communication protocol.

In a possible variation of the first possible configuration just described further the memory modules m m mk are based on dynamic random access memory DRAM in which the extraction of the data sets Dn D D is done in less than 2 two microseconds and the access D TR is done in less than 5 five microseconds.

In a second possible configuration of the second alternative embodiment described above further the obtaining obt includes writing by one or more of the compute elements c c the general communication messages mes into one or more of the memory modules m m mk and the obtaining obt includes also reading by the gateway compute node gate the general communication messages mes from the memory modules m m mk.

In a possible variation of the second possible configuration just described further the writing includes sending by one of the compute elements c c to one of the data interfaces k via the switching network using a packetized message associated with the first communication protocol one of the general communication messages mes and the writing further include writing one of the general communication messages mes by the specific data interfaces e.g. to the memory module e.g. m associated with that data interface using a random access write cycle.

In a possible option for the possible variation just described reading further includes reading one of the general communication messages mes by one of the data interfaces e.g. from the associated memory module e.g. m using a random access read cycle and reading also includes sending by the specific data interface e.g. to the gateway compute node gate via the switching network using a packetized message associated with the first communication protocol said one of the general communication messages mes.

In a third alternative embodiment to the system described above further the first communication protocol is a layer two L communication protocol in which layer three L traffic is absent from the memory network mem net thereby facilitating the first latency performance and the second communication protocol is a layer three L communication protocol in which layer three L functionality is added by the gateway compute element gate to the general communication messages mes thereby facilitating the transmission TR of general communication messages mes to those of the destinations DST that require layer three L functionality such as Internet Protocol IP addressing functionality.

In a fourth alternative embodiment to the system described above further the first communication protocol does not include a transmission control protocol TCP thereby facilitating the first latency performance and the second communication protocol includes a transmission control protocol in which relevant handshaking is added by the gateway compute element gate in conjunction with the general communication messages Imes when relaying the general communication messages to those destinations DST requiring a transmission control protocol.

In a fifth alternative embodiment to the system described above further the switching network is based on Ethernet.

In one configuration of the fifth alternative embodiment just described further the general communication network gn is at least one network of the Internet.

In a sixth alternative embodiment to the system described above further the first latency performance is a latency performance in which the access D TR D TR of any of the compute elements c c to any of the data sets D D Dn is done in less than 5 five microseconds.

In a seventh alternative embodiment to the system described above further the shared memory pool is a key value store in which said plurality of data sets D D Dn are a plurality of values v v v associated respectively with a plurality of keys k k k .

In one possible configuration of the seventh alternative embodiment just descried the system further includes a shared input output medium associated with a medium controller mc in which both the shared input output medium and the medium controller mc are associated with one of the compute elements c interchangeable with the compute element c illustrated in . Further the access D TR D TR is a high priority key value transaction kv tran . Further one of the compute elements e.g. c in conjunction with the first communication protocol is configured to initiate the high priority key value transaction kv tran in conjunction with the key value store via the shared input output medium . Further the medium controller mc in conjunction with the first communication protocol is configured to block lower priority transactions tran via the shared input output medium during at least parts of the high priority key value transactions kv tran thereby preventing delays in the access D TR D TR thereby further facilitating said first latency performance.

In a possible variation of the possible configuration just described further the shared input output medium is the switching network .

In an eighth alternative embodiment to the system described above further the obtaining obt includes sending by the compute elements c c the general communication messages mes to the gateway compute node gate using a packetized transmission associated with the first communication protocol directly via the switching network .

In a ninth alternative embodiment to the system described above the system further includes a first server S a second server S and a third server S . Further at least one of the compute nodes c c is located in the first server S at least a part of the shared memory pool such as a memory module m is located inside the second server S the gateway compute node gate is located inside the third server S and the switching network is located outside the first second and third servers. In this ninth alternative embodiment the memory network mem net facilitates memory disaggregation in the system .

One embodiment is a system configured to generate automatically a procedure operative to effectively combine fetching and processing of data sets. In a particular embodiment a first database DB is configured to store a code sequence code describing an action to be performed by a target system target on each of a plurality of data sets D D in which the action for each of the plurality of data sets comprises i fetching the data set from a data source data in the target system target and ii processing the data set fetched. Further a first compute element c has access to the first database DB and is configured to convert the code sequence code into a sequence of executable instructions exe includes a fetching sequence fetch in which the sequence of executable instructions exe when executed by a second compute element c of the target system target results in a fetching and processing procedure. Further the fetching and processing procedure is operative to fetch fetch from the data source data using said fetching sequence fetch a first data set D belonging to the plurality of data sets. The fetching and processing procedure is further operative to conclude that the first data set D has not yet been received R in the second compute element c due to a first latency associated with the fetch fetch and therefore in order to not lose time and instead of processing p the first data set D to fetch fetch from the data source data a second data set D belonging to the plurality of data sets. The fetching and processing procedure is further configured to process p the first data set D after the first data set D has been received R by the second compute element c.

In a first alternative embodiment to the system just described the fetching and processing procedure is further operative to process p the second data set D after it has been received by the second compute element c.

In a possible configuration of the first alternative embodiment just described the fetching and processing procedure is further operative to perform a sequence of additional fetches of additional ones of the plurality of data sets until all of the data sets have been fetched. In one embodiment of this possible configuration the additional fetches are done regardless of and asynchronously with reception of the additional ones of the plurality of data sets.

In one possible variation of the possible configuration just described the fetching and processing procedure is further operative to process each of such additional ones of the plurality of data sets as they are received until all of said data sets have been processed.

In a second alternative embodiment to the system described above to generate automatically a procedure operative to effectively combine fetching and processing of data sets the system from further includes the target system target and thus becomes a new system from .

In a first possible configuration of the second alternative embodiment just described further the conversion is performed by the first compute element c just before the second compute element c starts executing the sequence of machine executable instructions exe.

In a possible variation of the first possible configuration of the second alternative embodiment just described further the first compute element c communicates com the sequence of machine executable instructions exe to the second compute element c just before the second compute element c starts executing the sequence of machine executable instructions exe.

In a second possible configuration of the second alternative embodiment previously described further the first compute element c and the second compute element c are a single compute element.

In a third alternative embodiment to the system described to generate automatically a procedure operative to effectively combine fetching and processing of data sets further the code sequence code is written in a high level programming language and the conversion of the code sequence code into machine executable instructions exe is a compilation process.

One embodiment is a system configured to combine effectively fetching and processing of data sets according to an automatically generated procedure. In a particular embodiment the system includes a data source data configured to produce store or obtain a plurality of data sets D D a first compute element c and a second compute element c communicatively connected with said the source data. The system also includes a code sequence code describing an action to be performed on each of the plurality of data sets D D in which the action for each of the plurality of data sets comprises i fetching the data set from the data source data and ii processing the data set fetched. Further the first compute element c is configured to convert the code sequence code into a sequence of machine executable instructions exe including a fetching sequence fetch in which the sequence of machine executable instructions exe when executed by the second compute element c results in a procedure that combines effectively the fetching of the data sets and the processing of the data sets. Further the second compute element c is configured to receive com and execute the sequence of machine executable instructions exe thereby implementing the procedure for fetching and processing data sets. Further the fetching and processing procedure is operative to fetch fetch from the data source data using the fetching sequence fetch a first data set D belonging to the plurality of data sets. The fetching and processing procedure is further operative to conclude that the first data set D has not yet been received R in the second compute element c due to a first latency associated with the fetch fetch therefore in order to not lose time and instead of processing p the first data set D to fetch fetch from the data source data a second data set D belonging to the plurality of data sets. The fetching and processing procedure is further operative to process p the first data set D after it has been received R by the second compute element c.

In a first alternative embodiment to the system just described the system further includes a switching network and the data source data is a data interface k associated with a shared memory pool in which the data interface is communicatively connected with the second compute element c via the switching network . In this embodiment the second compute element c is interchangeable with the plurality of compute elements c c cn illustrated in .

In one possible configuration of the first alternative embodiment just described further the plurality of data sets D D is a plurality of values v v associated respectively with a plurality of keys k k . In this embodiment the switching network illustrated as element in is interchangeable with the switching network illustrated as element in . Further it should be understood that the key value system depicted as element in may be implemented in some embodiments using the shared memory pool illustrated as element in . In this sense the key value system and the shared memory pool may be considered interchangeable in the context of the data sets represented by the values associated with the keys in the key value system.

In one possible variation of the first possible configuration just described further the fetching sequence fetch is operative to send to the data interface k via the switching network one of the keys k in the plurality of keys k k k thereby allowing the data interface to find one of the values v in the plurality of values v v v that is associated with the one of the keys sent. The fetching sequence fetch is further operative to receive from the data interface k via the switching network the one of the values found v.

In one possible option to the possible variation just described further the first latency is associated with a time required for one of plurality of keys k k k to arrive at the data interface k. The first latency is further associated with a time required for finding the one of the values v. The first latency is further associated with the time required for the reception by the second compute element c of the one of the values v.

In a second alternative embodiment to the system described above that is configured to combine effectively fetching and processing of data sets according to an automatically generated procedure further the data source data is a shared input output medium . Further the fetching fetch fetch in conjunction with the data sets D D is associated with a memory transaction or a key value transaction kv tran conveyed via the shared input output medium .

In a first possible configuration of the second alternative embodiment just described further the first latency is associated with a delay in the fetching fetch fetch resulting from any general communication transaction tran conveyed via the shared input output medium .

In a second possible configuration of the second alternative embodiment described above further the shared input output medium is based on an interconnect element selected from a group consisting of i peripheral component interconnect express PCIE computer expansion bus ii Ethernet and iii Infiniband.

In a first alternative embodiment to the method just described further the second compute element s c conclusion that the first data set D has not yet been received R in the second compute element c is a default condition when no interrupt associated with reception of the first data set D has been detected by the second compute element c thereby indicating to the second compute element c that the first data set D is not yet ready for processing by the second compute element c thereby facilitating the fetching fetch of the second data set D by the second compute element c.

In one configuration of the first alternative embodiment just described further the second compute element s c conclusion that the first data set D has not yet been received R in the second compute element c is a passive conclusion in which the fetching fetch of the second data set D by the second compute element c is done automatically unless interrupted by the interrupt.

In a second alternative embodiment to the method described above for generating a sequence of executable instructions operative to combine effecting fetching and processing of data sets further the second compute element s c conclusion that the first data set D has not yet been received R in the second compute element c is reached by second compute element s c actively checking for reception R of the first data set D by the second compute element c.

In a third alternative embodiment to the method described above for generating a sequence of executable instructions operative to combine effecting fetching and processing of data sets further the code sequence code is a loop in which each iteration of the loop describes an action to be performed on one of the data sets for example on the first data set D of the plurality of data sets D D having an index associated with the iteration.

In a fourth alternative embodiment to the method described above for generating a sequence of executable instructions operative to combine effecting fetching and processing of data sets further the code sequence code describes a single instruction to be performed on multiple data in which the single instruction is associated with the processing and the multiple data are associated with the plurality of data sets D D. In some embodiments the single instruction to be performed on multiple data is an SIMD instruction.

One embodiment is a system configured to generate automatically a procedure operative to effectively combine fetching and processing of data sets. In a particular embodiment a first database DB is configured to store a code sequence code describing an action to be performed by a target system target on each of a plurality of data sets D D in which the action per each of the plurality of data sets comprises i fetching the data set from a data source data in the target system target and ii processing said data set fetched. Further a first compute element c having access to said first database DB is configured to convert the code sequence code into a sequence of executable instructions exe comprising a fetching sequence fetch in which said sequence of executable instructions exe when executed by a second compute element c of the target system target results in a fetching and processing procedure. Further the fetching and processing procedure is operative to fetch fetch from the data source data using said fetching sequence fetch a first data set D belonging to the plurality of data sets. The fetching and processing procedure is further operative to initiate a second fetch fetch from the data source data using said fetching sequence fetch for a second data set D belonging to the plurality of data sets in which the initiation is done prior to processing p of the first data set D that has not yet been received R in said second compute element c due to a first latency associated with the fetching fetch of the first data set D. The fetching and processing procedure is further configured to process p the first data set D after the first data set D has been received R by the second compute element c.

As shown in the first server S includes also a random access memory RAM which itself includes multiple change requirements each of which relates to a particular data set with a memory block of a flash memory flash. The various change requirements are accumulated in the random access memory RAM in the order in which they are received such that requirement D r is received before D r which is received before D r. These three change requirements all relate to a first data set D in a first memory block B within the flash memory flash. However the receipt of change requirements for a single data set will almost certainly be interspersed with other change requirements related to different data sets. As shown in change requirement D r related to data set D is received first then change requirement D r related to data set D in second memory block B then change requirement D r related to the first data set D then D r related to the second data set D and finally change requirement D r related to the first data set D.

The change requirements in random access memory RAM are executable as data entries where change requirement D r is executable as data entry D e then D r as D e D r as D e D r as D e and D r as D e. However in order to perform the execution of changes each executable data entry related to a particular data set is linked as part of a linked list from an earlier data entry to a later data entry. As shown in a first linked list LL B represented by a boldfaced line segment to the left of RAM links a first executable data entry D e related to a first data set D to a second executable data entry D e related to the first data set D and also the second executable data entry D e to a third executable data entry D e related to the first data set D. In the embodiment illustrated in the linkages are made through pointers. So for example D e has pointer pt which points at the physical address for D e and D e has pointer pt which points at the physical address for D e. In this way all of the executable data entries for a specific data set are linked and all of the data entries may be executed when the memory block holding the specific data set is rewritten. For example all of the data entries for the first data set D are executed when the first memory block B is rewritten.

Similarly a second linked list LL B shown in links all of the executable data entries related to the second data set D. In particular executable data entry D e related to the second data set D is linked via pointer pt to executable entry D e also related to the second data set D. When sufficient changes for the second data set D have been accumulated the second data block B is rewritten with all of these changes. Second linked list LL B is illustrated by the dotted line segment to the right of RAM connecting the dotted line boxes with the executable data entries D e and D e for the second data set D.

It should be noted that the last entry illustrated for the first data set D which is entry D e does not have a pointer and the reason is that there is nothing to point to because this is the last entry for that data set. If at some time a fourth change request is made for the first data set D then that fourth change request will generate a fourth executable data entry at which time a pointer will be added from the third entry D e to the new fourth executable data entry. Similarly there is no pointer for executable data entry D e related to the second data set D because this is the last executable data entry for the second data set D but if at some time a third change request is made for second data set D then that third change request will generate a third executable data entry at which time a pointer will be added from the second entry D e to the new third executable data entry.

The system illustrated in includes a flash memory flash which includes multiple memory blocks B B. As shown first memory block B delineated by a boldfaced rectangle includes a first data set D whereas second memory block B delineated by a rectangle created by a dotted line includes a second data set D. It will be understood that there may be thousands millions or even billions of memory blocks with the flash memory flash and that each memory block will hold one or more specific data sets. Also periodically the flash memory flash receives a request to read some data held in the flash memory. For example as shown in a request has been received to read some piece of data d of the first data set D stored in the first memory block B. In response to such request the first data set D is sent to the requesting compute element c which uses the aggregated change requirements D to update the first data set D into an updated data set D u which then extracts the required data d from the updated data set D u. It will be understood that the extracted first data d may or may not be identical to the original data requested d. If the aggregated change requirements D do not impact the specific data requested by the first compute element c then the original requested data d and the extracted data after update d will be exactly the same. If on the other hand the aggregated change requirements D do impact the specific data requested by the first compute element c then the original requested data d will be different from the extracted data after updated d where the first compute element c wishes to read and does read the updated data d .

As illustrated in the system includes also a second compute element c which executes many of the actions further described herein.

The embodiment illustrated in is illustrative only and non limiting in that alternative structures could achieve the same or similar effects. In alternative embodiments there may be multiple servers. For example a first server may include only compute elements c and cn whereas a second server may include the random access memory RAM a third server may include the flash memory flash and a fourth memory may include a second compute element c. As a different example the first server S may include in addition to the compute elements c cn and random access memory RAM as shown also the flash memory flash. As a different example the first server S may include only the compute elements c cn whereas a second server could include the random access memory RAM and the flash memory flash. Many different embodiments are conceivable in which one or more servers include various ones of the compute elements c c cn the random access memory RAM and the flash memory flash. The various alternative structures of the system may also be used to implement the various methods described below.

The embodiment illustrated in is illustrative and non limiting in that alternative pieces of hardware may execute some of the functions described above as being executed by some exemplary piece of hardware. For example as described above the first compute element c identifies linked lists traverses linked lists to accumulate change requirements makes requests to change a data set updates a data set and responds to requests by extracting data from an updated data set. Different ones of these functions may be executed by different hardware so for example one compute element might identify traverse and make requests but a separate compute element might update data sets and respond to requests by extracting data. As another example shows a single compute element c executing functions in the flash memory flash but these functions may be executed by multiple compute elements.

One embodiment is a system operative to use a plurality of linked lists LL B LL B for keeping track of changes to be made in data sets currently stored in a flash memory flash. In one particular embodiment the system includes a first compute element c and a first flash memory flash comprising a plurality of blocks B B currently storing respectively a plurality of data sets D D. The system further includes a first random access memory RAM comprising a plurality of linked lists LL B LL B associated respectively with the plurality of blocks B B in which each linked list for example LL B records a respective plurality of requirements D r D r to change the data set D of the respective block B and in which the plurality of requirements D r D r were made by the first compute element c and have been accumulated D e D e in the linked list LL B since the data set D was last written to the respective block B. Further the system is configured to identify one of the linked lists such as LL B as being currently in condition to be used for updating the respective block B. The system is further configured to traverse the linked list LL B identified in order to accumulate all the respective plurality of requirements D r D r into an aggregated requirement D to change the data set D of the respective block B. The system is further configured to change the data set D of the respective block B by performing a flash block write to the respective block B in conjunction with the aggregated requirement D .

In a first possible alternative to the system just described further each of the linked lists such as LL B comprises a plurality of data entries D e D e associated respectively with the plurality of requirements D r D r in which each of the data entries e.g. D e is linked to the next data entry D e using a pointer pt to an address associated with the next such data entry D e except that the most recent data entry does not yet have a pointer because it has no subsequent data entry to point to.

In a first possible configuration to the first possible alternative just described further the traversing of the linked list LL B is done by performing a random access read cycle to each of the data entries for example to D e in the linked list using the addresses as referenced by the pointers pt pt. As an example pointer pt stores the address associated with data entry D e such that data entry D e can be randomly accessed using the address stored in pointer pt. Entry D e can be randomly accessed using an address stored in a head pointer not depicted associated with block B in which such a head pointer points to the beginning of the linked list LL B and is thus used to start such traversing of the linked list LL B.

In a second possible configuration to the first possible alternative described above further all the pluralities of data entries as illustrated in first plurality D e D e D e and second plurality D e D e are stored in a single memory space for example a successive series of addresses within the first random access memory RAM thereby resulting is the plurality of linked lists LL B LL B being interlaced across the single memory space but such that the traversing is possible for each of the linked lists using the pointers for example pt and pt for linked list LL B .

In a second possible alternative to the system for using a plurality of linked lists as described above further identification of one of the linked lists LL B LL B as being currently in condition to be used for updated its respective block is based on the number of requirements in the plurality of requirements.

In one possible configuration of the second possible alternative just described further the linked list that is identified is the linked list associated with the highest number of requirements as compared with the other linked lists.

In a third possible alternative to the system for using a plurality of linked lists as described above further each of the plurality of requirements for example D r D r D r for a first data set D is stored in the respective linked list in this case first linked list LL B as part of a sequence of new data to be written to specific address in the respective block B.

In one possible configuration of the third possible alternative just described further the identification is based on the length of the sequences of new data such that the identification is made when the sequence of new data exceeds a certain threshold length.

In a fourth possible alternative to the system for using a plurality of linked lists as described above the system further comprises additional compute elements cn each of which is operative to make additional requirements to change the data set of at least some of the blocks and in which each such additional compute element necessitates an additional plurality of linked lists in support of the additional requirements thereby resulting is several pluralities of linked lists. Further the first random access memory is a shared memory pool in which is large enough to store the several pluralities of linked lists.

In a fifth possible alternative to the system for using a plurality of linked lists as described above the system further comprises a second compute element c associated with the first flash memory flash and a switching network in . Further the first random access memory RAM is located in a first server S together with the first compute element c. Further the first compute element c is configured to perform the identification traversing and accumulation in conjunction with the first random access memory RAM. Further the first compute element c is additionally configured to send the aggregated requirement D accumulated to the second compute element c via said switching network in . Further the second compute element c is configured to receive the aggregated requirement D and perform the changing using the aggregated requirement received.

In a sixth possible alternative to the system for using a plurality of linked lists as described above the system further comprises additional compute elements cn in which the first requirement D r is made by the first compute element c and the second requirement D r is made by one of the additional compute elements cn such that the link list LL B aggregates requirements from multiple sources.

One embodiment is a system operative to use a plurality of linked lists for keeping track of changes to be made in data sets currently stored in a flash memory flash. In one particular embodiment the system includes a first compute element c and a first flash memory flash comprising a plurality of blocks B B currently storing respectively a plurality of data sets D D. The system further includes a first random access memory RAM comprising a plurality of linked lists LL B LL B associated respectively with the plurality of blocks B B in which each linked list for example first linked list LL B records a respective plurality of requirements for first linked list LL B the requirements D r D r to change the data set D of the respective block B in which the plurality of requirements D r D r were made by the first compute element c and have been accumulated D e D e in the linked list LL B since the data set D was last written to the respective block B. Further the first compute element c is configured to make a new requirement D r to change the data set D of one of the blocks B. Further the system is configured to link the new requirement D r to the linked list LL B associated with said one of the blocks B thereby appending the new requirement D r to the plurality of requirements D r D r already associated with the one of the blocks B.

In a possible alternative to the system just described further each of the listed links for example first listed link LL B comprises a plurality of data entries D e D e associated respectively with the plurality of requirements D r D r in which each of the data entries for example D e for change requirement D r is linked to the next data entry in this example to D e using a pointer in this example pt to an address associated with the next such data entry except that the most recent data entry does not yet have a pointer because it has no subsequent data entry to point to. Further the new change requirement D r is associated with a new data entry D e. Further the linkage of the new requirement D r to the linked list LL B is performed by i adding in conjunction with the most recent data entry D e and using a first random access write cycle a new pointer pt to a new address to be associated with the new data entry D e and ii adding using a second random access write cycle at the new address the new data entry D e.

In a possible configuration of the possible alternative just described further the linkage is performed in less than one microsecond as a result of the first random access write cycle and the second random access write cycle being both random access cycles performed in conjunction with the first random access memory RAM.

In a first possible variation of the possible configuration just described further the first random access memory RAM is a first dynamic random access memory.

In a second possible variation of the possible configuration just described further the new data entry D e is less than 500 five hundred bytes in length thereby allowing low latency data writes in conjunction with small data objects such as the new data entry.

In a third possible variation of the possible configuration just described further the most recent data entry D e is located using a tail pointer not depicted associated with block B and thus preventing a need to traverse the linked list LL B in order to locate the most recent data entry D e thereby facilitating said linkage being performed in less than one microsecond.

One embodiment is a system operative to use a plurality of linked lists LL B LL B for keeping track of changes to be made in data sets currently stored in a flash memory. In one particular embodiment the system includes a first compute element c and a first flash memory flash comprising a plurality of blocks B B currently storing respectively a plurality of data sets D D. The system further includes a first random access memory RAM comprising a plurality of linked lists LL B LL B associated respectively with the plurality of blocks B B in which each linked list for example first linked list LL B records a respective plurality of requirements for first linked list LL B requirements D r D r to change the data set in this example first data set D of the respective block in this example B for first data set D in which the plurality of requirements in this example D r D r were made by the first compute element c and have been accumulated in this example D e D e in the linked list in this example LL B since the data set in this example D was last written to the respective block in this example B . Further the system is configured to i receive a first request to read a first data d associated with the data set in this example first data set D stored in one of the blocks in this example B ii read the data set in this example D from the one of the blocks in this example B and iii traverse the linked list in this example first linked list LL B associated with the one of the blocks in this example B in order to accumulate all the respective plurality of change requirements in this example D r D r into an aggregated requirement D to change the data set in this example first data set D of the one of the blocks in this example first memory block B . Further the system is configured i to update the data set read in this example first data set D using the aggregated requirement D into an updated data set D u and ii respond to the first request by extracting said first data d from the updated data set D u. In the embodiment herein described it is understood that first data d extracted may or may not be different than the original first data set d depending on whether the aggregated requirement D has changed the first data d. If the aggregated requirement D has changed the specific data that has been requested then d will be different and more updated than original and not yet updated d. If aggregated requirement D has not changed the specific data that has been requested then d will be the same as d since the original data would not be changed by a rewrite of the memory block in which the original data d is located.

In one embodiment the conversion of the code sequence code is a compilation process resulting in the sequence of executable instructions exe following which the sequence of executable instructions is distributed among the plurality of compute elements and data interfaces thereby facilitating the distributive fetching and processing procedure. The first compute element com which may act as a complier performing the conversion needs to recognize certain aspects of the data sets and certain aspects of how the data sets are to be processed before such first compute element continues with the conversion of code sequence code into the distributive fetching and processing procedure. As an example the first compute element com recognizes by inspecting the code sequence code that i the plurality of data sets D D D D D D are stored across the plurality of memory modules m m mk associated respectively with the plurality of data interfaces k that ii any one of said plurality of data sets can be processed before or after any other of said plurality of data sets and therefore that iii each fetch for one of the data sets can be done by any one of the plurality of compute elements in conjunction with the data interface associated with the memory module storing that data set asynchronously with any other of the fetches done by any other of the plurality of compute elements in conjunction with any one of the data interfaces. The abovementioned recognition may guarantee that the action described by code sequence code may indeed be successfully implemented by the distributive fetching and processing procedure.

In the embodiment illustrated in the target system and all of its components are not part of the base system although the base system and the target system are in communicative contact. In the embodiment illustrated in after the code source code has been converted by the first compute element com into the sequence of executable instructions exe the sequence of executable instructions exe may be executed immediately by the target system or alternatively may continue in an executable but not yet executed state until some time or condition or event occurs to have the target system execute the sequence of executable instructions exe.

In the system illustrated in the plurality of compute elements c c cn is configured to execute simultaneously the sequence of executable instructions exe resulting in a distributive fetching and processing procedure. By this procedure each of the plurality of compute elements for example c sends a plurality of data requests for example DR DR to at least some of the plurality of data interfaces for example in which each of the data requests is sent to one of the plurality of data interfaces for example DR sent to and DR sent to . Also by this procedure each of the plurality of compute elements for example c receives as a response to each of the data requests for example to data request DR in from the data interface receiving the data request for example a reply for example SR including a specific one of the data sets for example D stored in the memory module for example m associated with the data interface for example . Also by this procedure each of the compute elements processes the data sets it has received in response to the requests for data it has sent to one or more data interfaces. The sending of data requests receiving responses with data sets and processing of data sets received are continued by each of the plurality of compute elements c c cn until a first condition is met.

It is noted that in stating that the sequence of executable instructions exe is executed simultaneously by each of the plurality of compute elements c c cn of the target system it is understood that each of the compute elements c c cn runs an instance of the sequence of executable instructions exe in which each such instance runs asynchronously to the other instances of the sequence of executable instructions exe. One of the compute elements c c cn can run any part of the sequence of executable instructions exe while another of the compute elements c c cn can run any other part of the sequence of executable instructions exe at any given time.

One embodiment is a system configured to generate automatically a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules. In one particular form of such embodiment the system includes a first database DB configured to store a code sequence code describing an action to be performed by a target system on each of a plurality of data sets D D D D D D stored in a shared memory pool where the shared memory pool includes a plurality of memory modules m m mk associated respectively with a plurality of data interfaces k all belonging to the target system in which the action per each of the plurality of data sets includes i fetching the data set from the shared memory pool in the target system and ii processing said data set fetched. The system also includes a first compute element com having access to the first database DB and configured to convert the code sequence code into a sequence of executable instructions exe in which the sequence of executable instructions exe when executed simultaneously by each of a plurality of compute elements c c cn of the target system results in a distributive fetching and processing procedure. By this procedure each compute element for example c sends a plurality of data requests for example DR DR to at least some of the plurality of data interfaces for example in which each of the data requests is sent to one of the plurality of data interfaces. Also by this procedure the compute element for example c receives as a response to each of the data requests for example to data request DR in from the data interface receiving the data request for example a reply for example SR including a specific one of the data sets for example D stored in the memory module for example m associated with the data interface for example . Also by this procedure the compute element for example c processes the data sets it has received. The sending of data requests receiving of responses and processing of data sets received by each of the plurality of compute elements c c cn continues until a first condition is met.

In a first possible alternative to the system just described the specific one of the data sets for example D is selected by the data interface for example from the memory module for example m such as to guarantee that the specific one of the data sets has not previously been sent in conjunction with previous replies made by the data interface .

In one possible configuration of the first possible alternative just described further the sequence of executable instructions exe when executed simultaneously by each of the plurality of data interfaces k of the target system facilitates the selections the replies and therefore the guarantee.

It is noted that in stating that the sequence of executable instructions exe is executed simultaneously by each of the plurality of data interfaces k it is understood that each of the data interfaces k runs an instance of the sequence of executable instructions exe in which each such instance runs asynchronously to the other instances of the sequence of executable instructions exe. One of the data interfaces k can run any part of the sequence of executable instructions exe while another of the data interfaces k can run any other part of the sequence of executable instructions exe at any given time.

It is further noted that in stating that the sequence of executable instructions exe is executed simultaneously by each of the plurality of compute elements c c cn and that the sequence of executable instructions exe is also executed simultaneously by each of the plurality of data interfaces k it is understood that 

 i either the sequence of executable instructions exe is run by both the plurality of compute elements c c cn and the plurality of data interfaces k such that the plurality of compute elements c c cn run a first portion of the sequence of executable instructions exe that is dedicated for the compute elements while the plurality of data interfaces k run a second portion of the sequence of executable instructions exe that is dedicated for the data interfaces 

 ii or the sequence of executable instructions exe actually comprises a first set of executable instructions intended for the plurality of compute elements c c cn and a second set of executable instructions intended for the plurality of data interfaces k .

In a second possible configuration of the first possible alternative described above further each of the data interfaces k is configured to receive some of the plurality of data requests for example DR DR from any one of the plurality of compute elements for example from c c . Each of the data interfaces is configured also to identify from the data sets of the respective memory module for example m the ones of the data sets that were not yet served for example D D thereby facilitating the guarantee. Each of the data interfaces is configured also to serve as the reply for example SR SR to the data requests for example DR DR the data sets identified for example D D . Each of the data interfaces is also configured to keep track of the ones of the data sets already served thereby further facilitating the guarantee.

In a first possible variation of the second possible configuration just described further the data requests for example DR DR according to the sequence of executable instructions exe do not specify certain ones of the plurality of data sets to be served D D D D D D such that the identification and keeping track is the only way by which said plurality of data interfaces k know which one of the plurality of data sets D D D D D D is to be specifically served to the one of the plurality of compute elements c c cn making the data request and such that the identification and keeping track is the only way of making sure by the target system that none of the data sets D D D D D D is served more than once thereby facilitating the guarantee.

In a second possible variation of the second possible configuration described above further the sequence of executable instructions exe when executed simultaneously by each of the plurality of data interfaces k of the target system facilitates the identification the serving and the keeping track.

In a second possible alternative to the above described system configured to generate automatically a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules further the first condition is a condition in which the plurality of data sets D D D D D D is served and processed in its entirety by the plurality of compute elements c c cn . Further the plurality of compute elements c c cn is configured to execute distributively a first task associated with the plurality of data sets D D D D D D by performing the processing of the data sets received in which the execution of the first task can be done in any order of said processing of said plurality of data sets. In sum any one of said plurality of data sets can be processed before or after any other of said plurality of data sets.

In a third possible alternative to the above described system to generate automatically a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules further each of the compute elements c c cn is configured according to the sequence of executable instructions exe per each said sending of one of said data requests for example DR made by such compute element for example c to select one for example of the plurality of data interfaces k as a target of receiving such data request in which the selection is a round robin selection or a pseudo random selection. Further the selection of one of the plurality of data interfaces as a target of receiving such data request may be directed by load balancing considerations such that as an example a data interface that is relatively not loaded with data requests may be selected or such that data interfaces associated with a relatively large number of data sets may be selected more frequently.

In a fourth possible alternative to the above described system to generate automatically a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules further the code sequence code is written in a high level programming language and the conversion is a compilation process.

One embodiment is a system configured to generate and execute a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules. In one particular form of such embodiment the system includes a target system which itself includes a shared memory pool with a plurality of memory modules m m mk associated respectively with a plurality of data interfaces k which also includes a plurality of compute elements c c cn . The system also includes a first database DB configured to store a code sequence code describing an action to be performed by the target system on each of a plurality of data sets D D D D D D stored in the shared memory pool in which the action per each of the plurality of data sets includes i fetching the data set from the shared memory pool and ii processing the data set fetched. The system also includes a first compute element com having access to the first database DB and configured to convert the code sequence code into a sequence of executable instructions exe. Further the plurality of compute elements c c cn is configured to execute simultaneously said sequence of executable instructions exe resulting in a distributive fetching and processing procedure. By this procedure each of the plurality of compute elements c c cn sends a plurality of data requests for example DR DR to at least some of the plurality of data interfaces for example in which each of the data requests is sent to one of the plurality of data interfaces. Also by this procedure each of the compute elements receives as a response to each of the data requests for example to data request DR in from the data interface receiving the data request for example a reply for example SR comprising a specific one of the data sets for example D stored in the memory module for example m associated with the data interface for example . Also by this procedure each of the compute elements processes the data sets it has received. The sending of data requests receiving of responses and processing of data sets received by each of the plurality of compute elements c c cn continues until a first condition is met.

In a first possible alternative to the system just described further the first condition is a condition in which the plurality of data D D D D D D is served and processed in its entirety by the plurality of compute elements c c cn . Further the plurality of compute elements c c cn is configured to execute distributively a first task associated with said plurality of data sets D D D D D D by performing the processing of the data sets received in which the execution of said first task can be done in any order of said processing of said plurality of data sets. In sum any one of said plurality of data sets can be processed before or after any other of said plurality of data sets.

In a second possible alternative to the above described system configured to generate and execute a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules further the specific one of the data sets for example D is selected by the data interface for example from the memory module for example m such as to guarantee that said specific one of the data sets has not previously been sent in conjunction with previous replies made by the data interface for example .

In one possible configuration of the second possible alternative just described further the sequence of executable instructions exe when executed simultaneously by each of the plurality of data interfaces k facilitates the selections the replies and therefore the guarantee.

In a third possible alternative to the above described system configured to generate and execute a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules further the conversion is performed by the first compute element com just before the plurality of compute elements c c cn starts executing simultaneously the sequence of executable instructions exe.

In one possible configuration of the third possible alternative just described further the first compute element com communicates com the sequence of executable instructions exe to the plurality of compute elements c c cn just before the plurality of compute elements c c cn starts executing simultaneously the sequence of executable instructions exe.

In a fourth possible alternative to the above described system configured to generate and execute a procedure operative to distributively process a plurality of data sets stored on a plurality of memory modules further the first compute element com is one of the plurality of compute elements c c cn .

In a first possible alternative to the method described above in further the recognition is achieved by the first compute element com identifying in the code sequence code a loop in which each iteration of the loop describes the action to be performed on one of the plurality of data sets D D D D D D and further identifying in the code sequence code that the plurality of data sets D D D D D D are of a data type intended for storage across the plurality of memory modules m m mk .

In one possible configuration of the first possible alternative just described the recognition is further achieved by the first compute element com identifying in the loop that the processing of each of the plurality of data sets D D D D D D is unrelated to any other of the plurality of data sets D D D D D D.

In a second possible alternative to the method described above in the description of further the recognition is achieved by the first compute element com identifying in the code sequence code a single instruction to be performed on multiple data SIMD in which said single instruction is associated with the processing and the multiple data are associated with said plurality of data sets D D D D D D . Recognition is further achieved by the first compute element com identifying in the code sequence code that the plurality of data sets D D D D D D are of a data type intended for storage across said plurality of memory modules m m mk .

After conversion the instructions exe are sent to a second compute element c in the first server and a third compute element c in the second server . In the particular embodiment shown in both the first server and the second server are located outside the system each of these two servers is separate from the other. Further the first server includes the second compute element c the memory module m storing the first data set D prior to any processing and eventually a modified data set D that has been derived by the second compute element c from the first data set D using the first processing sub task in the instructions exe. The modified data set D is received by the second server where the third compute element c is configured to execute on this modified data set D the second processing sub task directed by the executable instructions exe.

One embodiment is a system configured to generate automatically a procedure operative to divide a processing task between at least two compute elements. In one particular form of such embodiment the system includes a first database DB configured to store a code sequence code describing an action to be performed on a first data set D stored in a memory module m in which the action comprises i fetching the first data set D from the memory module m and ii performing a first processing task on said first data set D fetched. The system further includes a first compute element c having access to the first database DB and configured to convert the code sequence code into a sequence of executable instructions exe in which the sequence of executable instructions exe is operative to instruct a second compute element c associated with the memory module m to i access the first data set D in the memory module m and ii perform a first processing sub task on said first data set accessed thereby resulting in a modified data set D . The sequence of executable instructions exe is further operative to instruct a third compute element c to i obtain the modified data set D and ii perform a second processing sub task on said modified data set obtained. The first processing sub task together with the second processing sub task constitute the first processing task thereby achieving the action distributively by the second compute element c and the third compute element c.

In a first possible alternative to the just described system configured to generate automatically a procedure operating to divide a processing task between at least two compute elements further the second compute element c is a data interface associated with the memory module m and the accessing of the first data set is a random access read cycle performed on the memory module m by the data interface . Such data interface may be the controller of the memory module m.

In a second possible alternative to the system configured to generate automatically a procedure operating to divide a processing task between at least two compute elements further the third compute element c is communicatively connected with the second compute element c via a switching network through which the obtaining of the modified data set D is done.

In a possible configuration of the second possible alternative just described further the memory module m is a part of a shared memory pool accessible to the third compute element c via the switching network .

In a possible variation of the possible configuration just described further the first processing sub task is a stored procedure.

In a second possible configuration of the second possible alternative described above further the second compute element c and the memory module m both reside in one server .

In a possible variation of the second possible configuration just described further third compute element c resides outside the one server and in a second server .

One embodiment is a system configured to generate and execute a procedure operative to divide a processing task between at least two compute elements. In one particular form of such embodiment the system includes a first database DB configured to store a code sequence code describing an action to be performed on a first data set D in which the action comprises i accessing the first data set D and ii performing a first processing task on the data set D accessed. The system further includes a first compute element c having access to the first database DB and configured to convert said the sequence code into a sequence of executable instructions exe. The system further includes a memory module m storing the first data set D. The system further includes a second compute element c associated with the memory module m and configured to execute the sequence of executable instructions exe resulting in a procedure operative to i access the first data set D in the memory module m and ii generate a modified data set D by performing a first processing sub task on the first data set D accessed. The system further includes a third compute element c configured to execute the sequence of executable instructions exe resulting in continuation of the procedure in which such continuation is operative to i obtain the modified data set D and ii perform a second processing sub task on said modified data set D obtained. The first processing sub task together with said second processing sub task constitute the first processing task thereby achieving the action distributively by the second compute element c and the third compute element c.

In a first possible alternative to the just described system configured to generate and execute a procedure operating to divide a processing task between at least two compute elements further the conversion is performed by the first compute element c just before the second compute element c starts executing the sequence of executable instructions exe.

In a possible configuration of the first possible alternative just described further the first compute element c communicates com the sequence of executable instructions exe to the second compute element c and to the third compute element c just before the second compute element c starts executing the sequence of executable instructions exe.

In a second possible alternative to the system configured to generate and execute a procedure operating to divide a processing task between at least two compute elements further the code sequence code is written in a high level programming language and further the conversion is a compilation process.

In a third possible alternative to the system configured to generate and execute a procedure operating to divide a processing task between at least two compute elements further the first compute element c and the second compute element c are a same compute element.

In a fourth possible alternative to the system configured to generate and execute a procedure operating to divide a processing task between at least two compute elements further the first compute element c and the third compute element c are a same compute element.

In a first possible alternative to the method described above in further the sequence of executable instructions exe is further operative to instruct the second compute element c associated with the memory module m to i receive the request ii access the first data set D in the memory module m as a response to the request iii perform the first processing sub task on the first data set accessed thereby resulting in the modified data set D and iv send the modified data set D to the third compute element c.

In a first possible configuration of the first possible alternative embodiment described above further the accessing is a random access read cycle performed by the second compute element c on the memory module m.

In a second possible configuration of the first possible alternative embodiment described above further the sequence of executable instructions exe or at least a portion thereof that is relevant for the second compute element c is delivered to the second compute element c by the third compute element c during run time of the third compute element c.

In a possible variation of the second possible configuration just described further the sequence of executable instructions exe is delivered to the third compute element c by the first compute element c just before run time of the third compute element c.

In a third possible configuration of the first possible alternative embodiment described above further the sequence of executable instructions exe is delivered to the second compute element c by the first compute element c just before run time of the third compute element c.

In a second possible alternative to the method described above in further the second compute element c and the memory module m are associated a priori with a stored procedure in which the stored procedure is the first processing sub task.

In a third possible alternative to the method described above in further the recognition that the first processing task is equivalent to performing the first processing sub task followed by performing the second processing sub task is achieved by matching the first processing task to a database comprising different combinations of sub tasks and resulting tasks.

In a fourth possible alternative to the method described above in further the recognition that the first processing task is equivalent to performing the first processing sub task followed by performing the second processing sub task is achieved by concluding from inspecting the code sequence code that the first processing task is explicitly described as the first processing sub task followed by the second processing sub task.

In a fifth possible alternative to the method described above in further the association of the second compute element c with the memory module m is identified by the first compute element c during the recognition by the first compute element s c concluding from inspecting the code sequence code that the first data set D is of a data type that is associated with the second compute element c and the memory module m.

In a sixth possible alternative to the method described above in further the association of the second compute element c with the memory module m is identified by the third compute element c during execution of the sequence of executable instructions exe by the third compute element c using information or operations available to the third compute element c during run time.

In some embodiments the second compute element c initiates the processing events by sending initiation requests to the compute elements c c. The sending of the initiation requests may be directed by a procedure pro of the system in which the procedure pro is associated with efficiently managing and communicating data across the system . Procedure pro may be used by the system to allocate a specific data set for processing in a specific compute elements such that as an example once compute element c initiates one of the processing events in conjunction with compute element c it is procedure pro that takes care of distributing the instance exe to compute element c and it is procedure pro that takes care of allocating the data set D to compute element c perhaps because the procedure pro is aware of a fact that data set D is stored in a memory module that is closer to compute element c than it is to compute element c. Each of the plurality of compute elements c c and the second compute element c are operative to execute at least a portion of the procedure pro relevant to that compute element. Procedure pro is aware of the underlying architecture of system such as communication network topology and data proximity to compute elements and is therefore best suited for making data allocation decisions as explained above. It is noted that the underlying architecture of system is not necessarily expressed in the code sequence code but it is known at the time of generating Procedure pro possibly to the designer of system and is therefore a property of system which is not derived from code sequence code. In addition it is noted that the description des of the first processing task to be performed in conjunction with the data sets D D is not known to the designer of system and therefore the system itself needs to convert description des into efficient executable instructions such as exe.

In various embodiments the system includes a sub system which is itself a distributed processing system d. This distributed processing system d includes in some embodiments the second compute element c with the plurality of general commands coms the first sequence of executable instructions exe the second c and third c compute elements and the procedure pro. In some embodiments the third compute element c includes data from the first data set D and the instance exe associated with the third compute element c plus the fourth compute element c includes data from the second data set D and the instance exe associated with the fourth compute element c.

The various embodiments as described herein provide a superior solution to the difficulty of effectively converting commands into instructions executable by compute elements within a system. On one hand it would be highly complex and possibly even unachievable to convert all of the high level commands in a code sequence code to efficient system tailored machine executable code. There are three reasons for this difficulty any one of which or all of which may be present in a data processing system. The first reason for the difficulty is that the instruction set is diverse. The second reason for the difficulty is that not all types of commands lend themselves to efficient conversion. The third reason for the difficulty is that not all types of commands can exploit or be tailored to a specific system architecture. On the other hand many high level commands do not need to be efficiently converted to executable instructions since the interpretation of such commands and or the conversion of such commands to even inefficient executable instructions presents a good enough way of following such commands. The way is good enough because execution of such commands does not present a real bottleneck when it comes to overall system performance and hence does not negatively impact system performance.

In short some commands and in particular specific task defining code within specific commands present problems of execution and must be handled specially whereas other commands in particular general commands do not need to be efficiently converted to executable instruction since the commands do not create a bottleneck and efficient conversion would not significantly improve system performance.

To solve these problems that is to improve system performance by giving special handling to specific task defining code within specific commands the embodiments proposed herein distinguish between multiple general commands coms on the one hand and on the other hand a specific command s com describing des a processing task to be performed in conjunction with multiple data sets. The specific command s com is recognized as special by the system which then converts at least the description des within the specific command into machine executable code that is optimally configured to be processed by the system . In this way the speed and efficiency of the system are enhanced. It is noted that the description des within the specific command s com which may be a mathematical expression or an algorithm written using the high level language may be relatively easily converted efficiently into executable instructions either directly or via an intermediary high level language such as the C language.

Various embodiments use a procedure pro in the system to communicate data across the system in which such procedure pro has various parts that may be executed by second compute element c and the multiple compute elements c c. In various embodiments the procedure pro may direct instances of the executable code exe to be executed by different compute elements for example c to execute exe and c to execute exe. In various embodiments the procedure pro may direct various of the data sets to different compute elements for example D to c and D to c. The procedure pro may be stored in any of the compute elements c c c c or memories thereof or may be stored in an entirely separate compute element. The procedure pro is optimized to take advantage of the requirements and capabilities of the specific elements in the distributed processing system d and in some embodiments of the requirements and capabilities of elements in the system other than those in the distributed processing system d. In some embodiments pro understands which data set such as D or D resides in a memory module associated with a specific compute element such as c or c and then directs the specific compute element to execute the task on the data set residing in that compute element for example compute element c executes a task on data set D and compute element c executes a task on data set D . In all of the embodiments in which the procedure pro is operable the procedure pro in some way or ways improves the speed and efficiency of the system . The procedure pro takes care of the management aspects of carrying our specific command s com in view of system architecture and may be independent of the description des appearing in s com while the first sequence of executable instructions exe which is totally dependent on the description des and directly derived from it takes care of efficiently executing the first processing task on compute elements c c.

In various embodiments the first compute elements c is aware of the requirements and capabilities of the specific compute elements c c c in the distributed processing system d. In these embodiments the first compute element c converts the description des in specific command s com in a manner to optimize the machine executable code exe for use and processing by these compute elements c c c. In these embodiments the efficiency of the conversion process is enhanced in order to improve the speed and efficiency of the system .

One embodiment is a system configured to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D. In one particular form of such embodiment the system includes a first code sequence code comprising i a plurality of commands coms and ii a specific command s com comprising a description des of a first processing task to be performed in conjunction with each of a plurality of data sets D D. The system includes also a first compute element c having access to the code sequence code and configured to i identify automatically by inspecting the specific command s com within the first code sequence code that the first processing task is to be performed in conjunction with each of said plurality of data sets D D and ii according to said identification convert specifically said description des into a first sequence of executable instructions exe constituting an efficient implementation of said first processing task. The system includes also a second compute element c having access to at least the plurality of commands coms and configured to first follow the plurality of commands coms until the specific command s com is reached and then initiate a plurality of processing events in which each of the processing events comprises executing an instance exe is one such instance exe is another of the first sequence of executable instructions exe in conjunction with one of the data sets D D e.g. a first processing event would be an execution of instance exe which may be identical to exe in conjunction with data set D thereby resulting in performing efficiently the first processing task in conjunction with each of the plurality of data sets D D.

In a first possible alternative to the system just described to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D the system further comprises a plurality of compute elements c c wherein the plurality of processing events occur respectively in the plurality of compute elements.

In a first possible configuration of the first possible alternative system described above further the instances exe exe of the first sequence of executable instructions exe are available to the plurality of compute elements c c through the system sending such instances exe exe to the plurality of compute elements c c or through the plurality of compute elements c c obtaining the instances exe exe from the system .

In a second possible configuration of the first possible alternative system described above further the plurality of data sets D D is available respectively to the plurality of compute elements c c through the plurality of compute elements c c respectively being pointed to the plurality of data sets D D by the system .

In a third possible configuration of the first possible alternative system described above further the initiation is done by the second compute element c sending a plurality of initiation requests respectively to the plurality of compute elements c c.

In a first possible variation the third possible configuration described above further the plurality of initiation requests also convey to the plurality of compute elements c c respectively the plurality of data sets D D that is data set D is conveyed to compute element c and data set D is conveyed to compute element c .

In a second possible variation of the third possible configuration described above further the plurality of initiation requests also convey to the plurality of compute elements c c respectively a plurality of instances exe exe of the first sequence of executable instructions exe that is instance exe is conveyed to compute element c and instance exe is conveyed to compute element c .

In a third possible variation of the third possible configuration described above further the sending of the plurality of initiation requests is directed by a procedure pro of the system in which the procedure pro is associated with managing and communicating data across the system such that each of the plurality of compute elements c c and the second compute element c is operative to execute at least a portion of the procedure pro relevant to that compute element.

In one possible option of the third possible variation described above further execution of the procedure pro together with execution of the instances exe exe of the first sequence of executable instructions exe implement the specific command s com.

In a fourth possible configuration of the first possible alternative system described above further the plurality of compute elements c c together with the second compute element c constitute at least a part of a distributed processing system d.

In a second possible alternative to the system described above to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D further the plurality of processing events occur in the second compute element c.

In a third possible alternative to the system described above to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D further the first compute element c and the second compute element c are a same compute element.

In a fourth possible alternative to the system described above to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D further the following of the plurality of commands coms is started after the conversion of the description des.

In a fifth possible alternative to the system described above to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D further the conversion of the description des is done after the following of at least some of the plurality of commands coms but before the initiation of the plurality of processing events.

In a possible configuration of the fifth possible alternative to the system described above further the conversion of the description des is performed just in time for the initiation of the plurality of processing events. That is to say the initiation of the plurality of processing events begins right after the conversion of the description des.

In a sixth possible alternative to the system described above to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D further the following of the plurality of commands coms is an interpretation process associated with the plurality of commands.

In a possible configuration of the sixth possible alternative to the system described above further the initiation of the plurality of processing events is performed instead of the second compute element c interpreting the specific command s com thereby further facilitating the performing efficiently of the first processing task in conjunction with each of the plurality of data sets D D.

In a seventh possible alternative to the system described above to perform efficiently a first processing task in conjunction with each of a plurality of data sets D D further the code sequence code is a source code written in a high level language the description of the first processing task is written using the same high level language and the conversion of the description des is a compilation process.

In a first possible alternative to the method described above in further the distributed system identifies by inspecting the specific command s com within the first code sequence code that the first processing task as described in the specific command s com is to be performed in conjunction with each of the plurality of data sets D D. Further the obtaining of the first sequence of executable instructions by the distributed processing system is done as a result of such identification.

In a possible configuration of the first possible alternative to the method described in further the obtaining of the first sequence of executable instructions exe by the distributed processing system comprises performing the conversion of the description des in which the conversion is done by the distributed processing system .

In a possible variation of the possible configuration just described further the code sequence code is a source code written in a high level language the description des of the first processing task is written using the high level language as part of the source code and the conversion of the description des is a compilation process.

In one option of the possible variation just described further the following the plurality of commands coms by the additional compute element c is an interpretation process associated with the plurality of commands coms.

In a second possible alternative to the method described above in further the specific command s com is a single instruction to be performed on multiple data SIMD in which the single instruction is associated with the first processing task and the multiple data is associated with the plurality of data sets D D. Further said identifying automatically that the first processing task is to be performed in conjunction with each of said plurality of data sets is done by identifying that the specific command is the single instruction to be performed on multiple data SIMD in which such identification may be directed by recognizing a certain format associated with the specific command.

In a third possible alternative to the method described above in further the specific command s com is a map function associated with a lambda operator and an array of data in which the lambda operator is the description des of the first processing task and the array of data is the plurality of data sets D D. Further said identifying automatically that the first processing task is to be performed in conjunction with each of said plurality of data sets is done by identifying that the specific command is the map function associated with the lambda operator and an array of data in which such identification may be directed by recognizing a certain format associated with the map function.

Various systems and methods are described herein to efficiently realize user defined matrix mathematical operations in a distributed processing system. The user defined operation will be executed on two matrices. Each matrix may include any number of scalar values but as in all matrix operations the number of scalar values in a row of one matrix will be equal to the number of scalar values in a column of the second matrix. The user defined operation may be repeated on any number of combinations provided that each combination includes a row of the first matrix or a portion of a row of the first matrix and a column of the second matrix or a portion of a row of the second matrix . Compute elements within the system identify and execute a command which references two matrices and defines a mathematical function to be executed on the two matrices.

In some embodiments various elements of the system are formed into a sub system d which may be referred to as a distributed processing system. It is noted that both the system and sub system d may be referred to as a distributed processing system in the sense that both include a plurality of processing elements c c c working distributively on a task. In one example illustrated in distributed processing system d includes the code sequence code in the second compute element c the executable instructions exe prior to distribution the executable instructions exe after distribution to the compute elements exe distributed to c exe distributed to c exe distributed to c. These combinations shown in are first three combinations listed above but it is understood that in this example all nine of the combinations will be distributed to and processed by the compute elements c c c or by additional compute elements not shown or that the number of compute elements exceeds nine in which each compute element will receive only a portion of a row and a column combination.

In some embodiments the system includes a procedure pro configured to send the initiation request to various compute elements c c c c each of which compute elements is operative to execute at least a portion of the procedure pro relevant to that compute element. For example procedure pro may direct third compute element c to execute instance exe on row R and column C as illustrated in . In various embodiments the execution of procedure pro and the execution of the instances exe exe exe of the executable instructions exe together implement the specific command s com.

An example of a specific format for command s com is A.matrix B lambda where A is a first matrix with a certain number of rows B is a second matrix with a certain number of columns and a matrix operation will be executed according to lambda. Lambda defines a function of two vectors in which the vectors are taken from different combinations of rows or portions of rows in the first matrix A and columns or portions of columns in the second matrix B. A compute element in the system takes lambda and compiles it into executable code. In the various embodiments that include procedure pro pro sends to multiple compute elements in the system different combination of rows from A or portions of rows form A and columns from B or portions of columns from B plus the executable code to implement lambda. After execution of the executable code by the compute elements the results is a third matrix of scalar values with the number of rows equal to the number rows in matrix A and the number of columns equal to the number of columns in matrix B.

The procedure pro may be stored in any of the compute elements c c c c c or may be stored in an entire separate compute element. The procedure pro may be created shortly before conversion of a specific command s com to machine executable code exe or may be rather created any prior time and stored within the system to become operable only when a specific command s com is to be so converted. The procedure pro is optimized to take advantage of the requirements and capabilities of the specific elements in the distributed processing system d and in some embodiments of the requirements and capabilities of elements in the system other than those in the distributed processing system d. In all of the embodiments in which the procedure pro is operable the procedure pro in some way or ways improves the speed and efficiency of the system .

In various embodiments the first compute elements c is aware of the requirements and capabilities of the specific compute elements c c c c in the distributed processing system d. In these embodiments the first compute element c converts the description des in a manner to optimize the machine executable code exe for later use and processing by these compute elements c c c c. In these embodiments the efficiency of the conversion process is enhanced in order to improve the speed and efficiency of the system .

As shown in one of the scalar values in the box at the upper right is f R C which is the result of mathematical function f in conjunction with vectors R and C. Similarly f R C is the result of mathematical function f in conjunction with vectors R and C f R C is the result of mathematical function if in conjunction with vectors R and C f R C is the result of mathematical function if in conjunction with vectors R and C f R C is the result of mathematical function f in conjunction with vectors R and C f R C is the result of mathematical function f in conjunction with vectors R and C f R C is the result of mathematical function f in conjunction with vectors R and C f R C is the result of mathematical function f in conjunction with vectors R and C and f R C is the result of mathematical function f in conjunction with vectors R and C. As indicated in scalar values f R C and f R C were calculated by different compute elements which as shown in were the third compute element c and the fifth compute element c respectively. The implication is that the various scalar values shown in have been calculated by different ones of the compute elements in . In four compute elements c c c c are illustrated that may calculate the scalar values. Since the number of scalar values or portions thereof to be calculated may be higher than the number of compute elements it is understood that some or all of the compute elements will calculate two or more scalar values or portions thereof. That is one case. It is possible however that there will be one compute element for each scalar value or a portion thereof to be computed in which case in some embodiments each compute element will calculate only one scalar value of a portion thereof although it is possible even in this situation to allocate two or more calculations to one compute element. It is also possible that in a specific situation there are more compute elements than scalar values to be calculated in which case some the scalar values will be calculated by only part of the compute elements.

One embodiment is a system configured to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B. In one particular form of such embodiment the system includes a code sequence code comprising a plurality of commands coms and a specific command s com in which the specific command i references the first matrix M A having a plurality of rows R R R and the second matrix M B having a plurality of columns C C C and ii describes des a mathematical function f of two vectors to be executed multiple times. Each execution is of a different combination of one of the rows R R R in matrix M A and one of the columns C C C of matrix M B acting as the two vectors. The system also includes a first compute element c having access to the code sequence code. The first compute element c is configured to i identify automatically by inspecting the specific command s com within the first code sequence code that the mathematical function f is to be executed multiple times in conjunction with the two matrices M A M B and ii according to said identification convert the mathematical function f into a sequence of executable instructions exe constituting an efficient implementation of the mathematical function f. The system also includes a second compute element c having access to at least the plurality of commands coms. The second compute element c is configured to i follow the plurality of commands coms until the specific command s com is reached and then ii initiate a plurality of processing events in which each of the processing events comprises executing an instance of the sequence of executable instructions exe in conjunction with one of the different combinations of one of the rows R R R and one of the columns C C C or a portion of the row and a respective portion of the column associated with the combination thereby resulting in performing efficiently the mathematical function f in conjunction with the two matrices M A M B.

In a first possible alternative to the system just described to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B system further includes a plurality of compute elements c c c and each of the plurality of processing events occurs in each of the plurality of compute elements respectively.

In a first possible configuration of the first possible alternative system described above further the instances exe exe exe of the sequence of executable instructions exe are available to the plurality of compute elements c c c through the system sending such instances to the plurality of compute elements c c c or through the plurality of compute elements obtaining such instances from the system.

In a second possible configuration of the first possible alternative system described above further the different combinations of one of the rows R R R and one of the columns C C C or the portions of the rows and respective portions of the columns are available respectively to the plurality of compute elements c c c through the plurality of compute elements respectively being pointed to the different combinations or portions by the system using as an example pointers to memory locations within system .

In a third possible configuration of the first possible alternative system described above further the initiation is done by the second compute element c sending a plurality of initiation requests to the plurality of compute elements c c. c respectively.

In a first possible variation of the third possible configuration just described further the plurality of initiation requests also convey to the plurality of compute elements c c. c respectively the different combinations or portions.

In a second possible variation of the third possible configuration described above further the plurality of initiation requests also convey to the plurality of compute elements c c. c respectively a plurality of instances exe exe exe of the sequence of executable instructions exe.

In a third variation of the third possible configuration described above further the sending of the plurality of initiation requests is directed by a procedure pro of the system in which the procedure pro is associated with managing and communicating data across the system and such that each of the plurality of compute elements c c. c and the second compute element c is operative to execute at least a portion of the procedure pro relevant to that compute element.

In one option of the third variation just described further execution of the procedure pro together with execution of the instances exe exe exe of the sequence of executable instructions exe implements the specific command s com.

In a fourth possible configuration of the first possible alternative system described above further the plurality of compute elements c c. c together with the second compute element c constitutes at least a part of a distributed processing system d.

In a second possible alternative to the system described above to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B further the plurality of processing events occur in the second compute element c.

In a third possible alternative to the system described above to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B further the first compute element c and the second compute element c are the same compute element.

In a fourth possible alternative to the system described above to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B further the following of the plurality of commands coms is started after the conversion of the mathematical function f.

In a fifth possible alternative to the system described above to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B further the conversion of the mathematical function if is done after the following of at least some of the plurality of commands coms but before the initiation of the plurality of processing events.

In a possible configuration of the fifth possible alternative system just described further the conversion of the mathematical function f is performed just in time for the initiation of the plurality of processing events.

In a six possible alternative to the system described above to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B further the following of the plurality of commands coms is an interpretation process associated with the plurality of commands coms.

In a possible configuration of the sixth possible alternative system just described further the initiation of the plurality of processing events is performed instead of the second compute element c interpreting the specific command s com thereby further facilitating the performing efficiently of the mathematical function f in conjunction with the two matrices M A M B.

In a seventh possible alternative to the system described above to perform efficiently a mathematical function f in conjunction with two matrices first matrix M A second matrix M B further the code sequence code is a source code written in a high level language the description of the mathematical function f is written using the high level language and the conversion of the mathematical function if is a compilation process.

In step upon following the code sequence code the system makes available to each of a plurality of compute elements c c c belonging to the system the description des of the mathematical function or a representation exe thereof together with at least one of the different combinations of one of the rows and one of the columns or a portion of the row and a respective portion of the column associated with the combination. For example in one embodiment a combination may be row R and column C and this combination may be made available to compute element c. For example in an alternative embodiment a row R may be apportioned in conjunction with a first row portion R R R and R together are the first row portion and a column C may be apportioned in conjunction with a respective first column portion C C C and C together are the first column portion . Then portions of rows and columns are made available to a compute element for example the first row portion R R and the respective first column portion C C are made available to compute element c.

In step each of the compute elements executes the mathematical function if in conjunction with the row and the column made available to the compute element or in conjunction with the portion of the row and the respective portion of the column made available to the compute element thereby realizing the mathematical function if in conjunction with the first matrix M A and the second matrix M B. For example a compute element receiving row R and column C may execute the mathematical function f to generate the result f R C. Alternatively a compute element receiving the first row portion R R and the respective first column portion C C may execute the mathematical function f to generate a first portion of the result f R C to be later combined with another portion of said result into the complete result f R C. The compute elements will continue to execute the mathematical function f on all the combinations of rows and columns or row portions and column portions received by the compute elements until the mathematical function if has been executed on all available vectors.

In a first possible alternative to the method described above in further the plurality of rows R R R constitute M rows for example M 3 the plurality of columns C C C constitute N columns for example N 3 and therefore a total of M N M times N different combinations of one of the rows and one of the columns exist for example 3 rows times 3 columns equals 9 combinations. Further according to the code sequence code the mathematical function f is to be executed for each of the M N different combinations resulting in a third matrix M result having M rows and N columns in which the third matrix has M N entries constituting respectively the following M N results f R C f R C f R C f R C f R C f R C f R C f R C and f R C. Further each of the compute elements c c c as well as possibly others not illustrated in after executing the mathematical function f holds at least one of the entries of the third matrix M result or a part of one of the entries of the third matrix M result corresponding to the mathematical function f applied on the portion of the respective row and the portion of the respective column.

In a first possible configuration of the first possible alternative to the method described in further each of the plurality of M rows is of length L each of the plurality of N columns is of the same length L and the mathematical function f is a length L dot product of the two vectors such that the third matrix M result is equal to the first matrix M A multiplied by the second matrix M B.

In a second possible configuration of the first possible alternative to the method described in further each of the plurality of M rows is of length L each of the plurality of N columns is of the same length L and the mathematical function f is a length L weighted dot product of the two vectors.

In a second possible alternative to the method described above in further the code sequence code comprises a specific command s com the specific command includes the references to the first matrix M A and the second matrix M B and the specific command s com further includes the description des of the mathematical function f.

In a first possible configuration of the second possible alternative to the method described in further interpreting by an additional compute element c belonging to the distributed processing system the specific command s com in which the interpretation constitutes at least part of the following of the code sequence code.

In a possible variation of the first possible configuration of the second alternative to the method described in further the additional compute element c detects by inspecting the specific command s com in the code sequence code that i the specific command s com makes said references to the first matrix M A and the second matrix M B and that ii the specific command s com describes des the mathematical function f. The additional compute element c extracts from the specific command s com the description des of the mathematical function f. Further the system facilitates based on the detection and extraction the availability of vectors and execution of mathematical function f in conjunction with the plurality of compute elements c c c.

In a second possible configuration of the second possible alternative to the method described in further the additional compute element c detects by inspecting the specific command s com in the code sequence code that i the specific command s com makes the references to the first matrix M A and the second matrix M B and that ii the specific command s com describes the mathematical function f. Further the additional compute element c extracts from the specific command s com the description des of the mathematical function f. Further the first compute element c converts the description des of the mathematical function f into a sequence of executable instructions exe in which the sequence of executable instructions exe constitutes said representation of the mathematical function f wherein each of the compute elements c c. c executes the mathematical function f in conjunction with the row R R R and the column C C C made available to that compute element or in conjunction with the portion of the row and the respective portion of the column made available to that compute element by executing the sequence of executable instructions exe or an instance exe exe exe of the sequence of executable instructions exe.

In a third possible alternative to the method described above in further the executing of the mathematical function f is done simultaneously by the plurality of compute elements c c c.

Various systems and methods are described herein to alternate system states between a first state in which a compute element conveys data write requests wherein each data write request includes a single data set extracted from a store command recently received in the compute element and a second state in which the compute element conveys aggregated data write requests wherein each aggregated data write request includes two or more data sets. The first state allows synchronization between incoming store commands and outgoing data write requests but increases the traffic load on a switching network within the system. The second state reduces significantly the traffic load on the switching network but at the cost of losing one to one synchronization between store commands and write requests. Various systems and methods described herein permit automatic switching between the first state and the second state and also between second state and the first state. If the system is operating in the first state and there are no attempts or negligible attempts to read back the data sets conveyed by the data write requests the system will automatically switch to the second state. If the system is operating in the second state and requests are received to read one or more of the data sets conveyed within the data write requests the system will automatically switch to the first state.

In some embodiments the second state includes two data sets per data write request. In alternative embodiments the second state includes some number of data sets greater than two within each data write request. In alternative embodiments each data write request can hold two or more data sets and the number of data sets per data write request will vary depending on various criteria such as but not by way of limitation the total number of data sets to be written into memory the rapidity of changes between the first system state and the second system state and other such criteria.

In some embodiments even one data read request will trigger the automatic switch from the second system state to the first system state. In alternative embodiments the switch from the second system state to the first system state occurs only have a predetermined number of data read requests have been received. In alternative embodiments the switch from the second system state to the first system state occurs only when the system experiences a predetermined rate of data read requests per unit of time.

In some alternative embodiments illustrated in the storing of those data sets D D performed in synchronization with the reception of data write requests is performed in conjunction with a data register r which is part of or associated with a particular data interface .

One embodiment is a system operative to automatically decide to aggregate data write requests in a distributed data store. In one particular form of such embodiment the system includes a distributed data store comprises a plurality of memory modules m m in which the plurality of memory modules m m are associated respectively with a plurality of data interfaces . The system further includes a switching network . The system further includes a first compute element c configured to receive sequentially a sequence seq of store commands associated respectively with a sequence of data sets D D D D D D D D in which each of the store commands for example. instructs the first compute element c to store the respective data set for example D in the distributed data store . For example store command may instruct the first compute element c to store data set D in memory module m of data store . Further the first compute element c is configured to initiate as a result of the sequence seq a series ser of data write requests respectively with some of the commands and in synchronization therewith in which i each of the data write requests for example comprises one of the data sets for example D received in conjunction with store command to be conveyed and ii each of the data write requests for example is sent via the switching network to one of the data interfaces for example that consequently stores in the respective memory module m the data set D conveyed. Further each of the data interfaces is configured to detect a condition in which i at least a certain number of the data write requests for example the two data write requests and have arrived at the data interface while ii there has been essentially no attempt or in some embodiments a negligible number of attempts to read any of the data sets conveyed to the data interface for example there was no attempt or perhaps only one attempt to read back the data sets D D conveyed respectively by the data write requests . The absence of data read requests or the low number of data read requests signifies that maintaining low latency through the synchronization is not critical at least at that point of time. Therefore in response to one of the data interfaces for example signaling that the condition has been detected the first compute element c is further configured to send at least one aggregated data write request to the data interface in which the aggregated data write request conveys at least two of the data sets D D that have not yet been conveyed. As a result of this change in system state from sending synchronized data write requests that is only one data set per data write request to a second state in of aggregated data write requests that is the data write requests include two or more data sets synchronization between data write requests and data sets is broken and consequently a latency associated with storage of the data sets D D conveyed is increased. However this change in system state also achieves a reduction of traffic load on the switching network .

In a first alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store each of the data interfaces for example is further configured to detect a departure from the condition by identifying an attempt to read any of the data sets D D D D conveyed to the data interface in this example thereby signifying that maintaining low latency through a renewed synchronization with the sequence of store commands seq is now important or even critical. Further as a response to one of the data interfaces signaling that said departure from the condition has been detected the first compute element c is further configured to stop aggregation of data sets D D into a next aggregated data write request and to send as quickly as possible the next aggregated data write request containing whatever data sets D D that have already been accumulated.

In a possible configuration of the first alternative embodiment as a further response to one of the data interfaces signaling the said departure from the condition has been detected the first compute element c is further configured to initiate a second series ser of data write requests in a renewed synchronization with the store commands that are still being received in which i each of the data write requests for example in the second series ser comprises one of the data sets D that has been recently received in one of the store commands in this example and ii each of the data write requests in the second series ser is sent via the switching network to one of the data interfaces for example that consequently stores in the respective memory module m the respective data set D D. As a result the synchronization is renewed and thereby a latency associated with storage of the data sets conveyed D D is decreased but in addition the traffic load on the switching network is increased.

In a first possible variation of the possible configuration further the renewed synchronization is done in conjunction with all of the data interfaces .

In a second possible variation of the possible configuration further the renewed synchronization is done in conjunction with only the data interface which has detected the departure from the condition.

In a second alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store the first compute element c as a further response to one of the data interfaces for example signaling that the condition has been detected is further configured to send at least another aggregated data write request to one of the data interfaces other than the data interface that has detected the condition and in which the another aggregated data write request conveys at least another two of the data sets D D that have not yet been conveyed.

In a possible configuration of the second alternative embodiment further the at least two data sets D D conveyed by the aggregated data write request are selected for aggregation in the aggregated data write request according to a decision by the first compute element c to store the at least two data sets D D in conjunction with one of the data interfaces . Further the at least another two data sets D D conveyed by the another aggregated data write request are selected for aggregation in the another aggregated data write request according to another decision by the first compute element c to store the at least another two data sets D D in conjunction with another of the data interfaces .

In a third alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store the breaking of synchronization between data write requests and data sets is done for all of the data interfaces .

In a fourth alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store the breaking of synchronization is done in conjunction with only the data interface for example that detected the condition.

In a fifth alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store further the first compute element c decides per each of the data write requests according to an input which of the data interfaces are the target of the data write request.

In a possible configuration of the fifth alternative embodiment further the distributed data store is a key value store KVS in . Also the sequence of data sets D D D is a sequence of values v v v in in which each of the data sets D D D is a value associated with a key k k k in . Also the input by which the first compute element c decides which data interface is the target for each data write request is the respective key of the data set.

In a sixth alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store further the sequence seq of store commands is sent to the first compute element c by a second compute element c in which the second compute element c is not aware whether or not the data sets D D D D D D D D in the sequence of store commands are conveyed to the data interfaces using aggregated data write requests.

In a possible configuration of the sixth alternative embodiment the first compute element c does not know when the second compute element c will decide to read back any of the data sets D D D D D D D D conveyed by the second compute element c.

In a seventh alternative embodiment to the system operating to automatically decide to aggregate data write requests in a distributed data store the sequence seq of store commands is generated by an application layer c app running on the first compute element c. Further the sequence of store commands are received by an interface layer c int running on the first compute element c. Further the series ser of data write requests and the at least one aggregated data write request are all sent by the interface layer c int. Further the application layer c app is not aware whether or not the interface layer c int sends the data sets D D D D D D D D to the data interfaces using aggregated data write requests.

In a possible configuration of the seventh alternative embodiment further the interface layer c does not know when the application layer c app will decide to read back any of the data sets conveyed D D D D D D D D.

In step the data interface stores each of the data sets conveyed D D as soon as the data set is conveyed such that said storing is performed in synchronization with the reception of the data write requests thereby facilitating low latency read back of the data sets already conveyed.

In step the data interface detects a condition in which i at least a certain number of the data write requests have arrived at the data interface ii and there has been essentially no attempts or a negligible number of attempts by the data interface to read back any of the data sets D D conveyed to the data interface.

In step as a result of the detection the data interface signals to the compute element c that the condition has been detected thereby implying to the compute element c that i maintaining a low latency in conjunction with the storage of the data sets D D is not critical and therefore ii that aggregation of data write requests is now possible.

In step as a result of the signaling the data interface receives from the compute element c at least one aggregated data write request in which the aggregated data write request conveys at least two additional data sets D D to be stored by the data interface .

In a first possible alternative to the method described above in further the data interface eventually stores in a memory module m the at least two additional data sets D D conveyed by the aggregated data write request .

In a possible configuration of the first possible alternative to the method described in further the data interface detects a departure from the condition by identifying an attempt to read any of the data sets D D D D conveyed to the data interface thereby signifying that maintaining the low latency in conjunction with the storage of data sets is now critical. Further the data interface signals to the compute element c that the departure from the condition has been detected thereby implying to the compute element c that i it is now critical again to maintain the low latency in conjunction with the storage of data sets and therefore ii that aggregation of data write requests is not possible anymore.

In a second possible alternative to the method described above in further the storing by the data interface of each of the data sets D D conveyed by the data write requests which is performed in synchronization with the reception of the data write requests is performed in conjunction with a memory module m associated with the data interface .

In a third possible alternative to the method described above in further the storing by the data interface of each of the data sets D D conveyed by the data write requests which is performed in synchronization with the reception of the data write requests is performed in conjunction with a data register r belonging to the data interface .

In a possible configuration of the third possible alternative to the method described in further transferring the data sets D D conveyed by the data write requests to the data interface from the data register r of the data interface to a memory module m associated with the data interface .

In a fourth possible alternative to the method described above in further the condition detected by the data interface is a condition in which i at least a first quantity Q of said data write requests have arrived at the data interface while ii there has been a second quantity Q of attempts to read back from the data interface any of the data sets D D conveyed to the data interface . Further first quantity Q is at least 10 ten times larger than said second quantity Q. This ratio implies that there were essentially no attempts or a negligible quantity of attempts to read any of the data sets conveyed to the data interface .

In a fifth possible alternative to the method described above in the condition detected is a condition in which i at least 10 ten of said data write requests have arrived at the data interface while ii there has been absolutely no attempt to read any of the data sets conveyed to the data interface.

Described herein are systems and methods that can reduce the number of data fetching operations or the duration of a data fetching operation in conjunction with executing a plurality of actions or algorithms. In particular where two or more compute elements or processes require the same data set but at different periods of time systems and methods are defined according to various embodiments by which a single data fetch operation is performed rather than a data fetch operation for each compute element or process. In some embodiments a data set is fetched by a first compute element process and the data set is stored in a memory location associated with that first compute element. Such memory location may be the cache memory of the first compute element or a memory module on the same computer board as the first compute element or another location that is in close proximity to the first compute element and that does not require an additional fetch operation nor any sending of the data set over a switching network after the first fetch operation. The first compute element process uses the data set in a processing task. In some embodiments a second compute element process accesses the data set after it has been used by the first compute element process. In these embodiments the second compute element process accesses the first data set from the memory location associated with the first compute element and uses the first data set to execute a second processing action. In other embodiments the system makes sure that the second process either i transfers or migrates to the first compute element or ii is scheduled to run on the first compute element while the first data set is still stored in the memory location of the first compute element. In such embodiments the system executes the processing action near the cached data. The effect is essentially the same although in the original embodiments the data set was transferred from the first memory location to the second compute element whereas in these embodiments the processing operation is either transferred or migrated from the second compute element to the first compute element or is scheduled for execution in the first compute element.

There are many permutations of the various embodiments described. All such permutations are within the scope of and are included within the invention described herein. For example although the embodiments heretofore described included only two compute elements processes there may be three or any higher number of compute elements processes in which two or more compute elements processes are accessing the data set placed in the memory location associated with a first compute element. For example for various reasons the specific data elements of the data set may be stored in different memory locations associated with the first compute element and all of these data elements may be accessed by a second or subsequent compute elements. For example a second compute element may access and use in processing multiple data sets in which all of the data sets are stored in one or more memory locations associated with the first compute element. For example a second compute element may access and use multiple data sets one or more of which are stored in one or more memory locations associated with the first compute element but one or more of the data sets may be stored in memories associated with additional compute elements and not with the first compute element. Many additional variations and permutations are also possible provided only that each compute element process seeking a data set will receive that data set form the memory associated with another compute element or another process that has previously fetched such data set via a switching network and therefore the load traffic on the switching network will be reduced.

It is understood that each application program AP AP may be any program layer algorithm or some other scheme for organizing the steps by which a process is executed.

It is understood that the two actions are not the same. For example one action may require counting the words in the first data set D whereas the other action may require using the data in the first data set D to compute a certain quantity. There is no limit to the different actions that may be executed using all or part of the data in the data set D provided that the two actions include different processing with different results.

The first algorithm alg can be executed independently of the second algorithm alg but this will result in two separate fetches of data set D one fetch as a result of request req appearing in alg and a second fetch as a result of request req appearing in alg . There is a better way of executing the two algorithms alg alg which is illustrated in .

One embodiment is a system operative to execute efficiently a plurality of actions. In one particular form of such embodiment there is a first compute element c associated with a first memory m and a first data set D associated with a first data source data . Further the system is configured to fetch f the first data set D from the first data source data to the first memory m. Further the first compute element c is configured to execute in conjunction with the first data set D now in the first memory m a first action act involving the first data set D. Further the system is configured to identify a second action act that like the first action act involves the first data set D. Further the system is configured to cause the second action act to be executed in the first compute element c and in conjunction with the first data set D already in the first memory m while the first data set D is still in the first memory m thereby avoiding a need to re fetch the first data set D from the data source.

In a first alternative embodiment to the system operative to execute efficiently a plurality of actions further the first memory m is a cache memory of the first compute element c. Also the execution of the first action act is directed by a first application program AP running on the first compute element m. Also the execution of the second action act is directed by a second application program AP . Also the causing of the second action act to be executed in the first compute element c is facilitated by the system instructing the first compute element c after said execution of the first action act to at least temporarily halt execution of the first application program AP while leaving the first data set D in the cache memory m and to temporarily execute the second application program AP at least until the second action act is performed by the second application program AP in conjunction with the first data set D still in the cache memory m.

In a first possible configuration of the first alternative embodiment further the identification of the second action act is facilitated by estimating by the system that the second application program AP if allowed to run on the first compute element c will reach execution of the second action act not later than a predetermined amount of time T after the allowance to run.

In a first possible variation of the first possible configuration further the predetermined amount of time T is 10 ten milliseconds.

In a second possible variation of the first possible configuration the system includes a second compute element c the estimation is done by the second compute element c by analyzing the second application program AP and the instructing of the first compute element c is done by the second compute element c.

In a first possible option of the second possible variation further the second application program AP is executed in the second compute element c before the estimation and the instructing take place and prior to the temporary halt and the temporary execution of the second application program AP in the first compute element c.

In a second possible option of the second possible variation further the second compute element c has access to both the first application program AP and the second application program AP thereby enabling the second compute element c to achieve the identification.

In a second alternative embodiment to the system operative to execute efficiently a plurality of actions further the first action act is associated with a first application program AP . Also the second action act is associated with a second application program AP . Also the identification of the second action act is facilitated by detecting in the second application program AP a fetching request req for the first data set D in which the fetching request req for the first data set is similar to another fetching request req in the first application program AP for the first data set D.

In a first possible configuration of the second alternative embodiment to the system operative to execute efficiently a plurality of actions further the identification of the second action act is further facilitated by detecting in the second application program AP that the fetching request req precedes the second action act while also detecting in the first application program AP that the another fetching request req precedes the first action act thereby realizing an association between the first action act and the second action act via the first data set D and the fetching thereof.

In a second possible configuration of the second alternative embodiment to the system operative to execute efficiently a plurality of actions further the second fetching request req is never executed such that the first fetching request req is the only one executed by the system .

In a third possible configuration of the second alternative embodiment to the system operative to execute efficiently a plurality of actions further the fetch f is a result of the first fetching request req in the first application program AP .

In a third alternative embodiment to the system operative to execute efficiently a plurality of actions further the first action act is a part of a first algorithm alg comprising a first part first and the first action act . Also the second action act is a part of a second algorithm alg comprising the first part first and the second action act . Also the system is further configured to execute the first part first prior to the execution of the first action act thereby facilitating execution of the first algorithm alg and the second algorithm alg up to a point where the first part first ends. Also the execution of the first action act follows the execution of the first part first such that the execution of the first action act is a continuation of the execution of the first algorithm alg . Also the causing of the second action act to be executed in the first compute element c is facilitated by the system instructing the first compute element c after the execution of the first action act to switch to executing the second action act .

In a possible configuration of the third alternative embodiment to the system operative to execute efficiently a plurality of actions further the identification of the second action act is facilitated by the system detecting that the first part first is a common part common of both the first algorithm alg and the second algorithm alg and that the first action act which follows the common part common is different than the second action act that also follows the common part common while both the first action act and the second action act are operating on the first data set D.

In a possible variation of the possible configuration of the third alternative embodiment further the identification of the second action act is further facilitated by detecting in the second algorithm alg a fetching request req for the first data set D in which the fetching request req for the first data set D is similar to another fetching request req in the first algorithm alg for the first data set D. Also the identification of the second action act is further facilitated by detecting that the first fetching request req terminates the common part common of the first algorithm alg and that the second fetching request req terminates the common part common of the second algorithm alg .

In a fourth alternative embodiment to the system operative to execute efficiently a plurality of actions further the first data source data is configured to produce store or obtain the first data set D.

In a possible configuration of the fourth alternative to the system operative to execute efficiently a plurality of actions further the first data source data is selected from a group consisting of i an element of a distributed data store for example as illustrated in and ii a memory module m for example as illustrated in iii a data interface for example as illustrated in and iv another compute element for example c as illustrated in .

In a fifth alternative embodiment to the system operative to execute efficiently a plurality of actions further the first fetch f is done via a switching network thereby introducing a first latency in conjunction with the fetch f in which the first latency is circumvented as a result of avoiding the need to re fetch the first data set D.

In a sixth alternative embodiment to the system operative to execute efficiently a plurality of actions further the first action act is selected form a group of actions consisting of i a matrix multiplication in which the first data set D is a first matrix that is multiplied by a second matrix ii a filtering or sorting process to be applied on the first data set D and iii any mathematical function involving the first data set D.

In a possible configuration of the sixth alternative to the system operative to execute efficiently a plurality of actions further the second action act is selected form a group of actions consisting of i a matrix multiplication in which the first data set D is the first matrix that is multiplied by a third matrix ii another filtering or sorting process to be applied on the first data set D and iii any other mathematical function involving the first data set D.

In a first possible alternative to the method described above in further the fetching f by the process AP is performed via a switching network and the compute element c on which the process AP runs is co located in a server s with the another c compute elements and the first memory m. The result is that the another process AP fetches the first data set D from the first memory m but does not fetch the first data set D from the first data source data . Therefore the switching network is not involved with this fetch the load on the switching network is reduced and latency associated with the fetching from the first memory m is reduced.

In a possible configuration of the first possible alternative to the method described in further the first memory m is a cache memory of the compute element c on which the process AP runs.

In a second possible alternative to the method described above in further the detection is facilitated by the another of the processes AP indicating to the distributed processing system that the process AP is about to fetch the first data set D from the first data source data .

In a third possible alternative to the method described above in further the prevention is facilitated by the distributed processing system signaling the another of the processes AP that the first data set D is also located in the first memory m thereby causing the another process AP to fetch the first data set D from the first memory m rather than from the first data source data .

In this description numerous specific details are set forth. However the embodiments cases of the invention may be practiced without some of these specific details. In other instances well known hardware materials structures and techniques have not been shown in detail in order not to obscure the understanding of this description. In this description references to one embodiment and one case mean that the feature being referred to may be included in at least one embodiment case of the invention. Moreover separate references to one embodiment some embodiments one case or some cases in this description do not necessarily refer to the same embodiment case. Illustrated embodiments cases are not mutually exclusive unless so stated and except as will be readily apparent to those of ordinary skill in the art. Thus the invention may include any variety of combinations and or integrations of the features of the embodiments cases described herein. Also herein flow diagram illustrates non limiting embodiment case example of the methods and block diagrams illustrate non limiting embodiment case examples of the devices. Some operations in the flow diagram may be described with reference to the embodiments cases illustrated by the block diagrams. However the method of the flow diagram could be performed by embodiments cases of the invention other than those discussed with reference to the block diagrams and embodiments cases discussed with reference to the block diagrams could perform operations different from those discussed with reference to the flow diagram. Moreover although the flow diagram may depict serial operations certain embodiments cases could perform certain operations in parallel and or in different orders from those depicted. Moreover the use of repeated reference numerals and or letters in the text and or drawings is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments cases and or configurations discussed. Furthermore methods and mechanisms of the embodiments cases will sometimes be described in singular form for clarity. However some embodiments cases may include multiple iterations of a method or multiple instantiations of a mechanism unless noted otherwise. For example a system may include multiple compute elements each of which is communicatively connected to multiple servers even though specific illustrations presented herein include only one compute element or a maximum of two compute elements.

Certain features of the embodiments cases which may have been for clarity described in the context of separate embodiments cases may also be provided in various combinations in a single embodiment case. Conversely various features of the embodiments cases which may have been for brevity described in the context of a single embodiment case may also be provided separately or in any suitable sub combination. The embodiments cases are not limited in their applications to the details of the order or sequence of steps of operation of methods or to details of implementation of devices set in the description drawings or examples. In addition individual blocks illustrated in the figures may be functional in nature and do not necessarily correspond to discrete hardware elements. While the methods disclosed herein have been described and shown with reference to particular steps performed in a particular order it is understood that these steps may be combined sub divided or reordered to form an equivalent method without departing from the teachings of the embodiments cases. Accordingly unless specifically indicated herein the order and grouping of the steps is not a limitation of the embodiments cases. Embodiments cases described in conjunction with specific examples are presented by way of example and not limitation. Moreover it is evident that many alternatives modifications and variations will be apparent to those skilled in the art. Accordingly it is intended to embrace all such alternatives modifications and variations that fall within the spirit and scope of the appended claims and their equivalents.

