---

title: Multi-tenant discovery and claiming of distributed storage nodes over an insecure network
abstract: A technique is introduced that enables a server to establish trust of and a secure channel of communication with an unverified client computer, which can be on a different insecure network. To establish trust, the server needs to ensure that the client computer is legitimate, and the client computer similarly needs to ensure that the server is legitimate. With mutual trust established, a secure channel of communication is established between the server and the client computer. With mutual trust and a secure channel of communication established, the client computer can safely communicate with the server, for example, to download software that enables the client computer to join a central management system at the server.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09577830&OS=09577830&RS=09577830
owner: SwiftStack, Inc.
number: 09577830
owner_city: San Francisco
owner_country: US
publication_date: 20150512
---
This application claims the benefit of U.S. Provisional Patent Application No. 61 994 779 filed on May 16 2014 entitled MULTI TENANT DISCOVERY AND CLAIMING OF DISTRIBUTED STORAGE NODES OVER AN INSECURE NETWORK which is incorporated herein by reference in its entirety.

At least one embodiment of the present disclosure pertains to a multi tenant highly scalable and durable object storage system and more particularly to multi tenant discovery and claiming of distributed storage nodes over an insecure network.

The pervasiveness of the Internet and the advancements in network speed have enabled a wide variety of different applications on storage devices. For example cloud storage or more specifically network distributed data storage system has become a popular approach for safekeeping data as well as making large amounts of data accessible to a variety of clients. As the use of cloud storage has grown cloud service providers aim to address problems that are prominent in conventional file storage systems and methods such as scalability global accessibility rapid deployment user account management and utilization data collection. In addition the system s robustness must not be compromised while providing these functionalities.

Among different distributed data storage systems an object storage system employs a storage architecture that manages data as objects as opposed to other storage architectures like file systems which manage data as a file hierarchy and block storage which manages data as blocks within sectors and tracks. Generally object storage systems allow relatively inexpensive scalable and self healing retention of massive amounts of unstructured data. Object storage is used for diverse purposes such as storing photos and songs on the Internet or files in online collaboration services.

As new users are added to a storage system storage space needs to be securely allocated to the new users. Complicating matters are that with cloud storage as well as some other types of network distributed data storage portions of a storage system may be accessible via insecure networks. To obtain needed disk space users need to discover and claim distributed storage nodes and need to be able to securely do so over an insecure network.

Introduced here is a technique for a first computer such as a server to establish trust and a secure channel of communication with an untrusted unverified second computer such as a client computer that is on a different network that may be insecure. A multi tenant central management system is a system which can manage resources such as computing resources storage resources etc. To join or register with a central management system a client computer may want to download and install client software that enables the client computer to be added to the central management system.

Before the client computer joins registers with the central management system the management system needs to ensure that the client computer has a legitimate request to join register with the system. The client computer has a similar need to ensure that the central management system is legitimate before downloading any software or other data from the management system. Further a mutual verification of legitimacy may need to work in an environment where the network is insecure. With mutual trust established a secure channel of communication needs to be established between the server and the client computer.

Without such mutual verification and secure communication security issues may arise such as from a man in the middle attack. With mutual trust and a secure channel of communication established the client computer can safely download software that enables the client to join register with the central management system.

A storage system can be a file based storage system an object based storage system etc. Some storage systems such as the OpenStack Object Storage system referred to herein as Swift are multi tenant highly scalable and durable storage systems designed to store large amounts of unstructured data at low cost. A highly scalable storage system such as Swift can scale from a few nodes and a handful of hard drives to thousands of clustered machines with multiple petabytes of storage and can be designed to be horizontally scalable so there is no single point of failure.

Some highly scalable storage systems can be used by businesses of various sizes service providers and research organizations worldwide. These storage systems can be used to efficiently store unstructured data such as documents Web and media content backups images virtual machine snapshots etc.

Network storage system in the example of used by Company X can be an object storage system e.g. OpenStack Object Storage system also known as Swift which is a multitenant highly scalable and durable object storage system designed to store large amounts of unstructured data at low cost. Network storage system is highly scalable because it can be deployed in configurations ranging from a few nodes and a handful of drives to thousands of machines with tens of petabytes of storage. Network storage system is designed to be horizontally scalable so there is no single point of failure. Storage clusters can scale horizontally simply by adding new servers. If a server or hard drive fails network storage system automatically replicates its content from other active nodes to new locations in the cluster. Therefore network storage system can be used by businesses of variable sizes service providers and research organizations worldwide. Network storage system can be used to store unstructured data such as documents web and media content backups images virtual machine snapshots etc. Objects and files can be written to multiple disk drives spread throughout servers in the data center with system software being responsible for ensuring data replication and integrity across the cluster.

Some characteristics of the network storage system differentiate it from some other storage systems. For instance in some embodiments network storage system is not a traditional file system or a raw block device instead network storage system enables users to store retrieve and delete objects with metadata associated with the objects in logical containers e.g. via a RESTful HTTP API . Developers can for example either write directly to an application programming interface API of network storage system can use one of the many client libraries that exist for many popular programming languages such as Java Python Ruby C etc. among others. Other features of network storage system include being natively designed to store and serve content to many concurrent users being able to manage storage servers with no additional vendor specific hardware needed etc. Also because in some embodiments network storage system uses software logic to ensure data replication and distribution across different devices inexpensive commodity hard drives and servers can be used.

Referring back to distributed storage cluster is a distributed storage system used for object storage. Distributed storage cluster is a collection of machines that run server processes and consistency services e.g. in the form of daemons . A daemon is a computer program that can run as a background process or service in contrast to being under the direct control of an interactive user. Each machine that runs one or more processes and or services is called a node. When there are multiple nodes running that provide all the processes needed to act as a distributed storage system such as network storage system the multiple nodes are considered to be a cluster e.g. distributed storage cluster . In some embodiments there are four server processes proxy account container and object. When a node has only the proxy server process running it is called a proxy node such as proxy nodes . A node running one or more of the other server processes account container or object is called a storage node such as storage nodes . Storage nodes contain data that incoming requests wish to affect e.g. a PUT request for an object would go to the appropriate nodes running the object server processes . Storage nodes can also have a number of other services running on them to maintain data consistency.

As illustrated in within a cluster the nodes can belong to multiple logical groups e.g. regions such as Region West and Region East and zones such as Zone with proxy server and storage nodes . Regions and zones are user defined and identify unique characteristics about a collection of nodes for example geographic location and points of failure such as all the power running to one rack of nodes. Having such groups zones etc. facilitate network storage system placing data across different parts of the cluster to reduce risk.

More specifically the proxy servers can function as an interface of network storage system as proxy servers can communicate with external clients. As a result proxy servers can be the first and last to handle an API request from for example an external client such as client user which can be a computer of a customer of Company X. Client user can be one of multiple external client users of network storage system . In some embodiments all requests to and responses from proxy servers use standard HTTP verbs and response codes. Proxy servers can use a shared nothing architecture among others. A shared nothing architecture is a distributed computing architecture in which each node is independent and self sufficient and there is no single point of contention in the system. For example none of the nodes in a shared nothing architecture share memory or disk storage. Proxy servers can be scaled as needed based on projected workloads. In some embodiments a minimum of two proxy servers are deployed for redundancy should one proxy server fail a second proxy server can take over.

The storage nodes are responsible for storage of objects on the drives of its node. In some embodiments objects are stored as binary files on the drive using a path that is made up in part of its associated partition and the timestamp of an operation associated with the object such as the timestamp of the upload write put operation that created the object. A path can be e.g. the general form of the name of a file directory object etc. The timestamp may allow for example the object server to store multiple versions of an object while providing the latest version for a download get request. In other embodiments the timestamp may not be necessary to provide the latest copy of object during a download get. In these embodiments the system can return the first object returned regardless of timestamp. The object s metadata standard and or custom can be stored in the file s extended attributes xattrs and the object s data and metadata can be stored together and copied as a single unit.

Although not illustrated in for simplicity a node that runs an account server process can handle requests regarding metadata for individual accounts or for the list of the containers within each account. This information can be stored by the account server process in SQLite databases on disk for example. Also a node that runs a container server process can handle requests regarding container metadata or the list of objects within each container. Note that in some embodiments the list of objects does not contain information about the location of the object and rather may simply contain information that an object belongs to a specific container. Like accounts the container information can be stored as SQLite databases. In some embodiments depending on the deployment some nodes may run some or all services. Although illustrated as separated in in some embodiments storage nodes and proxy server nodes may overlap.

In some embodiments network storage system optionally utilizes a load balancer . In general load balancer is used to distribute workload evenly among the proxy servers. In some embodiments load balancer is capable of prioritizing TCP and UDP traffic. Further load balancer can distribute requests for HTTP sessions among a number of resources in distributed storage cluster . Load balancer can be provided as one of the services run by a node can be provided externally such as via a round robin DNS a commercial load balancer etc. etc.

Illustrated in are two regions in distributed storage cluster Region West and Region East. Regions are user defined and can indicate that parts of a cluster are physically separate. For example regions can indicate that part of a cluster are in different geographic regions. In some embodiments a cluster can have one region. Distributed storage cluster uses two or more regions and is resultantly a multi region cluster. When a read request is made a proxy server may favor nearby copies of data as measured by latency. When a write request is made the proxy layer can write to all the locations simultaneously. In some embodiments an option called write affinity when activated enables the cluster to write all copies locally and then transfer the copies asynchronously to other regions.

In some embodiments within regions network storage system allows availability zones to be configured to for example isolate failure boundaries. An availability zone can be a distinct set of physical hardware whose failure would be isolated from other zones. In a large deployment example an availability zone may be configured as a unique facility in a large data center campus. In a single datacenter deployment example each availability zone may be a different rack. In some embodiments a cluster has many zones. A globally replicated cluster can be created by deploying storage nodes in geographically different regions e.g. Asia Europe Latin America America Australia or Africa . The proxy nodes can be configured to have an affinity to a region and to optimistically write to storage nodes based on the storage nodes region. In some embodiments the client can have the option to perform a write or read that goes across regions i.e. ignoring local affinity .

With the above elements of the network storage system in mind an application example of network storage system is introduced as follows. In this example network storage system is a storage system of Company X and client user is a computer of a customer of Company X. When a valid request is sent from client user through firewall to distributed storage cluster load balancer determines which proxy node in distributed storage cluster to which to route the request. The selected proxy node e.g. proxy verifies the request and determines among the storage nodes on which storage node s the requested object is stored based on a hash of the object name and sends the request to the storage node s . If one or more of the primary storage nodes is unavailable the proxy will choose an appropriate hand off node to which to send the request. The node s return a response and the proxy in turn returns the first received response and data if it was requested to the requester. A proxy server process can look up multiple locations because a storage system such as network storage system can provide data durability by writing multiple in some embodiments a target of 3 complete copies of the data and storing them in distributed storage cluster .

As previously mentioned proxy services handle external communication with clients and storage services handle storage and maintenance of data stored at network storage system . In some embodiments accounts are root storage locations for data in a storage cluster e.g. network storage cluster . Containers are user defined segments of the account that provide the storage location where objects are found. Accounts enable multiple users and applications to access the storage system at the same time. Accounts and containers store key information about themselves in separate databases that are distributed throughout the system. Accounts allow users who access them to create and store data in individual containers. Although containers cannot be nested they are conceptually similar to directories or folders in a file system.

Controller is the management system which provides cluster operator an interface e.g. browser based to facilitate management of nodes configuration of networking and management of user accounts for Company X s cluster. Cluster operator can be one of multiple cluster operators to which controller provides such an interface. In the example of controller executes at a server of a provider of storage software used by Company X. Cluster operator can also use controller for monitoring authentication integrations alerts system statistics reports etc. These statistics and reports can be based on accurately aggregated data and can enable cluster operator to determine storage utilization for chargeback billing and other purposes. Such capabilities can be useful for entities such as Company X who would like to leverage the multi tenancy of controller to enable their own customers to manage their storage clusters through controller .

In some embodiments controller can be accessed online. In some embodiments controller is installed and runs on a server on the protected side of firewall i.e. on the same side of firewall as distributed storage cluster rather than on the Internet wide of firewall e.g. the side of firewall on which client user is shown in the example of . In some of these embodiments controller which provides the management function of distributed storage cluster is independent of the proxy and data storage functions that the nodes of distributed storage cluster are performing.

A multi tenant central management system e.g. controller that manages and monitors an eventually consistent distributed storage system e.g. network storage system faces unique challenges when collecting and aggregating data metrics. Operators of any storage system need to know how much data a user is storing in order to charge the user accordingly. Similarly operators of network based storage systems often need to know how much data was transferred in or out of the system. Metrics that can satisfy these needs may be based on data which could include for example an account identification ID along with the following data per account the number of bytes used per storage category the number of bytes transferred into the cluster and the number of bytes transferred out of the cluster. An eventually consistent distributed storage system that uses replicas for durability has several factors e.g. robustness scalability and accuracy that can increase the difficulty of generating an accurate accounting and reporting of these metrics.

Accordingly in some embodiments controller employs one or more mechanisms for collecting storage and transfer utilization metrics for an account in network storage system that are more scalable and robust than conventional ways. As previously discussed controller along with other elements in network storage system accomplish this in part by having a portion of the processing take place on the storage nodes themselves which scale horizontally. On each of the proxy servers and storage nodes utilization data for that node is first collected essentially performing a first pass of aggregation. After the first pass of aggregation post aggregated data is sent to a central controller where the data is further aggregated and collated. Also in the case of some metrics for example transfer utilization metrics there can be an additional level of aggregation derived from the proxy access logs e.g. for improved accuracy . In addition the techniques can include several mechanisms to ensure robustness of the metrics collection mechanisms.

For example in some embodiments of distributed storage cluster storage metrics e.g. container count object count and total bytes used are stored in account databases e.g. Swift Account DBs that are distributed throughout network storage system . Raw transfer data are stored in log files on the proxy nodes. Overall the collection mechanism collects aggregates and stores 1 utilization data container count object count and total bytes stored from account databases and 2 transfer metrics bytes in bytes out and request count from all nodes across a distributed storage system. Based on methods of data collection aggregation and correction this collection mechanism produces metrics for storage utilization and transfer activity which can be used for reporting billing and or chargeback purposes. Both storage utilization and transfer metrics are collected and in some cases for example with transfer metrics there may also be some amount of preliminary computation at their respective nodes before they are sent to controller for aggregation storage and presentation. The metrics can be sent via for example a RESTful API. The raw proxy logs and the collected storage data can also be stored in distributed storage cluster itself to support resolution of any billing disputes.

In the example of which will be leveraged in the following description of Company X utilizes a storage system such as network storage system . In some embodiments Company X can download and install storage software from a storage software provider such as the company that operates controller . Company X can use the storage software as part of a storage system.

One of the issues facing a multi tenant central management system such as controller operating over an insecure network is establishing trust and a secure channel of communication between the central management system and an unverified and untrusted server such as node . Node can be on a different network than controller . For example node can be on the protected side of a firewall such as on the same side of firewall as is distributed storage cluster of and controller can be on the Internet side of the firewall such as on the same side of firewall as is controller of . Security issues exist for both controller and node . For example to ensure security node needs to ensure that it is communicating with and downloading software from an authentic server of the storage software company and controller of the storage software company needs to ensure that it is communicating with and sending software to an authentic computer of Company X. Company X is the client of the storage software company in the example of which is being leveraged in the example of .

A cluster operator such as a cluster operator that uses node can obtain a secure hypertext transfer protocol HTTPS address from a storage software provider such as the storage software provider that operates controller . For example the cluster operator can receive the HTTPS address via a phone call from an employee of the storage software provider via an email or a secure email from an employee of the storage software provider etc. Because the HTTPS address is from a trusted source e.g. the storage software provider and because HTTPS is a secure protocol that enables node to communicate securely with controller the cluster operator and node can trust that they are securely communicating with an authentic server of the storage software provider.

Based on trusting the HTTPS address the cluster operator runs a local command on node which sends an HTTPS install package request to controller message . Controller can be executed at for example the computing system of which can include multiple physical computers. In some embodiments the HTTPS install package request is sent to an alternate computing system rather than controller . The alternate compute system is coupled to the computing system that is executing controller . In these embodiments the alternate computing system rather than controller can perform some or all of the operations and or receives sends some or all of the communications shown in as being performed sent received by controller .

The HTTPS request uses secure socket layer transport layer security SSL TLS to ensure that communications between node and controller are secure for example protecting against a man in the middle attack. Usage of HTTPS ensures that the messages data sent between node and controller are encrypted. At this point node trusts controller as the operator obtained the HTTPS address from a trusted source. However controller does not yet trust node as the HTTPS request of message could come from any computer.

Controller replies and sends a software package such as platform repo 1.0 2.e16.noarch.rpm in the example above for installation by node message . As node installs the package the package sets up cryptographic trust on node for data hosted by controller . Node downloads and installs a node agent software package such as swiftstack node in the example above from controller . Node then starts a node agent process by executing the node agent software block . The node agent registers with controller securely via HTTPS message . The registration can also include node generating a digital fingerprint block and sending the digital fingerprint to controller message .

In some embodiments the digital fingerprint is generated based on the physical hardware of node and is a digital code that enables controller to reliably and or securely identify node . A digital fingerprint can be a unique identifier for a first computing system such as node and can enable a second computing system such as controller to be able to uniquely identify the first computing system. In some embodiments the registration includes certificate validation which can include validating a digital certificate against the certificate s signing authority by verifying the signature on the certificate with the signing authority s public key.

At block controller generates an identifier for node and a secret key for node block . The identifier can be a node identifier such as a universally unique identifier UUID . The secret key can be a security key which can have a value that is determined by encrypting a first digital value using an encryption key. In some embodiments the UUID and or the secret key are generated based on the digital fingerprint of message such as by using the digital fingerprint as an encryption key to generate the UUID and or the secret key. In some embodiments the identifier of block is a unique identifier. At block controller further creates a database entry for node and stores a node record e.g. the identifier and the secret key . The node record is anonymous meaning that node is not linked to any cluster or user of the storage system as controller has not yet established trust of node . Controller sends the identifier and the secret key to node message .

The node agent at node receives the identifier and secret key and uses them to generate a claim uniform resource locator URL such as claim URL of block . Claim URL of the example of includes address field address which is a URL for a resource at controller and an identifier field identifier which in this example is a UUID with a 128 bit value. A URL such as the URL of the address field is a reference to a resource that specifies the location of the resource on a computer network and a mechanism for retrieving it. In some embodiments some or all of the fields of the claim URL are encrypted prior to transmission via HTTPS. The identifier field is a field of the claim URL and can contain any value that can be used to identify node such as a UUID for node . In some embodiments the identifier field uniquely identifies node . The claim URL is stored at node and is communicated to user block such as by node displaying the claim URL to user by emailing the claim URL to user by texting the claim URL to user etc.

The node agent at node also generates a certificate uniform resource locator URL such as the certificate URL of based on the identifier and the secret key block . Certificate URL includes address field address which is a URL for a resource at controller an identifier field identifier which in this example is a UUID with a 128 bit value and a secret key field secret key which in this example is another 128 bit value. In some embodiments some or all of the fields of the certificate URL are encrypted prior to transmission via HTTPS. The identifier field is a field of the certificate URL and can contain any value that can be used to identify node such as a UUID for node . In some embodiments the identifier field uniquely identifies node . The secret field is a field of the certificate URL and can contain any value that can be used to verify that the entity requesting the certificate bundle is node and not an attacker who only has the node UUID. The certificate URL can be stored at node and is used by node to generate messages and which are all HTTPS GET requests.

The node agent of node initiates a GET request using the certificate URL message . If user has not accessed the claim URL yet such as by sending message the GET request receives a 404 Not Found response message . Node keeps looping and requesting the certificate URL from controller messages and controller keeps responding with a 404 Not Found error message message until controller establishes trust of node such as by user sending message . After controller establishes trust of node the GET request of message receives a certificate bundle in response message . The certificate URL can indicate for example that the node agent is requesting a virtual private network VPN certificate bundle via HTTPS. When controller receives a GET request using the certificate URL before controller establishes trust of node such as when controller receives messages and controller sends a message in response to the GET certificate request that indicates that controller could not find a resource associated with the certificate URL e.g. messages and . Controller can establish trust of node by for example verifying that node is an authentic node of Company X.

As discussed above controller continues responding with a 404 Not Found error message until controller validates the authenticity of node . In some embodiments the user validates the authenticity of node by going to a sign in URL of controller signing in via an authenticated account and accessing the claim URL message . As a first example the user after signing in can provide the value of the identifier to controller and controller can locate the node database record using the identifier thereby establishing the ownership of node by user . As a second example the user can open a web browser and using his credentials log into controller . The user can copy and paste or just type the claim URL into the browser to provide the value of the identifier to controller .

Because the node UUID is known to the legitimate owner of node and the owner can control who has access to the node UUID verification that an authenticated user knows the claim URL of node establishes that node is owned by user . After the validation controller can now safely link node with the account of the authorized user block and with the employer of the authorized user in this example Company X . Controller can further generate a certificate bundle for node which can include a configuration file a private client certificate and key and controller s public VPN certificate block . The generation of the certificate bundle can be based on the identifier for example the UUID.

Once controller establishes trust of node and controller receives an HTTPS GET request that includes the certificate URL from node message controller returns the certificate bundle via HTTPS block . The node agent at node receives the certificate bundle which is trusted because the bundle came over HTTPS from a trusted server and installs the certificate bundle. Once the certificate bundle is installed the node agent at node starts a secure communication agent such as VPN block . The secure communication agent establishes a secure connection between node and controller such as a VPN connection block . In some embodiments where the secure connection is VPN node s VPN connection verifies controller s certificate and controller s VPN verifies node s certificate resulting in a secure VPN tunnel between node and controller . This VPN tunnel provides a secure communication channel between controller and node . A secure connection between controller and node such as the VPN tunnel enables controller to sit in the cloud e.g. on the Internet side of a firewall while the data sits securely on the protected side of the firewall and behind other network security measures in a company s datacenter.

In some embodiments all network communications are initiated by node . The VPN tunnel includes a firewall that rejects all requests made to node other than those from the computer on the other side of the VPN tunnel e.g. controller . This reduces the security threats that node may encounter as it communicates over a less secure network with controller .

In the illustrated embodiment the processing system includes one or more processors memory a communication device and one or more input output I O devices all coupled to each other through an interconnect . The interconnect may be or include one or more conductive traces buses point to point connections controllers adapters and or other conventional connection devices. The processor s may be or include for example one or more general purpose programmable microprocessors microcontrollers application specific integrated circuits ASICs programmable gate arrays or the like or any combination of such devices. The processor s control the overall operation of the processing device . Memory may be or include one or more physical storage devices which may be in the form of random access memory RAM read only memory ROM which may be erasable and programmable flash memory miniature hard disk drive or other suitable type of storage device or any combination of such devices. Memory may store data and instructions that configure the processor s to execute operations in accordance with the techniques described above. The communication device may be or include for example an Ethernet adapter cable modem Wi Fi adapter cellular transceiver Bluetooth transceiver or the like or any combination thereof. Depending on the specific nature and purpose of the processing device the I O devices can include various devices e.g. a display which may be a touch screen display audio speaker keyboard mouse or other pointing device microphone camera etc.

Unless contrary to physical possibility it is envisioned that i the methods steps described above may be performed in any sequence and or in any combination and that ii the components of respective embodiments may be combined in any manner.

The techniques introduced above can be implemented by programmable circuitry programmed configured by software and or firmware or entirely by special purpose circuitry or by any combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware to implement the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

In this description references to an embodiment one embodiment or the like mean that the particular feature function structure or characteristic being described is included in at least one embodiment of the technique introduced here. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment. Note that any and all of the embodiments described above can be combined with each other except to the extent that it may be stated otherwise above or to the extent that any such embodiments might be mutually exclusive in function and or structure.

Although the disclosed technique has been described with reference to specific exemplary embodiments it will be recognized that the technique is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

