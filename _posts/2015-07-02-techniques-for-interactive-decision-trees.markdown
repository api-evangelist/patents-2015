---

title: Techniques for interactive decision trees
abstract: Techniques for providing interactive decision trees are included. For example, a system is provided that stores data related to a decision tree, wherein the data includes one or more data structures and one or more portions of code. The system receives input corresponding to an interaction request associated with a modification to the decision tree. The system determines whether the modification requires multiple-processing iterations of the distributed data set. The system generates an application layer modified decision tree when the generating requires no multiple-processing iterations of the distributed data set. The system facilitates server layer modification of the decision tree when the modification requires multiple-processing iterations of the distributed data set. The system generates a representation of the application layer modified decision tree or the server layer modified decision tree.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09495426&OS=09495426&RS=09495426
owner: SAS Institute Inc.
number: 09495426
owner_city: Cary
owner_country: US
publication_date: 20150702
---
This application claims priority to U.S. Provisional Patent Application No. 62 038 354 filed Aug. 17 2014 entitled A Three Tier In memory Architecture for Interactive Decision Tree on Large Scale Distributed Data Sets which is incorporated herein by reference in its entirety.

Decision tree learning algorithms are vital tools for predictive modeling approaches that use statistics data mining and machine learning. For example decision tree learning is commonly used in data mining to create a model that predicts the value of a target variable based on several input variables. Decision tree learning is one of the most successful techniques for supervised classification learning.

In accordance with the teachings included herein systems and methods are provided for providing interactive decision trees.

For example a system is provided for providing interactive decision trees. The system may include one or more processors. The system may further include one or more computer readable storage mediums comprising instructions to cause the one or more processors to perform operations. The operations may include receiving by the one or more processors a decision tree corresponding to a distributed data set. The operations may further include storing data related to the decision tree wherein the data includes one or more data structures and one or more portions of code. The operations may further include receiving input corresponding to an interaction request associated with a modification to the decision tree. The operations may further include determining whether the modification requires multiple processing iterations of the distributed data set. The operations may further include generating an application layer modified decision tree when the generating requires no multiple processing iterations of the distributed data set wherein the application layer modified decision tree is generated using the stored data. The operations may further include facilitating server layer modification of the decision tree when the server layer modification requires multiple processing iterations of the distributed data set wherein facilitating the server layer modification includes transmitting information related to the interaction request and receiving a server layer modified decision tree. The operations may further include generating a representation of the application layer modified decision tree or the server layer modified decision tree.

In another example a non transitory computer program product tangibly embodied in a non transitory machine readable storage medium is provided for providing interactive decision trees. The non transitory computer program product including instructions operable to cause a data processing apparatus to receive by a computing device a decision tree corresponding to a distributed data set. The computer program product may include further instructions operable to store data related to the decision tree wherein the data includes one or more data structures and one or more portions of code. The computer program product may include further instructions operable to receive input corresponding to an interaction request associated with a modification to the decision tree. The computer program product may include further instructions operable to determine whether the modification requires multiple processing iterations of the distributed data set. The computer program product may include further instructions operable to generate an application layer modified decision tree when the generating requires no multiple processing iterations of the distributed data set wherein the application layer modified decision tree is generated using the stored data. The computer program product may include further instructions operable to facilitate server layer modification of the decision tree when the server layer modification requires multiple processing iterations of the distributed data set wherein facilitating the server layer modification includes transmitting information related to the interaction request and receiving a server layer modified decision tree. The computer program product may include further instructions operable to generate a representation of the application layer modified decision tree or the server layer modified decision tree.

In still one further example a computer implemented method is provided for providing interactive decision trees. The method may include receiving by the one or more processors a decision tree corresponding to a distributed data set. The method may further include storing data related to the decision tree wherein the data includes one or more data structures and one or more portions of code. The method may further include receiving input corresponding to an interaction request associated with a modification to the decision tree. The method may further include determining whether the modification requires multiple processing iterations of the distributed data set. The method may further include generating an application layer modified decision tree when the generating requires no multiple processing iterations of the distributed data set wherein the application layer modified decision tree is generated using the stored data. The method may further include facilitating server layer modification of the decision tree when the server layer modification requires multiple processing iterations of the distributed data set wherein facilitating the server layer modification includes transmitting information related to the interaction request and receiving a server layer modified decision tree. The method may further include generating a representation of the application layer modified decision tree or the server layer modified decision tree.

In accordance with at least one embodiment a system method or non transitory computer program product may further include instructions and or operations that include facilitating a display of the representation of the application layer modified decision tree or server layer modified decision tree wherein facilitating the display uses a graphical interface related to the decision tree wherein the interaction request is generated using the graphical interface and wherein the representation of the application layer modified decision tree or the server layer modified decision tree is provided using the graphical interface.

In accordance with at least one embodiment the server layer modified decision tree corresponds to a portion of the decision tree.

In accordance with at least one embodiment the decision tree may include a set of nodes and a set of predictor variables wherein facilitating the server layer modification includes instructions to cause the one or more processors to perform operations including transmitting node data corresponding to a node transmitting predictor variable data corresponding to a predictor variable receiving potential node split data corresponding to a potential node split wherein the potential node split data is generated using the node and the predictor variable and associating the potential node split data with the node.

In accordance with at least one embodiment the multiple processing iteration of the data set requires more than a single read of the data set.

In accordance with at least one embodiment the interaction request relates to modifying a sub tree of the decision tree.

In accordance with at least one embodiment generating the server layer modified decision tree includes further instructions to cause the one or more processors to calculate a value of worth of a plurality of splits of a node of the decision tree.

In accordance with at least one embodiment a system method or non transitory computer program product may further include instructions and or operations that include determining a computational threshold value determining a computational requirement corresponding to the interaction request generating the application layer modified decision tree when the computational requirement is equal to or less than the computational threshold value and facilitating the server layer modification of the decision tree when the computational requirement exceeds the computational threshold value.

In accordance with at least one embodiment determining whether the application layer modification requires multiple processing iterations of the distributed data set contains further instructions to cause the one or more processors to perform operations including executing a query using a data store wherein the data store contains information related to a plurality of interaction request types and corresponding computational requirements for the plurality of interaction request types wherein the interaction request corresponds to an interaction request type and receiving information related to a computational requirement for the interaction request in response to executing the query.

In accordance with at least one embodiment the server layer modification of the decision tree is performed using a distributed file system.

In accordance with at least one embodiment the application layer modified decision tree uses a data structure and wherein the server layer modification of the decision tree uses a different data structure.

In accordance with at least one embodiment the graphical interface is provided using a thin client application.

In accordance with at least one embodiment the thin client application resides on a computing device that is separate from a device including the one or more processors.

In accordance with at least one embodiment facilitating the display uses the one or more processors to display the application layer modified decision tree and wherein facilitating the display uses processors other than the one or more processors to display the server layer modified decision tree.

In accordance with at least one embodiment the application programming interface is used to facilitate the display of a graphical interface.

In accordance with at least one embodiment an application programming interface is used to transmit information related to the interaction request and to receive the server layer modified decision tree.

Certain aspects of the disclosed subject matter relate to techniques for providing an interactive decision tree associated with a distributed data set. Decision tree algorithms are a type of machine learning algorithm that utilize recursive partitioning to segment the input data set into a tree structure that are used to make predictions and or decisions within each segment of the data. Decision tree models may be used for example for predictive modeling data stratification missing value imputation outlier detection and description variable selection and other areas of machine learning and statistical modeling. As used herein an interactive decision tree is intended to refer to a decision tree that may be modified by a user utilizing a graphical interface. Some common interactive actions include but are not limited to pruning a node splitting a node using a predictor e.g. a user specified predictor best splitting a node with a set of candidate predictors e.g. a set of candidate user specified predictors training a sub tree of a node removing the latest action from a user restoring a previous decision tree and the like. An interactive decision tree may be implemented in a variety of analytic software and may be used to combine user inputs and data mining algorithms for building analytics models.

Current techniques for providing interactive decision trees may be computationally expensive due to recursive nature of the underlying algorithms. This may be especially expensive when associated with large scale data such as a distributed data set. Techniques described herein reduce the computational burden by providing an architecture for managing calculations associated with providing an interactive decision tree for a distributed data set.

In accordance with at least one embodiment a tree can be learned by splitting the data set into subsets based on an attribute value test. This process may be repeated on each derived subset in a recursive manner. The recursion may be complete for example when the subset at a node has all the same value of the target variable or when splitting no longer adds value to the predictions. The recursive nature of such algorithms can make interactive decision trees difficult to implement with respect to distributed data sets especially large scale distributed data sets.

In accordance with at least one embodiment a multiple tier architecture may be configured to enable an interactive decision tree for a large scale data set where the large scale data set is stored in a distributed file system e.g. a Hadoop File System HDFS . For example the functionality of an interactive decision tree may be divided into three tiers including but not limited to a display tier an application tier and a server tier. In at least one example the display tier may include one or more display tier devices that operate a thin client application. The thin client application may be used for example for rendering visual representations of the interactive decision tree. The application tier may include one or more application tier devices that create and maintain a data structure for the interactive decision tree. The server tier may include one or more server tier devices. The server tier may in at least one example reside in a distributed file system and may be responsible for performing heavy duty computations that require multiple processing iterations on the distributed data set. As an illustrative example computationally extensive calculations such as training a decision tree or computing worth of splits for a set of candidate predictors may be computed by a device included in the server tier.

Thus the processes described below provide minimized communication between the server tier and the application tier resulting in fewer calculations by a server tier device than in a system that does not implement the multiple tier architecture disclosed herein. All tiers discussed herein may be developed in memory thus alleviating the need for memory disk input output operations once the data set has been loaded in a distributed file system of the server tier. Further as a result of the multiple tier architectures described herein operations related to an interaction request that do not require multiple processing iterations of a distributed data set may be executed by the application tier devices independently from the server tier saving server resources for more computationally intensive tasks. As used herein multiple processing iterations is intended to describe interactions between the user and the representation of a decision tree where the interaction requires generation of new statistics or data segmentation. Both tasks of generating new statistics and segmenting data require one or more processing iterations of the original data set. For example in order to split a node of a decision tree using a variable one processing iteration will be necessary to compute which of a number of variables specified by the user is the optimal variable to spit the node e.g. segmenting the data and another processing iteration will be necessary in order to compute chi square statistics e.g. generating statistics . Interactions that require a single interaction of the distributed data set require no new statistics or information from the original data such as for example undo ing a user action pruning a node and the like.

As a result of the multiple tier architectures described herein a real time interactive decision tree may be implemented for a large scale distributed file system. Such a design allows multiple display devices to build a decision tree and interact with a decision tree concurrently or substantially simultaneously. Additionally devices that lack the computational capability to execute multiple iterations of processing on large distributed data sets or devices that are merely undesirable to use for such purposes may nonetheless interact with data sets utilizing the tiers discussed herein as the most computationally intensive tasks are performed at the server level separate from the device.

In one example the computer implemented environment may include a stand alone computer architecture where a processing system e.g. one or more computer processors includes the system being executed on it. The processing system has access to a computer readable memory .

In one example the computer implemented environment may include a client server architecture. The users may utilize a PC to access the servers running the system on the processing system via the networks . The servers may access the computer readable memory .

Some or all of the process described in relation to providing an interactive decision tree may be performed under the control of one or more computer systems configured with specific computer executable instructions and may be implemented as code e.g. executable instructions one or more computer programs or one or more applications executing collectively on one or more processors by hardware or combinations thereof. The code may be stored on a non transitory computer readable storage medium for example in the form of a computer program including a plurality of instructions executable by one or more processors. The computer readable storage medium may be non transitory.

A disk controller can interface with one or more optional disk drives to the bus . These disk drives may be external or internal floppy disk drives such as a storage drive external or internal CD ROM CD R CD RW or DVD drives or external or internal hard drive . As indicated previously these various disk drives and disk controllers are optional devices.

A display interface may permit information from the bus to be displayed on a display in audio graphic or alphanumeric format. Communication with external devices may optionally occur using various communication ports . In addition to the standard computer type components the hardware may also include data input devices such as a keyboard or other input output devices such as a microphone remote control touchpad keypad stylus motion or gesture sensor location sensor still or video camera pointer mouse or joystick which can obtain information from bus via interface .

In accordance with at least one embodiment illustrates a first example architecture A for providing an interactive decision tree. The distributed file system of may include a head node . The head node may be responsible for storing metadata on a dedicated server and for coordinating data storage functions. The architecture A may include an edge node . The edge node may act as an interface between the distributed file system of and client devices of . The edge node may be used to run client applications and cluster administration tools with respect to data transmitted to and from distributed file system of . The architecture A may include one or more client devices .

In accordance with at least one embodiment a server tier including one or more server tier devices that may be configured within the architecture A. For example the server tier may include one or more data nodes and at least one head node . The server tier e.g. the data nodes of in conjunction with the head node may perform computationally expensive decision tree computations that require multiple iterations through the distributed file system of .

In accordance with at least one embodiment an application tier including one or more application tier devices that may be configured within the architecture A. For example the application tier may include edge node . The application tier may maintain and update one or more decision tree data structures. Various interactive features described throughout this specification may result in a modification or query to a decision tree structure stored in the application tier device s e.g. the edge node . The application tier e.g. the edge node of may perform computations for a decision tree algorithm that do not include multiple processing interactions.

In accordance with at least one embodiment a display tier including one or more display tier devices may be configured within the architecture A. For example the display tier may include the client devices of . The display tier in this example provides visual representations of interactive decision trees. In at least one example the computations performed at the display layer involve operations included in displaying visual representations of decision trees and communicating user interactions away from the client devices .

In accordance with at least one embodiment illustrates a second example architecture B for providing an interactive decision tree. The distributed file system of may include one or more of the data nodes of . The distributed file system of may include a head node . The head node may be responsible for storing metadata on a dedicated server and for coordinating data storage functions. The architecture B may include an edge node . The edge node may act as an interface between the distributed file system of and client devices of . The architecture B may include one or more client devices .

In accordance with at least one embodiment a server tier includes one or more server tier devices may be configured within the architecture B. For example server tier may include data nodes of and head node . The server tier e.g. the data nodes of in conjunction with head node may perform computationally expensive interactive decision tree computations that require multiple iterations through the distributed file system of .

In accordance with at least one embodiment client devices of may be configured to include both application tier functionality as well as display tier functionality. Thus the client devices of may perform operations including but not limited to maintaining and updating one or more decision tree data structures modifying or querying a decision tree structure stored on the client devices of performing computations that are not related to a multiple processing decision tree algorithm providing visual representations of decision trees and the like.

In accordance with at least one embodiment illustrates a third example architecture C for providing an interactive decision tree. The distributed file system of may include one or more of the data nodes of . The distributed file system of may include a head node . The head node may be responsible for storing metadata on a dedicated server and for coordinating data storage functions as well as providing an interface between distributed file system of and client devices of . The architecture C may include one or more client devices .

In accordance with at least one embodiment a server tier including one or more server tier devices may be configured within the architecture C that includes the data nodes of and the head node . The server tier e.g. the data nodes of in conjunction with head node may perform computationally expensive decision tree computations that require multiple iterations through the distributed file system of .

In accordance with at least one embodiment client devices of may be configured to include both application tier functionality as well as display tier functionality. Thus the client devices of may perform operations including but not limited to maintaining and updating one or more decision tree data structures modifying or querying a decision tree structure stored on the client devices of performing computations that are not related to a multiple processing decision tree algorithm providing visual representations of decision trees and the like.

The architectures A C are intended to be illustrative in nature. Any number and combination of data nodes head nodes edge nodes and client devices may be utilized and will be apparent to one skill in the art of configuring distributed file systems.

In accordance with at least one embodiment the application tier device may utilize at least the following structures. For example a data structure e.g. a Tree may be utilized by an application tier device. In at least one example the Tree may store interactive decision tree node information including but not limited to response profiles a root node one or more parent nodes one or more child nodes one or more spilt variables one or more split rules and the like. A DisplayTree data structure may be utilized by an application tier device. A DisplayTree data structure may be generated from a Tree data structure. The DisplayTree may be utilized by display tier devices to provide a visual representation of an interactive decision tree.

In accordance with at least one embodiment the application tier devices may utilize at least the following functions. For example a function e.g. Tree.Append may be utilized to append a sub tree generated by a server to a tree node in a decision tree. A function e.g. Tree.AppendWorth may be utilized to append worth of splits information e.g. via a table to a tree node in the current decision tree. A function e.g. Tree.Display may be utilized to generate a DisplayTree data structure. A function e.g. Tree.GetPath may be utilized to return a path from the root node to a specific node. The path may be used for filtering observations from the data set. A function e.g. Tree.GetSplitVar may be utilized to return a splitting variable and a splitting rule corresponding to a specific node in the Tree. A function e.g. Tree.GetSubTree may be utilized to return a sub tree structure associated with a specific node. A function e.g. Tree.Next may be utilized to return a current e.g. next snapshot of a Tree data structure. A function e.g. Tree.Previous may be utilized to return the previous snapshot of a Tree data structure. A function e.g. Tree.Remove may be utilized to remove a sub tree structure associated with a specific node. A function e.g. Tree.Save may be utilized to save a Tree data structure in memory e.g. on an application tier device . A function e.g. Tree.SetSplitValue may be utilized to manually update a splitting value associated with a specific node. A function e.g. Tree.UpdateSubTree may be utilized to update a sub tree structure associated with a specific node.

In accordance with at least one embodiment the server tier may include but is not limited to the following structures. For example a data structure e.g. a ServerTree may be utilized by server tier devices. In at least one example the ServerTree may store node information associated with each node the node information including but not limited to response profiles a parent node one or more child nodes one or more spilt variables one or more split rules and the like.

In accordance with at least one embodiment the server tier devices may utilize at least the following function calls. A function e.g. ServerTree.GetWorth may be utilized to return a table of worth of splits associated with one or more candidate variables that may be used to split a specific node. A function e.g. ServerTree.Train may be utilized to train a decision tree or sub tree of a node. A function e.g. ServerTree.Update may be utilized to provide update a summary of statistics in a Tree without changing the Tree structure. ServerTree.Update may include but is not limited to operations for copying a sub tree from one node to another and obtaining a summary of statistics on validation and or testing data sets or streaming data.

In the following examples described in let D be an input data set Y be a response variable in the decision tree model e.g. one column of the input data set and X be a set of predictors e.g. one or more predictors .

The process may begin at where a user may request e.g. via an interaction request a decision tree to be constructed using data set D response variable Y and a set of predictors X via an interaction request. For example a user may request a decision tree by utilizing a graphical user interface e.g. the graphical user interface of described below to identify a data set a response variable and a set of predictors. The interaction request may include but is not limited to interaction information corresponding to the data set D the response variable Y and the set of predictors X. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations to determine the computational requirements for the user request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request for a decision tree require multiple pass iterations of the data set D. In at least one example the determination of computational requirements for the user request may include a query of a data store responsible for storing such information. For example a table may be accessible that provides indications of the computational requirements for various function calls. When a function is called the table may be queried to determine whether or not the function will require multiple passes of the data set. The query may return an integer a Boolean or any suitable means for indicating computational requirements. Additionally or alternatively computational requirements of a user s request may be calculated using the size of the data set the computational capabilities of the device e.g. memory processing speed connection speed etc. the computational capabilities of the server s e.g. memory processing speed connection speed server farm size etc. the size of the decision tree structure a number of nodes user preferences information or any suitable combination thereof. In some embodiments if the computational requirement s for the user request is above a particular threshold value one or more remote devices e.g. one or more server devices may be used to process the request. In some cases if the computational requirement s for the user request is below a particular threshold value a local device e.g. the user s device may be used to process the request.

In accordance with at least one embodiment based on the decision at the interaction request information may be passed to the server tier device at . At the server tier device may execute operations e.g. ServerTree.Train of to train a decision tree using the data set D the response variable Y and the set of predictors X.

In accordance with at least one embodiment the server tier device may return a trained decision tree at . The application tier device may utilize the data returned at to maintain a tree structure e.g. a Tree data structure of . Maintaining a tree structure as used herein may include creating and or updating a Tree data structure. For example application tier device may utilize the function Tree.Save of to save the Tree data structure in memory. A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at .

It should be appreciated that while examples included through this description may discuss binary decision trees e.g. decision trees that have nodes that have at most two corresponding child nodes non binary trees e.g. decision trees having nodes with any number of corresponding child nodes may be utilized with any process function call interface etc. described herein.

The process may begin at where a user may request e.g. via an interaction request a splitting of a node K of a decision tree using a predictor variable P. For example a user may request to split a node of a decision tree by utilizing a graphical user interface e.g. the graphical user interface of described below to identify a node and a predictor variable. The interaction request may include but is not limited to user request information corresponding to the node K and the predictor variable P. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements for the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request to split a node require multiple pass iterations of the data set D.

In accordance with at least one embodiment at a path may be created from the root node of the decision tree to node K e.g. utilizing the function Tree.GetPath of . The path in at least one example may be used to filter observations of the data set D. The information corresponding to at least the path and P may be passed to server tier device at .

In accordance with at least one embodiment at the server tier device may execute operations to split a subset of data associated with node K into two partitions e.g. utilizing the function ServerTree.Train function of D Y P and the path created at . In accordance with at least one embodiment the server tier device may return a sub tree at . At the application tier device may create a snapshot of the Tree structure e.g. utilizing the Tree.Previous function of and may further utilize the data returned at to append the sub tree data to a Tree structure e.g. utilizing the Tree.Append function of . For an update to an existing sub tree the sub tree data of the Tree structure may be updated using the data returned at e.g. utilizing the Tree.UpdateSubTree function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at e.g. utilizing the graphical user interface of described below .

The process may begin at where a user may request e.g. via an interaction request pruning of a node K of a decision tree. For example a user may request pruning of a node K of a decision tree by utilizing a graphical user interface e.g. the graphical user interface of described below to identify a node to prune. The interaction request may include but is not limited to interaction request information corresponding to the node K. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements for the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request to prune a node do not require multiple pass iterations of the data set D.

In accordance with at least one embodiment the application tier device may execute operations to identify a sub tree below node K at e.g. utilizing the Tree.GetSubTree function of . At the application tier device may create a snapshot of the Tree structure e.g. utilizing the Tree.Next function of and may further utilize the data returned at to remove the sub tree data from the Tree structure e.g. utilizing the Tree.Remove function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at e.g. utilizing the graphical user interface of described below .

The process may begin at where a user may request e.g. via an interaction request a best split determination of a node K of a decision tree using a set of predictor variables X. For example a user may request a best split determination of a node of a decision tree by utilizing a graphical user interface e.g. the graphical user interface of described below to identify a node. The interaction request may include but is not limited to interaction request information corresponding to the node K and the set of predictor variables X. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements for the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request to determine a best split of a node require multiple pass iterations of the data set D.

At a path may be created from the root node of the decision tree to node K e.g. utilizing the function Tree.GetPath of . The path in at least one example may be used to filter observations of the data set D. The information corresponding to at least the path and the set of predictor variables X may be passed to server tier device at .

At the server tier device may execute operations to split a subset of data associated with node K into two partitions e.g. utilizing the function ServerTree.Train function of D Y X and the path created at . Two partitions are used as an example it should be appreciated that the node may be split in any number of partitions. In accordance with at least one embodiment the server tier device may return a sub tree at . At the application tier device may create a snapshot of the Tree structure e.g. utilizing the Tree.Previous function of and may further utilize the data returned at to append the sub tree data to the current Tree structure e.g. utilizing the Tree.Append function of . For an update to an existing sub tree the sub tree data of the Tree structure may be updated using the data returned at e.g. utilizing the Tree.UpdateSubTree function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . A split variable may be determined e.g. utilizing the function Tree.GetSplitVar of . Information corresponding to the generated DisplayTree data structure and the split variable may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure using the split variable at e.g. utilizing the graphical user interface of described below .

The process may begin at where a user may request e.g. via an interaction request training of a level L sub tree on a node K using a set of predictor variables X. For example a user may request a training a level sub tree on a node using a set of predictor variables by utilizing a graphical user interface e.g. the graphical user interface of described below to identify a node a set of predictor variables and a maximum depth value of the sub tree. The interaction request may include but is not limited to interaction request information corresponding to the node K the set of predictor variables X and a maximum depth value corresponding to L. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements for the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request to train a sub tree of a node with a set of predictor variables require multiple pass iterations of the data set D.

At a path may be created from the root node of the decision tree to node K e.g. utilizing the function Tree.GetPath of . The path in at least one example may be used to filter observations of the data set D. The information corresponding to at least the path the set of predictor variables X and a maximum depth value corresponding to L may be passed to server tier device at .

At the server tier device may execute operations to split a subset of data associated with node K into two partitions e.g. utilizing the function ServerTree.Train function of D Y X L and the path created at . In accordance with at least one embodiment the server tier device may return a sub tree at . At the application tier device may create a snapshot of the Tree structure e.g. utilizing the Tree.Previous function of and may further utilize the data returned at to append the sub tree data to the current Tree structure e.g. utilizing the Tree.Append function of . For an update to an existing sub tree the sub tree data of the Tree structure may be updated using the data returned at e.g. utilizing the Tree.UpdateSubTree function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at e.g. utilizing the graphical user interface of described below .

The process may begin at where a user may request e.g. via an interaction request restoring the previous action. For example a user may request to restore a prior decision tree by utilizing a graphical user interface e.g. the graphical user interface of described below . The interaction request may include but is not limited to interaction request information corresponding to the display tree. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements for the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request to restore a previous action do not require multiple pass iterations of the data set D.

In accordance with at least one embodiment at the application tier device may execute operations to restore a previous snapshot of the decision tree e.g. utilizing the Tree.Previous function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at .

The process may begin at where a user may request e.g. via an interaction request information related to a worth of a split on a node K related to a set of predictor variables X. The interaction request may include but is not limited to interaction request information corresponding to the node K and the set of predictor variables X. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements for the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request for information related to a worth of a split of a node using a set of predictor variables require multiple pass iterations of the data set D.

At a path may be created from the root node of the decision tree to node K e.g. utilizing the function Tree.GetPath of . The path in at least one example may be used to filter observations of the data set D. The information corresponding to at least the path and the set of predictor variables X may be passed to server tier device at .

At the server tier device may execute operations to determine a set of worth values e.g. utilizing the function ServerTree.GetWorth function of D Y X L and the path created at . In accordance with at least one embodiment the server tier device may return a set of worth values at . At the application tier device may create a snapshot of the Tree structure e.g. utilizing the Tree.Next function of and may further utilize the data returned at to append the data corresponding to the set of worth values to node K e.g. utilizing the Tree.AppendWorth function of . For an update to an existing sub tree the sub tree data of the Tree structure may be updated using the data returned at e.g. utilizing the Tree.UpdateSubTree function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at e.g. utilizing the graphical user interface of described below .

The process may begin at where a user may request e.g. via an interaction request a manual change of a splitting value S under a tree node K using a set of predictor variables X. In at least one example node K may have two child nodes K and K. The interaction request may include but is not limited to interaction request information corresponding to the node K the child nodes K and K the splitting value S and the set of predictor variables X. The interaction request information may be communicated by the display tier device to the application tier device . At the application tier device may execute operations that determine the computational requirements of the interaction request. For example the application tier device may execute operations that determine that the computational requirements of the interaction request e.g. a request to manually change the splitting value under a node using a set of predictor variables require multiple pass iterations of the data set D.

At the splitting rule stored in the Tree structure may be updated for node K e.g. utilizing the function Tree.SetSplitValue of . At new paths may be created from the root node of the decision tree to each of child nodes K and K e.g. utilizing the function Tree.GetPath of . The paths in at least one example may be used to filter observations of the data set D. The information corresponding to at least the paths and the set of predictor variables X may be passed to server tier device at .

At the server tier device may execute operations to train a decision tree on data associated with the path to K Path and the path to K Path separately using the set of predictor variables X e.g. utilizing the function ServerTree.Train function of D Y X L Path and Path . At the server tier device may return a sub tree corresponding to Path sub tree and a sub tree corresponding to Path sub tree . At the application tier device may create a snapshot of the Tree structure e.g. utilizing the Tree.Previous function of and may further utilize the data returned at to append sub tree and sub tree to the current Tree structure e.g. utilizing the Tree.UpdateSubTree function of . For an update to an existing sub tree the sub tree data of the Tree structure may be updated using the data returned at e.g. utilizing the Tree.UpdateSubTree function of . A DisplayTree data structure may be generated e.g. utilizing the function Tree.Display of at using the Tree structure described at . Information corresponding to the generated DisplayTree data structure may be communicated to display tier device at . The display tier device may display e.g. via a thin client application a visual representation of the DisplayTree data structure at .

The graphical interface may include a visual representation of an interactive decision tree . The graphical interface may include a set of variables included in data set corresponding to the interactive decision tree . A response variable may be specified utilizing graphical interface . A set of predictor variables may be included in graphical interface . A data set representation may be displayed using the graphical interface . The user may select an auto update indicator to indicate that he wishes the interactive decision tree to be updated as he interacts with graphical interface . Alternatively the user may select an update button to manually update the interactive decision tree at a time of his choosing.

In accordance with at least one embodiment upon selection of the option to split the node information related to splitting the node using each of a set of predictor variables may be displayed for example via a dialog box . As a non limiting example each predictor variable of the set of predictor variables may be displayed in column of dialog box . Additionally a corresponding log worth for each predictor variable may be displayed in column of dialog box . The user may select a particular predictor variable e.g. Television Region as the predictor variable with which to split the node . The user may complete the action by selecting the OK button .

In accordance with at least one embodiment upon selection of the option to train the sub tree of node each predictor variable of the set of predictor variables may be displayed in column of dialog box . Additionally a corresponding log worth for each predictor variable may be displayed in column of dialog box . The user may select a set of predictor variables e.g. including predictor variables Age Gender and Residential Neighborhood with which to train the sub tree of node . The user may specify for example via selector a maximum depth value for the trained sub tree. The user may complete the action by selecting the OK button .

In accordance with at least one embodiment an interaction request associated with a modification to the decision tree may be received at e.g. by the application tier devices of . An interaction request may indicate one or more user interactions with graphical interfaces . Further the interaction request may include user request information including but not limited to a data set a decision tree one or more nodes of a decision tree a set of predictor variables a particular predictor variable a response variable one or more paths from a root node of the decision tree to one or more nodes of the decision tree a splitting value a maximum depth value of the like.

At a determination regarding whether or not the modification requires multiple processing iterations of the distributed data set may be determined e.g. by the application tier devices of . Multiple processing iterations is intended to refer to operations that are executed using the data set that are recursive in nature.

At a locally modified decision tree may be generated e.g. by the application tier devices of . For example the locally modified decision tree may be generated given the determination at that the modification did not require multiple processing iterations of the distributed data set.

At remote modification of the decision tree may be facilitated e.g. by the application tier devices of . As a non limiting example facilitating remote modification of the decision tree may include transmitting information related to the interaction request and or receiving a remotely modified decision tree. Remote modification of the decision tree may be facilitated at when for example the determination at indicates that the modification requires multiple processing iterations of the distributed data set.

At a representation of the locally modified decision tree or the remotely modified decision tree may be generated e.g. by the application tier devices of .

Systems and methods according to some examples may include data transmissions conveyed via networks e.g. local area network wide area network Internet or combinations thereof etc. fiber optic medium carrier waves wireless networks etc. for communication with one or more data processing devices. The data transmissions can carry any or all of the data disclosed herein that is provided to or from a device.

Additionally the methods and systems described herein may be implemented on many different types of processing devices by program code comprising program instructions that are executable by the device processing subsystem. The software program instructions may include source code object code machine code or any other stored data that is operable to cause a processing system to perform the methods and operations described herein. Other implementations may also be used however such as firmware or even appropriately designed hardware configured to carry out the methods and systems described herein.

The data e.g. associations mappings data input data output intermediate data results final data results etc. may be stored and implemented in one or more different types of computer implemented data stores such as different types of storage devices and programming constructs e.g. RAM ROM Flash memory removable memory flat files temporary memory databases programming data structures programming variables IF THEN or similar type statement constructs etc. . It is noted that data structures may describe formats for use in organizing and storing data in databases programs memory or other computer readable media for use by a computer program.

A computer program also known as a program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules subprograms or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network. The processes and logic flows and figures described and shown in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.

Generally a computer can also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks . However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a tablet a mobile viewing device a mobile audio player a Global Positioning System GPS receiver to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of nonvolatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks . The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

The computer components software modules functions data stores and data structures described herein may be connected directly or indirectly to each other in order to allow the flow of data needed for their operations. It is also noted that a module or processor includes but is not limited to a unit of code that performs a software operation and can be implemented for example as a subroutine unit of code or as a software function unit of code or as an object as in an object oriented paradigm or as an applet or in a computer script language or as another type of computer code. The software components or functionality may be located on a single computer or distributed across multiple computers depending upon the situation at hand.

The computer may include a programmable machine that performs high speed processing of numbers as well as of text graphics symbols and sound. The computer can process generate or transform data. The computer includes a central processing unit that interprets and executes instructions input devices such as a keyboard keypad or a mouse through which data and commands enter the computer memory that enables the computer to store programs and data and output devices such as printers and display screens that show the results after the computer has processed generated or transformed data.

Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Implementations of the subject matter described in this specification can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus . The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated processed communication or a combination of one or more of them. The term data processing apparatus encompasses all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a graphical system a database management system an operating system or a combination of one or more of them .

While this disclosure may contain many specifics these should not be construed as limitations on the scope of what may be claimed but rather as descriptions of features specific to particular implementations. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be utilized. Moreover the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations and it should be understood that the described program components and systems can generally be integrated together in a single software or hardware product or packaged into multiple software or hardware products.

Some systems may use Hadoop an open source framework for storing and analyzing big data in a distributed computing environment. Some systems may use cloud computing which can enable ubiquitous convenient on demand network access to a shared pool of configurable computing resources e.g. networks servers storage applications and services that can be rapidly provisioned and released with minimal management effort or service provider interaction. Some grid systems may be implemented as a multiple node Hadoop cluster as understood by a person of skill in the art. Apache Hadoop is an open source software framework for distributed computing. Some systems may use the SAS LASR Analytic Server in order to deliver statistical modeling and machine learning capabilities in a highly interactive programming environment which may enable multiple users to concurrently manage data transform variables perform exploratory analysis build and compare models and score. Some systems may use SAS In Memory Statistics for Hadoop to read big data once and analyze it several times by persisting it in memory for the entire session.

Some aspects may utilize the Internet of Things IoT where things e.g. machines devices phones sensors can be connected to networks and the data from these things can be collected and processed within the things and or external to the things. For example with the IoT there can be sensors in many different devices and high value analytics can be applied to identify hidden relationships and drive increased efficiencies. This can apply to both Big Data analytics and realtime streaming analytics.

It should be understood that as used in the description herein and throughout the claims that follow the meaning of a an and the includes plural reference unless the context clearly dictates otherwise. Also as used in the description herein and throughout the claims that follow the meaning of in includes in and on unless the context clearly dictates otherwise. Finally as used in the description herein and throughout the claims that follow the meanings of and and or include both the conjunctive and disjunctive and may be used interchangeably unless the context expressly dictates otherwise the phrase exclusive or may be used to indicate situations where only the disjunctive meaning may apply.

