---

title: System and processor implemented method for improved image quality and generating an image of a target illuminated by quantum particles
abstract: According to some embodiments, system and methods for image improvement comprise: receiving a plurality of frames of a given region of interest, the frames comprised of a plurality of pixels; determining, based on a quantum property of the frames, a normalized pixel intensity value for each pixel of each of the plurality of frames; and generating an improved image of the given region of interest based on the plurality of frames and the corresponding normalized pixel intensity values for the frames, the order of the image being two. Also embodiments for generating an image of a target illuminated by quantum entangled particles, such as, photons, are disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09378542&OS=09378542&RS=09378542
owner: The United States of America as represented by the Secretary of the Army
number: 09378542
owner_city: Washington
owner_country: US
publication_date: 20150716
---
This application is a divisional application of U.S. patent application Ser. No. 14 303 078 filed Jun. 12 2014 which in turn is a continuation in part CIP application of and claims priority to U.S. patent application Ser. Nos. 14 086 463 and 14 022 148 filed Nov. 21 2013 and Sep. 9 2013 respectively both herein incorporated by reference in their entirety. Those applications in turn are a continuation in part CIP application of and claim priority to U.S. patent application Ser. No. 13 838 249 filed Mar. 15 2013 now U.S. Pat. No. 8 594 455 and U.S. patent application Ser. No. 13 247 470 filed Sep. 28 2011 now U.S. Pat. No. 8 532 427 herein incorporated by reference in their entirety. This application also claims priority to U.S. Provisional Application No. 61 834 497 titled System and Method for Image Enhancement and Improvement filed on Jun. 13 2013 herein incorporated by reference.

The invention described herein may be manufactured used and or licensed by or for the United States Government without the payment of royalties.

Image processing is a form of signal processing for which the input is an image such as a photograph or video frame and the output is either image or a set of characteristics or parameters related to the image. Forms of image processing include for example but are not limited to face detection feature detection medical image processing computer vision extraction of information from an image by a computer microscope image processing etc.

Image resolution relates at least in part to the detail that an image possesses. For satellite images generally speaking an image is considered to be more detailed as the area represented by each pixel is decreased. As used herein the term images include digital images electronic images film images and or other types of images. Cameras taking pictures from great distances such as aerial photos may not obtain detailed information about the subject matter. Consequently subtle or detail information may not present in the images.

An image may be captured by for example a monochrome camera a single charge coupled device CCD or complementary metal oxide semiconductor CMOS sensor and the image is formed via the light intensity projected onto the sensor therein.

In U.S. Pat. No. 7 536 012 to Meyers et al. hereby incorporated by reference entitled Entangled Quantum Communications and Quantum Imaging there is disclosed inter alia a quantum imaging system see Col. 8 line 50 et seq. in which the sender sends an image of an image mask using entangled photons and coincidence measurements to a receiver. The system differs from the conventional quantum imaging set up in that polarization beam splitters are placed in the path of the photons to provide two channels for each of the sender and the receiver as shown in FIG. 4 of the 012 patent. On the sender s side a photon beam is split by a beam splitter into first and second sub beams. The first sub beam is passed through a mask 164 which creates the image which is directed through a beam splitter 166 to bucket detectors 168 170 which are operatively connected to a coincidence circuit. The second sub beam is transmitted to the receiver without ever passing through the mask 164. In the embodiment of FIG. 4 of the 012 patent the receiver receives the second sub beam and an image of the mask is constructed based upon photon coincident measurements composited from two photon detectors 168 and 170 also referred to a bucket detectors. The image of a mask is transmitted via coincidences and the photons transmitting the image have never encountered the image mask. Because of the somewhat puzzling nature or circumstances of the transmission the process has been dubbed by some as Ghost Imaging while others have explained the effects as resulting from the quantum properties of light.

According to embodiments a processor implemented method for image improvement comprising receiving a plurality of frames of a given region of interest the frames comprised of a plurality of pixels determining based on a quantum property of the frames a normalized pixel intensity value for each pixel of each of the plurality of frames and generating an improved image of the given region of interest based on the plurality of frames and the corresponding normalized pixel intensity values for the frames the order of the image being two.

The frames of a region of interest are readily influenced by the effects of turbulence obscuration low signal to noise ratio bad or changing weather and or low lighting conditions. This leads to a poor image. Yet by employing the novel image improvement method of the present invention the image can be significantly improved ameliorating these negative effects. Unlike classical image improvement techniques conventionally employed the novel processing relies upon quantum properties.

The received frames may be generated by the detector directly or indirectly. For instance the frames could be output straight from the detector for processing or the frame may be generated by the detector stored in a memory and retrieved from the memory for processing at some later time. Not all frames need to be used for processing for all applications. Indeed in some implementations fewer than the total number of frames can be used to determine an improved image of the region of interest.

The normalized pixel intensity value may be determined based on a quantum property of the same frame data which comes from the detector. As such only one detector or measuring device may be needed in some embodiments. In general terms the normalized pixel intensity values may be thought as of averaged intensity values. For instance determining the normalized pixel intensity value of a frame may comprise determining pixel values within a frame summing pixel intensity values for determined pixel values within a frame and dividing each summed pixel value by the number of determined pixel values to form a normalized pixel intensity value for each pixel in the frame.

Generating the improved image of the region of interest can then comprise calculating i the average of the product of determined pixel values and the corresponding normalized pixel intensity values for the plurality of frames and ii the product of the average of the determined pixels values for each frame and the average of normalized pixel intensity values for the plurality of frames. And in some embodiments generating the improved image of the region of interest comprises may include further comprising taking the difference of i and ii .

Calculating i the average of the product of determined pixel values and the corresponding normalized pixel intensity values for the plurality of frames in some instances may comprise multiplying pixel values for determined pixels within each frame by the corresponding normalized pixel intensity values for that frame to produce a product for each frame summing the products of all the frames and determining the average of first product arrays by dividing the sum of product by the number of frames. And calculating ii the product of the average of the determined pixels values for each frame and the average of normalized pixel intensity values for the plurality of frames in some instances may comprise determining the average value of each pixel for each frame for the plurality of frames determining the average normalized pixel intensity value for each pixel for the plurality of frames and multiplying the average pixel values and the average of the normalized pixel intensity value for each pixel.

Determining pixels values within a frame for processing may be achieved by various methodologies including one or more of selecting all pixels within each frame selecting pixel based upon at least one predetermined criterion selecting pixels which are shifted a pre determined distance away from select pixels and or determining an average value of adjacent pixels for select pixels. Practicing the image improvement method may further includes selecting at least one measurable property for determining a normalized pixel intensity value for each pixel of each of the plurality of frames and using at least one different measurable property of the plurality of frames for generating the improved image. A measurable property may include wavelength or wavelength band color polarity polarization orbital angular momentum spin a quantum particle or any combination thereof.

Depending on the desired application the frames can comprise regions of interest that are radiation emitting. Or the frames of a region of interest comprise sparse image data and the improved image is generated using the sparse image data.

Select processing in generating the improvement image may be further employed in some embodiments based on differences among the frames. For instance the image improvement method can further include determining a frame intensity deviation value for each frame by subtracting the average frame intensity for the plurality of first frames from the frame intensity for each frame and classifying the frame intensity deviation values for each frame based on whether the frame intensity deviation values is positive or negative. Then selecting processing for generating an improved image may be carried out based on the aforementioned classification. In addition select processing may be carried out based on conditional product values for pixels. For example the method can further comprise calculating one or more conditional product values of the classified frame intensity deviation values for each frame and selecting one or more of the conditional product values to generate the improved image. More in some embodiments at least two calculated conditional product values are treated differently based upon their classification. Or all calculated conditional product values might be used to generate the improved image without any change thereto.

Additional refinements of the image improvement method may further be employed in many embodiments. These may comprise interpolating the pixel values for each frame to a finer resolution. Also filtering of the frame data the normalized pixel intensity value and or any data used in one or more calculations thereof can be utilized to additional improve the generation of the image.

Iterating or repeating certain processing may also be employed in embodiments. For instance providing an iterated improved image of the region of interest may comprises specifying one or more pixel locations to be normalized to form the normalized pixel intensity value for each pixel selecting new pixel locations based on a pre determined pixel selection criteria from the values of the improved image of the region of interest reviewing the new determined pixels to determine if the new determined pixel locations are substantially the same as the pixel locations previously determined pixel locations and repeating the aforementioned steps until a specified iteration criteria is met.

According to embodiments a system for image improvement comprises at least one processor and at least one input for receiving or inputting frames of data and at least one memory operatively associated with the at least one processor adapted to store frames of data taken of a region of interest each frame of data comprises an array of pixels with each pixel having a pixel value. The at least one processor is configured to process a plurality of frames of a given region of interest according to one or more or the aforementioned image improvement method embodiments. The input may be operably connectable to an input device such as a scanner a DVD player CMOS camera SPAD array video camera smart phone plenoptic camera cell phone lidar ladar television CCD or analog and or digital camera.

According to other embodiments a processor implemented method for image improvement comprises receiving a plurality of frames of a given region of interest the frames comprised of a plurality of pixels each pixel including a value of at least one measurable property of quantum particles specifying the order of the improved image to be generated the order being greater than or equal to two selecting at least one measurable quantum property for pixel values of the frames corresponding to the specified order determining based on the at least one measurable quantum property normalized pixel intensity values for each pixel of each of the plurality of frames up to the specified order to generate the improved image and generating an improved image of the given region of interest based on the plurality of frames and the corresponding normalized pixel intensity values for the frames. The at least measurable quantum property can comprise for example wavelength or wavelength band color polarity polarization orbital angular momentum spin quantum phase a quantum particle or any combination thereof.

Additionally according to embodiments a system for generating an image of a target illuminated by quantum entangled particles. The system includes an illumination system comprising at least one source of quantum entangled particle pairs and a beamsplitter receiving the quantum entangled particle pairs such that one particle from each pair of particle generated by each source interfere on the beamsplitter causing the interfered particles to be directed towards a target and the remaining particle pairs are not directed towards the target wherein the illumination system is configured so that the interfered particles interact with the target a measuring system comprising a first detector and a second detector that are configured to perform at least one spatially resolved measurement of particles where the first detector measures one of the remaining particle pairs and the second detector measures the other of the remaining particle pairs and a processor configured to generate an image of the target based upon the correlated measured values and spatial information from the first detector and the second detector.

In some implementations the system can further include electronics configured to determine coincidences based on measurements of the first and second detectors which occur within a predetermined time interval. The processor can further be configured to generate at least a second order image using the coincidences. Also the processor may be configured to apply an image improvement method for generating at least a second order image using at least one measurable quantum property.

In other implementations the system may further include an optical delay element configured to introduce a time delay for particles reaching the measuring system. This optical delay element may in some embodiment be further configured to be operated so as to generate an absorption image of the target a reflection image of the target or both.

Additionally the system may optionally include is some embodiment an optical delay line configured to ensure particle interference at the beamsplitter and or a phase modulator configured to modify the phase relationship between the particle pairs generated by the two sources of quantum entangled particle pairs respectively.

In one particular embodiment the illumination system comprises a single source of entangled particle pairs and a pair of beamsplitters receiving the entangled particles pairs such that one particle from each pair of particles generated by each source interfere on the beamsplitter causing the interfered particles to be directed towards a target and the remaining particle pairs to be retained wherein the illumination system is configured so that the interfered particles interact with the target causing absorption at the target entangling the retained particle pair.

In some embodiments the system may be configured so that the interfered particles interact with the target causing absorption at the target entangling the retained particle pairs. And it yet other embodiments the system may be configured so that the interfered particles interact with the target causing reflection at the target and further comprises optics or focusing components and measurement electronics wherein the measurement of the reflected entangled particles entangled the retained particle pairs.

These and other aspects of the embodiments herein will be better appreciated and understood when considered in conjunction with the following description and the accompanying drawings. It should be understood however that the following descriptions while indicating preferred embodiments and numerous specific details thereof are given by way of illustration and not of limitation. Many changes and modifications may be made within the scope of the embodiments herein without departing from the spirit thereof and the embodiments herein include all such modifications.

The embodiments herein and the various features and advantageous details thereof are explained more fully with reference to the non limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well known components and processing techniques are omitted so as to not unnecessarily obscure the embodiments herein. The examples used herein are intended merely to facilitate an understanding of ways in which the embodiments herein may be practiced and to further enable those of skill in the art to practice the embodiments herein. Accordingly the examples should not be construed as limiting the scope of the embodiments herein.

Rather these embodiments are provided so that this disclosure will be thorough and complete and will fully convey the scope of the invention to those skilled in the art. Like numbers refer to like elements throughout. As used herein the term and or includes any and all combinations of one or more of the associated listed items.

It will be understood that although the terms first second etc. may be used herein to describe various groups subgroups elements components regions layers and or sections these groups subgroups elements components regions layers and or sections should not be limited by these terms. For example when referring first and second groups or subgroups these terms are only used to distinguish one group subgroup element component region layer or section from another group subgroup region layer or section. Thus a first group subgroup element component region layer or section discussed below could be termed a second element component.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to limit the full scope of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

Unless otherwise defined all terms including technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs. It will be further understood that terms such as those defined in commonly used dictionaries should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.

The present invention provides a method and system for the enhancement of images using the quantum properties of light or matter. An embodiment of the present invention increases the image quality of an object or scene as seen by a detector. When a low quality detector is aimed at an object a high quality image is generated using the quantum properties of light. A low quality detector picks up quantum information on the object shape and its temporal relations to reference fields. The reference fields may be recorded by the same imager CCD camera etc. that acts as a bucket detector that is it does not necessarily contain spatial information . Current imaging methods are limited to the quality of the detector looking at the object being imaged. A preferred embodiment generates an improved quality image of the object without the object being imaged in high resolution directly. The preferred method may be used in connection with photographs taken during turbulent conditions.

An alternate embodiment comprises computing the average overall intensity of a plurality of frames and arranging the frames into two sets. A first set contains the frames having frame intensities greater than the average overall intensity for all frames the average overall intensity being the summation of the intensities for frames divided by the number of frames. The second set containing frames having an overall intensity less than the average overall intensity. Each of the first and second sets is processed by repeating steps a through i . The result obtained using the second set of frames is then subtracted from the result obtained using the first set of frames to create the image.

Yet another embodiment includes a method for image improvement that comprises providing a series of frames of a given region of interest determining the value of each pixel within each frame to form a first array of pixel values determining the integral of pixel values of each frame to form a second array of overall frame integrals partitioning the first array into two sets based on a predetermined criteria partitioning the second array into two sets based on a predetermined criteria using the first and second sets of the first and second arrays to compute a conditional product term for each of the four combinations of the first and second sets of the first and second arrays and combining the conditional product terms to provide an improved image of the region of interest.

And yet a further embodiment includes a method for image improvement comprising providing a series of frames of a given region of interest determining the value of each pixel at each location within each frame to form a first array of pixel values for each frame determining the overall intensity of each frame determining the product of the overall intensity and the array of pixel values for each frame determining the sum of the products by adding together the products of the overall frame intensity and first array of pixel values for each frame determining the average of the sum of products by dividing the sum of products by the number of frames in the series of frames determining the average value of each pixel at each pixel location for the series of frames to form a second array of average pixel values determining the average overall frame intensity for the series of frames determining a second product of the second array of average pixel values and the average overall frame intensity subtracting the second product from the first product to provide an improved image of the region of interest partitioning the improved image into at least two partitions based on a predetermined criteria mathematically operating upon the partitioned improved image to increase image contrast or image clarity.

The current invention utilizes the ability to increase the image quality of an object as seen by a detector using methods relating to the Quantum nature of light. For example when a low quality detector is aimed at an object then a high quality image may be generated based on the quantum properties of light. The high quality image is generated even in the presence of turbulence which might otherwise be disruptive to image clarity. Scattering of quantum particles such as photons off the object carries information of the object shape even when the quantum particles such as photons do not go directly into the camera or detector. An additional low quality bucket detector such as for example a detector lacking spatial information records quantum information on the object shape and its temporal relations to collocated reference fields. The reference fields may be recorded by the same type of imager CCD Camera etc. that looks at the object and which act like bucket detectors in U.S. Pat. No. 7 536 012 referenced in the foregoing.

Current imaging methods are limited to the quality of the detector looking at the object being imaged. Embodiments of this invention enable an image quality improvement by using techniques developed in the course of Ghost Imaging experiments and includes but is not limited to methods to generate a high quality image of the object without the object being imaged in high resolution directly i.e. low resolution images may be enhanced thereby enabling high quality imaging when only low quality images of the object are imaged directly.

Imaging of a scene or subject is typically accomplished by mapping an illuminated scene or subject onto an image sensor where there is a light measurement component such as film CCD or other sensing device. Light consists of a plurality of photons that may be measured. The illuminating light may be from one or more light sources either natural or artificial or both. Common sources of light include for example the sun coherent incoherent or partially coherent light entangled photons infrared radiation emitted by atoms and molecules accelerating charges lasers light bulbs light emitting diodes LEDs chaotic laser light pseudo thermal light generated by passing laser light through a rotating ground glass or other scattering material stars moons clouds planets space objects fluorescent lamps electrical discharges plasmas bio luminescence and stimulated emission. Although it is not absolutely necessary a lens is often used to perform this mapping. Imaging is often susceptible to adverse affects such as obscuration turbulence low signal to noise ratio such as when operating in low light conditions jitter and noise. Often this type of imaging is referred to as First Order imaging due to the time ensemble or mixed time ensemble averaging of the sensors involved. For instance a first order light intensity image I x y t can be produced by light interacting with a sensor for some time t i.e. shutter or integration time. A single instance of this may be referred to as a frame . Multiple frames of images I x y t may be averaged over some or all of the frames in a sequence of frames to generate an averaged first order image of the subject where indicates an ensemble average. A second order image involves averages of products of two first order intensity or normalized intensity measurements. An enhanced image results from the subtraction of products of averages of first order intensities from the average of the product of the intensities. An intensity or normalized intensity can be decomposed into a mean plus a deviation from the mean or average I .

The terms Iand Iare intensities or normalized intensities measured by sensors and I Iand I Iwith Iand Ibeing functions of space and time i.e. x y t. is the ensemble average of intensity or normalized measurements of sensor and Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor . is the ensemble average of intensity or normalized measurements of sensor and Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor . The deviation is often called a fluctuation.

Mathematically the second order enhanced image can be described by . Simplifying this expression yields .

I. . . Imay be selectable by user as follows. For instance an imaging system may be configured to provide at least one measurement value for at least one spatial location of a region of interest. The spatial location can be represented by position within an array of pixels . A frame of measured values may be composed of a plurality of pixels typically in an at least one dimensional array that together may form an image. Exemplary frames may be electronic image data such a TIFF or JPEG file. The measured values can be assigned to variables I I . . . Iin a variety of ways. As an example when the imaging system provides a single color or gray scale image of the region of interest per frame then each measured pixel value may be assigned to variable I and variable Imay be assigned the value of the sum of the measurement pixel values per frame. For a different embodiment Imay be for instance assigned the measured values of a particular pixel say pixel i 12 j 300 and variable Iassigned the measured values of pixel i 522 j 207 where i and j are indices into the 2D arrays of measured values. Generally each measured value assigned to one of the Ivariables may be selected by the user to meet the requirements for their specific application. At least one of the variables Imust be assigned positions that correspond to the pixel positions of the measured pixel value arrays provided by the imaging system at known pixel position coordinates.

In an example of a second order improved image Iand Imay refer to intensities measured by at least two sensors where one of the sensors measures spatial information of the light I coming from the scene or subject the Reference sensor and the other sensor measures a quantity representative of the intensity I coming from the scene or subject i.e. a the bucket sensor. One of the sensors may be a virtual sensor wherein for instance the representative intensity coming from the scene or subject is comprised of spatially integrating all or a selected subset of pixels on a CCD or CMOS camera or even consist of a single pixel from a CCD or CMOS camera. The enhanced image is contained in which has a function like correspondence between points on the object and points on the image sensor and is largely unaffected by the degrading effects of turbulence obscuration low signal to noise ratio such as when operating in low light conditions jitter and noise. See for example Meyers et al. Turbulence free ghost imaging Appl. Phys. Lett. 98 111115 2011 herein incorporate by reference.

A preferred method for practicing the present invention may be correlated to the mathematical representations as follows. Expressed using the terms Iand I a preferred method for image improvement comprises inputting a series of frames of an image determining the value of each pixel at each location within each frame to form a pixel value array for each frame summing the pixel values in each frame to obtain the frame intensity for each frame correlates to determining I multiplying the pixels in pixel value array by the frame intensity to produce a frame intensity multiplied pixel value array correlates to determining the product II summing the frame intensity multiplied pixel value arrays together and dividing by the number of frames to obtain an average of the frame intensity multiplied pixel value arrays correlates to determining using the pixel value arrays creating an array of average pixel values correlates to determining determining the average frame intensity for the series of frames correlates to determining multiplying the array of average pixel values by the average frame intensity for all of the inputted frames correlates to the product and subtracting the array of average pixel values multiplied by average frame intensity from the average of the frame intensity multiplied pixel value arrays correlates to to provide an array of modified pixel values to form an improved image second order image which from the previously expresses mathematical equations correlates to the equation .

Other preferred methods may include the normalizing of the intensity to produce an enhanced image. There are several ways to normalize intensity. One way is to divide the Reference pixel intensity values by a non zero value bucket intensity J I I. This normalization would give J Jand I Iwith Jand Ibeing functions of space and time i.e. x y t. Where Jand Iare normalized intensities and intensities measured by sensors and . is the ensemble average of intensity or normalized measurements of sensor and Jis the deviation from the mean for the normalized intensity measurements of sensors and . is the ensemble average of intensity or normalized measurements of sensor and Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor . The deviation is often called a fluctuation.

Mathematically the second order enhanced image can be described by . Simplifying this expression yields rearranging terms yields wherein the enhanced image is contained in . The enhanced image may be normalized by the product of the standard deviations of Iand Ito generate an enhanced image that displays the correlation of Iand I. Other alternative ways to normalize the enhanced image include dividing or by the product or respectively.

One embodiment of the current invention comprises the subject area illumination being generated by one or more light sources which can be internal external or a mixture of external and internal light sources. Examples of external light sources include the sun coherent incoherent or partially coherent light illuminating the subject area generated by natural or artificial means indoors or out of doors propagating through any transmissive or partially transmissive media such as the air water or biological tissue including cells. Examples of internal light sources include the subject emanating light in the infrared given off by atoms and molecules. Light received may be reflected scattered or emanated from the subject into at least one first receiver at predetermined time intervals. Light may be received at the at least one second receiver at corresponding time intervals from the light source which may be reflected or partially reflected from the subject and contains spatial information. The first and second receivers may be selected from for example one or more arrays of pixels from one or more cameras imagers CCDs etc. In a preferred embodiment the measured values are transmitted from the first and second receivers to the at least one processor. The measured values of the at least one first receiver are then correlated with the spatially resolved measurements of the at least one second receiver at the corresponding intervals of time. A first image of the subject is then created based upon the correlated measured values and spatial information by combining the spatial information from at least one second receiver at predetermined intervals of time with the measured values from at least one first receiver at the corresponding intervals of time. An enhanced second image of the subject is generated by removing the blurred distorted or noisy averaged first order image part from the first image. The first order image part may be removed by subtraction or other equivalent mathematical operation.

It is to be appreciated that the methods and techniques described in this invention can be applied to microscopy. Microscopy of biological samples in particular can be degraded by the transmission and scattering of light propagating through scattering and absorbing media that can significantly degrade the quality of the image. It is to be appreciated that substituting a microscope objective for a telescope as described in certain embodiment only alters the focal length of the optical system and does not affect the image enhancement properties of this invention.

Another embodiment would entail generating enhanced images using intensity products where more than two intensity measurements are available. This is especially useful for when the intensity deviations do not follow Gaussian statistics. This would involve simultaneous measurements of three or more sensors at a time. Our method would be applied to generate enhanced images of the form . . . . This has application to the investigation of turbulence finding of non classical photon behavior and as a research tool exploring higher order correlation statistics the investigation of the fundamental nature of quantum physics such as non local correlations Bell inequalities and EPR effects.

There is a trend in modern imaging devices i.e. cameras to provide more measured quantities at each pixel. Thus the intensity measurements may include measurements such as wavelength color or derived color mappings such as RGB CMY CMYK etc. polarization Stokes parameters spatial modes orbital angular momentum OAM spin etc. For example a color camera may provide separate wavelength measurements red R green G and blue B intensity values at each pixel the polarization Stokes parameters at each pixel and modern infrared IR cameras can provide measurements of long wave infrared LWIR and mid wave infrared MWIR intensities at each pixel of the imaging device or combinations of these measurements. In the current invention at least one of the available measured quantities is selected to provide the frame data for the generation of the improved image of the region of interest.

It is to be appreciated that measurements of quantities such as wavelength and polarization are typically dependent on the responsiveness of the measurement device to the quantity being measured. As an example color cameras typically use band pass filters arranged in a pattern over the pixels of the measurement device. These filters are usually labeled Red Green and Blue R G and B . The wavelengths that each of the R G G filters pass is centered at a particular wavelength and also passes nearby wavelengths with wavelengths being more distant from the center wavelength being more highly attenuated. This effect is referred to as the bandwidth of the filter. Polarization filters have similar bandwidths with respect to the orientation of the filter. The responsiveness to wavelength polarization etc. of an element on a measurement may also be adjusted by applying for example larger or smaller voltages to increase or decrease the degree to which each element pixel reacts to the wavelength or polarization of light that interacts with that pixel.

It is to be further appreciated that information that can be extracted from measurements made by at least one sensor may exist over a spectrum or bandwidth of time and space scales and that extraction of such information may be facilitated by electronic computational and or optical filtering. For example electronic filtering of measurements to emphasize lower or higher frequencies may enhance the observation of correlations between sensor measurements and improve the generated enhanced images. The character and amount of electronic and or optical filtering needed to optimize enhanced images may vary with the types and makeup of the sensors used the physical properties of the area of interest the physical properties of the source of illumination the nature and physical properties of the intervening regions e.g. atmosphere ocean biological tissue material outer space obscurants etc between the sensors illuminators and the region of interest. By physical properties we also mean to include physical chemical biological and electronic properties.

Referring now to in accordance with one preferred embodiment in Box a series of frames are inputted into the memory or input of a processor or image processor. The frame may be composed on a plurality of pixels typically in a two dimensional 2D array that together form an image. Exemplary frames may be electronic image data such a TIFF or JPEG file. As used herein the terminology processor or image processor as used in the following claims includes a computer multiprocessor CPU minicomputer microprocessor or any machine similar to a computer or processor which is capable of processing algorithms. The frames may comprise photographs of the same region of interest. The region of interest may be a scene landscape an object a subject person or thing. In Box the frame data or value of each pixel at each pixel location is determined for a frame. In Box the overall intensity of the frame is determined. The overall intensity correlates to a bucket value determination in that overall intensity value does not comprise spatial information. Instead it correlates to the summation of the light intensity of a frame. In the case of a picture the overall intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels the overall intensity is the summation each pixel value at each pixel location within a given frame. At Box the values in Box are multiplied by the value determined in Box . Box represents the Frame Data Intensity Product for the frame. Inasmuch as the Frame Data is an array of pixel values the Frame Data Intensity Product is also an array of values. At Box the products of Box Frame Data Intensity Product are repeated for each frame in a selected plurality of frames. As an example one hundred frames may be selected. At Box the summation of the Frame Data Intensity Products for the plurality of frames determined in Box is divided by the number of frames such as for example one hundred to determine the Frame Data Intensity Product Average for the plurality of frames. As noted in Box this Product Average is an array containing pixel values at each pixel location within the frame.

Box represents the multiplication of Boxes and to form the Average Frame Data Average Intensity Product which is an array. As shown in the bottom portion of the Average Frame Data Average Intensity Product is subtracted from the Frame Data Intensity Product Average to form the refined image of Box .

It is postulated that the preferred methodology in effect subtracts out or negates the effects or errors due to the effects of turbulence or the like. Most fluctuations caused by turbulence occur at the edges of objects. The algorithm focuses on the edges of letters objects etc. to refine the image edges.

In the lower portion of note that Box A is repeated as shown by the arrow. In Box A the average frame data or average value of each pixel at each pixel location is determined for the first set of frames by averaging the pixel values at each pixel location for the first set of frames to determine an array of average pixel values for the first set. In Box A the average overall intensity for the first set of frames is determined. This is similar to the determination of Box A except that Box A is a determination for a frame and Box A is an average for a plurality of frames. As stated with respect to Box A the overall frame intensity correlates to a bucket value determination in that overall intensity value does not comprise spatial information. Instead it correlates to the summation of the light intensity of a frame. In the case of a picture the overall frame intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels the overall intensity is the summation each pixel value at each pixel location within a given frame. The average overall intensity is the summation of the values for a plurality of frames divided by the number of frames.

Box represents the multiplication of Boxes A and A to form the Average Frame Data Average Intensity Product which is an array. As shown in the bottom portion of the Average Frame Data Average Intensity Product is subtracted from the Frame Data Intensity Product Average to form the refined image of Box A.

The steps are comparable in effect to the similarly numbered frames in as denoted by the addition of a letter B suffix to the correlating element number. In Box B the frame data or value of each pixel at each pixel location is determined for a frame. In Box B the overall intensity bucket value of the frame is determined. In the case of a picture the overall intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels the overall intensity is the summation each pixel value at each pixel location within a given frame. At Box the values in Box B are multiplied by the value determined in Box B. Box B represents the Frame Data Intensity Product for the frame. Inasmuch as the Frame Data is an array of pixel values the Frame Data Intensity Product is also an array of values. At Box B the products of Box B Frame Data Intensity Product are repeated for each frame in a second set of frames. At Box B the summation of the Frame Data Intensity Products for the plurality of frames determined in Box B is divided by the number of frames such as for example one hundred to determine the Frame Data Intensity Product Average for the second set of frames. As noted in Box B this Product Average is an array containing pixel values at each pixel location within the frame.

In the lower portion of note that Box B is repeated as shown by the arrow. In Box B the average frame data or average value of each pixel at each pixel location is determined for the first set of frames by averaging the pixel values at each pixel location for the first set of frames to determine an array of average pixel values for the first set. In Box B the average overall intensity for the second set of frames is determined. This is similar to the determination of Box B except that Box B is a determination for a frame and Box B is an average for a plurality of frames. As stated with respect to Box B the overall frame intensity correlates to a bucket value determination in that overall intensity value does not comprise spatial information. Instead it correlates to the summation of the light intensity of a frame. In the case of a picture the overall frame intensity correlates to the sum of the reflected illumination. In the case of an electronic display formed by pixels the overall intensity is the summation each pixel value at each pixel location within a given frame. The average overall intensity is the summation of the values for a plurality of frames divided by the number of frames.

Box represents the multiplication of Boxes B and B to form the Average Frame Data Average Intensity Product which is an array. As shown in the bottom portion of the Average Frame Data Average Intensity Product is subtracted from the Frame Data Intensity Product Average to form the refined image of Box B.

Another alternate preferred method of the present invention applies the use of techniques from the field of Compressive Imaging or Compressive Sensing. In this embodiment the bucket values for each frame of the series is computed by integrating the values of the pixels within each frame. This bucket data is stored for use per Eq. 5 below. The pixel values for each frame of the series are stored as a row in a matrix J. The improved image is computed by application of a Compressive Imaging inversion algorithm such as GPSR to solve Eq. 6. The improved image is returned in the matrix R.

Virtual Ghost Imaging refers to an imaging process which creates an enhanced image from a series of frames of an imaging subject based on a process related to Ghost Imaging.

Virtual Ghost Imaging in the current instance applies the following process to a series of frames of an imaging subject. Inasmuch as the overall frame intensity value determined in Box correlates to the bucket value a brief discussion of ghost imaging and reflective ghost imaging follows. Typically ghost imaging uses two detectors one to observe the light source and the other single pixel or bucket detector to observe the light scattering and reflecting from the target object. 1 where denotes an ensemble average. If Iand Iare recorded from the same target object Imay be computed as 2 Basic Virtual Ghost Imaging

Results of an experiment conducted through turbulence using chaotic laser or pseudo thermal light are presented in . shows the same target computed with data taken using a typical two path configuration.

A relatively new mathematical field named Compressive Sensing CS or Compressive Imaging CI can be used to good effect within the context of ghost imaging. The first use of compressive techniques in the context of Ghost Imaging was performed by the Katz group see O. Katz et al. Compressive Ghost Imaging Appl. Phys. Lett. 95 131110 2009 hereby incorporated by reference who demonstrated a ghost like imaging proposal of Shapiro see J. Shapiro Computational Ghost Imaging Phys. Rev. A 78 061802 R 2008 . Their demonstration was limited to a transmission object.

More recently the present inventors have performed experiments of this nature using reflection objects.

The inventors use of CS and CI is based on finding approximate solutions to the integral equations using the GPSR mathematical methodology where 3 and 4 is the object reflectance. The term J is a matrix where the rows are the illumination patterns at time k and the B vector 5 represents the bucket values. In cases where the system is underdetermined too few B then Lconstraints are applied to complete the system and sparseness is used 

Compressive imaging CI results for the ARL target are presented using Eq. 2 and varying the parameter. is an example of a result where is too large and most of the pixel values are driven to 0. One can sense that the letters ARL are in the figure. Decreasing to a value of 5e7 shown in more portions of the ARL letters appear. When is set to 2.5e7 the R is quite clear in but the appearance of the A and the L are still degraded. Continuing with the examination of the effect of the parameter the value of is set to 1e7. These results are shown in .

Finally as a lower image quality bound is set to equal 1e6. The ARL presented in is quite degraded. These GPSR calculated Virtual Ghost imaging results highlight the sensitivity of the calculations to an external parameter which has no connection to the underlying physics.

Results were computed using Eq. 1 subject to the self bucketing concept of Eq. 2. These results are generated from a few hundred shots of the Air Force resolution target imaged at a 100 m distance through turbulence.

A single image from this data set is presented in . This image illustrates the distorting effect of turbulence on imaging. A simple averaging of 335 frames of the dataset that was performed is shown in . This average image has some better qualities that the single frame image of but one can still only resolve the coarser scale features of the Air Force target.

Using the self bucket ghost imaging concept on this dataset an initial result using only 2 frames of the dataset is displayed in . Some of the edges in this image are very distinct and superior to areas in either the instantaneous or the average images. When the entire dataset is used as presented in the results are striking. In particular the 4 and 5 on the right side of the target are legible and the small horizontal and vertical bars to the left of the numbers are distinct whereas those regions in the instantaneous and average images are simply blurs.

In a preferred embodiment there is provided a method to partition the values in the measured data sets i.e. frames into two or more groups for the frame data reference fields and overall frame intensities bucket values . These groups are then used to compute products or cross correlations between the different groupings. These individual product terms can be mathematically combined via addition and or subtraction processes to generate improve images of the target or scene. This method further adapts the techniques presented in the parent application. One key advantage to this method is that it is possible with the following embodiments to generate all positive valued images and largely eliminate background and noise effects. Other advantages include the ability to operate on a computed partitioned image using functions such as logarithms and exponentials to further increase contrast or better identify objects and information on their properties.

The following embodiments are predicated on the appreciation that other operations involving the partitioned sets of above average and below average measurements are beneficial to improve image quality in adverse conditions such as turbulence. These operations would include but are not limited to cross correlations between above average bucket overall frame intensities and below average reference fields. Typically four correlation types are available when data is partitioned into two distinct sets such as above the average and below the average values. In a non normalized form this can be written as

The product terms that comprise a particular Rmay be computed conditionally. The Rcan be called conditional product terms. For instance Rmay be computed for the set of pixel values Ithat are above the mean for those frames of data with bucket values Ithat are above the mean. For example 

Another alternative embodiment may include computing a Gimproved image. This improved image is then partitioned into pixels that are for example above the spatial mean Gand pixels that are below the spatial mean G. These alternative positive negative Gpartitioned improved images can display higher contrast and can be further operated upon by using mathematical operations such as logarithms to increase the dynamic range. It is to be recognized that other partitions are possible to tailor results needed for specific applications.

Another alternative embodiment may include computing a Rcorrelation image by calculating the correlation coefficient between the Iand Ipartitions where the Iand Iare not aligned in time or frame. For instance at a particular pixel i j there may be 10 frames in which that pixel is above the mean value of that pixel for all frames and there may only be say 5 frames for which the Ivalues is above the mean of I. A correlation coefficient may be computed between these two sets of data using 

As an example for imaging measurements of pixel values and bucket values that have been grouped into two sets each of above and below their respective averages there is a total of 80 possible ways to present for output the results of the computed conditional product terms. For instance each conditional product term may be presented for display individually with either a positive or negative sign. Thus individually for the four conditional product terms there is a total of 8 ways to present them for display. Combinations of two conditional product terms with signs allows for 24 options to present for display combinations of three conditional product terms allows for 32 and combinations of all four conditional product terms allows for 16 ways to present for output and display.

Continuing in in Box P if overall intensity deviation is positive for a given frame the frame is grouped into a first group P. In Box P for each frame in the first group select all pixels in each frame with a positive deviation from the array of average pixel values and place in a first subgroup PP.

In Box P for each frame in the first group select all pixels in each frame with a negative deviation from the array of average pixel values and place in a second subgroup PN. In Box P for each frame in the first group select all pixels in each frame with a zero deviation from the array of average pixel values and place in a either the first or second subgroup PP or PN .

Continuing in in Box N if the overall intensity deviation is negative for a given frame the frame is grouped into a second group N. In Box N for each frame in the second group select all pixels in each frame with a positive deviation from the array of average pixel values and place in a third subgroup NP. In Box N for each frame in the second group select all pixels in each frame with a negative deviation from the array of average pixel values and place in a fourth subgroup NN. In Box N for each frame in the second group select all pixels in each frame with a zero deviation from the array of average pixel values and place in a either the first or second subgroup NP or NN . In Box for each pixel value for each frame in the each subgroup multiply each pixel value by overall intensity deviation for that frame to obtain a first product array or conditional product array for each frame in the subgroup PN PP NP NN . In Box for each subgroup add up the product arrays in the subgroup and divide by the number of frames in the subgroup to obtain a second or average conditional product array for each subgroup. In Box one of more second product arrays is selected to generate an image.

The method proceeds to . In Box a series of frames are entered into the memory or input of a processor or image processor. The frames may comprise photographs of the same region of interest. The region of interest may be a scene landscape an object a subject person or thing. These frames are the same frames used in Box and may be reused if they are still available in the memory or input of the processor or image processor. In Box the Average Frame Data is determined by computing the average value of each pixel at each pixel location for the plurality of frames. In Box the Deviation Frame Data is determined by subtracting the Average Frame Data from the Frame Data for all pixels in each frame for the plurality of frames.

The method proceeds via path to which shows how to generate a third set of data which is referred to here at SET . SET data includes conditional product terms using frames having a positive overall intensity deviation and positive deviation frame data pixels. SET may be determined as follows In Box frames with a Positive Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Positive Frame Data Deviation pixels within that set of frames. In Box pixel locations a the square of the Positive Overall Intensity Deviation b the product of the Positive Overall Intensity Deviation the Positive Deviation Frame Data pixels and the square of the Positive Deviation Frame Data pixels are recorded and accumulated. In Box the pre normalized Positive Positive Product pixel values are determined by dividing the product of the Positive Overall Intensity Deviation the Positive Deviation Frame Data Frame Data pixels by . The average of the squares of the Positive Overall Intensity is determined by dividing by . The average of the squares of the Positive Deviation Frame Data pixels is determined by dividing by .

Continuing via path to the method proceeds to In Box the standard deviation of the Positive Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Positive Overall Intensity Deviation . In Box the standard deviations of the Positive Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Positive Deviation Frame Data pixels . Box determines the Normalized Positive Positive Product pixel values by dividing the pre Normalized Positive Positive Product by the product of the standard deviation of the Positive Overall Intensity Deviation and the standard deviation of the Positive Deviation Frame Data pixels .

The method proceeds via path to which shows how to generate a fourth set of data which is referred to here at SET . SET frame data includes conditional product terms using frames having a negative overall intensity deviation and negative deviation frame data pixels. SET may be determined as follows In Box frames with a Negative Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Negative Frame Data Deviation pixels within that set of frames. In Box pixel locations a the square of the Negative Overall Intensity Deviation b the product of the Negative Overall Intensity Deviation the Negative Deviation Frame Data pixels and the square of the Negative Deviation Frame Data pixels are recorded and accumulated. In Box the pre normalized Negative Negative Product pixel values are determined by dividing the product of the Negative Overall Intensity Deviation the Negative Deviation Frame Data Frame Data pixels by . The average of the squares of the Negative Overall Intensity is determined by dividing by . The average of the squares of the Negative Deviation Frame Data pixels is determined by dividing by .

From Box in the method can proceed via path to . In Box the standard deviation of the Negative Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Negative Overall Intensity Deviation . In Box the standard deviations of the Negative Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Negative Deviation Frame Data pixels . Box determines the Normalized Negative Negative Product pixel values by dividing the pre Normalized Negative Negative Product by the product of the standard deviation of the Negative Overall Intensity Deviation and the standard deviation of the Negative Deviation Frame Data pixels . At the conclusion of Box the method proceeds via path to for determining an improved image data or to for determining alternative improved image data by an alternative embodiment.

Returning to the method also can concurrently proceeds via path to which shows how to generate a fifth set of data which is referred to here at SET . SET frame data includes conditional product terms using frames having a negative overall intensity deviation and positive deviation frame data pixels. SET may be determined as follows In Box frames with a Negative Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Positive Frame Data Deviation pixels within that set of frames. In Box pixel locations a the square of the Negative Overall Intensity Deviation b the product of the Negative Overall Intensity Deviation the Positive Deviation Frame Data pixels and the square of the Positive Deviation Frame Data pixels are recorded and accumulated. In Box the pre normalized Positive Negative Product pixel values are determined by dividing the product of the Negative Overall Intensity Deviation the Positive Deviation Frame Data Frame Data pixels by . The average of the squares of the Negative Overall Intensity is determined by dividing by . The average of the squares of the Positive Deviation Frame Data pixels is determined by dividing by .

From Box in the method can proceed via path to . In Box the standard deviation of the Negative Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Negative Overall Intensity Deviation . In Box the standard deviations of the Positive Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Positive Deviation Frame Data pixels . Box determines the Normalized Positive Negative Product pixel values by dividing the pre Normalized Positive Negative Product by the product of the standard deviation of the Negative Overall Intensity Deviation and the standard deviation of the Positive Deviation Frame Data pixels .

At the conclusion of Box in the method proceeds via path to for determining an improved image data or to for determining alternative improved image data by an alternative embodiment.

Similar as discussed above with respect to the fifth set of data SET returning to the method also can concurrently proceed via path to which shows how to generate a sixth set of data which is referred to here at SET . SET frame data includes conditional product terms using frames having a positive overall intensity deviation and a negative deviation frame data pixels. SET may be determined as follows In Box frames with a Positive Overall Intensity Deviation multiply the value of the Overall Intensity Deviation by the Negative Frame Data Deviation pixels within that set of frames. In Box pixel locations a the square of the Positive Overall Intensity Deviation b the product of the Negative Overall Intensity Deviation the Negative Deviation Frame Data pixels and the square of the Negative Deviation Frame Data pixels are recorded and accumulated. In Box the pre normalized Negative Positive Product pixel values are determined by dividing the product of the Positive Overall Intensity Deviation the Negative Deviation Frame Data Frame Data pixels by . The average of the squares of the Positive Overall Intensity is determined by dividing by . The average of the squares of the Negative Deviation Frame Data pixels is determined by dividing by .

From Box in the method can proceed via path to . In Box the standard deviation of the Positive Overall Intensity Deviation is determined by taking the square root of the average of the squares of the Positive Overall Intensity Deviation . In Box the standard deviations of the Negative Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the Negative Deviation Frame Data pixels . Box determines the Normalized Negative Positive Product pixel values by dividing the pre Normalized Negative Negative Product by the product of the standard deviation of the Positive Overall Intensity Deviation and the standard deviation of the Negative Deviation Frame Data pixels .

At the conclusion of Box in the method proceeds via path to for determining an improved image data or to for determining alternative improved image data by an alternative embodiment.

Objects within the field of view of a light sensing device typically reflect and scatter light from an external source of illumination as well as emitting light. Depending on the source of illumination and the material composition of the objects within the scene the contributions from reflected scattered light and emitted light are often quite different. Light that is reflected from an object typically polarizes the reflected light while emitted light is often unpolarized. Emitted light may be from physical processes that include but are not limited to luminescence fluorescence black body and thermal radiation.

The present invention relates to a method that can be applied to improve images of many types of low light target objects and areas. Objects and target areas of interest are often very faint either due but not limited to to low photon emission rate being very distant radiation intensity with distance from a source follows an inverse square law i.e. 1 r where r is the distance from the center of the emitter to the receiving device the reflectance of the object or target area of interest being very small the effective integration or shutter time of the measurement device being very short the efficiency of the detector may be small or through attenuation and or scattering due to the media through which the radiation propagates. Low light conditions could generally said to exist when the quality of the image degrades with the reduction of illuminating light or light received from the imaged area of interest by the eye or a sensor such as a camera. A low signal to noise ratio sensed by a sensor such as a CCD CMOS or single photon detector may also indicate low light imaging conditions when the noise of the sensor exceeds the measured light by the sensor. Outdoors between dusk and dawn would typically be considered low light conditions and indoors without bright overhead illumination may also produce low light conditions. In an environment when obstacles occlude the light source such as in a cave or thick forest also produce low light conditions. Conditions that cause the use of intensity amplification are considered low light conditions. is an average image of a distant 2.33 km target taken under low light conditions. demonstrates an improved enhanced image of the same target as shown in . A close up picture of the target is shown in . It was determined that the conditions were low light because the average image deteriorated to be virtually free from recognizable patterns as the intensity of the sun decreased by more than a factor or two late in the afternoon.

Referring now to in accordance with one preferred embodiment in Box a series of frames are inputted into the memory or input of a processor or image processor. The frame may be composed on a plurality of pixels typically in a two dimensional 2D array that together form an image. Exemplary frames may be electronic image data such a TIFF or JPEG file. As used herein the terminology processor or image processor as used in the following claims includes a computer multiprocessor CPU GPU minicomputer microprocessor or any machine similar to a computer or processor which is capable of processing algorithms. The frames may comprise photographs of the same region of interest. The region of interest may be a scene landscape an object a subject person or thing. In Box the frame data or value of each pixel at each pixel location is determined for a frame. In Box Q a predetermined number of pixels are selected rather than the entire overall intensity as was the case in regard to the preferred embodiment of . The intensity value of the predetermined pixels correlates to a bucket value determination. It correlates to the summation of the light intensity at the predetermined pixel locations. In the case of a picture the overall intensity correlates to the sum of the reflected illumination at the predetermined pixel locations. In the case of an electronic display formed by pixels the intensity is the summation each pixel value at each pixel location. At Box the values in Box are multiplied by the value determined in Box Q. Box C represents the frame data sum of intensity of selected pixels product for the frame which will henceforth be referred to as the First Product Array which is also an array of values. At Box C the sum of the First Product Arrays for a plurality of frames 1 100 products is obtained. As an example one hundred frames may be selected. At Box C the summation of the First Product Arrays determined in Box C is divided by the number of frames such as for example one hundred to determine the Average Of The First Product Arrays.

Box represents the multiplication of Boxes and to form the Average Frame Data Average Intensity Product which is an array. As shown in the bottom portion of the Average Frame Data Average Intensity Product is subtracted from the Average Of The First Product Arrays to form the refined image of Box .

The methods and techniques described in conjunction with the present invention can be applied to improve and enhance imaging of subject areas objects and scenes that would otherwise be degraded by the influence of scattering particulates bad weather conditions including but not limited to rain or fog turbulence and obscuration which may create at least one of the effects of artifacts distortion or noise in the measured image data. The invention has been demonstrated to improve and enhance images as shown by . Turbulence a type of obscuring property of the atmosphere degrades the conventional image. In like manner a conventional image degraded by bad or changing weather low light and or obscuration would also be enhanced and improved by the current invention.

Centriods or center of intensity calculations are commonly used in first order imaging applications such as fluorescence microscopy. Many images of an illuminated target are measured and the locations of fluorescing emitters are determined to greater resolution than the average of the images by computing a center of intensity or centroid of the measured fluorescence. Where the location of the emitting source is determined by

 1 Separation of different orders of interference with measurements at different space time locations for better contrast visibility and different views of the physics. 2 With two levels i.e. there are 80 different views. 3 Separating the orders of photon interference as manifest in images at the data ensemble levels is useful for quantum imaging and quantum computing.

Improved stereoscopic images and movies can be generated with application of the methods and techniques described in this invention. A typical stereoscopic image or movie is produced by viewing a region of interest with two distinct and separated imaging sensors such as two cameras or two human eyes. The optical path from the region of interest to each sensor is slightly different and each sensor also emphasizes slightly different portions of the region of interest. Furthermore the field of view between each sensor and the region of interest is typically subject to different degrees of obscuration and degrading turbulence effects. The methods and techniques described in this invention can be applied to each of the stereoscopic channels individually or in other embodiments cross channel products between the reference fields and the buckets may be used i.e. a right channel reference with a left channel bucket and a left channel reference with a right channel bucket. Range or depth maps of a region of interest can be generated from stereographic images. Since the stereographic images are adversely affected by obscurants and turbulence the methods and techniques described herein can be applied to generate improved stereographic images which may then be used to generate more accurate range or depth maps of the region of interest.

Turbulence causes changes in the index of refraction in the atmosphere. These changes in the index of refraction cause a change in the speed of light through media thereby causing ranging problems. For example light travels slower thru air with higher index of refraction. Other factors that influence the index of refraction in the atmosphere include but are not limited to temperature and concentrations of other materials such as water. The current invention can be applied to mitigate the effects of the anisotropic inhomogeneous and temporally fluctuating changes in the index of refraction along an optical path from a sender to a receiver. As an example a current type of LADAR system would direct a laser pulse to the region of interest on sending the pulse the receiver detector would start taking a series of measurements for N consecutive measurements or bins. Each of the measurements would correspond to a dT where light returning from the target area would be measured. The particular dT out of the N dT measurements that measured the greatest number of photons would be assumed to give the range to the region of interest c M dT 2 where c is the speed of light M is the index of the measurement time with the greatest number of photons measured dT is the time width of each bin. The factor of 2 corrects for the time of the pulse to travel from the laser to the target and back from the target to the receiver. However index of refraction variations generally spread out the pulse sometimes to the extent where no single dT bin has the greatest number of photon counts. Applying the methods and techniques described in this invention can mitigate this variable index of refraction induced degradation and generate improved depth or distance to target maps of the region of interest.

An alternative embodiment of the current invention may use different groupings of pixels or spatially resolved measurements for the reference field and the bucket field measurements. As an example the pixels of a CCD type sensor may be grouped into a checker board type pattern where the equivalent pixels to a red even or black odd box on a checker board are treated as two distinct sensors. The even odd pixel set is then used as a set of reference measurements while the odd even pixel set is summed over and used as the set of bucket measurements. A Gtype image for each pairing even odd or odd even can then be computed using the methods and techniques described herein to generate improved images of a region of interest. Each of the computed improved images may be examined separately or the can be added or subtracted from each other to highlight different features of the region of interest. It is to be appreciated that other groupings of pixels or spatially resolved measurements would be advantageous for specific applications or characteristics and details in the region of interest.

Many current imaging devices are able to measure distinct portions of the electro magnetic and optical spectrum. These devices such as color CCD cameras smart phone cameras and video recorders often split the colors of a region of interest into three color bands i.e. red green and blue. Turbulence and obscurants typically impose an optical frequency dependent degradation on color images. The current invention can mitigate these degradations on a per color band basis. For example using the methods and techniques described in this invention the measured red green and blue color bands could be used to generate an improved red image improved green image and an improved blue image. These three improved color band images could then be collected into a single improved color image. Other alternative embodiments would include using for instance a red bucket with a blue reference field. These types of cross color calculations may be useful to highlight or suppress desired features of the region of interest.

Embodiments of the present invention can be used to determine the dynamics and relative dynamics contained in measurements of the region of interest. The dynamics would consist of the information on the physical chemical and biological processes as they evolve in space and time. This would include but is not limited to the dynamics of light sources surface characteristics such as reflection and scattering in the transmission media.

Using ghost imaging we have shown that it is possible to generate an improved image of an area of interest using two detectors. The first detector is a sensitive light meter that measures light scattered or reflected from the area of interest. The second detector can be a camera such as a charged coupled device CCD CMOS or other device capable of imaging the relevant light source such as the sun stars and moon outdoors or light bulbs indoors that illuminate the area of interest. In the case where the area of interest emits radiation such as infrared radiation detector one measures the combined reflected scattered and emitted radiation. Detector two measures the effects of the reflected scattered and emitted radiation sources. By combining coincident measurements of first detector and the second detector a ghost image can be computed of the area of interest. It turns out that the ghost image can achieve higher resolution or greater clarity than an image of the area of interest using the camera type device alone taking a picture classically. This has been demonstrated in experiments and publications of peer reviewed articles. An explanation for the improvement of the image in turbulence is demonstrated by so called two photon models of coincident imaging found in our publication. There it is shown that turbulence aberrations cancel and do not affect the improved resolution of the ghost image. Sometimes a two photon ghost image is referred to as a second order image. Whereas the camera image taken by classical means is referred to as a first order image. Classical theory and experiment show that first order images are smeared by turbulence.

It would be desirable to generate a second order ghost image with high resolution and clarity even through turbulence while using just a single camera which may or may not be a color camera. In the case of a color camera ghost imaging system and method can be applied to each color that the color camera measures a color ghost image will result. Also the camera could measure in infrared UV or more than one other wavelengths and pseudo coloring can be used to display the ghost image depicting the intensity in the different wavelength bands. One embodiment of how to do this would be to treat the area of interest as a mirror or an imperfect mirror for the light sources. Thus an array of pixel on the camera can image the area of interest as a measure of the light sources just as the second detector above. The first detector which measures the reflected and scattered light from the area of interest can be other or the same array of pixels in the camera.

One method would be to use a black and white checkerboard pattern of pixels where the white pixels act as detector two and the sum of the measurements on the black pixels act as detector one. By combining coincident measurements of first detector and the second detector a ghost image can be computed of the area of interest.

An alternate embodiment of this would be to place a beam splitter between the camera and the area of interest. The beam splitter can split the radiation into two parts one part towards detector one and the other part towards detector two. For example with a 50 50 beam splitter half of the light from the area of interest would be directed to the pixels of the camera which act as the second detector. The remaining half of the light would be directed to a separate sensor which acts as the first detector. The separate sensor could be attached to the camera or even be a single pixel of the usual camera array. This second embodiment is not as simple as the first embodiment. However it may be more accurate if the first detector has higher sensitivity or accuracy. By combining coincident measurements of first detector and the second detector a ghost image can be computed of the area of interest.

Another alternate embodiment would be to use all of the pixels of the camera to act as detector two and the sum of the measurements of all of the pixels act as detector one. By combining coincident measurements of first detector and the second detector a ghost image can be computed of the area of interest. Various combinations of the pixels including those which overlap or do not overlap of the camera can used as detector one and detector two. It has been demonstrated with experiments that these combinations described can be useful to obtain improved quality ghost images.

In calculating the improved quality ghost image the processor creates a correlation product of measurements from detector one times measurements from detector two at coincident times for each pixel location measured by detector two. Summing and averaging these correlations over an ensemble of coincidence times yields a measure of the ghost image. This ghost image is combined of both first order and second order image information. Subtracting off the first order image which may be smeared by turbulence yields a second order ghost image which will be improved over the first order image in that it will have higher resolution and less distortion due to turbulence. This image is often referred to as a Gimage. When for instance thermal photons have Gaussian statistics then the Gimage is expected to be positive. However Gaussian statistics may not accurately characterize the photon measurements made by cameras imaging areas of interest. In addition turbulence and other physical characteristics of the illuminating source area of interest camera detectors one and two and intervening media may combine in ways to create photon measurements that would be best characterized as non Gaussian. Some of the physical characteristics that may contribute to the non Gaussian photon statistics can include any or all of inhomogeneous non stationary anisotropic non ergodic nonlinear and quantum processes.

As it turns out empirically Ggives a good image of the area of interest. However it is important to try to improve the Gimage contrast. Sometimes Gcontrast is not as high as ideally desirable. One way to improve the image contrast is to artificially scale the ghost image by subtracting off any background. An analysis of the Gsign shows that when Gis positive then the intensity deviations that comprise it are correlated. When Gis zero then the intensity deviations are uncorrelated. When Gis negative then intensity deviations are anti correlated. When the negative parts of Gsubtracts from the positive parts of Gthen an all positive G Gcorrelated Ganti correlated can be computed. This method can be imaged by a display to have high contrast since the minimum is zero and other values are positive.

As described in the specification for U.S. patent application Ser. No. 13 247 470 filed Sep. 28 2011 ARL 11 03 herein incorporated by reference there are many possibilities to combine results from the conditional product terms R R R and R computations to generate and display an improved image of the region of interest. This sum RI is a weighed sum of correlated products. These correlated products would be formed from results of measurements of quantum particles. There are many applications for the method described below including imaging using photons and other quantum particles measurement of gravitational waves measurement of the quantum properties of sound improved LIDARs LADARs improved medical imaging improved Earth surveillance from space improved space surveillance from Earth or other places in space. It is helpful to put the weighted sum of correlated products in a form where a user could compare the value of one vs. others for their application.

In particular one useful way would be to weight terms in R R R R by cos and sin factors i.e. cos sin where is an angle. This arrangement allows for continuously varying the contributions of the conditional product terms to display improved images to include an all positive image an all negative image a Gimage and the negative of the Gimage.

While the conditional product terms can be displayed independently as an image the alternative means described here to present and display the results would be useful for teaching and for detailed examination and analysis of the properties of the region of interest.

In practice a movie displaying RIor RIwould be made that scans through values of by the desired amount. For example the movie could start at 0 and each for each successive frame could be incremented by one degree until takes all values from 0 to 360 degrees. This will show the continuous change of RIor RIexhibiting varying contrast visibility and resolution of the computed image of the region of interest. For example when the symmetric terms are added to the asymmetric terms then a Gimage results when the asymmetric terms are subtracted from the symmetric terms then an all positive image results. Which image is better for the user depends on the needs of the user. For example from RIone may obtain high contrast for 180 degrees for an all positive image with high visibility. In some cases more resolution of the region of interest may be found for 0 degrees.

Color cameras often have pixel sensors that are covered by color filters. That way light directed towards a pixel sensor first passes through a filter before interacting with the pixel sensor. The filters are often laid out on a grid of pixel sensors in patterns that are variations of the Bayer pattern. For example the filters over each pixel may be red green or blue arranged in a Bayer pattern. Usually 50 of the pixels are green 25 are blue and 25 are red so that the pixels only respond to the color of light that transmits through their color filter. There is a process for converting a Bayer pattern image to a RGB image where a processor computes a RGB value for each pixel. For example at a pixel that ordinarily measures a green value the RGB values may be completed by interpolating surrounding red pixel values and surrounding blue pixel values to that pixel. One method to record the RGB image is a variation of the AVI standard. Video sequences or individual pictures from a color camera are often recorded in AVI. Producing ghost images from AVI may result in high quality ghost images. Occasionally visual artifacts may be present in the ghost images due to the Bayer to RGB interpolation and conversion process. For some images that have been converted from Bayer to RGB it is possible to reverse the process to investigate what the ghost image would look like in the original Bayer pattern format.

For example one means to convert back to a Bayer pattern image from a RGB image would be to extract the Bayer pixel color for each pixel location and zero out the interpolated color components. This would recover the underlying Bayer pattern image which could then be used to compute a ghost image that would be absent any artifacts from the interpolative or Bayer to RGB process.

This section contains information on the conditional product terms. Conditional product terms are terms formed by correlations between values measured above or below the mean value. The values may represent intensity polarization or other physical quantities that are able to be measured. When these quantities have quantum properties or are able to be represented by quantum physics such as photons or other quantum particles then the resolution and visibility of the conditional product terms may be enhanced beyond conventional limits.

The values of a particular Gcan be either positive or negative. A positive Gindicates that the measurements at an x y pixel are correlated with the bucket measurements. A negative Gindicates that the two measurements are anti correlated.

CASE 1 As an example assume that an ensemble of measurements is made at location a and location b . For this case assume that the ensemble consists of two measurement realizations 1 and 2. For realization 1 I 1 8 and I 1 8. For realization 2 I 2 2 and I 2 2. G where indicates and average over the ensemble of realizations. For this ensemble G 34 25 9. Examining Iand Iit is easy to see that the values of both of the measurements decrease at the same and typically referred to as correlated.

CASE 2 In a case where the ensemble of measurements is made at location a and location b for realizations 1 and 2 I 1 2 I 1 8 and I 1 8 I 2 2. Then G 16 25 9. In this example Iincreases in magnitude from 2 to 8 while Idecreases from 8 to 2. This is typically referred to as anti correlated.

Bucket pixels can be allocated down to an individual pixel or a group of pixels in particular frames. The pixels in a group of pixel selected to be summed to a bucket integrated intensities value for a frame need not be contiguous. The bucket pixel or pixels may also be chosen on a per frame basis. For instance all pixels within 10 of the maximum pixel value for a frame may be chosen to be summed over for the bucket value for that frame. The spatial distribution of those pixels within 10 of the maximum pixel value is likely to change from frame to frame based on lighting conditions and turbulence obscuration effects between the target area and the measurement system. Furthermore single pixel buckets or multiple pixel buckets may be utilized.

The criteria for selecting a set of pixels to use as a bucket value includes but is not limited to finding those pixels which contribute to improved contrast of the refined image improving the convergence rate of the refined image improving resolution of the refined image improving the refined image fidelity to the imaged scene or target finding optimal placement selection of pixel sensors as buckets to minimize noise in refined image providing enhanced resolution to comply with certain Gmathematical constraints such as positivity and to explore the computational mathematical and or physical effects of changing the bucket configuration. For example we can compute a Gusing all the pixels in a frame to represent the bucket. A Grefined image may also be computed using only one pixel out of a set of pixels in a frame. We may see from comparison of two Gimages that the Gimage with the many pixel bucket provides different images than a Gimage using a single pixel. Furthermore the Gimage changes with the pixel location or other physical property such as wavelength in the ensemble of frames array. This comparison provides image information about the effect of a single pixel bucket its location and the many pixel bucket average of the ensemble of such pixels. From this information the effect of bucket size number of pixels used for the bucket and shape distribution of pixels in space and time used as the bucket on the properties of the refined image can be determined which provides theoretical and practical benefits such as determining and implementation of optimized bucket selection to provide enhanced images of the target or region of interest.

Each pixel within a set of measurements I typically intensity values taken using a camera may be indexed by a location x y k i.e. I xi yj k . When the locations of the measurement elements of the sensor are not arbitrarily distributed a pixel value can be indexed as I i j k . The values of i vary from 1 to IMAX where IMAX is the number of horizontal pixels on the sensor j varies from 1 to JMAX where JMAX is the number of vertical pixels on the sensor and k is the identifier for the frame number which varies from 1 to NFRAMES. For example a sensor may have 8 pixels in the horizontal IMAX 8 and 8 pixels in the vertical JMAX 8 . If 10 images where measured with this sensor then NFRAMES would equal 10. In the current invention a bucket value B i j k is determined on a per pixel basis for each pixel in the set of measured values. The bucket for say pixel i 2 j 6 k 3 could be a normalized summation of pixels values. For example B 2 6 3 could be I 1 1 1 I 3 8 7 2. B 3 1 1 I 2 1 2 I 1 9 3 I 10 10 9 3. Then these bucket values B i j k can be used to compute a Gimage as where indicates an ensemble average over the number of frame. In the current invention a previous embodiment would have the bucket or overall frame intensity defined as B k I i j k where the summation is over all i 1 to IMAX and j 1 to JMAX. The B k value for this embodiment would be applied to compute a Gimages as G .

Reference values and bucket integrated intensities of pixels values may be allocated based upon the wavelength or color of the measured photons. For instance measured green values on a color CCD or CMOS camera may be used to provide the reference field pixel measurements and measured red values may be allocated or summed to provide the value of the bucket measurement for a frame.

Modern advanced infrared cameras may provide per pixel co located measurements of infrared wavelengths in for instance the mid wave infrared MWIR and long wave infrared LWIR wavelength bands. One band may be used to provide reference pixel values and pixel in the other band can be allocated or summed to provide bucket values. A further local Gtype calculation may take place wherein deviations from the ensemble mean of one wavelength band can be multiplied with deviations from the ensemble mean of the other wavelength band. These deviation products are performed for each pixel in a frame and summed over all the frames of data in the ensemble. A preferred methodology may therefore comprise dividing this deviation product sum by the total number of frames in the ensemble to yield a Gimage of the area of interest.

Taken together outline the steps of an alternate preferred methodology involving selecting a set of pixels to generate a normalized sum to use as a bucket value for each measured pixel value in the plurality of frames. Boxes which are not labeled with a suffix S T or U are the same as described with respect to . Box T represents determining the selecting sets of pixels to generate a normalized sum to use as a bucket values for each pixel for each frame. Box U represents normalizing the sum of the selected sets of pixels by dividing the sum of the pixel values in each set of pixel by the number of pixels within each set of selected pixels. Box T represents the frame data normalized pixel intensity sum product. Box T is the sum of the frame data normalized pixel intensity sum products for a plurality of frames 1 100 . Box T represents the division by the number of frames e.g. 100 to obtain frame data s normalized pixel intensity sum product average contains pixel values at each pixel location . Box T represents the determining of the average normalized pixel intensity sum average of each normalized pixel intensity sum at each pixel location for the plurality of frames . Box T represents the average frame data average normalized pixel intensity sum product which is subtracted from Box T to produce the enhanced image.

Box indicates a distant target scene area. Box indicates the target which is an area of the distant target scene for the particular region of interest selected for improvement and enhancement. Box indicates optionally present turbulence or other obscuring or image degrading effects along photon path between light source and target and light source and imaging system or photon measurement devices or components. Box is a telescope. Telescope although other apparatus or direct viewing may be used without departing from the scope of the invention . Telescope may zoom in on or expand view of the selected region of interest as well as zoom out. Telescope is used to transmit entangled photon pairs to the region of interest and receive entangled photon pairs reflected or scattered from the region of interest. Box is an optical circulator An optical circulator is an optical element that transfers incoming photons to the next port of the circulator unlike a beam splitter where photon paths are split to two or more ports. Box is an entangled photon source. The entangled photon source generates entangled photon pair that are entangled in time energy H V polarization or between other conjugate pair properties of the photons. Exemplary examples of entangled photo sources include entangled photons generated via Spontaneous Parametric Down conversion SPDC in a nonlinear crystal such a Beta Barium Borate BBO or potassium titanyl phosphate KTP entangled photons generated in a quasi phase matched nonlinear media such as periodically poled KPT PPKTP or periodically poled Lithium Niobate PPLN and entangled photons generated in a four wave mixing process in a nonlinear optical fiber.

Box is a polarizing beamsplitter dichroic mirror or other optical element that operates to direct one portion of an entangled photon pair towards spatially resolving detector and directs the remaining portion of an entangled photon pair toward spatially resolving detector . The portions of the entangled photon that are directed towards for example detectors and are selected by the element in Box . As an example an entangled photon produced in an entangled polarization state HH VV VV HH may be generated with frequencies vand v the frequencies are not correlated with the polarizations. Then Box could be configured with a dichroic mirror that would operate to direct for example vtowards detector and vtowards detector for measurement. The choice of the element for Box of course would depend on the configuration of the system and or the type of entanglement being used to illuminate the target.

Element is a lens used to focus the photons onto detector and detector . Box indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one part of an entangled pair that has interacted with the remote scene target or subject. Box indicates spatially resolving detector . Spatially resolving detector measures time and spatial x y location of the second part of an entangled pair that has interacted with the remote scene target or subject. Detectors placed at multiple diverse locations allow imaging from separate vantage points. One can also generate stereo views from different locations and composite imaging to provide a large field of view increased depth texture and resolution of the region of interest. Furthermore the measurements from separate sensors can be utilized for higher order images of the region of interest.

A volumetric ghost image of a region of interest would be able to provide a 3D space filling representation of the region of interest that would not be degraded by the adverse effects of obscurants or turbulence. The 3D space filling representation could then be output to a 3D computer rendering system 3D printer or computer numerical control CNC device for visualization or for the generation of a physical model of the region of interest that was volumetrically ghost imaged. One could have a series of such volumetric images to form a 3D movie or a moving representation of the 3D volume.

Some exemplary detectors which may be used for the detector and detector include charge coupled devices CCD CMOS SPAD arrays quantum well LIDAR LADAR video device spatial sensor light field plenoptic camera gyro stabilized camera spatial phase sensitive camera or range sensor.

Box indicates coincidence and timing electronics that operates to register when a pixel on detector and a pixel on detector occur inside within a user defined coincidence window T. A coincidence window is a time difference within which two photon measurements are defined to be co incident. The timing electronics further operate to record the time that has elapsed since a user chosen laser pulse and the first coincidence pair detection for ranging calculations. Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software operationally configured to perform the image improvement and enhancement processes. The processing corresponding to Box may include at least one of the inventive methods described for example by to compute a second or higher order improved image of the target area. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest.

Box is an entangled photon source. The entangled photon source generates entangled photon pair that are entangled in time energy H V polarization or between other conjugate pair properties of the photons. Box is a polarizing beamsplitter dichroic mirror or other optical element that operates to direct one portion of an entangled photon pair towards spatially resolving detector and directs the remaining portion of an entangled photon pair toward spatially resolving detector . Element is a lens used to focus the photons onto detector and detector . Box indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one part of an entangled pair that has interacted with the remote scene target or subject. Box indicates spatially resolving detector . Spatially resolving detector measures time and spatial x y location of the second part of an entangled pair that has interacted with the remote scene target or subject. Box indicates coincidence and timing electronics that operates to register when a pixel on detector and a pixel on detector occur inside within a user defined coincidence window T. A coincidence window is a time difference within which two photon measurements are defined to be co incident. The timing electronics further operate to record the time that has elapsed since a user chosen laser pulse and the first coincidence pair detection for ranging calculations. Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software operationally configured to perform the image improvement and enhancement processes. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest.

Box is a polarizing beamsplitter dichroic mirror or other optical element that operates to direct one portion of the photons with distinguishable properties towards spatially resolving detector and directs the remaining portion of the photons with distinguishable properties toward spatially resolving detector . Element is a lens used to focus the photons onto detector and detector . Box indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one part of an entangled pair that has interacted with the remote scene target or subject. Box indicates spatially resolving detector . Spatially resolving detector measures time and spatial x y location of the second part of an entangled pair that has interacted with the remote scene target or subject. Box A indicates image measurement readout and timing electronics that operates to read the measurements from detector and detector . Box A differs from Box in that Box includes a capability to register coincident measurements between pixels located on detectors and . Coincidence electronics are typically more complex and expensive than usual timing electronics and are often only employed when a very high degree of precision in determining if two measurement events happened within a coincidence window Tis required.

The images from detector and detector are transferred to processor . Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software operationally configured to perform the image improvement and enhancement processes. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest.

Box accepts an initial Gimage provided by for example the processes described in and or . and are exemplary means to compute an initial Gimage in particular to and may use the methods described by and or other methods to compute the initial Gimage. In Box pixel locations within the provided Gimage are selected based upon a predetermined criteria such as those pixel locations where the value of the Gimage is greater than 0. At least one pixel would need to be selected in Box to begin the iterative process. Should no pixels be selected this can be construed to mean that the Gimage is already converged to an image where further iteration would provide no corresponding improvement or enhancement to the generated image. In the measured image intensities first array values at the selected pixel locations are summed to provide new bucket values over all frame intensity values or normalized pixel intensity sum values. In a new Gimage is calculated using current computed bucket values. Box performs a determination as whether to complete the iteration process which may include for example testing all values of the new Gimage being positive or the pixels selected for the current bucket values have not changed from the previous pixel location selected or the calculated using current computed bucket values or that the currently computed Gimage has not changed significantly from the previous computed Gimage i.e. the maximum difference between the two images is less than 1e 6. If the test is true then the process proceeds to box otherwise if the test is false then the processes continues to box . In Box the iteration process is complete and the computed Gimage of the region of interest is available for storage or display.

In Box a sequence of images taken of the region of interest are provided. Box is the start of the loop to compute per frame new Gimages for a set of M frames where M

Box indicates a distant target scene area. Box indicates the target which is an area of the distant target scene for the particular region of interest selected for improvement and enhancement. Box indicates optionally present turbulence or other obscuring or image degrading effects along photon path between light source and target and light source and imaging system or photon measurement devices or components. A is a first telescope although other apparatus or direct viewing may be used without departing from the scope of the invention . Telescope A may zoom in on or expand view of the selected region of interest as well as zoom out. Used to transmit illuminating photons to the region of interest. Box B is a second telescope although other apparatus or direct viewing may be used without departing from the scope of the invention . Telescope B may zoom in on or expand view of the selected region of interest. B is configured to receive illuminating photons reflected or scattered from the region of interest. Box A Illuminating photon source. The illuminating photon source generates photons with distinguishable properties such as two or more wavelengths polarizations or entanglements of conjugate photon properties. Element is a lens used to focus the photons onto detector . Box A indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one photons that have interacted with the remote scene target or subject. Detector A may be able to resolve wavelengths color or polarizations etc. Box B indicates image measurement readout and timing electronics that operates to read the measurements from detector . The timing electronics further operate to record the time since a user chosen illumination pulse and the first photon detections for ranging calculations. The images from detector are transferred to processor . Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software instructions for operationally configuring a processor to perform the image improvement and enhancement processes. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest.

Gtype images may be calculated on a per pixel basis by shifting the pixels in a frame of data for instance if a frame of measured is I i j k where i j indicates a pixel location and k is the time or frame index then Ican be set to be equal to a pixel shifted set of measured values I i j k I i 1 j k . Deviations of Iand Ifrom their respective ensemble averages can then be computed and the average of the products of those deviations is a Gimage computed from space or coordinate shifted pixels of the measured values. Other statistical quantities such as standard deviations may also be computed from the spatially shifted arrays.

The concept of time frame shift of pixel values enhancement can be beneficial in identifying objects in a scene that are in motion. Shifting of pixel values can also occur between frames wherein for instance I i j k I i j k 1 . Deviations and averages of the products of those deviations can be computed to yield Gimages of the area of interest.

Gimproved images can be normalized by dividing the improved Gimage by for example the product of the standard deviations of the reference values and the bucket values or dividing by the product of the ensemble averages of the reference values and the bucket values. Other normalizations i.e. G I I where Iand Ican be pixel values over time and G I I Where indicates an ensemble average and indicates the standard deviation. is typically computing using the following equation 1 

Sub ensembles such as utilizing a portion of an full ensemble e.g. 1000 frames can be averaged subset blocks of G I I i.e. 10 blocks of 10 frames for frames 1 to 100. Examples of sub ensemble Gs are as follows.

An example of a sub ensemble used in a preferred method of the present invention comprises utilizing sum of Gper frame where G k I k I k and the sum of the G k over a set of i 1 to total number of frames in the ensemble is the improved image. A smaller subset of G k can be summed over a sub ensemble k 1 to N where N is less than the total number of frames in the ensemble to provide an improved image. Note that the full set of measurements may be greater than the N being used for the current ensemble.

A further example of a sub ensemble used in a preferred method of the present invention comprises a Gimage that may be computed by summing and averaging per frame Gimages over a set of frames that is less than the total number of frames in the ensemble of measurements. A per frame Gwould be G k I k I k where the k indicates the frame number out of the ensemble of frames. and are the reference and bucket averages over the ensemble of measurements. When the G k are summed and averaged over a set of frames that is less than the total number of frame this constitutes a sub ensemble Gcalculation. This sub ensemble improved Gimage can highlight different features of the area of interest and may further provide a means to generate improved image frames for a movie of the area of interest.

The movies produced by this method can separate scale of turbulence. Separate scales of turbulence can be better images by the ability to choose different sizes of an ensemble and different size sets within an ensemble to use as the sub ensemble.

An iterative means to calculate in improved Gimage could involve progressively incorporating more or fewer pixels of the measured frames to be summed over for a bucket value. One such method would be to choose a single pixel location to provide the bucket values over all frames. A Gimage would then be computed using those bucket values. A new set of bucket values would be selected based on some criteria such as choosing pixel locations where the Gvalue is greater than 0. A new Gimage is computed and the process iterated until no new pixels are being added to or removed from the set of pixels being used to provide bucket values. Criteria other than the positivity of the Gmay be used to emphasize or converge to differing features in the Gimage of the area of interest.

Iterative techniques may generally refer to methods wherein a sequence of operations is repeated to achieve a desired result. These types of methods sometimes have stopping criteria that are based on an estimated error parameter falling below some specified value. When this criterion is met the iteration is said to have converged to a solution. However due to sometimes slow or erratic convergence a counter of the number of times the sequence of operations has been performed is often tracked. This number is sometimes called the iteration number. A further halting criterion beyond that of the iterative technique achieving the specified convergence is a maximum iteration number. This maximum iteration number criterion is desirable when computational operations must be completed within a specified time and to prevent so called infinite loops where the iterative technique is unable due to round off or other error to achieve the convergence criterion.

When an area of interest is illuminated by a source of photons such as for example the Sun laser LED etc the illuminated area acts as an imperfect mirror of the illuminating source. A perfect mirror would acts to reflect the photons from the illuminating source where the angle of incidence would equal the angle of reflection. Furthermore the perfect mirror would not absorb any of the incident photons. However a region of interest may contain many differing scattering and absorbing elements. All of these elements are illuminated by the source and when the reflected and scattered photons from the region of interest are measured some information about the spatial distribution of photons of the light source is retained. It is this retained information of the spatial distribution of photons that enables a series of images measured with a single sensor of a region of interest to act much as a typical two sensor ghost imaging system wherein one of the sensors for a ghost imager measures only undisturbed spatial distributions of photons from the light source.

Next in step each R G B value per pixel is summed over all frames and stored in an array RS GS BS. The array RS GS and BS are divided by the number of frames to yield average and arrays in step . Finally in step the per pixel product of is subtracted from the average array to yield a third order image of the scene.

In step the product of the I I and I array values are computed on each frame for all frames. Next in step the I I I array values are summed over all frames on data on a per pixel basis. In step the sum of the I I I pixel products is divided by the number of frames to yield an average array. In step each I I I value per pixel is summed over all frames and stored in an array IS IS IS.

In step the array IS and the sums IS IS are divided by the number of frames to yield average array and values. Last in step the per pixel product of is subtracted from the average array to yield a third order image of the scene.

In step the color frame data is partitioned into 2 arrays e.g. red R and green G pixel values. In step the R and G pixel value arrays are summed for each frame to yield bucket value arrays RB and GB. In step the product of the R RB G GB values are computed on each frame for all frames.

Continuing to step the R RB G GB pixel values are summed over all frames on data on a per pixel basis. In step the sum of the per pixel R RB G GB pixel products are divided by the number of frames to yield an average array. In step each RB and GB values is summed for all frames and stored as variables RBS GBS. In step each R G value per pixel is summed over all frames and stored in an array RS GS.

Next in step the arrays RS and GS are divided by the number of frames to yield average AND and the variables RBS AND GBS are divided by the number of frames to yield average .

Entangled photons may be used to determine range or distance to a remote target. In one embodiment a pair of entangled photons with nearly the same transverse momentum from an entangled photon source can be collinearly propagated towards a remote target. The entangled photons may be directed towards specific areas on the remote target using a pointing and aiming device. The entangle photons that reflect off of a remote target can be measured by photon detectors. When the entangled photon source operates in a pulsed manner then an elapsed time from a particular pulse to the first entangled photon pair measurement that returns from target may be determined. The time that it takes from the pulsed source to the target and back to the photon detectors indicates the distance of the target from the entangled photon source and detector system. Since entangled photons may be recognized by consideration that the entangled photons are formed at virtually the same time and therefore if they take the same path they must arrive at the detectors at nearly the same time or in coincidence. An advantage of this type of coincidence measurement is that background noise from single photon measurements can be ignored and only detection measurements that happen in pairs within some small time interval will be used to provide a more accurate determination of distance to a remote target. Consider for instance the case where a single pulse of entangle photons is propagated and when the pulse contains more than one pair of entangled photons then several coincidences may be measured after reflecting off of the remote target. The first pair of entangled photons to be detected would be indicative of reflection of the entangled photons from a nearer target distance than later entangled photon measurements generated by the same pulse. The locus of target points indicated by the detection events would describe the shape and distance of the target from the setup comprising the entangled photon source and detectors. Photons that were received after the first entangled photons are indicative of photons that have traveled further than the nearest points of the target and may have been scattered by the environment or within the body of the target over a longer path before being measured. Scatterers in the environment may include the atmosphere dust fog air pollution etc. It should be appreciated that this process would apply to propagation in water liquids oceans solids such as glass and other media such as biological tissues.

In the non collinearly propagating entangled photon case when the photons are directed towards a remote target then each photon of the pair likely will be reflected off of different areas of the remote target.

Two detectors can measure the photons in a small time interval i.e. coincidence time window. One detector can act as a bucket detector and the other detector can determine the reference location of the measured photon. Thus the entangled photons can be used for ghost imaging higher order imaging but also determine the distance of each measured pixel from the entangled photon source to the detectors setup. Of course the timing must consider the path from the source to the target and back to the detectors.

The first entangled photon pair back may be calculated from the time of the pulse. One or more optical delays may be included to allow a longer path and thereby allow longer time delays to measure a coincidence event. Typical optical delays that may be used are for example lenses quartz rods fiber free space optical delay line slow light media or quantum memories.

Ghost imaging sometimes called higher order or Gimaging may be used to identify the motion of objects within a region of interest. If the time period for forming a ghost imaging is too short then the motion cannot be detected in a single ghost imaging frame. A ghost imaging movie can be made by stringing together such a series of frames covering a time period over which an object is moving. This way the object motion does not smear the ghost image but rather resolves each of the individual frames in the series. However there is another way to indicate motion of an object within a region of interest via ghost imaging. If the object moves at least one pixel a series of measurement frames are collected then a ghost image using those frames may resolve the object. But signatures of the object motion will appear on the ghost image. In the case of the all positive form for a ghost image using conditional products R R R R then edges of the moving object can be distinguished by above background positive magnitudes. The ghost image made from positive and negative quadrants R R R R will still visually appear as an improved image. The separate terms R R R R may have both edge and improved image qualities. It is to be noted that Gmay take on both positive and negative values depending on location within the image. That is some points will be positive or zero valued and other points will have negative values. The ghost images produced can still acquire the turbulence free ghost imaging properties even though the signs may change. That is the images will still be enhanced and improved over conventional images. A constant can be added to the ghost image to guarantee its positivity to satisfy certain rendering requirements. Thus ghost imaging of moving objects has the benefit in that it can produce edge maps of the scene and indicate objects that are moving such as to separate them from stationary objects in the same region of interest. Such edge maps and identification of moving objects can be very useful for many human endeavors requiring such benefits that may include machine vision for robotics intelligence surveillance reconnaissance moving object pointing and tracking medical imaging of moving organs law enforcement military situational awareness documentation of flow lines of moving media earthquake motion or missile motion. For example transmission of a single ghost image identifying moving objects in a scene is more efficient than transmitting a movie with many frames. As a further example the edge image generated by ghost imaging can be mapped to a 1 bit per pixel image where only those edges greater than some threshold are set to a value of 1 and all other pixels are set to zero to greatly increase transmission efficiency. Ghost imaging of the Earth from satellite or aircraft can indicate changes in terrain features and crop evolution. Ghost images of outer space from the Earth may indicate the outlines of moving satellites more clearly and distinguish them from background. Ghost imaging can also be applied to indicate the motion of underwater objects more clearly or objects in moving media more clearly.

It must be appreciated that all positive enhanced images for higher order i.e. G G . . . G images can be calculated by combining the products of the higher order quadrant terms may be grouped such that an individual conditional product term R has a positive or a negative value. The all positive Gcan then be determined by adding together those Rterms with positive values and subtracting the negative valued Rterms. In the following Ris shorted to R for ease is reading. For example an all positive Gwould have R R R R R R R R conditional product terms. The conditional product terms that have positive values are R R R and R. The conditional terms with negative values are R R R and R. So an all positive Gwould have the form R R R R R R R R . Similar groupings can be determined and used to calculate other all positive Genhanced images.

Box is an entangled photon source. The entangled photon source generates entangled photon pair that are entangled in time energy H V polarization or between other conjugate pair properties of the photons. Box is a polarizing beamsplitter dichroic mirror or other optical element that operates to direct one portion of an entangled photon pair towards spatially resolving detector and directs the remaining portion of an entangled photon pair toward spatially resolving detector . Element is a lens used to focus the photons onto detector and detector . Box indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one part of an entangled pair that has interacted with the remote scene target or subject. Box indicates spatially resolving detector . Spatially resolving detector measures time and spatial x y location of the second part of an entangled pair that has interacted with the remote scene target or subject. Box C indicates coincidence and timing electronics that operates to register when a pixel on detector and a pixel on detector occur inside within a user defined coincidence window T. A coincidence window is a time difference within which two photon measurements are defined to be co incident. The timing electronics further operate to record the time that has elapsed since a user chosen laser pulse and the first coincidence pair detection for ranging calculations. Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software operationally configured to perform the image improvement and enhancement processes. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest. Box indicates optionally present pointing and aiming e.g. beam steering components that may be used to direct the entangled photon pairs to a specific point within the region of interest.

Filtering may be used in various embodiments for enhanced image processing. The filtering processes can be performed electronically digitally computationally or a combination thereof for example and may be performed at different stages of processing. In some embodiments quantum filtering techniques may be employed including Quantum computations on classic computers as discussed for instance in U.S. Pat. No. 7 353 148.

As an example consider that detector and detector where each detector can measure photons. The measurements from the two detectors may be used to determine Gcorrelations of the measured photons. Gcorrelations appear when the photons are measured within a specified time interval of each other. The Gcorrelations can be used to generate enhanced images as discussed above. As shown in filtering can further improve the enhanced images.

There may be situations such as involving sunlight or incoherent light in which the two detector measurements could be virtually uncorrelated. More particularly the time frequency properties of the detectors may be such that they do not resolve the Gcorrelations which would normally yield an enhanced image. These detectors measurements in these situations may not produce a readily appreciable image. But the inventors have found that with filtering these measurements can still nonetheless produce an enhanced image processing. Thus in some embodiments filtering i the individual detector measurement events before determining the Gcorrelations and or ii during the determination of the joint coincidences may yield enhanced images since the resolvable filtered components of the Gwould be processed.

Entanglement swapping is a quantum process by which particles that are not entangled become entangled with each other. For example consider that particles 1 P1 and 2 P2 are entangled with each other and that particles 3 P3 and 4 P4 are entangled with each other. To entangle P1 and P4 particles P2 and P3 are interfered on a beam splitter and then are measured. The interference and measurement swaps the entanglements P1 P2 and P3 P4 to P1 P4. Particles P2 and P3 are also affected by the measurement device and may be absorbed. The process of entanglement swapping has previously been verified. See e.g. Bouwmeister et al. Physical Review Letters 80 3891 3894 May 1998 which described a process of entanglement swapping with experimental verification using entangled photons. Swapping may be considered as the teleportation of an unknown photon particle state onto anther photon particle.

The process of entanglement swapping has many potential applications in the development of quantum technology. Thus far relatively few applications have found uses for entanglement swapping. Potential applications for entanglement swapping in quantum technology include quantum computing quantum communications and in the current invention quantum imaging. There are potentially many benefits to using entanglement swapping for quantum imaging that have not yet been described or exploited. The reason for this is that entanglement swapping has required high precision in its implementation and great expense for equipment that achieves the high precision. The lack of robust applications for entanglement swapping has been another drawback to its implementation in technology. This technology is being miniaturized in solid state devices and some components are being tested on chips. These quantum chips can generated entangled particles and perform interference operations and measurements of quantum states.

It would be beneficial to have an entanglement swapping application that is robust and can be implemented with both available and evolving technologies. One way to make entanglement swapping useful would be to apply it information transfer sharing or communication without the need for a classical communications channel. For example the current Internet radio and telephone are generally considered to be classical communications channels. Another way to make entanglement swapping useful would be to be able to transfer share or communicate by quantum means without the sender or receiver needing access to information or resources held by the other. For example the sender having access to photons P2 P3 and the receiver having access to photons P1 P4 is sufficient to transfer information from sender to receiver. Repetition of this process allows the transfer of images without sending classical information and by only sharing entanglement. This type of communication of information such as data and or images would be difficult to detect by an external observer since there would be no particle or radiation going between the sender and the receiver which an observer would be able to sense and follow. Military and domestic applications requiring stealth and or security would benefit from this capability.

Benefits of entanglement swapping for quantum imaging may include performing an entanglement swap to optimize photon detection efficiency while simultaneously optimizing transmission properties from an illumination source to a target. Another benefit is that an entanglement swap may be used to measure absorption maps of a target without the need to measure reflected photons. Furthermore entanglement swapping may be used to help compute the product of the absorption values at two locations on a target. Using the environment to enable entanglement swapping provides a direct and remote measurement on the environment. For example absorption of photons by a remote target can be sensed by the enabling of quantum swapping of entangled particles which can be measured remotely without need for the return of photons from the target. It should be noted that besides images of absorption fields of targets any property can be imaged by enabling quantum swapping when the quantum particle is sensitive to the effects of object. Furthermore with time sequencing this provides range information from for example the source of entangled quantum particles to target features. It should be further realized that the source or sources of the entangled quantum particles need not be located with the equipment used to direct particles towards a target sender or located with the equipment that measured those entangled particles that never directly interacted with the target receiver . For example the source or sources of the entangled particles may be on a satellite that would send the entangled particle pairs to the sender equipment and receiver equipment. Alternately both the sender and receiver may have a single entangled quantum particle source and each shares one particle of their entangled particle pairs with the other. The identification of which particles are entangled with each other relative to initial entangled pair creation times may be achieved using an auxiliary time stamp e.g. a laser pulse encoded with time information for each entangled photon pair created that propagates with each particle of each entangled particle pair. Also the use of an entanglement source such as the one described in does not have an issue or question as to the identification of which particles are entangled as there is only a single source that sequentially generates entangled particles. Although not obvious we consider it possible to use thermal light photon number fluctuations and their correlations and quantum illumination for variations of teleportation and swapping in our current inventions with swapping.

Further benefits of entanglement swapping applied to quantum imaging using measurements of reflected photons may include application to quantum imaging of remote targets and microscopy with the images being generated for the user at a distant location with entangled photons that did not interact directly with the target. The reflected photons may be further used to compute the product of reflectance or the product of reflected intensities of at least two locations on the target. Current imaging systems such as cameras are dependent on producing imaging using photons that have directly interacted with the target. The sharing of images taken by a camera normally requires communication by electromagnetic radiation that takes specific paths to communicate a facsimile of the image between sender and receiver. Even quantum teleportation requires a classical communication channel using electromagnetic radiation that takes specific paths to communicate. It would be beneficial to use entanglement swapping to communicate images or quantum images that does not require a classical communications channel to complete the transfer of images between a sender and a distant user at the receiver in order to avoid having the classical communications channel blocked which would also block image communication. Communication information transfer using entanglement swapping would be an entirely quantum process. The speed of quantum information has been recently been reported as being greater than or equal to 1.37 10times the speed of light See J. Yin et al. Physical Review Letters 110 26047 June 2013 . The benefits of utilizing swapping in the process of quantum communications is that communications would be at the speed of the quantum information even if it is faster than the speed of light which can be beneficial for many applications.

The system is specifically configured to transmit entangled photon pairs to the region of interest. Box is an entangled photon source such as of the types further illustrated by . The entangled photon source generates entangled photon pairs that are entangled in time energy H V polarization or between other conjugate pair properties of the photons. Element is an optical delay line that introduces a time delay for particles reaching the measuring system. Box is a polarizing beamsplitter dichroic mirror or other optical element that operates to direct one portion of an entangled photon pair towards spatially resolving detector and directs the remaining portion of an entangled photon pair toward spatially resolving detector . Boxes are lenses used to focus photons onto detector and detector . Box indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one part of an entangled pair that has interacted with the remote scene target or subject. Box indicates spatially resolving detector . Spatially resolving detector measures time and spatial x y location of the second part of an entangled pair that has interacted with the remote scene target or subject. Box indicates coincidence and timing electronics that operates to register when a pixel on detector and a pixel on detector occur inside within a user defined coincidence window T. A coincidence window is a time difference within which two photon measurements are defined to be coincident. The timing electronics further operate to record the time that has elapsed since a user chosen laser pulse and the first coincidence pair detection for ranging calculations. Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software operationally configured to perform the image improvement and enhancement processes. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest. Box indicates optionally present pointing and aiming e.g. beam steering components that may be used to direct the entangled photon pairs to a specific point within the region of interest.

Box is an entangled photon source such as of the types illustrated by . The entangled photon source generates entangled photon pairs that are entangled in time energy H V polarization or between other conjugate pair properties of the photons. Box is a polarizing beamsplitter dichroic mirror or other optical element that operates to direct one portion of an entangled photon pair towards spatially resolving detector and directs the remaining portion of an entangled photon pair toward spatially resolving detector . Element is a lens used to focus the photons onto detector and detector . Box indicates spatially resolving detector . Spatially resolving detector measures the time and spatial x y location of one part of an entangled pair that has interacted with the remote scene target or subject. Box indicates spatially resolving detector . Spatially resolving detector measures time and spatial x y location of the second part of an entangled pair that has interacted with the remote scene target or subject. Box B indicates coincidence and timing electronics that operates to register when a pixel on detector and a pixel on detector occur inside within a user defined coincidence window T. A coincidence window is a time difference within which two photon measurements are defined to be co incident. The timing electronics further operate to record the time that has elapsed since a user chosen laser pulse and the first coincidence pair detection for ranging calculations. Box indicates a processor memory and algorithms to generate enhanced average second order images of the region of interest. Box indicates memory associated with processor to store input images algorithms intermediate computations and enhanced second order images of the region of interest. Box indicates software operationally configured to perform the image improvement and enhancement processes. Box is a display operationally connected to processor to display the generated enhanced second order or higher order image of the region of interest. Box indicates optionally present pointing and aiming e.g. beam steering components that may be used to direct the entangled photon pairs to a specific point within the region of interest.

A second set of entangled photon pairs generated by are directed towards element A. Element is an optical delay line that introduces a time delay for particles reaching the measuring system. Element A is similar to and directs portions of the entering entangled photon pairs towards measurement devices and . Coincidence measurements of the entangled photons directed to element A are used to generate a reflection image of the target with information that is provided by the shared entanglement properties of the entangled photons that were directed from to telescope A. It is to be appreciated that this invention will operate to generate an improved image of the target where the target may be partially absorbing and or partially reflecting.

Gimaging in the infrared may be useful for applications in art medical imaging of blood vessels identify and face recognition imaging of blood vessels medical imaging of blood vessel flow over the face and other parts of the body and remote feature extraction.

A further application is terahertz ghost imaging. The methods and techniques of this invention apply to all electromagnetic wavelengths. Gamma rays ultraviolet visible infrared microwave and radio waves and terahertz radiation can be utilized to produce improved images of a region of interest.

The potential extent of possible use of this invention is described in the following. However this description should not be construed as limited to the statements. Potential applications include high resolution imaging remote sensing microscopic sensing phase contrast imaging or microscopy astronomical imaging physics chemistry biology medical applications quality control surveillance surface tampering detection imaging partially occluded scenes spectroscopy raman spectroscopy satellite imaging detection of exoplanets identification of hidden or concealed objects remote biometrics design of new sensors and image processing methods design of new types of stealth technology design of new types of communications devices. Furthermore the methods and techniques can be used to determine characteristics of imaging sensors and discover favorable or unfavorable artifacts and properties including but not limited to spatial and temporal noise.

Speed Traffic Enforcement Current local governments use traffic enforcement cameras to enforce traffic regulation violations. A traffic enforcement camera also road safety camera road rule camera photo radar speed camera Gatso is an automated ticketing machine. It may include a camera which may be mounted besides on or over a highway or installed in an enforcement vehicle to detect traffic regulation violations including speeding vehicles going through a red traffic light unauthorized use of a bus lane for recording vehicles inside a congestion charge area and others. The latest automatic number plate recognition ANPR systems can be used for the detection of average speeds and use optical character recognition on images to read the license plates on vehicles. There are a number of possible factors that affect the ANPR software performance. One of these important factors is poor image resolution usually because the plate is too far away but sometimes resulting from the use of a low quality camera. In the case of camera recording a video a sequence of images this invention can process the recorded images to improve image quality of the license plate on vehicle. The enhanced license plate images are used to improve the performance of ANPR software. The invention is especially useful when the images are acquired from a far away distance and or from a low quality camera.

The invention may be utilized in conjunction with large crowd event security and management. Events involving a large crowd especially the types of events including circuses sporting events theatrical events concerts rallies parades etc. the security task is to prevent where possible crimes including theft vandalism or assault through the deployment of trained and licensed security personnel. Camera monitoring is an important component in this type of event security and management. The invention can be used to improve image details of a human face nomenclature on a jersey or a moving object vehicle etc. from a distance or from the periphery of the event location. Also at football games a preferred embodiment could be used to enhance the readability of numbers and or names on football uniforms.

As used herein the terminology subject means an area a scene an object or objects a landscape overhead view of land or an object or objects or a combination thereof.

As used herein the terminology frame means a picture an image or one of the successive pictures on a strip of film or video.

As used herein the terminology process means an algorithm software subroutine computer program or methodology.

As used herein the terminology algorithm means sequence of steps using computer software process software subroutine computer program or methodology.

As used herein the terminology image sensor means a camera bucket detector CMOS SPAD quantum well LIDAR LADAR charge coupled device CCD video device spatial sensor light field plenoptic camera gyro stabilized camera spatial phase sensitive camera or range sensor. The image sensor may comprise a device having a shutter controlled aperture that when opened admits light enabling an object to be focused usually by means of a lens onto a surface thereby producing a photographic image OR a device in which the picture is formed before it is changed into electric impulses.

The terminology camera as used in the following claims includes devices which measure intensity of photons to produce images. Cameras which can take a series of images may be termed video cameras. Many individual cameras have circuitry to allow the capture and storage of video sequences and hence are called video cameras. Cameras are often embedded or attached to computers or smart cell phones or other types of phones. Cameras may be sensitive to one or more wavelengths of light. Color cameras are sensitive to more than one wavelength band of light and can distinguish between those wavelength bands. For example color cameras may sense red green and blue at separate pixels. Color cameras may use a Bayer pattern for collection of more than one wavelength band and use methods for Bayer interpolation to provide intensity for each wavelength band at each pixel. Digital cameras are readily interfaced to digital computers. Analog cameras need analog to digital A D circuitry to transfer images from the camera to a digital computer processor. For example a digital camera is a device used for measuring images of a region of interest. Some digital cameras such as charged coupled devices CCDs have an advantage of being able to achieve low image noise especially when cooled. High quality CCDs may have an electronic cooling capability to lower image noise. CCD cameras may have readout or processing noise. CMOS cameras also called complementary metal oxide cameras. CMOS cameras may have an advantage of lower cost and may have lower readout noise but may have higher image noise. CMOS cameras can also be cooled to achieve improved properties. Analog cameras have been used in television industry and may have advantages where digital cameras should not be used. For example analog cameras may be less susceptible to digital computer viruses. Analog cameras may already be used as sensors in a wide variety of applications. Analog cameras may be made using photo diodes. Photo diodes measure single or multiple photons and in general the total intensity of light falling on the diode. Photo diodes have a small or large active area which allows them to be used as point or area detectors such as photon bucket detectors. Plenoptic cameras utilize an alternate means to record information of a scene or region of interest than conventional cameras. Plenoptic cameras record light coming into the camera through more than one lens. The measurements are saved in a manner that can be combined to achieve refocusing of the image after the measurements are made. As plenoptic cameras improve they will be able to store measured light with higher resolution and speed.

The terminology SPAD as used in the following claims means Single Photon Avalanche Diode. SPAD arrays can be used to form an image of an area of interest in terms of single photon counts for a specified period of time at each SPAD location. Currently SPADs can operate in the nano seconds and pico seconds but it is expected that their speeds may improve even further. Since SPADs can be individually addressed with electronic logic they may be suitable for a wide variety of multi photon interference applications such as for producing enhanced images in cases with and without obscurants and or turbulence. SPADs are becoming more common because the technology is advancing to lower the cost of SPAD arrays. SPADs may also be coupled with illumination timing circuits such as laser triggers to be able to measure time of flight for an emitted laser pulse to travel to a target and return to a particular SPAD pixel. Thus each pixel of a SPAD array in this case can be used to represent a distance or depth map between the SPAD sensor and a corresponding location on a target in the region of interest. Such a distance or depth map would indicate not only the intensity of light reflected and returned but also the distance to the target at each corresponding point.

The terminology LIDAR as used in the following claims means Light Detection and Ranging. LIDAR devices use lasers to measure distances to an object. Often LIDARs are pulsed but they may alternatively be modulated in periodic or aperiodic ways in order to determine distance such as by use of phase modulation. LIDARs may be scanned to produce an image and range map also giving depth and 3D information of the target in the region of interest.

The terminology LADAR as used in the following claims means Laser Radar which produces an image or a map of objects at a distance from the laser source. LADARs may use scanning or flash methods. Flash methods are distinguished from scanning methods in that a larger region of target is illuminated at nearly the same time. Whereas a single laser beam scan illuminates different parts of the target at sequential times. However arrays of lasers may also be used to illuminate different parts of a target at approximately the same time. Each laser when used in combination with a synchronized sensor produces a distance to the target in addition to a strength of intensity return.

The terminology display device as used in the following claims include a display capable of rendering a display. These ways may include rendering on a screen cathode ray tube CRT glasses liquid crystal diodes LCDs light emitting diode LED arrays plasma screens projectors projecting onto a surface or even to the eyes. Modern televisions TVs sometimes called smart TVs not only render images for viewing but also render the images into other formats which may be interfaced to other imaging multi media computing or recording devices such as digital video recorders DVRs through free space e.g. WiFi or Bluetooth electrical wiring and optical fiber.

The terminology processor or image processor as used in the following claims includes a computer multiprocessor CPU GPU FPGA minicomputer microprocessor or any machine similar to a computer or processor which is capable of processing algorithms.

The terminology operations as used in the following claims includes steps a series of operations actions processes subprocesses acts functions and or subroutines.

As used herein the terminology succession means the act or process of following in order or sequence but is not limited to sequential order. As used herein the terminology succession refers to a later taken image being compared with an earlier taken image.

As used herein the terminology array refers to a systematic arrangement of data in rows and columns. An example of an array is a matrix which is a rectangular array of numbers symbols or expressions. Examples of arrays include a matrix which is a rectangular array of numbers symbols or expressions and a vector which is a linear array of numbers symbols or expressions.

As used herein the terminology phase refers to a property of waves that measures the advance of one wave relative to another wave or that wave at an earlier place in space time. Quantum particles such as photons having some wave properties may exhibit phase properties. Phase differences may be manifest as interference or diffraction fringes. Since images are the result of interaction of quantum particles with reflective or scattering objects they can exhibit interference fringes as signatures of instantaneous or average phase variations. Often objects that exhibit some fringes can be referred to as phase objects. Phase information refers to images that provide indications such as interference or diffraction fringes induced by the target and its environment. Phase information can be useful to distinguish features of the target that are generally not as recognizable without the phase information. Phase is discussed in R. E. Meyers K. S. Deacon and Y. H. Shih Turbulence free ghost imaging Appl. Phys. Lett. 98 111115 2011 R. E. Meyers K. S. Deacon and Y. H. Shih Positive negative turbulence free ghost imaging Appl. Phys. Lett. 100 131114 2012 . As used herein the terminology SPAD refers to an array of Single Photon Counting Detectors that is used for imaging.

As used herein the terminology synchronous means data or frames that are acquired at the time or are time coincident.

As used herein the terminology asynchronous means data or frames that are not acquired at the same time or are not time coincident.

As used herein the terminology light is meant to describe electro magnetic energy of any portion of the electro magnetic spectrum to include but not limited to cosmic rays x rays ultra violet visible infra red terahertz microwave and radio waves. Light may be measured in terms of its photons or fields.

As used herein for those embodiments that indicate two or more sensors the sensors need not be co located and may be distant from the other sensor s each sensor would have optionally present optical elements such as lenses or telescopes each sensor system could optionally consist of a processor and memory and further include means to exchange measurement values with other remotely located sensor system locations.

Super resolution generally refers to methods and techniques that enhance the resolution of an imaging system. This resolution increase can mean exceeding the diffraction limit L 1.22 f D where L is the spatial resolution f is the focal length of the lens the wavelength of the light and D the diameter of the lens aperture or super resolution may involve extracting sub pixel features from an image or set of images using digital image processing techniques.

For the current invention one way to enhance the resolution is to distribute the measured intensity value at pixels onto a finer scale array of pixel. For example a 2 2 array of pixels could be expanded to a 4 4 pixel array. Each pixel of the coarse 2 2 array may be partitioned into another 2 2 pixel where each pixel of the fine scale array would be apportioned of the value of the parent coarse pixel. This type of distribution is sometimes referred to as injection . Another method to distribute values to a finer grid from a coarse grid would involve interpolation techniques such as bilinear interpolation where values interior to four surrounding points are a bounded linear combination of the values at the four surrounding points.

The invention can be used with measurements of quantum particles. There are many quantum particles including but not limited to photons electrons neutrons protons atoms ions mesons and positrons. Photons mesons neutral atoms or molecules are bosons. Fermions include quantum particles such as electrons ionized atoms and ionized molecules sometimes referred to as ions.

The invention can be used to generate improved images through a variety of gaseous solid or liquid media or mixtures of these that are at least partially transparent to quantum particles. These media may include but are not limited to for instance glasses diamond silicon water and air. As an example images captured with an underwater camera can be used as input for the inventive process for enhancement as well as images taken through say an air water interface such as an imaging system on a boat looking down into the water or a submerged imaging looking into the air above the water.

Although various preferred embodiments of the present invention have been described herein in detail to provide for complete and clear disclosure it will be appreciated by those skilled in the art that variations may be made thereto without departing from the spirit of the invention.

It should be emphasized that the above described embodiments are merely possible examples of implementations. Many variations and modifications may be made to the above described embodiments. All such modifications and variations are intended to be included herein within the scope of the disclosure and protected by the following claims. The invention has been described in detail with particular reference to certain preferred embodiments thereof but it will be understood that variations and modifications can be effected within the spirit and scope of the invention. All references mentioned herein are hereby incorporation by reference herein.

