---

title: Intelligent snapshot point-in-time management in object storage
abstract: Various embodiments for managing data objects stored in a tiered data object storage environment, by a processor device, are provided. In one embodiment, a method comprises using an application to provide directives to the tiered data object storage environment for manipulating and managing stored data objects such that data objects with a pending management operation are refrained from being migrated from a higher storage tier to a lower storage tier.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09423962&OS=09423962&RS=09423962
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09423962
owner_city: Armonk
owner_country: US
publication_date: 20151116
---
The present invention relates in general to computing systems and more particularly to various embodiments for managing data objects stored in a tiered data object storage environment.

In today s society computer systems are commonplace. Computer systems may be found in the workplace at home or at school. As computer systems become increasingly relied upon convenient and portable the Internet has grown exponentially. Now more than ever individuals and businesses rely upon distributed storage systems commonly referred to as the cloud to store information and data. As wide strides in technological advancement relating to data access devices have been accomplished there is an ever growing demand for growth and development within the back end supporting systems that provide and store the data content.

Various embodiments for managing data objects stored in a tiered data object storage environment by a processor device are provided. In one embodiment a method comprises using an application to provide directives to the tiered data object storage environment for manipulating and managing stored data objects such that data objects with a pending management operation are refrained from being migrated from a higher storage tier to a lower storage tier.

In addition to the foregoing exemplary embodiment various other system and computer program product embodiments are provided and supply related advantages. The foregoing summary has been provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in the background.

Described embodiments and illustrative figures of various embodiments for managing data objects stored in a tiered data object storage environment are to follow. In the interest of clarity not all features of an actual implementation are described in this Specification. It will of course be appreciated by the skilled artisan that in the development of any such actual embodiment numerous implementation specific decisions must be made to achieve the developers specific goals such as compliance with system related and business related constraints which will vary from one implementation to another. Furthermore it will be appreciated that such a development effort may be complex and labor intensive but would nevertheless be a routine undertaking for those of ordinary skill in the art having the benefit of this Disclosure.

Various methods exist for creating data redundancy. For example a function such as a snapshot function enables an administrator to make point in time full volume copies of data with the copies immediately available for read or write access. The snapshot can be used with standard backup tools that are available in the environment to create backup copies on tape. A snapshot function creates a copy of a source volume on a target volume. This copy as mentioned above is called a point in time copy. When a snapshot operation is initiated a relationship is created between a source volume and target volume. This relationship is a mapping of the source volume and the target volume. This mapping allows a point in time copy of that source volume to be copied to the associated target volume. The relationship exists between this volume pair from the time that the snapshot operation is initiated until the storage unit copies all data from the source volume to the target volume or the relationship is deleted.

When the data is physically copied a background process copies data from the source volume to the target volume. The amount of time that it takes to complete the background copy depends on various criteria such as the amount of data being copied the number of background copy processes that are running and any other activities that are presently occurring. The snapshot function works in that the data which is being copied does not actually need to be copied instantaneously it only needs to be copied just prior to an update causing on overwrite of any old data on the source volume. So as data changes on the source volume the original data is copied to the target volume before being overwritten on the source volume.

Therefore a snapshot is a feature supported on various storage devices that allows a user or an automated process to make nearly instantaneous copies of entire logical volumes of data. A copy of a source disk is made on a target disk. The copies are immediately available for both read and write access. A common feature of snapshot like implementations is the ability to reverse the copy. That is to populate the source disk of a snapshot map with the contents of the target disk. It is also possible to use snapshot in cascaded implementations in which a target disk later becomes the source disk for a further snapshot or vice versa.

A cascaded configuration of storage volumes is described in detail in U.S. Pat. No. 7 386 695. It is also possible to create multiple cascades of storage volumes which are interlocking at a logical level. A first cascade may comprise storage volumes A B C and D which are arranged in a cascade as follows ABCD while at a later time a new backup of A may be started that ultimately leads to the creation of AEF. Many different combinations of snapshot functions and reversed functions are possible potentially creating complicated multiple cascading storage volumes.

A typical use case for multiple target snapshot is to create a number of application consistent snapshots of a production disk the source volume of the flashcopies . This is done using some form of a consistency group in order to guarantee that all the copies are identical.

The mechanisms illustrated below may be applicable to a variety of network topologies and network components as will be further described. Notwithstanding the illustration of some of the functionality attendant to the various embodiments one of ordinary skill will appreciate that the methodologies herein may be adapted to a wide variety of implementations and scenarios as noted above.

Turning now to a schematic pictorial illustration of a data processing storage subsystem is shown in accordance with a disclosed embodiment of the invention. The particular subsystem shown in is presented to facilitate an explanation of the invention. However as the skilled artisan will appreciate the invention can be practiced using other computing environments such as other storage subsystems with diverse architectures and capabilities.

Storage subsystem receives from one or more host computers input output I O requests which are commands to read or write data at logical addresses on logical volumes. Any number of host computers are coupled to storage subsystem by any means known in the art for example using a network. Herein by way of example host computers and storage subsystem are assumed to be coupled by a Storage Area Network SAN incorporating data connections and Host Bus Adapters HBAs . The logical addresses specify a range of data blocks within a logical volume each block herein being assumed by way of example to contain 512 bytes. For example a 10 KB data record used in a data processing application on a given host computer would require 20 blocks which the given host computer might specify as being stored at a logical address comprising blocks 1 000 through 1 019 of a logical volume. Storage subsystem may operate in or as a SAN system.

Storage subsystem comprises a clustered storage controller coupled between SAN and a private network using data connections and respectively and incorporating adapters and again respectively. In some configurations adapters and may comprise host SAN adapters HSAs . Clustered storage controller implements clusters of storage modules each of which includes an interface in communication between adapters and and a cache . Each storage module is responsible for a number of storage devices by way of a data connection as shown.

As described previously each storage module further comprises a given cache . However it will be appreciated that the number of caches used in storage subsystem and in conjunction with clustered storage controller may be any convenient number. While all caches in storage subsystem may operate in substantially the same manner and comprise substantially similar elements this is not a requirement. Each of the caches may be approximately equal in size and is assumed to be coupled by way of example in a one to one correspondence with a set of physical storage devices which may comprise disks. In one embodiment physical storage devices may comprise such disks. Those skilled in the art will be able to adapt the description herein to caches of different sizes.

Each set of storage devices comprises multiple slow and or fast access time mass storage devices herein below assumed to be multiple hard disks. shows caches coupled to respective sets of storage devices . In some configurations the sets of storage devices comprise one or more hard disks which can have different performance characteristics. In response to an I O command a given cache by way of example may read or write data at addressable physical locations of a given storage device . In the embodiment shown in caches are able to exercise certain control functions over storage devices . These control functions may alternatively be realized by hardware devices such as disk controllers not shown which are linked to caches .

Each storage module is operative to monitor its state including the states of associated caches and to transmit configuration information to other components of storage subsystem for example configuration changes that result in blocking intervals or limit the rate at which I O requests for the sets of physical storage are accepted.

Routing of commands and data from HBAs to clustered storage controller and to each cache may be performed over a network and or a switch. Herein by way of example HBAs may be coupled to storage modules by at least one switch not shown of SAN which can be of any known type having a digital cross connect function. Additionally or alternatively HBAs may be coupled to storage modules .

In some embodiments data having contiguous logical addresses can be distributed among modules and within the storage devices in each of the modules. Alternatively the data can be distributed using other algorithms e.g. byte or block interleaving. In general this increases bandwidth for instance by allowing a volume in a SAN or a file in network attached storage to be read from or written to more than one given storage device at a time. However this technique requires coordination among the various storage devices and in practice may require complex provisions for any failure of the storage devices and a strategy for dealing with error checking information e.g. a technique for storing parity information relating to distributed data. Indeed when logical unit partitions are distributed in sufficiently small granularity data associated with a single logical unit may span all of the storage devices .

While not explicitly shown for purposes of illustrative simplicity the skilled artisan will appreciate that in some embodiments clustered storage controller may be adapted for implementation in conjunction with certain hardware such as a rack mount system a midplane and or a backplane. Indeed private network in one embodiment may be implemented using a backplane. Additional hardware such as the aforementioned switches processors controllers memory devices and the like may also be incorporated into clustered storage controller and elsewhere within storage subsystem again as the skilled artisan will appreciate. Further a variety of software components operating systems firmware and the like may be integrated into one storage subsystem .

Host computer A comprises a processor A a memory A and an adapter A. Adapter A is coupled to SAN A via a data connection A.

As described supra module A is coupled to storage devices A via data connections A and comprises adapters A and A a cache A and an interface A. Module A also comprises a processor A and a memory A. As explained in detail herein below processor A is configured to establish metrics that indicate a connectivity status of host computer A and store the metrics to memory A. In some embodiments processor A may store metrics to storage devices A.

Host computer B comprises a processor B a memory B and an adapter B. Adapter B is coupled to SAN B via a data connection B.

As described supra module B is coupled to storage devices B via data connections B and comprises adapters B and B a cache B and an interface B. Module B also comprises a processor A and a memory B.

Processors A B A and B typically comprise general purpose computers which are programmed in software to carry out the functions described herein. The software may be downloaded to host computers A and B and modules A and B in electronic form over a network for example or it may be provided on non transitory tangible media such as optical magnetic or electronic memory media. Alternatively some or all of the functions of the processors may be carried out by dedicated or programmable digital hardware components or using a combination of hardware and software elements.

Examples of adapters A B A B A and B include switched fabric adapters such as Fibre Channel FC adapters Internet Small Computer System Interface iSCSI adapters Fibre Channel over Ethernet FCoE adapters and Infiniband adapters.

While the configuration shown in shows storage host computers A and B coupled to storage controllers A and B via SANs A and B other configurations are to be considered within the spirit and scope of the present invention. For example host computers A and B can be coupled to a single storage controller via a single SAN .

Distributed storage i.e. cloud storage is primarily comprised of object storage at massive scale. Object storage provides very little control over manipulating point in time snapshot copies of data generated by storage subsystems such as members of the IBM Storwize family IBM Spectrum Control NetApp FAS6200 etc. For example object storage offers simple Representational State Transfer REST Application Programming Interface API commands such as GET PUT and DELETE.

In the case of backup restore and copy data management a storage controller such as the IBM Storwize V7000 may generate an initial full backup followed by several incremental backups and push them to the object storage. The backups may be controlled via a policy such that a backup is taken every few hours e.g. 4 hours . Over time the amount of backup points in time stored in the environment grows tremendously and it is unnecessary to keep multiple point in time copies of data at these intervals once the points in time are have aged more than a week old as an example . In order to save space there is a need to merge collapse point in time backups in the object storage according to a user defined policy. No interface currently exists to perform these operations within object storage.

There may be the need to update the point in time of the full copy in the object storage in order to improve efficiency in the event of a disaster recovery operation. The most likely scenario during a disaster recovery is to recover data from the point in time nearest to production. In order to recover from a disaster it is necessary to restore a full volume from the object storage and then apply restore points from incremental backups repetitively until the desired point in time is achieved. If a substantial amount of time has passed since the initial full backup was obtained it would mean performing many incremental restore operations from the object storage after performing the full restore.

Additionally a user may request to delete point in time snapshots from the object storage. In order to maintain the ability to restore from any point in time from the object storage a snapshot bitmap cascade must be preserved. Deleting snapshot bitmaps in the middle of the data chain from the object storage inevitably breaks the data chain.

The object storage target that stores the snapshot backups as objects and that also supports the above commands and operations collapse move full forward delete etc. may have a low cost storage tier associated with it which has much different performance characteristics.

The underlying problem is that point in time management operations may need to access data from the lower tier which causes unnecessary overhead in the object storage. Further these management operations may interfere with customer initiated commands e.g. GET commands from the top end of the object store which translates to having to recall data from the lower tier.

Diagram illustrates a move full forward operation A from point in time T Tgt Disk to point in time T Tgt Disk for production vdisk A . This results in a recall of T Tgt Disk from the object storage. In parallel a GET command operation B is issued for T Tgt Disk for production vdisk B . This operation is blocked since the lower tier is already performing a recall for the move full forward command in operation.

Accordingly the mechanisms of the present invention prevent unnecessary recalls from the lower tier in the object store from taking place when a management operation is pending such as performing a delete move full forward prune and collapse of point in time copies of production volumes mapped to objects. Further these mechanisms also prevent writing data to tape only to have it deleted a short time later. These features are employed by adding intelligence in the overall system to ensure that data objects are not migrated to a lower tier e.g. tape if they have the pending management operation associated with them.

The mechanisms of the present invention enable applications such as IBM Spectrum Virtualize to provide directives to the object storage service pertaining to the management of full and incremental snapshot points in time stored within the environment. The storage service provider will interpret these directives in order to perform functions such as collapse points in time move the full backup forward in time delete points in time reconcile points in time update directives and provide status on operations.

In one embodiment the application may set directives to the object storage providing guidance with respect to the time to migrate specific objects to the lower tape tier. These directives are tightly integrated with point in time management operations to ensure that data is not unnecessarily recalled from tape.

Continuing to a method for managing data objects stored in a tiered data object storage environment is illustrated in accordance with one embodiment of the present invention. Starting at step an application is used to provide directives to the tiered data object storage environment for manipulating and managing stored data objects such that data objects with a pending management operation are refrained from being migrated from a higher storage tier to a lower storage tier step . The method ends step .

Under the mechanisms illustrated herein the tagging of data objects to keep them in the higher warm tier may be performed using a variety of methods between the application and the object storage service. Examples of these methods include but are not limited to a REST API an extensible markup language XML document and or a separate socket interface.

In one embodiment using the example of a REST API the application may provide a new API command for the data object store to interpret that controls the management of placing data on the lower tier which is coordinated with the point in time management tasks. Additionally or alternatively the API may set extended attributes in each data object which provides instructions that the object store extracts and interprets. The application may also be able to query the location of a data object determining whether on a lower tape tier or higher disk tier as well as the status of in progress management operations.

In case of unexpected placement of data on lower storage tiers in the object store the application may instruct the object store to prefetch the appropriate data from the lower tier ahead of the operation in order to streamline production. Further in the case of error recovery such as a situation where the production environment becomes disconnected from the object store for an extended period of time such that the object store does not receive any directives an existing policy and or history operations may be used to make an intelligent estimation about which data objects should be migrated between storage tiers and at what time.

Additionally if such were the case where data is inadvertently migrated to a lower tier and a point in time management operation needs to take place the data objects associated with snapshot bitmaps may be left on the higher tier such that the bitmaps may be manipulated to point at different objects on the lower tier without having to recall and rewrite the tape on the lower tier. In this model old data is simply marked for delete or overwrite.

The coordination and intelligence of the application to correlate point in time management operations with the optimal placement of data in the appropriate object storage tier and more specifically its ability to be aware of production in a lower tier in an object store along with the object store s ability to take the appropriate action based on the instruction of the application provides an optimal end to end solution.

The present invention may be a system a method and or a computer program product. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.

The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present invention may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present invention.

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

While one or more embodiments of the present invention have been illustrated in detail the skilled artisan will appreciate that modifications and adaptations to those embodiments may be made without departing from the scope of the present invention as set forth in the following claims.

