---

title: Methods and systems for voice conversion
abstract: A device may receive data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The device may receive an input indicative of speech associated with second voice characteristics of a second voice. The device may map at least one portion of the speech of the second voice to one or more speech sounds of the plurality of speech sounds of the first voice. The device may compare the first voice characteristics with the second voice characteristics based on the map. The comparison may include vocal tract characteristics, nasal cavity characteristics, and voicing characteristics. The device may determine a given representation configured to associate the first voice characteristics with the second voice characteristics. The device may provide an output indicative of pronunciations of the one or more speech sounds of the first voice according to the second voice characteristics based on the given representation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09613620&OS=09613620&RS=09613620
owner: Google Inc.
number: 09613620
owner_city: Mountain View
owner_country: US
publication_date: 20150225
---
This application claims priority to U.S. Provisional Patent Application Ser. No. 62 020 812 filed on Jul. 3 2014 the entirety of which is herein incorporated by reference.

Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

Voice conversion systems are speech processing systems configured to alter speech from a first speaker to have voice characteristics of a second speaker. By way of example a text to speech TTS device may be employed to generate synthetic speech from text by concatenating one or more recorded speech sounds from the first speaker. Such TTS for example may utilize a voice conversion system to modify the one or more recorded speech sounds to have the voice characteristics of the second speaker. In this example the synthetic speech may be perceived as speech from the second speaker.

In one example a method is provided that includes a device receiving data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The device may include one or more processors. The method also includes receiving an input indicative of speech associated with second voice characteristics of a second voice. The method also includes mapping at least one portion of the speech of the second voice to one or more speech sounds of the plurality of speech sounds of the first voice. The method also includes comparing the first voice characteristics with the second voice characteristics based on the mapping. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. The voicing characteristics may be associated with a glottal format or a spectral tilt. The spectral tilt may be between spectral features of a first speech sound of the first voice and corresponding spectral features of a second speech sound of the second voice. The method also includes determining a given representation configured to associate the first voice characteristics with the second voice characteristics based on the comparison. The method also includes providing an output indicative of pronunciations of the one or more speech sounds of the first voice according to the second voice characteristics of the second voice based on the given representation.

In another example a method is provided that includes a device receiving data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The method also includes receiving a request for provision of speech content. The request may be indicative of the speech content having second voice characteristics of a second voice. The method also includes determining a sequence of speech sounds that corresponds to the speech content indicated by the request from within the plurality of speech sounds of the first voice. The method also includes receiving a plurality of representations configured to associate the first voice characteristics with the second voice characteristics. The plurality of representations may be indicative of a comparison between the first voice characteristics and the second voice characteristics. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. The voicing characteristics may be associated with a glottal formant or a spectral tilt. The spectral tilt may be between spectral features of the first voice characteristics and corresponding spectral features of the second voice characteristics. The method also includes modifying the sequence of speech sounds of the first voice to have the second voice characteristics of the second voice based on the plurality of representations. The method also includes providing the speech content having the second voice characteristics of the second voice based on the modification.

In yet another example a device is provided that comprises one or more processors and data storage configured to store instructions executable by the one or more processors. The instructions may cause the device to receive data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The instructions may also cause the device to receive an input indicative of speech associated with second voice characteristics of a second voice. The instructions may also cause the device to map at least one portion of the speech of the second voice to one or more speech sounds of the plurality of speech sounds of the first voice. The instructions may also cause the device to compare the first voice characteristics with the second voice characteristics based on the map. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. The voicing characteristics may be associated with a glottal formant or a spectral tilt. The spectral tilt may be between spectral features of a first speech sound of the first voice and corresponding spectral features of a second speech sound of the second voice. The instructions may also cause the device to determine a given representation configured to associate the first voice characteristics with the second voice characteristics based on the comparison. The instructions may also cause the device to provide an output indicative of pronunciations of the one or more speech sounds of the first voice according to the second voice characteristics of the second voice based on the given representation.

In still another example a system is provided that comprises a means for a device receiving data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The device may include one or more processors. The system further comprises a means for receiving an input indicative of speech associated with second voice characteristics of a second voice. The system further comprises a means for mapping at least one portion of the speech of the second voice to one or more speech sounds of the plurality of speech sounds of the first voice. The system further comprises a means for comparing the first voice characteristics with the second voice characteristics based on the mapping. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. The voicing characteristics may be associated with a glottal formant or a spectral tilt. The spectral tilt may be between spectral features of a first speech sound of the first voice and corresponding spectral features of a second speech sound of the second voice. The system further comprises a means for determining a given representation configured to associate the first voice characteristics with the second voice characteristics based on the comparison. The system further comprises a means for providing an output indicative of pronunciations of the one or more speech sounds of the first voice according to the second voice characteristics of the second voice based on the given representation.

In still another example a system is provided that comprises a means for receiving data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The system further comprises a means for receiving a request for provision of speech content. The request may be indicative of the speech content having second voice characteristics of a second voice. The system further comprises a means for determining a sequence of speech sounds that corresponds to the speech content indicated by the request from within the plurality of speech sounds of the first voice. The system further comprises a means for receiving a plurality of representations configured to associate the first voice characteristics with the second voice characteristics. The plurality of representations may be indicative of a comparison between the first voice characteristics and the second voice characteristics. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. The voicing characteristics may be associated with a glottal formant or a spectral tilt. The spectral tilt may be between spectral features of the first voice characteristics and corresponding spectral features of the second voice characteristics. The system further comprises a means for modifying the sequence of speech sounds of the first voice to have the second voice characteristics of the second voice based on the plurality of representations. The system further comprises a means for providing the speech content having the second voice characteristics of the second voice based on the modification.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying figures.

The following detailed description describes various features and functions of the disclosed systems and methods with reference to the accompanying figures. In the figures similar symbols identify similar components unless context dictates otherwise. The illustrative system device and method embodiments described herein are not meant to be limiting. It may be readily understood by those skilled in the art that certain aspects of the disclosed systems devices and methods can be arranged and combined in a wide variety of different configurations all of which are contemplated herein.

Speech processing systems such as text to speech TTS systems automatic speech recognition ASR systems vocoder systems etc. may be deployed in various environments to provide speech based user interfaces or other speech based output. Some of these environments may include residences businesses vehicles etc. To facilitate speech synthesis in such systems a speech corpus may be provided that includes recorded speech from a speaker that has a particular voice e.g. male female child adult high pitch low pitch etc. or the speech corpus may include representations of the recorded speech e.g. parametric representations etc. .

In some instances a speech synthesis system may be configured to provide synthetic speech associated with more than one voice. For instance a TTS may provide synthetic speech having the particular voice for a first application e.g. email etc. and synthetic speech having another voice for a second application e.g. calendar etc. . In some examples the TTS may include a speech corpus for each voice to provide such functionality. However in other examples including multiple speech corpuses may be undesirable. In one example memory limitations may only allow a limited number of speech corpuses. In another example a speech corpus for a desired voice may be unavailable. For example a device may be configured to provide synthetic speech associated with a given voice that was unknown when the speech corpus was generated.

Accordingly in some examples a voice conversion system may be employed for speech synthesis to allow converting first voice characteristics of recorded speech or representations thereof of a first voice to second voice characteristics associated with a second voice. For example the voice conversion system may apply various signal processing techniques e.g. filters transforms etc. to adjust the first voice characteristics of the recorded speech to correspond to the second voice characteristics associated with the second voice. In some examples the first voice may correspond to the linguistics term source voice or source speaker and the second voice may correspond to the linguistic term target voice or target speaker. 

In some examples such voice conversion system may be configured to associate the first voice characteristics with the second voice characteristics based on a comparison between speech sounds associated with the first voice and speech sounds associated with the second voice. However in some scenarios there may be limitations for the comparison of the voice conversion system. Example limitations may include an amount of training data available e.g. first voice speech sounds second voice speech sounds etc. speed requirements of the comparison according to the voice conversion application etc.

Therefore within examples systems devices and methods are provided for fast voice conversion that is compatible with limited training data. In one example a device is provided that is configured to receive data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. The device may also be configured to receive input indicative of speech associated with second voice characteristics of a second voice. The device may also be configured to map at least one portion of the speech of the second voice to one or more speech sounds of the plurality of speech sounds of the first voice. Various techniques for the mapping in accordance with speed requirements of the comparison are possible and are described in greater detail within embodiments of the present disclosure.

The device may also be configured to compare the first voice characteristics with the second voice characteristics based on the mapping. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. By way of example the device may compensate for difference in the vocal tract characteristics using a signal processing technique such as frequency warping. Further for example the device may compensate for difference in the nasal cavity characteristics and or the voicing characteristics using a signal processing technique such as frequency weighting. In some examples the device may be configured to perform the comparison between the voice characteristics simultaneously e.g. simultaneous frequency warping and frequency weighting etc. . In turn for example such implementation may allow the comparison to be both fast and compatible with a limited amount of speech sounds associated with the second voice.

The device may also be configured to determine a given representation for associating the first voice characteristics with the second voice characteristics based on the comparison. For example the device may determine a frequency warping and weighting filter e.g. the given representation that associates the first voice characteristics with the second voice characteristics. The device may also be configured to provide an output indicative of pronunciations of the one or more speech sounds of the first voice according to the second voice characteristics of the second voice based on the given representation.

Some embodiments herein may therefore allow online voice conversion. Within examples online voice conversion may correspond to voice conversion training and implementation during the speech synthesis process such as for example a computing device receiving speech input from a user and responsively providing synthetic speech having voice characteristics of the user e.g. speech translation etc. . In other embodiments systems and methods herein may also allow offline voice conversion that is compatible with limited training data. As an example of offline voice conversion a user of an example device may provide a few utterances of speech associated with a first voice and the example device may update a speech corpus associated with a second voice to have voice characteristics of the first voice. Other examples are possible as well and are described in greater detail within embodiments of the present disclosure.

Referring now to the figures illustrates an example device in accordance with at least some embodiments described herein. The device includes an input interface an output interface a processor and data storage .

The device may include a computing device such as a smart phone digital assistant digital electronic device body mounted computing device personal computer server or any other computing device configured to execute program instructions included in the data storage to operate the device . The device may include additional components not shown in such as a camera an antenna or any other physical component configured based on the program instructions executable by the processor to operate the device . The processor included in the device may comprise one or more processors configured to execute the program instructions to operate the device .

The input interface may include an audio input device such as a microphone or any other component configured to provide an input signal comprising audio content associated with speech to the processor . Additionally or alternatively the input interface may include a text input device such as a keyboard mouse touchscreen or any other component configured to provide an input signal comprising text content and or other linguistic content e.g. phonemic content etc. to the processor .

The output interface may include an audio output device such as a speaker headphone or any other component configured to receive an output signal from the processor and output speech sounds that may indicate synthetic speech content based on the output signal. Additionally or alternatively the output interface may include a display such as a liquid crystal display LCD light emitting diode LED display projection display cathode ray tube CRT display or any other display configured to provide the output signal comprising linguistic content e.g. text .

Additionally or alternatively the input interface and or the output interface may include network interface components configured to respectively receive and or transmit the input signal and or the output signal described above. For example an external computing device e.g. server etc. may provide the input signal e.g. speech content linguistic content etc. to the input interface via a communication medium such as Wi Fi WiMAX Ethernet Universal Serial Bus USB or any other wired or wireless medium. Similarly for example the external computing device may receive the output signal from the output interface via the communication medium described above.

The data storage may include one or more memories e.g. flash memory Random Access Memory RAM solid state drive disk drive etc. that include software components configured to provide the program instructions executable by the processor to operate the device . Although shows the data storage physically included in the device in some examples the data storage or some components included thereon may be physically stored on a remote computing device. For example some of the software components in the data storage may be stored on a remote server accessible by the device . The data storage may include speech dataset and the program instructions .

In some examples the speech dataset may include a plurality of speech sounds associated with first voice characteristics of a first voice. For example the plurality of speech sounds may include recorded speech segments from a speaker that has the first voice that are each assigned to a linguistic term or a linguistic class. The linguistic term or linguistic class for example may include various phonetic features such as phonemes phones diphones triphones etc.

A phoneme may be considered to be a smallest segment or a small segment of an utterance that encompasses a meaningful contrast with other segments of utterances. Thus a word typically includes one or more phonemes. For example phonemes may be thought of as utterances of letters however some phonemes may represent multiple letters. An example phonemic representation for the English language pronunciation of the word cat may be k ae t including the phonemes k ae and t from the English language. In another example the phonemic representation for the word dog in the English language may be d aw g including the phonemes d aw and g from the English language.

Different phonemic alphabets exist and these alphabets may have different textual representations for the various phonemes therein. For example the letter a in the English language may be represented by the phoneme ae for the sound in cat by the phoneme ey for the sound in atc and by the phoneme ah for the sound in beta. Other phonemic representations are possible. As an example in the English language common phonemic alphabets may contain about 40 distinct phonemes. In some examples a phone may correspond to a speech sound. For example the letter s in the word nods may correspond to the phoneme z which corresponds to the phone s or the phone z depending on a position of the word nods in a sentence or on a pronunciation of a speaker of the word. In some examples a sequence of two phonemes e.g. k ae may be described as a diphone. In this example a first half of the diphone may correspond to a first phoneme of the two phonemes e.g. k and a second half of the diphone may correspond to a second phoneme of the two phonemes e.g. ae . Similarly in some examples a sequence of three phonemes may be described as a triphone.

Additionally in some examples the plurality of speech sounds in the speech dataset may be associated with context features e.g. linguistic classes etc. such as prosodic context preceding and following phonemes position of speech sound in syllable position of syllable in word and or phrase position of word in phrase stress accent length features of current preceding following syllables distance from stressed accented syllable length of current preceding following phrase end tone of phrase length of speech sound within the speech signal etc. By way of example a pronunciation of the phoneme ae in the word cat may be different than a corresponding pronunciation of the phoneme ae in the word catapult. 

Additionally or alternatively in some examples the speech dataset may include parametric representations of the plurality of speech sounds of the first voice. For example the plurality of speech sounds may be represented in the speech dataset by samples of spectral and or aperiodicity envelopes that describe the plurality of speech sounds. By way of example a synthetic speech signal of the device may include a hidden Markov model HMM chain that corresponds to the acoustic feature parameters indicated in the speech dataset . For example an HMM may model a system such as a Markov process with unobserved i.e. hidden states. Each HMM state may be represented as a Gaussian distribution a von Mises distribution or any other statistical distribution that characterizes statistical behavior of the state. For example a statistical distribution may include the acoustic feature parameters of a phoneme that corresponds to a given speech sound of the plurality of speech sounds indicated by the speech dataset . Additionally each state may also be associated with one or more state transitions that specify a probability of making a transition from a current state to another state e.g. based on context features etc. .

When applied to the device in some examples the combination of the statistical distributions and the state transitions for each state may define a sequence of acoustic feature parameters that may be processed to generate a synthetic speech output.

The program instructions comprise various software components including a parameter unit a mapping module a voice characteristics modification unit and a speech synthesis module . The various software components may be implemented for example as an application programming interface API dynamically linked library DLL or any other software implementation suitable for providing the program instructions to the processor .

The parameter unit may be configured to receive a speech signal e.g. via the input interface and provide an acoustic feature representation for the speech signal. The acoustic feature representation for example may include a parameterization of spectral aperiodicity aspects e.g. spectral envelope aperiodicity envelope etc. for the speech signal that may be utilized to regenerate a synthetic pronunciation of the speech signal. Example spectral parameters may include Cepstrum Mel Cepstrum Generalized Mel Cepstrum Discrete Mel Cepstrum Log Spectral Envelope Auto Regressive Filter Line Spectrum Pairs LSP Line Spectrum Frequencies LSF Mel LSP Reflection Coefficients Log Area Ratio Coefficients deltas of these delta deltas of these a combination of these or any other type of spectral parameter. Example aperiodicity parameters may include Mel Cepstrum log aperiodicity envelope filterbank based quantization maximum voiced frequency deltas of these delta deltas of these a combination of these or any other type of aperiodicity parameter. Other parameterizations are possible as well such as maximum voiced frequency or fundamental frequency parameterizations.

In some examples the parameter unit may be configured to sample the acoustic feature parameters described above at harmonic frequencies of the speech signal. For example a spectral envelope of the speech signal e.g. Mel Cepstrum Log spectral envelope etc. may be sampled at the harmonic frequencies and or any other frequencies e.g. regular frequencies irregular frequencies etc. of the speech signal.

In some examples where the speech dataset includes parametric representations of the plurality of speech sounds of the first voice types of the parametric representations may be similar to types of the acoustic feature parameters described above for input speech signal processed by the parameter unit . In other examples where the speech dataset includes the plurality of speech sounds the parameter unit may also be configured to determine the parametric representations similarly to the determination of the acoustic feature parameters of the input speech signal.

In some examples the speech signal may be associated with second voice characteristics of a second voice of a speaker of the speech signal. For example a user of the device may provide the input speech signal having the second voice characteristics of the user e.g. the second voice .

Accordingly the mapping unit may be configured to map one or more speech sounds of the speech dataset e.g. the first voice speech sounds with at least one portion of the input speech signal e.g. the second voice speech sounds . By way of example the mapping unit may identify the one or more speech sounds of the speech dataset that correspond to a linguistic class e.g. pronunciations of the phoneme a in various contexts etc. . On one hand if the one or more speech sounds are included in the speech dataset as recorded speech segments the mapping module may obtain the acoustic features parameters for the one or more speech sounds via the parameter unit for example. On the other hand if the one or more speech sounds are included in the speech dataset as statistical distributions e.g. HMM states etc. the mapping unit may obtain representations of the statistical distributions e.g. Gaussian means etc. for example. Further in the example the mapping unit may identify the at least one portion of the input speech signal that are similar to the one or more speech sounds of the speech dataset . For example the mapping unit may perform a comparison between the acoustic feature parameters of the one or more speech sounds e.g. first voice speech sounds and corresponding parameters generated by the parameter unit of the input speech signal to identify the at least one portion of the input speech signal associated with similar speech sounds to the one or more speech sounds of the speech dataset . Example methods for operation of the mapping unit are described in greater detail within embodiments of the present disclosure.

The voice characteristics modification unit may be configured to compare the first voice characteristics with the second voice characteristics based on the mapping. The comparison may include vocal tract characteristics e.g. vocal tract length vocal tract shape etc. nasal cavity characteristics and voicing characteristics. The voicing characteristics for example may be associated with a glottal formant or a spectral tilt. The glottal format and or the spectral tilt for example may pertain to spectral energy distribution indicated by the acoustic feature parameters of the one or more speech sounds of the speech dataset e.g. the first voice speech sounds and corresponding acoustic feature parameters of the at least one portion of the input speech signal e.g. the second voice speech sounds .

To compensate for differences in the vocal tract characteristics for example the voice characteristics modification unit may determine a warping component. By way of example a first speech sound of the first voice may have particular spectral features e.g. maximums minimums etc. at the frequencies 1450 Hz 1600 Hz 1750 Hz. Similarly in the example a second speech sound of the second voice may have corresponding spectral features at the frequencies 1455 Hz 1595 Hz 1740 Hz. The difference in the example may be due to differences between the vocal tract characteristics of the first voice and the second voice. Thus for example the warping component determined by the voice characteristics modification unit may associate the various frequencies accordingly. Further for example to compensate for differences in the nasal cavity characteristics e.g. spectral nulls at high frequencies etc. and or the voicing characteristics the voice characteristics modification unit may determine a weighting component. In the example above amplitudes of the acoustic feature parameters e.g. the spectral envelopes at a particular frequency e.g. 1750 Hz for the first voice and 1740 Hz for the second voice etc. may differ by a factor and thus the weighting component may compensate for such difference.

In some examples the warping component may be sensitive to the weighting component. For example an effect of the weighting component may bias the identification of corresponding frequencies by the warping component. Therefore to facilitate fast operation of the voice characteristics modification unit in some examples herein the comparison performed by the voice characteristics modification unit including the warping and the weighting components may be performed simultaneously. To that end for example the comparison including the vocal tract characteristics the nasal cavity characteristics and the voicing characteristics may be performed simultaneously by the voice characteristics modification unit .

Accordingly the voice characteristics modification unit may also be configured to determine a given representation to associate the first voice characteristics with the second voice characteristics based on the comparison. The given representation for example may include both the warping and the weighting components. An example implementation for operation of the voice characteristics modification unit to determine the given representation based on the comparison is presented below.

S may correspond to the n th source speaker e.g. the first voice acoustic feature parameters at frequency e.g. radians etc. . Similarly T may correspond to the n th target speaker e.g. the second voice acoustic feature parameter at frequency e.g. radians etc. . Further for example S and T may be matched in accordance with the mapping unit . Additionally w may correspond to a frequency warping function and b may correspond to a frequency weighting function. In some examples the frequency warping function w may satisfy the condition illustrated by equation 1 below. 0 0 1 

Further in accordance with the definitions above an estimate of the target speaker e.g. the second voice spectral envelope circumflex over T at a given frequency that is based on the frequency warping function w and the frequency weighting function b applied to the source speaker e.g. the first voice spectral envelope S is illustrated in equation 2 below. 2 

The voice characteristics modification unit may then determine the given representation that minimizes a criterion of average log spectral distortion D as illustrated in equations 3 4 below.

In some examples the frequency may be quantized to K equal intervals e.g. where k 1 . . . K . Accordingly the integral in equation 4 may be approximated as a sum as illustrated in equation 5 below.

Further in some examples the distortion D may be augmented with a regularization term to penalize rapid variations in frequency weighting function b that may cause audible artifacts as illustrated in equation 6 below.

In equation 6 may correspond to a smoothness regularization constant. Further for example a condition b b may be utilized to determine b . As illustrated in equation 6 the distortion D is convex quadratic on b . In turn an optimal frequency weighting function circumflex over b may be determined based on a partial derivative of the distortion D of equation 6 as illustrated in equation 7 below.

It is noted that in equation 7 the optimal frequency weighting function circumflex over b is determined based on the optimal frequency weighting function at a previous frequency circumflex over b and on the frequency warping function w . Accordingly the distortion D in equation 6 may be modified to include the optimal frequency weighting function circumflex over b of equation 7 as illustrated in equation 8 below.

At this point in some examples a Dynamic Frequency Warping DFW process such as the DFW process used in a pitch synchronous overlap and add PSOLA system or any other DFW process may be modified to utilize the distortion D of equation 8 to determine values for optimal frequency warping and optimal frequency weighting circumflex over b simultaneously. Such values may correspond to the given representation determined by the voice characteristics modification unit and may be utilized to adjust speech sounds associated with the first voice e.g. the speech dataset to have the second voice characteristics of the second voice e.g. the second voice characteristics of the input speech signal . Similarly for example the process described by equations 1 8 may be repeated for other linguistic classes e.g. other phonemes etc. to determine values for optimal frequency warping and optimal frequency weighting circumflex over b that are associated with the other linguistic classes. In turn for example to generate synthetic speech sounds having the second voice characteristics based on speech sounds having the first voice characteristics the device may apply a filter characterized by an equation similar to equation 2 using corresponding optimal frequency warping and weighting OFWW values computed in accordance with equations 7 8 e.g. circumflex over b etc. .

The speech synthesis unit may be configured to receive a request for provision of speech content having the second voice characteristics of the second voice based on speech sounds having the first voice characteristics of the first voice e.g. speech sounds in the speech dataset etc. . Further the speech synthesis unit may also be configured to responsively generate an output audio signal e.g. via the output interface etc. that corresponds to a pronunciation of the speech content indicated by the request having the second voice characteristics.

By way of example the speech synthesis unit may determine a sequence of speech sounds or HMMs from within the speech dataset e.g. having the first voice characteristics that correspond to the speech content of the request. Further for example the speech synthesis unit may apply the given representation determined by the voice characteristics modification unit on the sequence of speech sounds or the HMMs to adjust the sequence of speech sounds to have the second voice characteristics. In this example the speech synthesis unit may be operated in an online mode that corresponds to performing voice conversion during speech synthesis. Alternatively for example the device may modify the speech dataset based on the given representation determined by the voice characteristics modification unit . In this example the sequence of speech sounds or the HMMs may already have the second voice characteristics. In this example the speech synthesis unit may be operated in an offline mode that corresponds to performing the voice conversion prior to the speech synthesis. Other examples are possible as well and are described in greater detail within embodiments of the present disclosure.

In some examples the given representation from the voice characteristics modification unit e.g. circumflex over b etc. may not correspond to the exact frequencies in the speech sounds having the first voice characteristics. In these examples an interpolation technique may be utilized to determine the optimal frequency warping and weighting OFWW values for such frequencies. Example interpolation techniques may be based on Kullback Leibler Euclidean distance or any other distance between the frequencies of the speech sounds and the frequencies in the given representation.

In some examples the speech synthesis unit may utilize a vocoder to provide the modified speech sounds of the speech dataset having the second voice characteristics. In one example the modified HMMs or other statistical models e.g. having the second voice characteristics associated with speech sounds of the speech dataset may be provided to the vocoder to generate a synthetic speech signal. In another example the sequence of speech sounds based on the request may be provided to the vocoder along with the given representation from the voice characteristics modification unit and the vocoder may apply for example the warping and weighting components to modify the sequence of speech sounds. Other examples are possible as well. Example vocoders that may be utilized by the speech synthesis unit may include sinusoidal vocoders e.g. AhoCoder Harmonic plus Noise Model HNM vocoder Sinusoidal Transform Codec STC etc. and or non sinusoidal vocoders e.g. STRAIGHT etc. .

It is noted that functional blocks of are illustrated for convenience in description. In some embodiments the device may be implemented using more or less components configured to perform the functionalities described in . For example the parameter unit the mapping unit the voice characteristics modification unit and or the speech synthesis unit may be implemented as one two or more software components. Further in some examples components of the device may be physically implemented in one or more computing devices according to various applications of the device . For example the speech dataset may be included in a remote computing device e.g. server accessible by the device . Other configurations are possible as well.

In addition for the method and other processes and methods disclosed herein the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard each block may represent a module a segment a portion of a manufacturing or operation process or a portion of program code which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium for example such as a storage device including a disk or hard drive. The computer readable medium may include non transitory computer readable medium for example such as computer readable media that stores data for short periods of time like register memory processor cache and Random Access Memory RAM . The computer readable medium may also include non transitory media such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. The computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

In some examples for the method and other processes and methods disclosed herein each block may represent circuitry that is wired to perform the specific logical functions in the process.

At block the method includes receiving data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. In one example the data may include a speech corpus similar to the speech dataset of the device which includes the plurality of speech sounds as recorded speech utterances. In another example the data may include parametric representations of the plurality of speech sounds such as HMM models. In yet another example the data may include a combination of recorded speech sounds and HMM models. Other configurations of the data are possible as well.

At block the method includes receiving an input indicative of speech associated with second voice characteristics of a second voice. The input for example may include an articulation of a phrase by the second voice that is associated with a speaker other than the speaker of the plurality of speech sounds at block .

At block the method includes mapping at least one portion of the speech of the second voice to one or more speech sounds of the plurality of speech sounds of the first voice. By way of example the method may identify the one or more speech sounds of the first voice that are associated with a linguistic class e.g. phoneme a and may match the one or more speech sounds with portions of the speech of the second voice that are similar to pronunciations of the linguistic class.

Accordingly in some examples the method may also include determining the one or more speech sounds from within the plurality of speech sounds of the first voice based on an association between the one or more speech sounds and a linguistic term. The linguistic term for example may include a phoneme or text. In these examples the method may also include determining the at least one portion of the speech of the second voice based on the at least one portion of the speech being associated also with the linguistic term.

At block the method includes comparing the first voice characteristics with the second voice characteristics based on the mapping. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. For example the comparison may be similar to the comparison performed by the voice characteristics modification unit of the device e.g. based on equations 1 8 etc. .

In some examples the voicing characteristics may be associated with a glottal formant or a spectral tilt. The spectral tilt may be between spectral features of a first speech sound of the first voice and corresponding spectral features of a second speech sound of the second voice. In some examples the vocal tract characteristics may be associated with one or more of a vocal tract length or a vocal tract shape.

In some examples the method may also include determining acoustic feature representations for the one or more speech sounds of the first voice and the at least one portion of the speech of the second voice. In some examples the glottal formant and or the spectral tilt may be indicated by the acoustic feature representations. Further in some examples the nasal cavity characteristics may be associated with spectral nulls indicated by the acoustic feature representations.

Additionally in these examples the method may also include identifying the first voice characteristics and the second voice characteristics based on the acoustic feature representations. By way of example a device of the method may determine statistical models e.g. HMMs for the plurality of speech sounds of the first voice and acoustic feature parameters e.g. Mel Cepstrum parameters etc. for the at least one portion of the speech of the second voice. The device may then proceed to identify the first voice characteristics and the second voice characteristics similarly to the voice characteristics modification unit of the device e.g. warping and weighting components etc. .

Accordingly in some examples the method may also include determining one or more statistical models associated with the one or more speech sounds of the first voice. The one or more statistical models may be indicative of the first voice characteristics. In these examples the method may also include modifying the one or more statistical models such that the one or more statistical models are indicative of the second voice characteristics. The one or more statistical models for example may include Hidden Markov Models HMMs or Deep Neural Network DNN nodes. Other examples are possible as well.

At block the method includes determining a given representation configured to associate the first voice characteristics with the second voice characteristics based on the comparison. The given representation for example may include the optimal frequency warping and weighting OFWW described in the voice characteristics modification unit of the device .

In some examples the method may include determining a distortion representation that includes a frequency warping component and a frequency weighting component based on the comparison. The distortion representation for example may correspond to the distortion D of equation 8 . Accordingly the frequency warping component may be configured to associate given vocal tract characteristics of the first voice with corresponding vocal tract characteristics of the second voice. The frequency weighting component may be configured to associate given voicing characteristics of the first voice with corresponding voicing characteristics of the second voice. The frequency weighting component may also be configured to associate given nasal cavity characteristics of the first voice with corresponding nasal cavity characteristics of the second voice. In these examples determining the given representation at block may be based on the distortion representation. For example the frequency warping component may be similar to the optimal frequency warping function described in the voice characteristics modification unit and the frequency weighting component may be similar to the optimal frequency weighting function circumflex over b described in the voice characteristics modification unit . Further in some examples the method may include modifying the frequency weighting component based on a smoothing modulation factor. For example the smoothing modulation factor may correspond to the smoothness regulation constant of equation 6 .

At block the method includes providing an output indicative of pronunciations of the one or more speech sounds of the first voice according to the second voice characteristics of the second voice based on the representation. In one example the method may apply the given representation e.g. OFWW to the plurality of speech sounds based on use of a vocoder to modify the plurality of speech sounds. In another example the method may apply the given representation to statistical models of the plurality of speech sounds e.g. Gaussian means etc. .

In some examples the method may also include receiving a speech corpus that includes the plurality of speech sounds associated with the first voice characteristics. The speech corpus for example may be similar to the speech dataset of the device . In these examples the method may also include modifying the speech corpus to include the one or more speech sounds associated with the second voice characteristics based on the given representation. For example the OFWW filter described above may be applied to the speech corpus to adjust voice characteristics of the speech corpus to correspond to the second voice characteristics.

At block the method includes receiving data indicative of a plurality of speech sounds associated with first voice characteristics of a first voice. In one example the data may include a speech corpus similar to the speech dataset of the device which includes the plurality of speech sounds as recorded speech utterances. In another example the data may include parametric representations of the plurality of speech sounds such as HMM models. In yet another example the data may include a combination of recorded speech sounds and HMM models. Other configurations of the data are possible as well.

At block the method includes receiving a request for provision of speech content. The request may be indicative of the speech content having second voice characteristics of a second voice. For example a navigation device of the method may receive the request for providing instructions to an address. The request for example may include speech from a user that has the second voice characteristics. In turn for example the navigation device may be configured to provide the instructions to the address as the speech content having the second voice characteristics.

At block the method includes determining a sequence of speech sounds from within the plurality of speech sounds that corresponds to the speech content indicated by the request. In the example of the navigation device the device may obtain the sequence of speech sounds from a speech corpus that includes the plurality of speech sounds of the first voice. The sequence of speech sounds in this example may correspond to the instructions to the address.

In some examples the method may also include determining a sequence of linguistic terms that corresponds to the speech content indicated by the request. In these examples the plurality of speech sounds of the first voice may be associated with the plurality of linguistic terms. In the example of the navigation device the device may obtain the sequence of speech sounds from the speech corpus that correspond to the sequence of linguistic terms e.g. text of the instructions to the address etc. .

At block the method includes receiving a plurality of representations configured to associate the first voice characteristics with the second voice characteristics. The plurality of representations may be indicative of a comparison between the first voice characteristics and the second voice characteristics. The comparison may include vocal tract characteristics nasal cavity characteristics and voicing characteristics. For example the plurality of representations may be similar to the OFWW transforms described in the voice characteristics module of the device . Further for example the plurality of representations may be associated with linguistic terms pertaining to the sequence of speech sounds.

At block the method includes modifying the sequence of speech sounds of the first voice to have the second voice characteristics of the second voice based on the plurality of representations. For example a device of the method may perform the functions of the voice characteristics module and or the speech synthesis module of the device to modify the sequence of speech sounds to have the second voice characteristics e.g. frequency warping frequency weighting etc. .

At block the method includes providing the speech content having the second voice characteristics of the second voice based on the modification. In the example of the navigation device the method at block may provide the instructions to the address having the second voice characteristics of the user based on modifying the sequence of speech sounds of the first voice to have the second voice characteristics.

The method in some examples may perform the functions of the mapping unit and or other components of the device .

At block the method includes receiving data indicative of a plurality of first voice speech sounds and a plurality of second voice speech sounds. The plurality of first voice speech sounds and the plurality of second voice speech sounds may be associated with a linguistic term. For example the plurality of first voice speech sounds may correspond to the one or more speech sounds in the speech dataset of the device that are associated with the linguistic term e.g. linguistic class such as a phoneme. Further for example the plurality of second voice speech sounds may correspond to the at least one portion of the speech received by the input interface of the device that have the second voice characteristics.

At block the method includes determining a first vector that includes representations of the plurality of first voice speech sounds and a second vector that includes representations of the plurality of second voice speech sounds. In some examples the representations of the plurality of first voice speech sounds may be stored in a dataset such as the speech dataset of the device . For example the speech dataset may include Hidden Markov Models HMMs that simulate the first voice speech sounds. In some examples the representations of the plurality of second voice speech sounds may be determined at the block as acoustic feature parameters of spectral envelopes of the second voice speech sounds. For example a Mel Cepstral representation of a spectral envelope may be sampled.

By assembling the first vector with Gaussian distributions of the first voice speech sounds and the second vector with Gaussian distributions of the second voice speech sounds the method may then associate the first voice speech sounds with the second voice speech sounds by solving an optimization problem for example. In some examples the first vector and the second vector may include or be based on the means of the Gaussian distributions and not the weights e.g. probabilities of the Gaussian distributions. The optimization problem for instance may attempt to formulate the following transformation function 9 

In equation 9 above x belongs to an X space that includes the Gaussian distributions of the first vector and y belongs to a Y space that includes the Gaussian distributions of the second vector. For example the X space may correspond to an input space and the Y space may correspond to an output space. Thus for example equation 9 may represent a solution for the optimization problem that corresponds to the mapping module of the system . For example equation 9 may provide parametric mapping from the X space to the Y space. Further the equation 9 may be utilized to associate second voice speech sounds with first voice speech sounds to compensate inter speaker differences between the first voice and the second voice for example. On the other hand the equation 9 may also provide non parametric mapping from the Y space to the X space for example.

In examples where there are N first voice speech sounds in the x space and Q second voice speech sounds in the y space a given first voice speech sound and a given second voice speech sound may be represented respectively as yand x where q and n are real integers in the ranges respectively 1 to Q and 1 to N. Further a distortion metric d y x between the first vector x and the second vector y may be expressed as 10 

The distortion metric in equation 10 describes the distortion e.g. distance between yand x. Additionally in equation 10 Wis a weighting matrix depending on the second vector y. The weighting matrix may allow incorporating frequency weighting in the distortion metric for example. Further in some examples a joint probability of matching yand pressed as p y x . For example the average distortion D for all possible vector combinations in equation 9 may be expressed as 11 

The association probabilities p x y in equation 11 indicate the mapping and or association between a given first voice speech sound corresponding to xand a given second voice speech sound corresponding to y Additionally in some examples the y space probabilities p y in equation 11 may be to be uniformly distributed for all Q. In other words 

Additionally in this example by minimizing the average distortion D and the association probabilities p x y simultaneously in equation 11 the optimization problem may be solved to determine the optimal mapping of the equation 9 .

At block the method includes determining a third vector that includes association probabilities between the first vector and the second vector. Below is an example computation for determining the association probabilities p x y .

The method for example may simulate the distortion metric of equation 10 as a distance function. Thus in this example the computation may involve determining the minimum distance between a given Gaussian distribution mean of the first vector e.g. x and a given Gaussian distribution mean of the second vector e.g. y . In other words the method may be configured to identify a given first voice speech sound that is proximal to a given second voice speech sound for example. For instance for a given y space vector a nearest neighbor search I q for the nearest x space vector may be expressed as argmin 12 

In this example the association probabilities in the third vector may then be computed by the method as the expression 

For example according to equation 13 the association probabilities map second voice speech sounds from the Y space to first voice speech sounds from the X space.

It is noted that the example above is for example purposes and other computations may be possible to determine the association probabilities. For example deterministic annealing computations involving an annealing temperature may be alternatively used.

At block the method includes mapping the plurality of first voice speech sounds to the plurality of second voice speech sounds based on the third vector. The mapping for example may be represented as the transformation function of equation 9 .

For example the optimization problem described in the previous blocks may be solved based on the third vector in block . One example solution may involve performing a linear regression. For example the transformation function of equation 9 may be represented as a mixture of linear regressions function expressed as 14 where is a bias vector of the linear regression is a linear transformation matrix of the k th class and p k x is the probability that xto the k th class. Note that the linear transformation matrix is in matrix form. In some examples the matrix form may be inconvenient thus the linear transformation matrix may be reformulated by using a vector operator vec and a Kronecker product vec vec 15 where is a vectorized transformation matrix of and Iis an identity matrix. Note that the operator vec is simply rearranging the parameters of by stacking together the columns of the matrix.

In some examples the transform vector may be structured. In these examples the structure may be incorporated in equation 15 with a repetition matrix R. The repetition matrix R may include zeros and ones at the appropriate location to incorporate the structure of the transform vector for example. The inclusion of the repetition matrix R may yield the following expression 16 where includes free parameters of the structured matrix and Xis an space data matrix that includes recasted information of x.

In some examples the data matrix Xmay be sparse. In these examples the linear transformation matrix may be constrained as the block transform matrix expression 

Further the transformation function F x e.g. mapping may then be expressed as a simple linear regression 

Further the optimization problem may then be solved by minimizing the average distortion D of equation 11 with respect to the linear regression variable . For example a corresponding partial derivative may be equated to zero 

Note that the solution in equation 24 includes the association probabilities p x y from the third vector determined at block . Thus at block the mapping e.g. equation 9 may be determined based on the third vector. In some examples the linear regression computation may involve repeating blocks and until convergence of the linear regression variables for example in the equation 18 .

Although shows three programmable devices distributed application architectures may serve tens hundreds thousands or any other number of programmable devices. Moreover the programmable devices and or any additional programmable devices may be any sort of computing device such as an ordinary laptop computer desktop computer network terminal wireless communication device e.g. a tablet a cell phone or smart phone a wearable computing device etc. and so on. In some examples the programmable devices and may be dedicated to the design and use of software applications. In other examples the programmable devices and may be general purpose computers that are configured to perform a number of tasks and may not be dedicated to software development tools. For example the programmable devices may be configured to provide speech processing functionality similar to that discussed in . For example the programmable devices may include a device such as the device .

The server devices and can be configured to perform one or more services as requested by programmable devices and or . For example server device and or can provide content to the programmable devices . The content may include but is not limited to text web pages hypertext scripts binary data such as compiled software images audio and or video. The content can include compressed and or uncompressed content. The content can be encrypted and or unencrypted. Other types of content are possible as well.

As another example the server device and or can provide the programmable devices with access to software for database search computation e.g. vocoder speech synthesis graphical audio e.g. speech content video World Wide Web Internet utilization and or other functions. Many other examples of server devices are possible as well. In some examples the server devices and or may perform at least some of the functions described in .

The server devices and or can be cloud based devices that store program logic and or data of cloud based applications and or services. In some examples the server devices and or can be a single computing device residing in a single computing center. In other examples the server devices and or can include multiple computing devices in a single computing center or multiple computing devices located in multiple computing centers in diverse geographic locations. For example depicts each of the server devices and residing in different physical locations.

In some examples data and services at the server devices and or can be encoded as computer readable information stored in non transitory tangible computer readable media or computer readable storage media and accessible by programmable devices and and or other computing devices. In some examples data at the server device and or can be stored on a single disk drive or other tangible storage media or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.

As noted above in some embodiments the disclosed techniques e.g. methods can be implemented by computer program instructions encoded on a computer readable storage media in a machine readable format or on other media or articles of manufacture e.g. the program instructions of the device or the instructions that operate the server devices and or the programmable devices in . is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device arranged according to at least some embodiments disclosed herein.

In one embodiment the example computer program product is provided using a signal bearing medium . The signal bearing medium may include one or more programming instructions that when executed by one or more processors may provide functionality or portions of the functionality described above with respect to . In some examples the signal bearing medium can be a computer readable medium such as but not limited to a hard disk drive a Compact Disc CD a Digital Video Disk DVD a digital tape memory etc. In some implementations the signal bearing medium can be a computer recordable medium such as but not limited to memory read write R W CDs R W DVDs etc. In some implementations the signal bearing medium can be a communication medium e.g. a fiber optic cable a waveguide a wired communications link etc. . Thus for example the signal bearing medium can be conveyed by a wireless form of the communications medium .

The one or more programming instructions can be for example computer executable and or logic implemented instructions. In some examples a computing device such as the processor equipped device of and or programmable devices of may be configured to provide various operations functions or actions in response to the programming instructions conveyed to the computing device by one or more of the computer readable medium the computer recordable medium and or the communications medium . In other examples the computing device can be an external device such as server devices of in communication with a device such as the device and or the programmable devices 

The computer readable medium can also be distributed among multiple data storage elements which could be remotely located from each other. The computing device that executes some or all of the stored instructions could be an external computer or a mobile computing platform such as a smartphone tablet device personal computer wearable device etc. Alternatively the computing device that executes some or all of the stored instructions could be remotely located computer system such as a server. For example the computer program product can implement the functionalities discussed in the description of .

It should be understood that arrangements described herein are for purposes of example only. As such those skilled in the art will appreciate that other arrangements and other elements e.g. machines interfaces functions orders and groupings of functions etc. can be used instead and some elements may be omitted altogether according to the desired results. Further many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components in any suitable combination and location or other structural elements described as independent structures may be combined.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims along with the full scope of equivalents to which such claims are entitled. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting.

