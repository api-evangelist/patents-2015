---

title: System and method for runtime grouping of processing elements in streaming applications
abstract: A method, computer program product, and computer system for dynamically grouping and un-grouping processing operators and processing elements used by a streaming application. A distributed processing elements utilization of resources may be monitored to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. At runtime, via at least one of parallelization and fusion, the grouping and un-grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09535763&OS=09535763&RS=09535763
owner: International Business Machines Corporation
number: 09535763
owner_city: Armonk
owner_country: US
publication_date: 20151130
---
Many times in solving problems with large data sets there may be issues acquiring enough computing power to address the problem in the desired timeframe. The result may be that often time a distributed processing job overloads a number or cluster of the machines that may be dedicated to computing the solution. In example MapReduce style computations or the like a resource manager may be used to allocate machines at each of the Map and Reduce steps to attempt to alleviate stresses placed on a subset of the computing cluster. Streaming computing may present additional alternative issues.

For example since the computation may be constantly occurring for every processing phase at all times it may be difficult to use conventional resource managers e.g. used for non streaming computing to allocate resources. For instance there may never be an ending of a streaming computing process thus resource managers may see constant usage and without knowledge of the stream processing topology the resource managers may not be able to safely remove a machine without potentially introducing a break in the processing graph. Adding additional resources may also present similar complications.

In one example implementation a method performed by one or more computing devices may include but is not limited to dynamically grouping and un grouping by a computing device processing operators and processing elements used by a streaming application. A distributed processing elements utilization of resources may be monitored to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. At runtime via at least one of parallelization and fusion the grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted.

One or more of the following example features may be included. A first processing element of the identified candidate processing elements may be identified with the distributed processing elements resource utilization exceeding an upper threshold a second processing element of the identified candidate processing elements may be identified with the distributed processing elements resource utilization below a lower threshold and a fusion of the first and second processing elements may be executed. Responsive to changes in system topology based upon at least in part outside use of a resource manager an upper threshold of a first system by a first processing element of the identified candidate processing elements may be detected and the first processing element may be parallelized to a newly available resource from the resource manager. Responsive to changes in the system topology based upon at least in part outside use of the resource manager the lower threshold of a second system by a second processing element of the identified candidate processing elements may be detected and the second processing element may be fused with a third processing element of the identified candidate processing elements to release the second system to the resource manager. Executing fusion may include splitting a portion of operators grouped in the first processing element and grouping the portion of the operators in the second processing element. Parallelization may include a graph transform. The grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted at runtime via at least one of parallelization and fusion based upon at least in part a tuple flow rate of the operators. The dynamic adjusting may be performed without user intervention.

In another example implementation a computing system includes a processor and a memory configured to perform operations that may include but are not limited to dynamically grouping and un grouping processing operators and processing elements used by a streaming application. A distributed processing elements utilization of resources may be monitored to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. At runtime via at least one of parallelization and fusion the grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted.

One or more of the following example features may be included. A first processing element of the identified candidate processing elements may be identified with the distributed processing elements resource utilization exceeding an upper threshold a second processing element of the identified candidate processing elements may be identified with the distributed processing elements resource utilization below a lower threshold and a fusion of the first and second processing elements may be executed. Responsive to changes in system topology based upon at least in part outside use of a resource manager an upper threshold of a first system by a first processing element of the identified candidate processing elements may be detected and the first processing element may be parallelized to a newly available resource from the resource manager. Responsive to changes in the system topology based upon at least in part outside use of the resource manager the lower threshold of a second system by a second processing element of the identified candidate processing elements may be detected and the second processing element may be fused with a third processing element of the identified candidate processing elements to release the second system to the resource manager. Executing fusion may include splitting a portion of operators grouped in the first processing element and grouping the portion of the operators in the second processing element. Parallelization may include a graph transform. The grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted at runtime via at least one of parallelization and fusion based upon at least in part a tuple flow rate of the operators. The dynamic adjusting may be performed without user intervention.

In another example implementation a computer program product resides on a computer readable storage medium that has a plurality of instructions stored on it. When executed by a processor the instructions cause the processor to perform operations that may include but are not limited to dynamically grouping and un grouping processing operators and processing elements used by a streaming application. A distributed processing elements utilization of resources may be monitored to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. At runtime via at least one of parallelization and fusion the grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted. At runtime via at least one of parallelization and fusion the grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted.

One or more of the following example features may be included. A first processing element of the identified candidate processing elements may be identified with the distributed processing elements resource utilization exceeding an upper threshold a second processing element of the identified candidate processing elements may be identified with the distributed processing elements resource utilization below a lower threshold and a fusion of the first and second processing elements may be executed. Responsive to changes in system topology based upon at least in part outside use of a resource manager an upper threshold of a first system by a first processing element of the identified candidate processing elements may be detected and the first processing element may be parallelized to a newly available resource from the resource manager. Responsive to changes in the system topology based upon at least in part outside use of the resource manager the lower threshold of a second system by a second processing element of the identified candidate processing elements may be detected and the second processing element may be fused with a third processing element of the identified candidate processing elements to release the second system to the resource manager. Executing fusion may include splitting a portion of operators grouped in the first processing element and grouping the portion of the operators in the second processing element. Parallelization may include a graph transform. The grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted at runtime via at least one of parallelization and fusion based upon at least in part a tuple flow rate of the operators. The dynamic adjusting may be performed without user intervention.

The details of one or more example implementations are set forth in the accompanying drawings and the description below. Other possible example features and or possible example advantages will become apparent from the description the drawings and the claims. Some implementations may not have those possible example features and or possible example advantages and such possible example features and or possible example advantages may not necessarily be required of some implementations.

As will be appreciated by one skilled in the art aspects of the present disclosure may be embodied as a system a method and or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.

Any combination of one or more computer readable medium s may be utilized. The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present invention may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data configuration data for integrated circuitry or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Smalltalk C or the like and procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present invention.

Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages including an object oriented programming language such as Java Python Scala Ruby and Node.js Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the blocks may occur out of the order noted in the Figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

Referring now to there is shown workload process that may reside on and may be executed by a computer e.g. computer which may be connected to a network e.g. network e.g. the internet or a local area network . Examples of computer and or one or more of the client electronic devices noted below may include but are not limited to a personal computer s a laptop computer s mobile computing device s a server computer a series of server computers a mainframe computer s or a computing cloud s . Computer may execute an operating system for example but not limited to Microsoft Windows Mac OS X Red Hat Linux or a custom operating system. Microsoft and Windows are registered trademarks of Microsoft Corporation in the United States other countries or both Mac and OS X are registered trademarks of Apple Inc. in the United States other countries or both Red Hat is a registered trademark of Red Hat Corporation in the United States other countries or both and Linux is a registered trademark of Linus Torvalds in the United States other countries or both .

As will be discussed below in greater detail workload process may dynamically group and un group by a computing device processing operators and processing elements used by a streaming application. A distributed processing elements utilization of resources may be monitored to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. At runtime via at least one of parallelization and fusion the grouping and un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted.

The instruction sets and subroutines of workload process which may be stored on storage device coupled to computer may be executed by one or more processors not shown and one or more memory architectures not shown included within computer . Storage device may include but is not limited to a hard disk drive a flash drive a tape drive an optical drive a RAID array a random access memory RAM and a read only memory ROM .

Network may be connected to one or more secondary networks e.g. network examples of which may include but are not limited to a local area network a wide area network or an intranet for example.

Computer may include a data store such as a database e.g. relational database object oriented database triplestore database etc. and may be located within any suitable memory location such as storage device coupled to computer . Any data metadata information etc. described throughout the present disclosure may be stored in the data store. In some implementations computer may utilize any known database management system such as but not limited to DB2 in order to provide multi user access to one or more databases such as the above noted relational database. The data store may also be a custom database such as for example a flat file database or an XML database. Any other form s of a data storage structure and or organization may also be used. Workload process may be a component of the data store a standalone application that interfaces with the above noted data store and or an applet application that is accessed via client applications . The above noted data store may be in whole or in part distributed in a cloud computing topology. In this way computer and storage device may refer to multiple devices which may also be distributed throughout the network.

Computer may execute a resource manager application e.g. resource manager application examples of which may include but are not limited to e.g. any application that allows for allocation and or temporary allocation loaning and or requesting back of machines and their resources such as e.g. primary memory and secondary memory processor s bandwidth graphics and sound networks cache etc. at each of the Map and Reduce steps or otherwise to attempt to alleviate stresses placed on a subset of the computing cluster. Workload process and or resource manager application may be accessed via client applications . Workload process may be a standalone application or may be an applet application script extension that may interact with and or be executed within resource manager application a component of resource manager application and or one or more of client applications . Resource manager application may be a standalone application or may be an applet application script extension that may interact with and or be executed within workload process a component of workload process and or one or more of client applications . One or more of client applications may be a standalone application or may be an applet application script extension that may interact with and or be executed within and or be a component of workload process and or resource manager application . Examples of client applications may include but are not limited to e.g. any application that allows for allocation and or temporary allocation loaning and or requesting back of machines and their resources such as e.g. primary memory and secondary memory processor s bandwidth graphics and sound networks cache etc. at each of the Map and Reduce steps or otherwise to attempt to alleviate stresses placed on a subset of the computing cluster a standard and or mobile web browser an email application e.g. an email client application a textual and or a graphical user interface a customized web browser a plugin an Application Programming Interface API a streaming application platform or a custom application. The instruction sets and subroutines of client applications which may be stored on storage devices coupled to client electronic devices may be executed by one or more processors not shown and one or more memory architectures not shown incorporated into client electronic devices .

Storage devices may include but are not limited to hard disk drives flash drives tape drives optical drives RAID arrays random access memories RAM and read only memories ROM . Examples of client electronic devices and or computer may include but are not limited to a personal computer e.g. client electronic device a laptop computer e.g. client electronic device a smart data enabled cellular phone e.g. client electronic device a notebook computer e.g. client electronic device a tablet not shown a server not shown a television not shown a smart television not shown a media e.g. video photo etc. capturing device not shown and a dedicated network device not shown . Client electronic devices may each execute an operating system examples of which may include but are not limited to Android Apple iOS Mac OS X Red Hat Linux or a custom operating system.

One or more of client applications may be configured to effectuate some or all of the functionality of workload process and vice versa . Accordingly workload process may be a purely server side application a purely client side application or a hybrid server side client side application that is cooperatively executed by one or more of client applications and or workload process .

One or more of client applications may be configured to effectuate some or all of the functionality of resource manager application and vice versa . Accordingly resource manager application may be a purely server side application a purely client side application or a hybrid server side client side application that is cooperatively executed by one or more of client applications and or resource manager application . As one or more of client applications workload process and resource manager application taken singly or in any combination may effectuate some or all of the same functionality any description of effectuating such functionality via one or more of client applications workload process resource manager application or combination thereof and any described interaction s between one or more of client applications workload process resource manager application or combination thereof to effectuate such functionality should be taken as an example only and not to limit the scope of the disclosure.

Users may access computer and workload process e.g. using one or more of client electronic devices directly through network or through secondary network . Further computer may be connected to network through secondary network as illustrated with phantom link line . Workload process may include one or more user interfaces such as browsers and textual or graphical user interfaces through which users may access workload process .

The various client electronic devices may be directly or indirectly coupled to network or network . For example client electronic device is shown directly coupled to network via a hardwired network connection. Further client electronic device is shown directly coupled to network via a hardwired network connection. Client electronic device is shown wirelessly coupled to network via wireless communication channel established between client electronic device and wireless access point i.e. WAP which is shown directly coupled to network . WAP may be for example an IEEE 802.11a 802.11b 802.11g Wi Fi and or Bluetooth including Bluetooth Low Energy device that is capable of establishing wireless communication channel between client electronic device and WAP . Client electronic device is shown wirelessly coupled to network via wireless communication channel established between client electronic device and cellular network bridge which is shown directly coupled to network .

Some or all of the IEEE 802.11x specifications may use Ethernet protocol and carrier sense multiple access with collision avoidance i.e. CSMA CA for path sharing. The various 802.11x specifications may use phase shift keying i.e. PSK modulation or complementary code keying i.e. CCK modulation for example. Bluetooth including Bluetooth Low Energy is a telecommunications industry specification that allows e.g. mobile phones computers smart phones and other electronic devices to be interconnected using a short range wireless connection. Other forms of interconnection e.g. Near Field Communication NFC may also be used.

Referring also to there is shown a diagrammatic view of client electronic device . While client electronic device is shown in this figure this is for example purposes only and is not intended to be a limitation of this disclosure as other configurations are possible. Additionally any computing device capable of executing in whole or in part workload process may be substituted for client electronic device within examples of which may include but are not limited to computer and or client electronic devices .

Client electronic device may include a processor and or microprocessor e.g. microprocessor configured to e.g. process data and execute the above noted code instruction sets and subroutines. Microprocessor may be coupled via a storage adaptor not shown to the above noted storage device s e.g. storage device . An I O controller e.g. I O controller may be configured to couple microprocessor with various devices such as keyboard pointing selecting device e.g. touchpad touchscreen mouse etc. custom device e.g. device USB ports not shown and printer ports not shown . A display adaptor e.g. display adaptor may be configured to couple display e.g. touchscreen monitor s plasma CRT or LCD monitor s etc. with microprocessor while network controller adaptor e.g. an Ethernet adaptor may be configured to couple microprocessor to the above noted network e.g. the Internet or a local area network .

As will be discussed below a constantly running real time system may need a constant assessment of its resource consumption as data flow may be less predictable. Adding and removing capacity from a system may have an inherent latency to it since e.g. real time systems may not be able to afford to constantly shuffle resources in and out as it may impact the real time nature of their processing. In some implementations workload process may run as part of a streaming application platform such as the IBM InfoSphere Streams platform and may constantly assess the capacity of the system and may in some implementations only surrender capacity when resource usage has fallen below a threshold such that two processing elements within the system topology may be fused together. The fusion and replication processes of workload process may be unique to streaming applications and thus not something that may be easily externalized to a third party resource manager such as resource manager application .

As discussed above and referring also at least to workload process may dynamically group un group by a computing device processing operators and processing elements used by a streaming application. A distributed processing elements utilization of resources may be monitored by workload process to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. At runtime via at least one of parallelization and fusion the grouping un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted by workload process .

As noted above many times in solving problems with large data sets there may be issues acquiring enough computing power to address the problem in the desired timeframe. The result may be that often time a distributed processing job overloads a number or cluster of the computer machines that may be dedicated to computing the solution. In example MapReduce style computations or the like a resource manager may be used to allocate entire machines at each of the Map and Reduce steps to attempt to alleviate stresses placed on a subset of the computing cluster. Streaming computing may present additional alternative issues. For example since the computation may be constantly occurring for every processing phase at all times it may be difficult to use conventional resource managers e.g. for use with non streaming computing to allocate resources. For instance there may never be an ending of a streaming computing process thus resource managers may see constant usage and without knowledge of the stream processing topology the resource managers may not be able to safely remove a machine without potentially introducing a break in the processing graph.

As will be described in greater detail workload process may manage workloads within the stream processing graph e.g. for streaming applications and may enable the offering and requesting of machines from a conventional resource manager e.g. resource manager application without introducing data processing latency to the streaming application. Workload process may enable the manipulation of the stream processing topology during runtime while being cognizant of any congestion or backpressure created. In some implementations this may enable workload process to build a streaming data resource manager that may then interact with a conventional resource manager. In some implementations workload process e.g. with the streaming data resource manager may interact with a resource manager at application submission time and or runtime. It will be appreciated that workload process may be used with variable rate data flows which may encompasses smaller data sets . As such the description of using large data sets should be taken as example only.

Assume for example purposes only that a user e.g. user writes via a user interface of workload process an application as a series of steps or operators . In the example the operators may be submitted to the above noted streaming application platform portion of workload process which may be turned into processing elements when they are deployed to a system. In some implementations processing elements may be generally described as a container that includes one or more operators. For example and referring at least to an example user application operator graph is shown. In the example four operators are shown e.g. operator A operator B operator C and operator D . Each of the boxes may represent an individual step that the user has coded. In the example if the user viewed their code as a series of linked steps this is how it may appear. This may be commonly referred to as a logical view as it may be each of the logical steps of the application. In some implementations such as the above noted streaming application platform portion of workload process workload process may form each of these operators into their own processing element at compilation time which may mean that the operators may run as their own execution processing which may require their own network handlers heap etc. In some implementations and referring at least to and example implementation deployment to two systems is shown. For the example shows the systems. The black perimeter outline represents a physical system with the vertical grey line in the middle segmenting the cores. Ex This is 2 systems each has 2 processor cores. The systems are marked as System .Core for the examples. Referring at least to an example processing element PE is shown. The user operators A B C D from may not go to a system without being wrapped in a processing element. A processing element may contain 1 or more user operators.

However in some implementations the deployment of may have more than 4 operators. For example imagine instead of 4 operators there are e.g. . In the example operator A may no longer require an output network handler and operator B may no longer require an input network handler. A and B may be formerly both separate processes on the same machine. They may communicate through a socket e.g. network handler . When they are fused they may become 1 process. Within a process they may simply access each other s memory making the data transfer instant and eliminating the need for the communication code that may have been previously required. In the example the operating system e.g. via workload process may not have to context switch between operator A and operator B. For example in an effort to be fair to all programs running Operating Systems may do a round robin and give CPU time to all running processes. If a machine has 2 cores and only 2 processes it may never need to do this round robin which means workload process may not have to save process state and load in the new one. In the example case A and B fuse because there is one process and they may rather pool their time slots than exist separately and be forced to load unload every time it may be their turn to run. In some implementations workload process may perform the reverse process. As will be discussed in greater detail below these processes may be called fusion. It will be appreciated that any number of operators and or processing elements may be used without departing from the scope of the disclosure.

In some implementations as will be discussed in greater detail parallelization may include a graph transform by workload process . For instance and referring at least to an example deployment in example stages e.g. of parallelization is shown. In the example workload process may perform parallelization which via workload process may perform a graph transform e.g. from the original operator A D graph in where operator B may be congested because it may be a very slow process compared to operators A C and D. In the example 2 system 2 core 4 user operator may be the default and ideal deployment. This may be the placement output at least initially. In some implementations it may follow that guideline of workload process creating 1 processing elements per processing core. That gives 4 processing elements which matches nicely with 4 user operators. Referring to with previous implementations it may be noticed that there are 2 processing elements on each processor core. That may not be idea ideal because they may now have to take turns for who is actively running. This may become more of a problem when workload process scales up the number of things that take turns but for example purposes only two are described. Referring to it is shown that there are only 2 processors where workload process may decide to execute fusion. In some implementations fusion may prefer grabbing the nearest neighbor. As such workload process may look at source A and sink D and pull in their neighbors thus creating AB and CD. Workload process successfully has made it down to the desired processing element count and may be finished for now. Referring now to it will be appreciated that there may be less clean scenarios ones where the division is more difficult. For example assume the same 4 operators and 3 processors. When deciding who gets fused workload process may use historical metrics if available to decide. If historical metrics are not available workload process may fuse the last two together. Once it is running workload process may change it if metrics indicate it isn t optimal. In some implementations it tends to be that users filter data out so as workload process goes along the graph less is being processed. If the ingest rate is X workload process may be processing X or less by the end rarely ever more. Hence the default setting may be to fuse from the end if workload process does not have metric information to tell which ones should fuse.

As discussed throughout in some implementations workload process may dynamically group un group by a computing device processing operators and processing elements used by a streaming application. For example workload process may group ungroup e.g. operators A B C D and or portions of operators A B C D into any processing element container. In some implementations this may be accomplished by not compiling the operator s to an executable format itself but instead compiling the operator to a library and then implementing a loading mechanism of workload process to be able to move the operator into out of any process. In some implementations when compiling a language like C the entire application may be built where workload process may know where everything is and has optimized for that. Workload process may also build code into libraries where workload process may have each operator built as a library. Workload process may then load the library at runtime as needed. The loading mechanism in Java may be the Java Classloader. It will be appreciated that other languages such as those noted above may be used without departing from the scope of the disclosure.

It will be appreciated that operators may have placement constraints on them. For instance workload process may not only determine which processing elements e.g. containers each operator may enter but may also take into consideration metrics to decide if e.g. operator A should merge with e.g. operator B or if e.g. operator B should merge with e.g. operator C etc. The threshold constraints at which fusion and or parallelization of operators may occur may be a configurable option via a user interface associated with workload process such that a user e.g. system administrator may make resource allocation more or less strict e.g. if they believe their data flows are burst like in nature and that the time to rebalance load may cause undue latency within their system. Other examples of constraints may include e.g. collocation exlocation partition processor architecture OS required software etc. Other examples of metrics may include e.g. tuples sec backpressure queue size CPU processing time memory consumption network utilization etc.

In some implementations the grouping un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted by workload process at runtime via at least one of parallelization and fusion based upon at least in part a tuple flow rate of the operators. Since Streams may be used for real time data processing it may be important how long it takes to process a piece of data from start to finish. Therefore if workload process sees somewhere being exceptionally slow workload process may want to get more machines doing that operation so that workload process does not hold up all the data flows. Workload process may want to facilitate the flow of data so additional processing avenues may open up for it. Workload process may detect the tuple flow rate issues via that metric and the backpressure metric which may tell workload process how many tuples are waiting to be processed at each operator large number may indicate this step is slower than its predecessor .

In some implementations a distributed processing elements utilization of resources may be monitored by workload process to identify candidate operators and candidate processing elements for at least one of parallelization and fusion. For example workload process may monitor per operator statistics regardless of the number of operators in a processing element e.g. container . Workload process may monitor processing element statistics. Monitoring may enable workload process to see inside the processing element and make an informed decision as to which processing elements may benefit from fusion parallelization movement. In some implementations the statistics may be transient or temporary. For instance one or more of the above noted metrics and constraints may be transient or temporary. Notably this is something a conventional resource manager is not understood to be able to do. For example it will be appreciated that applications are seen as black boxes to resource managers so they may only move processes. By contrast workload process may allow movement e.g. via fusion parallelization of processes and or parts of processes. In some implementations a processing element may be an operating system process and an operator may be a portion of a processing element. As noted above workload process may execute dynamic loading unloading. Workload process may use this to do the add remove of the operator from the processing element

In some implementations at runtime via at least one of parallelization and fusion the grouping un grouping of the identified candidate operators and candidate processing elements may be dynamically adjusted by workload process . Runtime may be generally described as the period during which an application is executing. Thus in the example implementations runtime may be described as a period during which an the identified candidate operators and candidate processing elements of a streaming application are being executed. In some implementations runtime fusion may enable via workload process an application to continuously be optimized in an environment that may not be stable e.g. five hosts one day six hosts the next back to five hosts the next day etc. In some implementations while system resource utilization may be monitored at runtime other resources may be monitored at runtime as well. For example workload process may flow data or tuples one at a time across the network. In some cases e.g. due to the network topology workload process may detect sub optimal throughput. In the example workload process may group un group operators and or processing elements identified as candidates responsive to the monitoring by changing during runtime placement of operators or fusion based on statistics about network performance. For instance if a specific operator also has network issues workload process may implement parallelization to allow the operator more access to throughput than any one system may provide.

In some implementations the dynamic adjusting may be performed by workload process without user intervention. For example how does workload process know when to change the workload Workload process may trigger on events be it resource manager metrics system crash etc. How does workload process know the new optimal workload Workload process may score each possible graph transform doing its best to estimate resource utilization and tuple flow rates and trying to maximize both based on historical data.

In some implementations a first processing element of the identified candidate processing elements may be identified by workload process with the distributed processing elements resource utilization exceeding an upper threshold a second processing element of the identified candidate processing elements may be identified by workload process with the distributed processing elements resource utilization below a lower threshold and a fusion of the first and second processing elements may be executed by workload process . Executing fusion may include workload process splitting a portion of operators grouped in the first processing element and grouping the portion of the operators in the second processing element. For example assume for example purposes only that user writes a large number of operators e.g. and submits them via workload process to ten hosts. It will be appreciated that any number of operators may be used without departing from the scope of the disclosure. As an initial pass workload process may try to place one processing element container per processor and 2 GB of RAM. In the example assume ten hosts with four cores per host the system may not have enough core to go around. For example assume for example purposes only that there are 200 operators where it is desired to have 1 processing element per CPU core. In the example there are 10 systems with 4 cores per system. 10 4 40 processing element slots. 200 Operators 40 Slots 5 Operators per processing element. Workload process may walk the graph and attempt to put 5 neighboring Operators together until there are 40 blocks. Thus in the example workload process may execute fusion of operators together. In some implementations this execution of fusion may occur blindly as workload process has not yet executed the application to obtain the proper analysis statistics during runtime and without the analysis workload process may not have sufficient knowledge of how the user s code may perform. Continuing with the example during runtime workload process may split the application into e.g. forty blocks of five operators to the best extent possible . Again this when deployed during runtime still may not be the optimal configuration. For example ingest operators like operator A may tend to deal with higher traffic volumes because there is no filter so workload process may determine that it may be better to run operator A in its own processing element and the following seven operators together in the same processing element. Thus in some implementations the above noted monitoring and dynamic grouping un grouping may occur constantly or at predetermined intervals during runtime to obtain the optimal configuration.

In some implementations workload process may obtain this information by looking at the per operator statistics and if monitoring identifies a processing element exceeding its upper predetermined profile box threshold e.g. 1 core 2 GB memory workload process may iteratively determine whether the processing element may be split and fused to other parts of the system e.g. other operators and or processing elements to make more sense by grouping parts of the processing element e.g. the operators contained within the processing element to another processing element s that may be identified as using less than a lower predetermined profile box threshold of a core e.g. 50 of one core . Conversely in some implementations workload process may analyze and identify two or more processing elements with low load and decide to fuse them which may open up a system for a higher load processing to split or to return a resource to a resource manager etc. It will be appreciated that other examples of upper and lower thresholds may be used without departing from the scope of the disclosure. For example the above noted metrics and constraints may be used with upper and lower thresholds.

In some implementations responsive to changes in system topology based upon at least in part outside use of a resource manager the upper threshold of a first system by a first processing element of the identified candidate processing elements may be detected by workload process and the first processing element may be parallelized by workload process to a newly available resource from the resource manager. Responsive to changes in the system topology based upon at least in part outside use of the resource manager the lower threshold of a second system by a second processing element of the identified candidate processing elements may be detected by workload process and the second processing element may be fused by workload process with a third processing element of the identified candidate processing elements to release the second system to the resource manager. For example and referring at least to an example deployment is shown with six stages e.g. and . In some implementations by default workload process may not perform parallelization unless the user directs it. The optimal deployment in some implementations may be 1 user operator per processing element and 1 processing element per system. However as congestion metrics etc. demands workload process may parallelize operations. Referring to it is shown beginning with an example ideal deployment but with a 3rd system added. As mentioned above the third system may be unused initially since there is 1 operator per processing element per host. Referring at least to assume that B is suffering from congestion. It has more traffic than it can process. If B could go faster the whole system may process more data per second. This is an example of tuple flow rate metric driven change. Workload process may now decide to fuse or parallelize. Since workload process has a spare system workload process may decide the first solution is to expand. Thus workload process may replicate B so there will be two copies of it going and split the traffic evenly between the two of them. They may both still send their results to C. The bracket notation is for referencing multiple copies of the same operator. Referring at least to assume there is a need to do this with just the original two systems. Workload process may take the lowest load system use metrics and assume for example purposes only that it is C. Workload process may then duplicate B and fuse B with C. Referring at least to resource manager operations may include e.g. request new host request host return new host available etc. In a non ideal deployment there was only 1 system available. In the example cluster there may be this 2nd system that does Map Reduce jobs overnight e.g. via workload process . So when it is not in use workload process may borrow it. Referring at least to an example resource manager event of map reduce job ended new host available is shown. In the example workload process may grab the host and move to the above noted example ideal deployment. Referring at least to an example resource manager event of map reduce job starting request host return is shown. In the example workload process may return the host and go back to the next best ideal example deployment. For example referring at least to assume that B starts to become congested but there is only the 2 systems available. In the example workload process may ask the resource manager for another system to alleviate the congestion. Referring at least to an example resource manager event of request new host is shown. In the example the problem is solved. If denied workload process may consider performing fusion with itself.

In some implementations the present disclosure may enable the sharing of as many resources as possible with the cluster while still maintaining processing capacity. The thresholds at which fusion and or parallelization of operators may occur may be a configurable option such that a user e.g. system administrator may make resource allocation more or less strict e.g. if they believe their data flows are burst like in nature and that the time to rebalance load may cause undue latency within their system. However it will be appreciated that workload process may work wells with many streaming applications and or non streaming applications that may be constantly receiving data from users during business hours but may then surrender much of their respective computing capacity in off hours which may also be the time larger batch jobs tend to run and may take advantage of the extra capacity.

The terminology used herein is for the purpose of describing particular implementations only and is not intended to be limiting of the disclosure. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps not necessarily in a particular order operations elements and or components but do not preclude the presence or addition of one or more other features integers steps not necessarily in a particular order operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents e.g. of all means or step plus function elements that may be in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present disclosure has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the disclosure in the form disclosed. Many modifications variations substitutions and any combinations thereof will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the disclosure. The implementation s were chosen and described in order to explain the principles of the disclosure and the practical application and to enable others of ordinary skill in the art to understand the disclosure for various implementation s with various modifications and or any combinations of implementation s as are suited to the particular use contemplated.

Having thus described the disclosure of the present application in detail and by reference to implementation s thereof it will be apparent that modifications variations and any combinations of implementation s including any modifications variations substitutions and combinations thereof are possible without departing from the scope of the disclosure defined in the appended claims.

