---

title: Asynchronous submission of commands
abstract: A system and method for allocating commands in processing is disclosed. The system and method includes an application running on a computer system that provides commands to be executed on one of a plurality of processors capable of executing the commands, the commands provided through an application programming interface, a device driver that buffers the streamed commands and converts the streamed commands into a format used by a GPU, and an operating system that builds a command buffer by grouping a plurality of converted commands based on an allocation for an available processor, wherein the available processor is determined in the interface between the device driver and the operating system. The available processor is one of the plurality of processors capable of executing the commands that receives the command buffer from the operating system, queues the command buffer and performs an asynchronous submission of the command buffer to the GPU, and the GPU executes the command buffer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09632848&OS=09632848&RS=09632848
owner: ADVANCED MICRO DEVICES, INC.
number: 09632848
owner_city: Sunnyvale
owner_country: US
publication_date: 20151229
---
The disclosed embodiments are generally directed to submission of commands to processors and in particular to the asynchronous submission of commands to a processor.

A driver for a processing unit is a component which takes a command stream from an application such as by an operating system component as input and generates another command stream as output. The command stream that is output may be submitted to an operating system in order to be sent to the hardware for processing. For example a graphics driver for a graphics processing unit GPU is a component which takes a command stream from an application for graphics processing as an input and generates another command stream as an output to the GPU to enable the GPU to perform the needed processing. This output command stream may be submitted to an operating system and other driver components in order to be sent to the GPU for execution.

Submission of the command stream such as the processing of the command stream by the operating system and level drivers for example may use a significant percentage of the cost of delivering the command stream from the application to the processing unit.

Generally modern computing systems include many individual processing unit cores. The usage of these cores in any given snapshot may not be well balanced. That is some of the cores may be heavily loaded while others are not. A benefit in efficiency may result in moving work from a heavily loaded processing unit core to a more lightly loaded core.

In addition on a central processing unit CPU bottlenecked application the driver thread may be the factor limiting performance. A driver thread is the CPU sequence of instructions which has as its results a command sequence for the GPU. For example the conversion from Application Programming Interface API command to GPU command is a sequential process which may be bottlenecked by the CPU. The ability to alleviate this bottleneck may therefore increase performance.

Present solutions to this problem focus on reducing the bottleneck at the top of the driver stack. Therefore a need exists to alleviate this bottleneck at other places within the driver stack to eliminate the visibility of the time spent in the operating system components.

A system and method for allocating commands in processing is disclosed. The system and method includes an application running on a computer system that provides commands to be executed on one of a plurality of processors capable of executing the commands the commands provided through an application programming interface a device driver that buffers the streamed commands and converts the streamed commands into a format used by a GPU and an operating system that builds a command buffer by grouping a plurality of converted commands based on an allocation for an available processor wherein the available processor is determined in the interface between the device driver and the operating system. The available processor is one of the plurality of processors capable of executing the commands and receives the command buffer from the operating system queues the command buffer and performs an asynchronous submission of the command buffer to the GPU and the GPU executes the command buffer.

The method for allocating commands in processing includes streaming commands from an application through the application programming interface buffering the streamed commands in a device driver converting in the device driver the buffered commands into a format used by a GPU allocating the commands for an available processor building a command buffer via an operating system by grouping a plurality of commands for the available processor queuing the command buffer as a work unit for asynchronous submission to the GPU asynchronously submitting the command buffer to the GPU and executing the command buffer in the GPU.

A system and method for allocating commands to eliminate bottlenecks in processing is disclosed. The system and method includes an application running on a computer system that provides commands to be executed on one of a plurality of processors capable of executing the commands the commands provided through an application programming interface a device driver that buffers the streamed commands and converts the streamed commands into a format used by a GPU and an operating system that builds a command buffer by grouping a plurality of converted commands based on an allocation for an available processor wherein the available processor is determined in the interface between the device driver and the operating system. The available processor is one of the plurality of processors capable of executing the commands and receives the command buffer from the operating system queues the command buffer and performs an asynchronous submission of the command buffer to the GPU and the GPU executes the command buffer.

The method for allocating commands to eliminate bottlenecks in processing includes streaming commands from an application through the application programming interface buffering the streamed commands in a device driver converting in the device driver the buffered commands into a format used by a GPU allocating the commands for an available processor building a command buffer via an operating system by grouping a plurality of commands for the available processor queuing the command buffer as a work unit for asynchronous submission to the GPU asynchronously submitting the command buffer to the GPU and executing the command buffer in the GPU. The system and method for allocating commands to eliminate bottlenecks in processing may be described in more detail below with reference to .

The processor may include a central processing unit CPU a graphics processing unit GPU a CPU and GPU located on the same die or one or more processor cores wherein each processor core may be a CPU or a GPU. The memory may be located on the same die as the processor or may be located separately from the processor . The memory may include a volatile or non volatile memory for example random access memory RAM dynamic RAM or a cache.

The storage may include a fixed or removable storage for example a hard disk drive a solid state drive an optical disk or a flash drive. The input devices may include a keyboard a keypad a touch screen a touch pad a detector a microphone an accelerometer a gyroscope a biometric scanner or a network connection e.g. a wireless local area network card for transmission and or reception of wireless IEEE 802 signals . The output devices may include a display a speaker a printer a haptic feedback device one or more lights an antenna or a network connection e.g. a wireless local area network card for transmission and or reception of wireless IEEE 802 signals .

The input driver communicates with the processor and the input devices and permits the processor to receive input from the input devices . The output driver communicates with the processor and the output devices and permits the processor to send output to the output devices . It is noted that the input driver and the output driver are optional components and that the device will operate in the same manner if the input driver and the output driver are not present.

Application may take the form of an application program that generally may be a computer program designed to perform a group of coordinated functions tasks or activities for the benefit of the user. For example application may be a video game that provides a displayed output to a user.

API may include a set of routines protocols and tools for building software applications. Generally an API expresses a software component in terms of its operations inputs outputs and underlying types. An API defines functionalities that are independent of their respective implementations which allows definitions and implementations to vary without compromising the interface for accessing databases or computer hardware such as hard disk drives or video cards. API commands are the commands or routines used to interact with hardware components. For example an API may be used by application to provide video for display to a user. In such an example the API may interface with a video card.

APIs often come in the form of a library that includes specifications for routines data structures object classes and variables. In other cases notably SOAP and REST services an API is simply a specification of remote calls exposed to the API consumers.

An API specification can take many forms including an International Standard such as POSIX vendor documentation such as the Microsoft Windows API or the libraries of a programming language e.g. the Standard Template Library in C or the Java APIs.

Operating system may include any system software that manages computer hardware and software resources and provides common services for computer programs. Operating system is a component of the system software in a computer system. Application programs such as application usually require an operating system to function. Operating systems may schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time mass storage printing and other resources.

For hardware functions such as input output and memory allocation operating system acts as an intermediary between programs and the computer hardware. The application code is usually executed directly by the hardware and makes system calls to an operating system function or to be interrupted by it. Operating system interacts with driver to schedule and buffer the command. Examples of popular modern operating systems include Android BlackBerry BSD Chrome OS iOS Linux OS X QNX Steam OS Microsoft Windows and variant Windows Phone and z OS.

Driver may be a device driver that is a computer program that operates or controls a particular type of device that is attached to a computer. Driver provides a software interface to hardware devices enabling operating systems and other computer programs to access hardware functions without needing to know precise details of the hardware being used. For example driver may take the form of a video driver when the application includes rendering images.

Driver typically communicates with the hardware through a computer bus not shown or a communications subsystem not shown to which the hardware connects. When a calling application invokes a routine in driver driver issues commands to the hardware . Once hardware sends data back to driver driver may invoke routines in the original calling application . Drivers are hardware dependent and operating system specific. Drivers usually provide the interrupt handling required for any necessary asynchronous time dependent hardware interface. Driver may then submit the command to the hardware for processing.

There are multiple different techniques or ways of arranging the communication between operating system driver and hardware . For example as generally described herein operating system may control or otherwise own the communication with hardware . Other techniques may focus on driver control or ownership or be less dependent on operating system for example.

Hardware may take the form of any physical part or component of a computer including by non limiting example monitors or displays mouses keyboards storage devices graphics devices or cards sounds cards memory motherboards and other tangible parts of a computer.

More specifically and by way of non limiting example application may include a 3D simulator and a renderer . Operating system may include an API and a scheduler . Driver may include a user mode API driver and a kernel mode driver . Hardware may include a command processor .

In this example application may seek to provide a simulation using 3D simulator . Renderer may provide the necessary commands to display the images on the screen. The operating system receives the commands from application via renderer . Operating system utilizes API to decode the commands from application and may utilize scheduler to plan the processing of the commands via driver to processor of hardware . Driver may include two parts a user mode driver that interacts with API and outputs back to operating system and a kernel mode driver that receives commands from scheduler and outputs a command buffer to the hardware .

In operation 3D simulator may request rendering from renderer within application . Application may generate the API command stream from its renderer . This may be driven as a simulation from a simulation engine depicted as 3D simulator .

Renderer may provide API commands to the API of operating system . API may take the form of a Direct3D OpenGL or the like. For example in Direct3D API performs parameter validation and some translation parameters. In other APIs component API may be thin or even non existent in system .

API of operating system may communicate with user mode API driver of driver . User mode API driver may translate the API calls into command buffers suitable for execution on the installed hardware . User mode API driver may also generate any metadata required for scheduling the command buffer. This may include a list of referenced memory allocations and any dependencies on previous command buffers for example. User mode API driver may communicate the command buffer and metadata to scheduler of operating system .

The command buffer and metadata may then be processed by the operating system for processing. In order for the scheduling such as in kernel mode to be maximally efficient preparatory work may be performed to minimize time in the kernel modes as opposed to time in kernel mode later in the process if preparatory work is not performed. Scheduler may include user and kernel mode components for example. Scheduler may provide command buffer to the kernel mode driver of driver . When dependencies are satisfied the command buffer becomes runnable and at some point the command buffer may be selected by scheduler for execution.

The command buffer details are delivered to the video driver s kernel mode component such as kernel mode driver which may then provide the instructions to hardware to read and execute the command buffer. Command buffer may be read via DMA to command processor of hardware . Command processor may take the form of rendering hardware for example.

The described interactions are simplified for ease of understanding the present invention and a person having ordinary skill in the pertinent arts would understand that more details of the interactions between components are necessary and occur. The example present includes all of the key components involved in the translation of application API requests into pixels visible to the user for example.

Referring now to there is shown a data flow along with the main data processing operations. In data flow an application generates commands . These commands may be sequential API commands for example. Application may provide these commands to a driver .

Driver may take the application thread and may perform lightweight buffering for example. Driver may provide internal buffering of submitted commands to a driver .

Driver may take the driver thread and may perform the more elaborate conversion to GPU buffers. At this point in the data flow the driver may provide information to the GPU command buffer and other associated data such as memory allocations for example to an operating system .

Operating system upon receiving the command buffer from driver may prepare the commands for execution in the GPU by sending the command buffer to the hardware such as GPU . GPU may then process the commands.

The thread boundary between driver and operating system may provide an opportunity to balance the load between processors by moving work from more heavily loaded processors to more lightly loaded processors. This may eliminate the driver thread from limiting performance.

As discussed hereinabove at the boundary between driver and operating system there may be an opportunity to redirect processing to a different processor. This spreading of work to additional and or alternative CPUs may involve a two step process. First under the control of application in and or second under the control of the operating system in . The workload may be compared with the available resources. This may be achieved by counting the threads that a CPU may simultaneously execute by estimating the likely workload profile of the application at application start up time or by applying a specific profile for a given application or using a combination of these techniques. Once the decision has been made a new thread may be created. From the creation of the new thread on the data coming out of the calculation step is buffered and provided to the new thread via known patterns such as by queuing work units into a producer consumer buffer.

Once the new thread is created the operating system scheduler in may track the utilization of each thread on the system and allocate the threads to individual available CPU threads of execution. This technique may provide beneficial results when the number of busy threads in the system is lower than the number of threads the CPU can simultaneously execute. This condition is generally true given that new threads are created when the application is created as described above.

This may also occur at the interface between driver and driver or between operating system and hardware . The boundary between driver and operating system provides an opportunity to alleviate any bottleneck that has occurred in processing.

The methods provided may be implemented in a general purpose computer a processor or a processor core. Suitable processors include by way of example a general purpose processor a special purpose processor a conventional processor a digital signal processor DSP a plurality of microprocessors one or more microprocessors in association with a DSP core a controller a microcontroller Application Specific Integrated Circuits ASICs Field Programmable Gate Arrays FPGAs circuits any other type of integrated circuit IC and or a state machine. Such processors may be manufactured by configuring a manufacturing process using the results of processed hardware description language HDL instructions and other intermediary data including netlists such instructions capable of being stored on a computer readable media . The results of such processing may be maskworks that are then used in a semiconductor manufacturing process to manufacture a processor which implements aspects of the embodiments.

The methods or flow charts provided herein may be implemented in a computer program software or firmware incorporated in a non transitory computer readable storage medium for execution by a general purpose computer or a processor. Examples of non transitory computer readable storage mediums include a read only memory ROM a random access memory RAM a register cache memory semiconductor memory devices magnetic media such as internal hard disks and removable disks magneto optical media and optical media such as CD ROM disks and digital versatile disks DVDs .

It should be understood that many variations are possible based on the disclosure herein. Although features and elements are described above in particular combinations each feature or element may be used alone without the other features and elements or in various combinations with or without other features and elements.

