---

title: Multisensory speech detection
abstract: A computer-implemented method of multisensory speech detection is disclosed. The method comprises determining an orientation of a mobile device and determining an operating mode of the mobile device based on the orientation of the mobile device. The method further includes identifying speech detection parameters that specify when speech detection begins or ends based on the determined operating mode and detecting speech from a user of the mobile device based on the speech detection parameters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09570094&OS=09570094&RS=09570094
owner: Google Inc.
number: 09570094
owner_city: Mountain View
owner_country: US
publication_date: 20150629
---
This Application is a continuation application of and claims priority to U.S. application Ser. No. 14 645 802 titled Multisensory Speech Detection filed on Mar. 12 2015 which claims priority to U.S. application Ser. No. 12 615 583 entitled Multisensory Speech Detection Now U.S. Pat. No. 9 009 053 filed on Nov. 10 2009 which claims priority under 35 U.S.C. 119 e to U.S. Provisional Application Ser. No. 61 113 061 titled Multisensory Speech Detection filed on Nov. 10 2008 the disclosures of which are incorporated herein by reference.

As computer processors have decreased in size and expense mobile computing devices have become increasingly widespread. Designed to be portable many mobile computing devices are lightweight and small enough to be worn or carried in a pocket or handbag. However the portability of modern mobile computing devices comes at a price today s mobile computing devices often incorporate small input devices to reduce the size and weight of the device. For example many current mobile devices include small keyboards that many people especially those with poor dexterity find difficult to use.

Some mobile computing devices address this problem by allowing a user to interact with the device using speech. For example a user can place a call to someone in his contact list by simply speaking a voice command e.g. call and the name of the person into the phone. However speech can be difficult to distinguish from background noise in some environments and it can hard to capture user speech in a manner that is natural to the user. In addition it can be challenging to begin recording speech at the right time. For example if recording begins after the user has started speaking the resulting recording may not include all of the user s voice command. Furthermore a user may be notified that a spoken command was not recognized by the device after the user has spoken which can be frustrating for users.

In general this document describes systems and techniques for detecting speech. In some implementations a mobile computing device can determine whether a user is speaking or is about to speak to the device based on the changing orientation i.e. distance from or proximity to a user and or angle of the device. For example the device may use one or more sensors to determine if the user has made a particular gesture with the device such as bringing it from in front of the user s face to a normal talk position with the device at the user s ear. If the gesture is detected the device may emit a sound to indicate that the user may start speaking and audio recording may commence. A second gesture of moving the device away from the user s ear can be used as a trigger to cease recording.

In addition the device may determine whether it is in a specified pose that corresponds to a mode of interacting with the device. When the device is placed into a predefined pose the device may begin sound recording. Once the device has been removed from the pose sound recording may cease. In some cases auditory tactile or visual feedback or a combination of the three may be given to indicate that the device has either started or stopped recording.

In one implementation a computer implemented method of multisensory speech detection is disclosed. The method comprises determining an orientation of a mobile device and determining an operating mode of the mobile device based on the orientation of the mobile device. The method further includes identifying speech detection parameters that specify when speech detection begins or ends based on the detected operating mode and detecting speech from the user of the mobile device based on the speech detection parameters.

In some aspects detecting an orientation of a mobile device further comprises detecting an angle of the mobile device. In yet further aspects detecting an orientation of a mobile device further comprises detecting a proximity of the mobile device to the user of the mobile device. Also determining an operating mode of a mobile device comprises using a Bayesian network to identify a movement of the mobile device.

In another implementation a system for multisensory speech detection is disclosed. The system can include one or more computers having at least one sensor that detects an orientation of a mobile device relative to a user of the mobile device. The system can further include a pose identifier that identifies a pose of the mobile device based on the detected orientation of the mobile device. In addition the system may include a speech endpointer that identifies selected speech detection parameters that specify when speech detection begins or ends.

In certain aspects the system can include an accelerometer. The system can also include a proximity sensor. In addition the system may also include a gesture classifier that classifies movements of the mobile device.

The systems and techniques described here may provide one or more of the following advantages. First a system can allow a user to interact with a mobile device in a natural manner. Second recorded audio may have a higher signal to noise ratio. Third a system can record speech without clipping the speech. Fourth a system may provide feedback regarding audio signal quality before a user begins speaking. The details of one or more embodiments of the multisensory speech detection feature are set forth in the accompanying drawings and the description below. Other features and advantages of the multisensory speech detection feature will be apparent from the description and drawings and from the claims.

This document describes systems and techniques for detecting speech. In some implementations a mobile device can determine its distance from a user as well as its angle relative to the user. Based on this information the device can initiate or stop voice recording. In an illustrative example the user may place the device in a predetermined position e.g. next to his ear. The device may detect that it has entered this position and begin voice recording. Once the user moves the device out of this position the device may stop recording user input. The recorded speech may be used as input to an application running on the device or running on an external device.

In the illustrative example of multisensory speech detection shown in the user may conduct the search by simply placing the mobile device in a natural operating position and saying the search terms. For example in some implementations the device may begin or end recording speech by identifying the orientation of the device . The recorded speech or text corresponding to the recorded speech may be provided as input to a selected search application.

The letters A B and C in represent different states in the illustrative example of multisensory speech detection. In State A the user is holding the device in a non operating position that is a position outside a predetermined set of angles or too far from the user or in some cases both. For example between uses the user may hold the device at his side as shown in or place the device in a pocket or bag. If the device has such an orientation the device is probably not in use and it is unlikely that the user is speaking into the mobile device . As such the device may be placed in a non recording mode.

When the user wants to use the device the user may place the device in an operating mode position. In the illustrative example shown in the diagram the device may determine when it is placed in selected operating positions referred to as poses. State B shows the mobile device in several example poses. For example the left most figure in State B illustrates a telephone pose . A telephone pose can in some implementations correspond to the user holding the mobile device in a position commonly used to speak into a telephone. For example as shown in the device may be held to a side of the user s head with the speaker of the device held near the user s ear. Holding the device in this way can make it easier for the user to hear audio emitted by the device and speak into a microphone connected to the device .

The middle figure shown in State B depicts the user holding the device in a PDA pose . For example as shown in PDA pose may correspond to the user holding the mobile device at nearly arm s length and positioned so that the user can see and interact with the mobile device . For instance in this position the user can press buttons on the keypad of the device or a virtual keyboard displayed on the device s screen. In some cases the user may also enter voice commands into the device in this position.

Finally the right most figure shown in State B illustrates a walkie talkie pose . In some cases a walkie talkie pose may comprise the user holding the mobile device to his face such that the device s microphone is close the user s mouth. This position may allow the user to speak directly into the microphone of the device while also being able to hear sounds emitted by a speakerphone linked to the device .

Although shows three poses others may be used. For instance in an alternative implementation a pose may take into account whether a mobile device is open or closed. For example the mobile device shown in may be a flip phone that is a phone having a form factor that includes two or more sections typically a lid and a base that can fold together or apart using a hinge. For some of these devices a pose may include whether the phone is open or closed in addition to or in lieu of the orientation of the phone. For instance if the mobile device is a flip phone the telephone pose shown in may include the device being open. Even though the current example describes a flip phone other types or form factors e.g. a phone that swivels or slides open may be used.

When the device is identified as being in a predetermined pose the device may begin recording auditory information such as speech from the user . For example State C depicts a user speaking into the device while the device is in the telephone pose. Because in some implementations the device may begin recording auditory information when the device is detected in the telephone pose the device may begin recording just before or as the user starts speaking. As such the device may capture the beginning of the user s speech.

When the device leaves a pose the device may stop recording. For instance in the example shown in after the user finishes speaking into the device he may return the device to a non operating position by for example placing the device by his side as shown at State A. When the device leaves a pose telephone pose in the current example the device may stop recording. For example if the device is outside a selected set of angles and or too far from the user the device can cease its recording operations. In some cases the information recorded by the device up to this point can be provided to an application running on the device or on a remote device. For example as noted above the auditory information can be converted to text and supplied to a search application being executed by the device .

The device may include one or more sensors that can be used to detect speech readiness among other things. For example the device can include an accelerometer . The accelerometer may be used to determine an angle of the device. For example the accelerometer can determine an angle of the device and supply this information to other device components.

In addition to the accelerometer the device may also include a proximity sensor . In some cases the proximity sensor can be used to determine how far the device is from a user. For example the proximity sensor may include an infrared sensor that emits a beam of infrared light and uses the reflected signal to compute the distance to an object. In alternative implementations other types of sensors may be used. For example the sensor may be capacitive photoelectric or inductive among other kinds of sensors.

The device can also include a camera . Signals from the camera can be processed to derive additional information about the pose of the device . For example if the camera points toward the user the camera can determine the proximity of the user. In some cases the camera can determine the angle of the user using features having a known angle such as the horizon vehicles pedestrians etc. For example if the camera is pointing at a general scene that does not include a user the camera can determine its orientation in the scene in an absolute coordinate system. However if the camera can see the user the camera can determine its orientation with respect to the user. If the camera can see both the general scene and the user the camera can determine both its orientation with respect to the user and the scene and in addition can determine where the user is in the scene.

The device may also include a central processing unit that executes instructions stored in memory . The processor may comprise multiple processors responsible for coordinating interactions among other device components and communications over an I O interface . The device may communicate with a remote computing device through the Internet . Some or all of the processing performed by the gesture classifier pose identifier speech detector speaker identifier and speech endpointer can be performed by the remote computing device .

A microphone may capture auditory input and provide the input to both a speech detector and a speaker identifier . In some implementations the speech detector may determine if a user is speaking into the device . For example the speech detector can determine whether the auditory input captured by the microphone is above a threshold value. If the input is above the threshold value the speech detector may pass a value to another device component indicating that the speech has been detected. In some cases the device may store this value in memory e.g. RAM or a hard drive for future use.

In some cases a speech detector can determine when a user is speaking. For example the speech detector can determine whether captured audio signals include speech or consist entirely of background noise. In some cases the speech detector may assume that the initially detected audio is noise. Audio signals at a specified magnitude e.g. 6 dB above the initially detected audio signal may be considered speech.

If the device includes a camera the camera may also provide visual signals to the speech detector that can be used to determine if the user is speaking. For example if the user s lips are visible to the camera the motion of the lips may be an indication of speech activity as may be correlation of that motion with the acoustic signal. A lack of motion in the user s lips can in some cases be evidence that the detected acoustic energy came from another speaker or sound source.

The speaker identifier in some cases may be able to determine the identity of the person speaking into the device . For example the device may store auditory profiles e.g. speech signals of one or more users. The auditory information supplied by the microphone may be compared to the profiles a match may indicate that an associated user is speaking into the device . Data indicative of the match may be provided to other device components stored in memory or both. In some implementations identification of a speaker can be used to confirm that the speech is not background noise but is intended to be recorded.

The speaker identifier can also use biometric information obtained by the camera to identify the speaker. For example biometric information captured by the camera can include but is not limited to face appearance lip motion ear shape or hand print. The camera may supply this information to the speaker identifier . The speaker identifier can use any or all of the information provided by the camera in combination with or without acoustic information to deduce the speaker s identity.

The device may also include a gesture classifier . The gesture classifier may be used to classify movement of the device . In some cases the accelerometer can supply movement information to the gesture classifier that the gesture classifier may separate into different classifications. For example the gesture classifier can classify movement of the phone into groups such as shake and flip. In addition the gesture classifier may also classify motion related to gestures such as to mouth from mouth facing user to ear and from ear. 

A pose identifier included in the device may infer detect different poses of the device . The pose identifier may use data provided by the proximity sensor and the gesture classifier to identify poses. For example the pose identifier may determine how far the device is from an object e.g. a person using information provided by the proximity sensor . This information combined with a gesture classification provided by the gesture classifier can be used by the posture identifier to determine which pose if any the device has been placed in. In one example if the gesture classifier transmits a to ear classification to the pose identifier and the proximity sensor indicates that the device is being held close to the user the pose identifier may determine that the device is in telephone pose. A camera can also be used to provide evidence about movement. For example the optical flow detected by the camera may provide evidence of movement.

The device may also include a speech endpointer . The speech endpointer in some implementations can combine outputs from the pose identifier speaker identifier and speech detector to determine inter alia whether a user is speaking into the device beginning to speak into the device or has stopped speaking into the device. For example the pose identifier may transmit information to the endpointer indicating that the device is not in an operating position. Inputs from the speech detector and speaker identifier may indicate that the user is not currently speaking. The combination of these inputs may indicate to the endpointer that the user has stopped speaking.

However if a record button press is not detected at step the process can proceed to step where it is determined whether a record gesture has been detected. For example a user may be holding the device in PDA pose. When the user brings the device to his mouth the gesture classifier may classify this motion as a to mouth gesture and cause the device to execute a recording application. In some implementations other gestures such as shaking or flipping the phone can be a record gesture. In response the process may proceed to step where a recording process is started and a recording confirmation is displayed as described above. If not the process may return to step where it determines if a record button has been pressed.

The process may load settings into an endpointer at step . In some cases the device may load pose specific speech detection parameters such as a speech energy threshold that can be used to detect speech. For example in some cases the speech energy threshold for a pose may be compared to detected auditory information. If the auditory information is greater than the speech energy threshold this may indicate that a user is speaking to the device. In some implementations poses may have an associated speech energy threshold that is based on the distance between the device and a user when the device is in the specified pose. For instance the device may be closer to a user in telephone pose than it is in PDA pose. Accordingly the speech energy threshold may be lower for the PDA pose than it is for the telephone pose because the user s mouth is farther from the device in PDA pose.

At step an endpointer may run. For example device may execute endpointer . In response the endpointer can use parameters loaded at step to determine whether the user is speaking to the device and related events such as the start and end of speech. For example the endpointer may use a speech energy threshold along with inputs from the pose identifier speech detector and speaker identifier to determine whether the user is speaking and if so whether the speech is beginning or ending.

At step an end of speech input may be detected. As discussed above the endpointer may determine whether speech has ended using inputs from other device components and a speech energy threshold. If the end of speech input has been detected recording may cease and an end of input EOI display indicating that recording has ended may be provided at step . For example a message may appear on the screen of the device or a sound may be played. In some cases tactile feedback e.g. a vibration may be provided.

At step it is determined whether the device is in phone pose. For example the pose identifier can use inputs from the proximity sensor and the gesture classifier to determine if the device is in phone pose. In some cases the pose of the device can be identified by determining how far the device is from the user and whether the device is within a set of predetermined angles. If the device is in phone pose a sound confirming that recording has begun may be played at step . In some implementations another type of feedback e.g. a vibration or a display of a message may be provided with or instead of the audio confirmation.

At step phone pose settings may be loaded into an endpointer. For example a speech energy threshold associated with the phone pose may be read from memory into the endpointer .

Similarly at step it is determined whether the device is in walkie talkie pose. As noted above the pose identifier can use inputs from the gesture classifier and the proximity sensor to determine the pose of the device. If the device is in walkie talkie pose confirmation that recording has begun may be displayed on the screen in some cases confirmation may also be tactile or auditory at step and walk talkie pose settings may be loaded into an endpointer at step .

At step it is determined whether the device is in PDA pose. In some cases the pose of the device can be determined as described in regards to steps and above. If the device is not in PDA pose the method can return to step . If the device is in PDA pose it can be determined whether a record button has been pressed at step . If a record button has not been pressed the method proceeds to step where it is determined if a record gesture has been detected. For example as discussed in relation to step of above the device may detect a movement of the device toward a user s mouth. In some cases the device may interpret this motion as a record gesture.

If a record button was pressed at step or a record gesture was detected at step a message confirming that recording has begun can be displayed on the screen of the device at step . In some cases the device may vibrate or play a sound to indicate that recording has started. Subsequently settings associated with the PDA pose may be loaded into an endpointer at step . For example a speech energy threshold may be loaded into the endpointer .

For each of the poses described above after the appropriate pose settings are read into an endpointer the endpointer may be run at step . For example a processor associated with the device may execute instructions stored in memory that correspond to the endpointer . Once the endpointer has begun executing the endpointer may determine whether an end of speech input has been detected at step . For example the endpointer may determine whether an end of speech input has been detected using outputs from the pose identifier speech detector speaker identifier and parameters associated with the pose that have been loaded into the endpointer . For example the endpointer may determine when the device is no longer in one of the specified poses using outputs from the previously mentioned sources. At step the process may play or display a confirmation that speech recording has ceased. For example an end of recording message may be displayed on the device s screen or a sound may be played. In some cases the device may vibrate.

For example acceleration data provided by the accelerometer may be smoothed by for instance using a digital filter e.g. an infinite impulse response filter . In some cases the accelerometer may have a sample frequency of 10 Hz. In addition the infinite impulse response filter may have a filtering factor of 0.6. The magnitude of the instantaneous acceleration may be calculated from the residual of the filter. A resulting gravity vector may be projected onto XY and YZ planes of the coordinate system and the angle subtended by the projected components may be calculated using the inverse tangent of the components. The resulting two angles can be projected onto a new plane such as the one shown in and critical angle bounding boxes and can be defined around the left and right hand positions of the phone to a user s ear. As described in further detail below these bounding boxes can be used to detect gestures among other things.

If the proximity sensor detects an object within a preset distance of the device the state machine transitions to state where it waits for an angle. In some cases if the proximity sensor detects a user within the predetermined distance and the device was previously determined to be at the critical angles e.g. the state machine was activated because the device was placed at the critical angles the state machine transitions to the next state . If the device was not previously placed at the critical angles the device may wait for a preset period for the device to be placed at the critical angles this preset period may allow any acceleration noise to settle. In some cases the preset period may be one second. If the device is not placed at the critical angles within the predetermined period the state machine may transition back to state . However if the device is detected at the critical angles within the predetermined threshold the state machine transitions to state where a gesture is detected. In some cases the gesture classifier may classify the detected gesture. For example the gesture may fall into the following categories to mouth from mouth facing user to ear and from ear. In some implementations other categories may be defined. If the device is determined to no longer be at the critical angles the state machine may transition to state where the gesture has expired. In some implementations a minimum debounce period may prevent this transition from happening because of angle bounce. For example the minimum debounce period may be 1.7 seconds.

The state machine begins in an initial state . If an acceleration above a threshold is detected the machine transitions to state where it waits for proximity detection after the detected acceleration. In some implementations the acceleration threshold may be 0.6 g. In some cases the wait may be 0.5 seconds. If the device is proximate an object such as a user the state machine transitions to state where it waits a predetermined time for the device to placed at the critical angles. In some cases the wait may be one second. If the device is not placed at the critical angles within the specified time the state machine returns to its initial state . However if the device is placed at the critical angles the state machine transitions to state where a gesture is detected in the manner described above. When the device is no longer within the critical angles the state machine transitions to state where the gesture has expired. These transitions may correspond to a fast gesture.

In some cases after acceleration has been detected the device may be placed in critical angles and as such the state machine can proceed to state where it waits for a proximity detection. If no proximity detection is made within a preset time the state machine can transition to state where the waiting proximity time has expired and subsequently return to its initial state . In some cases the preset time may be one second. However if a proximity detection is made before the preset time expires the state machine can transition to states and as described above. In some cases this series of transitions may correspond to a medium speed gesture.

If the state machine is in its initial state and the device has been placed at the critical angles the state machine can transition to state where the state machine waits for proximity detection. If proximity detection occurs before a timeout period the state machine proceeds to state where a gesture is detected. If the device is moved from the critical angles the state machine transitions to state where the gesture has expired. This series of transitions may correspond to a gesture made at relatively slow pace.

In some implementations the posterior probability for EP can be used as input to an endpointer state machine. For example illustrates an endpointer state machine . In the illustrative implementation shown in an EP posterior probability can be thresholded and a time frame may be determined to contain either noise or speech. In this example noise may be represented by a zero value and speech can be represented by a one value. A circular buffer of thresholded values may be stored. A one value in a buffer can be used to drive the endpointer state machine shown in . For example if the initial state is pre speech and the number of one values in the circular buffer exceeds a threshold the machine moves to state Possible Onset. If the number of one values fall below the threshold the machine moves back to the Pre Speech state . The state machine can transition backward and forward among the Speech Present Possible Offset and Post Speech states in a similar fashion.

In some cases as the user is speaking the graphical user interface can provide feedback on the quality of the audio captured by the device. For example a visual indication of the amplitude of the recorded audio can be displayed on the screen while the user is speaking. This may provide the user an indication of whether background noise is interfering with sound recording or whether the user s commands are being properly recorded. After the user has finished speaking the graphical user interface may display a representation of the captured voice commands to the user.

The illustrative graphical user interface can also include a visual audio level indicator . In an illustrative implementation the visual audio level indicator can indicate the amplitude of audio captured by a mobile device. For example as a user is speaking the indicator can go up an amount related to the amplitude of the detected speech. In some circumstances the indicator may allow a user to determine whether background noise is interfering with speech recording. For example if the indicator goes up before the user begins speaking background noise may interfere with speech recording. If the indicator does not go up while the user is speaking this may indicate the user s voice commands are not being properly recorded.

In some cases the audio level indicator can display a representation of the log of the Root Means Square RMS level of a frame of audio samples. The log RMS level of the frame of audio samples may represent a background noise level. In some cases the RMS value may be equal to

In some cases audio level indicator may display a representation of a signal to noise ratio i.e. strength of a speech signal relative to background noise. For example the signal to noise ratio can be calculated using the following equation 

In an alternative implementation the audio level indicator can display a representation of a combination of the log RMS level of a frame of audio samples and a signal to noise ratio. For example this combination can be determined as follows 5 In this equation and can be variables that can scale the background noise and signal to noise. For example a can scale the RMS level of a frame of audio samples to represent decibel values e.g. such that 100 db equals a full scale RMS level of a frame of audio . can used to scale a signal to noise ratio in a similar fashion.

In some implementations one or more of the background noise level signal to noise ratio or a combination of the two can be displayed on the graphical user interface . For example one or more of these measures may be displayed on the screen in different colors or in different areas of the screen. In some cases one of these measures may be superimposed on one of the others. For example data representing a signal to noise ratio may be superimposed on data representing a background noise level.

The illustrative waveform may permit the user to recognize when a device has failed to record audio. For example after a user has spoken an voice command the application can show a waveform that represents the captured audio. If the waveform is a flat line this may indicate that no audio was recorded.

In some cases the device can correlate a particular noise level or type of environmental sound to recognition accuracy. For example a noise level NL of 40 dB fan noise may correspond to a word error rate WER of 20 while the WER might be 50 when the noise is 70 dB assuming the user speaks at 80 dB on average . These values may be transmitted to a server e.g. remote device that can collect statistics to make a table from NL to WER.

Some noise types may be worse than others. For example 50 dB cafeteria noise might have the same WER as 70 dB fan noise. The device can perform environment characterization of this type by sending the audio to a server such as remote device for mode determination.

If the background noise and or signal to noise ratio is above the background level threshold the process proceeds to step where a voice search button is displayed as shown in . If not a dialog box or message may be displayed advising a user to use the device in phone position at step . Regardless the method returns to after step or step .

Optionally at step noise and speech levels may be initialized. For instance if noise and speech levels have not already been set as may be the case when the method is executed for the first time noise and speech levels may be initialized using an RMS level of an audio sample. In an illustrative example the noise and speech levels may be set using the following equations 1 6 1 2 7 In equations 6 and 7 RMS can be an RMS level of an audio sample and a is a ratio of a previous estimate of noise or speech and a current estimate of noise or speech. This ratio may be initially set to zero and increase to

At step a noise level may be updated. For example a noise level can be compared with a RMS level of an audio sample and the noise level can be adjusted according to the following equation UpdateRate UpdateRate 8 Like equation 7 RMS can be an RMS level of an audio sample. In some cases the sum of UpdateRateand UpdateRatecan equal one. If the noise level is less than an RMS level of an audio sample UpdateRatemay be 0.995 while UpdateRatemay be 0.005. If the noise level is greater than the RMS level of an audio sample the noise level may be adjusted using equation 8 but UpdateRatemay be 0.95 and UpdateRatemay be 0.05.

At step a speech level may be updated. For example a speech level can be compared with an RMS level of an audio sample and the speech sample can be adjusted according to the following equation UpdateRate UpdateRate 9 

If the speech level is greater than an RMS level of the audio sample UpdateRatemay equal 0.995 and UpdateRatecan equal 0.005. If the speech level is less than an RMS level of the audio sample UpdateRatemay equal 0.995 and UpdateRatecan equal 0.005. After the speech level is updated the method may return to step .

In some implementations other background noise level estimation methods may be used. For example the methods disclosed in the following papers which are herein incorporated by reference may be used Assessing Local Noise Level Estimation Methods Application to Noise Robust ASR Christophe Ris Stephane Dupont. Speech Communication 34 2001 141 158 DySANA Dynamic Speech and Noise Adaptation for Voice Activity Detection Ron J. Weiss Trausti Kristjansson ICASSP 2008 Noise estimation techniques for robust speech recognition H. G. Hirsch C Ehrlicher Proc. IEEE Internat. Conf. Audio Speech Signal Process v12 i1 59 67 and Assessing Local Noise Level Estimation Methods Stephane Dupont Christophe Ris Workshop on Robust Methods For Speech Recognition in Adverse Conditions Nokia COST249 IEEE pages 115 118 Tampere Finland May 1999.

Referring now to the exterior appearance of an exemplary device that implements the multisensory speech detection methods described above is illustrated. In more detail the hardware environment of the device includes a display for displaying text images and video to a user a keyboard for entering text data and user commands into the device a pointing device for pointing selecting and adjusting objects displayed on the display an antenna a network connection a camera a microphone and a speaker . Although the device shows an external antenna the device can include an internal antenna which is not visible to the user.

The display can display video graphics images and text that make up the user interface for the software applications used by the device and the operating system programs used to operate the device . Among the possible elements that may be displayed on the display are a new mail indicator that alerts a user to the presence of a new message an active call indicator that indicates that a telephone call is being received placed or is occurring a data standard indicator that indicates the data standard currently being used by the device to transmit and receive data a signal strength indicator that indicates a measurement of the strength of a signal received by via the antenna such as by using signal strength bars a battery life indicator that indicates a measurement of the remaining battery life or a clock that outputs the current time.

The display may also show application icons representing various applications available to the user such as a web browser application icon a phone application icon a search application icon a contacts application icon a mapping application icon an email application icon or other application icons. In one example implementation the display is a quarter video graphics array QVGA thin film transistor TFT liquid crystal display LCD capable of 16 bit or better color.

A user uses the keyboard or keypad to enter commands and data to operate and control the operating system and applications that provide for multisensory speech detection. The keyboard includes standard keyboard buttons or keys associated with alphanumeric characters such as keys and that are associated with the alphanumeric characters Q and W when selected alone or are associated with the characters and 1 when pressed in combination with key . A single key may also be associated with special characters or functions including unlabeled functions based upon the state of the operating system or applications invoked by the operating system. For example when an application calls for the input of a numeric character a selection of the key alone may cause a 1 to be input.

In addition to keys traditionally associated with an alphanumeric keypad the keyboard also includes other special function keys such as an establish call key that causes a received call to be answered or a new call to be originated a terminate call key that causes the termination of an active call a drop down menu key that causes a menu to appear within the display a backward navigation key that causes a previously accessed network address to be accessed again a favorites key that causes an active web page to be placed in a bookmarks folder of favorite sites or causes a bookmarks folder to appear a home page key that causes an application invoked on the device to navigate to a predetermined network address or other keys that provide for multiple way navigation application selection and power and volume control.

The user uses the pointing device to select and adjust graphics and text objects displayed on the display as part of the interaction with and control of the device and the applications invoked on the device . The pointing device is any appropriate type of pointing device and may be a joystick a trackball a touch pad a camera a voice input device a touch screen device implemented in combination with the display or any other input device.

The antenna which can be an external antenna or an internal antenna is a directional or omni directional antenna used for the transmission and reception of radiofrequency RF signals that implement point to point radio communication wireless local area network LAN communication or location determination. The antenna may facilitate point to point radio communication using the Specialized Mobile Radio SMR cellular or Personal Communication Service PCS frequency bands and may implement the transmission of data using any number or data standards. For example the antenna may allow data to be transmitted between the device and a base station using technologies such as Wireless Broadband WiBro Worldwide Interoperability for Microwave ACCess WiMAX 10GPP Long Term Evolution LTE Ultra Mobile Broadband UMB High Performance Radio Metropolitan Network HIPERMAN iBurst or High Capacity Spatial Division Multiple Access HC SDMA High Speed OFDM Packet Access HSOPA High Speed Packet Access HSPA HSPA Evolution HSPA High Speed Upload Packet Access HSUPA High Speed Downlink Packet Access HSDPA Generic Access Network GAN Time Division Synchronous Code Division Multiple Access TD SCDMA Evolution Data Optimized or Evolution Data Only EVDO Time Division Code Division Multiple Access TD CDMA Freedom Of Mobile Multimedia Access FOMA Universal Mobile Telecommunications System UMTS Wideband Code Division Multiple Access W CDMA Enhanced Data rates for GSM Evolution EDGE Enhanced GPRS EGPRS Code Division Multiple Access 2000 CDMA2000 Wideband Integrated Dispatch Enhanced Network WiDEN High Speed Circuit Switched Data HSCSD General Packet Radio Service GPRS Personal Handy Phone System PHS Circuit Switched Data CSD Personal Digital Cellular PDC CDMAone Digital Advanced Mobile Phone System D AMPS Integrated Digital Enhanced Network IDEN Global System for Mobile communications GSM DataTAC Mobitex Cellular Digital Packet Data CDPD Hicap Advanced Mobile Phone System AMPS Nordic Mobile Phone NMP Autoradiopuhelin ARP Autotel or Public Automated Land Mobile PALM Mobiltelefonisystem D MTD Offentlig Landmobil Telefoni OLT Advanced Mobile Telephone System AMTS Improved Mobile Telephone Service IMTS Mobile Telephone System MTS Push To Talk PTT or other technologies. Communication via W CDMA HSUPA GSM GPRS and EDGE networks may occur for example using a QUALCOMM MSM7200A chipset with an QUALCOMM RTR6285 transceiver and PM7540 power management circuit.

The wireless or wired computer network connection may be a modem connection a local area network LAN connection including the Ethernet or a broadband wide area network WAN connection such as a digital subscriber line DSL cable high speed internet connection dial up connection T 1 line T 10 line fiber optic connection or satellite connection. The network connection may connect to a LAN network a corporate or government WAN network the Internet a telephone network or other network. The network connection uses a wired or wireless connector. Example wireless connectors include for example an INFRARED DATA ASSOCIATION IrDA wireless connector a Wi Fi wireless connector an optical wireless connector an INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS IEEE Standard 802.11 wireless connector a BLUETOOTH wireless connector such as a BLUETOOTH version 1.2 or 10.0 connector a near field communications NFC connector an orthogonal frequency division multiplexing OFDM ultra wide band UWB wireless connector a time modulated ultra wide band TM UWB wireless connector or other wireless connector. Example wired connectors include for example a IEEE 1394 FIREWIRE connector a Universal Serial Bus USB connector including a mini B USB interface connector a serial port connector a parallel port connector or other wired connector. In another implementation the functions of the network connection and the antenna are integrated into a single component.

The camera allows the device to capture digital images and may be a scanner a digital still camera a digital video camera other digital input device. In one example implementation the camera is a 10 mega pixel MP camera that utilizes a complementary metal oxide semiconductor CMOS .

The microphone allows the device to capture sound and may be an omni directional microphone a unidirectional microphone a bi directional microphone a shotgun microphone or other type of apparatus that converts sound to an electrical signal. The microphone may be used to capture sound generated by a user for example when the user is speaking to another user during a telephone call via the device . Conversely the speaker allows the device to convert an electrical signal into sound such as a voice from another user generated by a telephone application program or a ring tone generated from a ring tone application program. Furthermore although the device is illustrated in as a handheld device in further implementations the device may be a laptop a workstation a midrange computer a mainframe an embedded system telephone desktop PC a tablet computer a PDA or other type of computing device.

The CPU can be one of a number of computer processors. In one arrangement the computer CPU is more than one processing unit. The RAM interfaces with the computer bus so as to provide quick RAM storage to the CPU during the execution of software programs such as the operating system application programs and device drivers. More specifically the CPU loads computer executable process steps from the storage medium or other media into a field of the RAM in order to execute software programs. Data is stored in the RAM where the data is accessed by the computer CPU during execution. In one example configuration the device includes at least 128 MB of RAM and 256 MB of flash memory.

The storage medium itself may include a number of physical drive units such as a redundant array of independent disks RAID a floppy disk drive a flash memory a USB flash drive an external hard disk drive thumb drive pen drive key drive a High Density Digital Versatile Disc HD DVD optical disc drive an internal hard disk drive a Blu Ray optical disc drive or a Holographic Digital Data Storage HDDS optical disc drive an external mini dual in line memory module DIMM synchronous dynamic random access memory SDRAM or an external micro DIMM SDRAM. Such computer readable storage media allow the device to access computer executable process steps application programs and the like stored on removable and non removable memory media to off load data from the device or to upload data onto the device .

A computer program product is tangibly embodied in storage medium a machine readable storage medium. The computer program product includes instructions that when read by a machine operate to cause a data processing apparatus to store image data in the mobile device. In some embodiments the computer program product includes instructions that perform multisensory speech detection.

The operating system may be a LINUX based operating system such as the GOOGLE mobile device platform APPLE MAC OS X MICROSOFT WINDOWS NT WINDOWS 2000 WINDOWS XP WINDOWS MOBILE a variety of UNIX flavored operating systems or a proprietary operating system for computers or embedded systems. The application development platform or framework for the operating system may be BINARY RUNTIME ENVIRONMENT FOR WIRELESS BREW JAVA Platform Micro Edition JAVA ME or JAVA 2 Platform Micro Edition J2ME using the SUN MICROSYSTEMS JAVASCRIPT programming language PYTHON FLASH LITE or MICROSOFT .NET Compact or another appropriate environment.

The device stores computer executable code for the operating system and the application programs such as an email instant messaging a video service application a mapping application word processing spreadsheet presentation gaming mapping web browsing JAVASCRIPT engine or other applications. For example one implementation may allow a user to access the GOOGLE GMAIL email application the GOOGLE TALK instant messaging application a YOUTUBE video service application a GOOGLE MAPS or GOOGLE EARTH mapping application or a GOOGLE PICASA imaging editing and presentation application. The application programs may also include a widget or gadget engine such as a TAFRI widget engine a MICROSOFT gadget engine such as the WINDOWS SIDEBAR gadget engine or the KAPSULES gadget engine a YAHOO widget engine such as the KONFABULTOR widget engine the APPLE DASHBOARD widget engine the GOOGLE gadget engine the KLIPFOLIO widget engine an OPERA widget engine the WIDSETS widget engine a proprietary widget or gadget engine or other widget or gadget engine that provides host system software for a physically inspired applet on a desktop.

Although it is possible to provide for multisensory speech detection using the above described implementation it is also possible to implement the functions according to the present disclosure as a dynamic link library DLL or as a plug in to other application programs such as an Internet web browser such as the FOXFIRE web browser the APPLE SAFARI web browser or the MICROSOFT INTERNET EXPLORER web browser.

The navigation module may determine an absolute or relative position of the device such as by using the Global Positioning System GPS signals the GLObal NAvigation Satellite System GLONASS the Galileo positioning system the Beidou Satellite Navigation and Positioning System an inertial navigation system a dead reckoning system or by accessing address internet protocol IP address or location information in a database. The navigation module may also be used to measure angular displacement orientation or velocity of the device such as by using one or more accelerometers.

The operating system can generally be organized into six components a kernel libraries an operating system runtime application libraries system services and applications . The kernel includes a display driver that allows software such as the operating system and the application programs to interact with the display via the display interface a camera driver that allows the software to interact with the camera a BLUETOOTH driver a M Systems driver a binder IPC driver a USB driver a keypad driver that allows the software to interact with the keyboard via the keyboard interface a WiFi driver audio drivers that allow the software to interact with the microphone and the speaker via the sound interface and a power management component that allows the software to interact with and manage the power source .

The BLUETOOTH driver which in one implementation is based on the BlueZ BLUETOOTH stack for LINUX based operating systems provides profile support for headsets and hands free devices dial up networking personal area networking PAN or audio streaming such as by Advance Audio Distribution Profile A2DP or Audio Video Remote Control Profile AVRCP . The BLUETOOTH driver provides JAVA bindings for scanning pairing and unpairing and service queries.

The libraries include a media framework that supports standard video audio and still frame formats such as Moving Picture Experts Group MPEG 11 H.264 MPEG 1 Audio Layer 10 MP3 Advanced Audio Coding AAC Adaptive Multi Rate AMR Joint Photographic Experts Group JPEG and others using an efficient JAVA Application Programming Interface API layer a surface manager a simple graphics library SGL for two dimensional application drawing an Open Graphics Library for Embedded Systems OpenGL ES for gaming and three dimensional rendering a C standard library LIBC a LIBWEBCORE library a FreeType library an SSL and an SQLite library .

The operating system runtime includes core JAVA libraries and a Dalvik virtual machine . The Dalvik virtual machine is a custom virtual machine that runs a customized file format .DEX .

The operating system can also include Mobile Information Device Profile MIDP components such as the MIDP JAVA Specification Requests JSRs components MIDP runtime and MIDP applications as shown in . The MIDP components can support MIDP applications running on the device .

With regard to graphics rendering a system wide composer manages surfaces and a frame buffer and handles window transitions using the OpenGL ES and two dimensional hardware accelerators for its compositions.

The Dalvik virtual machine may be used with an embedded environment since it uses runtime memory very efficiently implements a CPU optimized bytecode interpreter and supports multiple virtual machine processes per device. The custom file format .DEX is designed for runtime efficiency using a shared constant pool to reduce memory read only structures to improve cross process sharing concise and fixed width instructions to reduce parse time thereby allowing installed applications to be translated into the custom file formal at build time. The associated bytecodes are designed for quick interpretation since register based instead of stack based instructions reduce memory and dispatch overhead since using fixed width instructions simplifies parsing and since the 16 bit code units minimize reads.

The application libraries include a view system a resource manager and content providers . The system services includes a status bar an application launcher a package manager that maintains information for all installed applications a telephony manager that provides an application level JAVA interface to the telephony subsystem a notification manager that allows all applications access to the status bar and on screen notifications a window manager that allows multiple applications with multiple windows to share the display and an activity manager that runs each application in a separate process manages an application life cycle and maintains a cross application history.

The applications include a home application a dialer application a contacts application a browser application and a multispeech detection application .

The telephony manager provides event notifications such as phone state network state Subscriber Identity Module SIM status or voicemail status allows access to state information such as network information SIM information or voicemail presence initiates calls and queries and controls the call state. The browser application renders web pages in a full desktop like manager including navigation functions. Furthermore the browser application allows single column small screen rendering and provides for the embedding of HTML views into other applications.

Some processes can be persistent. For example processes associated with core system components such as the surface manager the window manager or the activity manager can be continuously executed while the device is powered. Additionally some application specific process can also be persistent. For example processes associated with the dialer application may also be persistent.

The processes implemented by the operating system kernel may generally be categorized as system services processes dialer processes browser processes and maps processes . The system services processes include status bar processes associated with the status bar application launcher processes associated with the application launcher package manager processes associated with the package manager activity manager processes associated with the activity manager resource manager processes associated with a resource manager that provides access to graphics localized strings and XML layout descriptions notification manger processes associated with the notification manager window manager processes associated with the window manager core JAVA libraries processes associated with the core JAVA libraries surface manager processes associated with the surface manager Dalvik virtual machine processes associated with the Dalvik virtual machine LIBC processes associated with the LIBC library and multispeech detection processes associated with the multispeech detection application .

The dialer processes include dialer application processes associated with the dialer application telephony manager processes associated with the telephony manager core JAVA libraries processes associated with the core JAVA libraries Dalvik virtual machine processes associated with the Dalvik Virtual machine and LIBC processes associated with the LIBC library . The browser processes include browser application processes associated with the browser application core JAVA libraries processes associated with the core JAVA libraries Dalvik virtual machine processes associated with the Dalvik virtual machine LIBWEBCORE processes associated with the LIBWEBCORE library and LIBC processes associated with the LIBC library .

The maps processes include maps application processes core JAVA libraries processes Dalvik virtual machine processes and LIBC processes . Notably some processes such as the Dalvik virtual machine processes may exist within one or more of the systems services processes the dialer processes the browser processes and the maps processes .

Computing device includes a processor memory a storage device a high speed interface connecting to memory and high speed expansion ports and a low speed interface connecting to low speed bus and storage device . Each of the components and are interconnected using various busses and may be mounted on a common motherboard or in other manners as appropriate. The processor can process instructions for execution within the computing device including instructions stored in the memory or on the storage device to display graphical information for a GUI on an external input output device such as display coupled to high speed interface . In other implementations multiple processors and or multiple buses may be used as appropriate along with multiple memories and types of memory. Also multiple computing devices may be connected with each device providing portions of the necessary operations e.g. as a server bank a group of blade servers or a multi processor system .

The memory stores information within the computing device . In one implementation the memory is a volatile memory unit or units. In another implementation the memory is a non volatile memory unit or units. The memory may also be another form of computer readable medium such as a magnetic or optical disk.

The storage device is capable of providing mass storage for the computing device . In one implementation the storage device may be or contain a computer readable medium such as a floppy disk device a hard disk device an optical disk device or a tape device a flash memory or other similar solid state memory device or an array of devices including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that when executed perform one or more methods such as those described above. The information carrier is a computer or machine readable medium such as the memory the storage device memory on processor or a propagated signal.

The high speed controller manages bandwidth intensive operations for the computing device while the low speed controller manages lower bandwidth intensive operations. Such allocation of functions is exemplary only. In one implementation the high speed controller is coupled to memory display e.g. through a graphics processor or accelerator and to high speed expansion ports which may accept various expansion cards not shown . In the implementation low speed controller is coupled to storage device and low speed expansion port . The low speed expansion port which may include various communication ports e.g. USB Bluetooth Ethernet wireless Ethernet may be coupled to one or more input output devices such as a keyboard a pointing device a scanner or a networking device such as a switch or router e.g. through a network adapter.

The computing device may be implemented in a number of different forms as shown in the figure. For example it may be implemented as a standard server or multiple times in a group of such servers. It may also be implemented as part of a rack server system . In addition it may be implemented in a personal computer such as a laptop computer . Alternatively components from computing device may be combined with other components in a mobile device not shown such as device . Each of such devices may contain one or more of computing device and an entire system may be made up of multiple computing devices communicating with each other.

Computing device includes a processor memory an input output device such as a display a communication interface and a transceiver among other components. The device may also be provided with a storage device such as a microdrive or other device to provide additional storage. Each of the components and are interconnected using various buses and several of the components may be mounted on a common motherboard or in other manners as appropriate.

The processor can execute instructions within the computing device including instructions stored in the memory . The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide for example for coordination of the other components of the device such as control of user interfaces applications run by device and wireless communication by device .

Processor may communicate with a user through control interface and display interface coupled to a display . The display may be for example a TFT LCD Thin Film Transistor Liquid Crystal Display or an OLED Organic Light Emitting Diode display or other appropriate display technology. The display interface may comprise appropriate circuitry for driving the display to present graphical and other information to a user. The control interface may receive commands from a user and convert them for submission to the processor . In addition an external interface may be provide in communication with processor so as to enable near area communication of device with other devices. External interface may provide for example for wired communication in some implementations or for wireless communication in other implementations and multiple interfaces may also be used.

The memory stores information within the computing device . The memory can be implemented as one or more of a computer readable medium or media a volatile memory unit or units or a non volatile memory unit or units. Expansion memory may also be provided and connected to device through expansion interface which may include for example a SIMM Single In Line Memory Module card interface. Such expansion memory may provide extra storage space for device or may also store applications or other information for device . Specifically expansion memory may include instructions to carry out or supplement the processes described above and may include secure information also. Thus for example expansion memory may be provide as a security module for device and may be programmed with instructions that permit secure use of device . In addition secure applications may be provided via the SIMM cards along with additional information such as placing identifying information on the SIMM card in a non hackable manner.

The memory may include for example flash memory and or NVRAM memory as discussed below. In one implementation a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that when executed perform one or more methods such as those described above. The information carrier is a computer or machine readable medium such as the memory expansion memory memory on processor or a propagated signal that may be received for example over transceiver or external interface .

Device may communicate wirelessly through communication interface which may include digital signal processing circuitry where necessary. Communication interface may provide for communications under various modes or protocols such as GSM voice calls SMS EMS or MMS messaging CDMA TDMA PDC WCDMA CDMA2000 or GPRS among others. Such communication may occur for example through radiofrequency transceiver . In addition short range communication may occur such as using a Bluetooth WiFi or other such transceiver not shown . In addition GPS Global Positioning System receiver module may provide additional navigation and location related wireless data to device which may be used as appropriate by applications running on device .

Device may also communicate audibly using audio codec which may receive spoken information from a user and convert it to usable digital information. Audio codec may likewise generate audible sound for a user such as through a speaker e.g. in a handset of device . Such sound may include sound from voice telephone calls may include recorded sound e.g. voice messages music files etc. and may also include sound generated by applications operating on device .

The computing device may be implemented in a number of different forms as shown in the figure. For example it may be implemented as a cellular telephone . It may also be implemented as part of a smartphone personal digital assistant or other similar mobile device.

Various implementations of the systems and techniques described here can be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium computer readable medium refers to any computer program product apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

To provide for interaction with a user the systems and techniques described here can be implemented on a computer having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

The systems and techniques described here can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here or any combination of such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network . Examples of communication networks include a local area network LAN a wide area network WAN and the Internet.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

In addition the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

