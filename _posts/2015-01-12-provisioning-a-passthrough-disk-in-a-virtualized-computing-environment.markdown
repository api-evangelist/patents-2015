---

title: Provisioning a pass-through disk in a virtualized computing environment
abstract: A physical host machine determines that a storage device from a network storage system is available to the host machine as a pass-through disk. Virtualization software running on the host machine assigns the pass-through disk to a temporary resource group on the host machine. The pass-through disk is logically attached to the virtual machine running on the host machine and made available to an operating system and application running on the virtual machine.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09098324&OS=09098324&RS=09098324
owner: NetApp, Inc.
number: 09098324
owner_city: Sunnyvale
owner_country: US
publication_date: 20150112
---
This application is a Continuation of and claims the priority benefit of U.S. application Ser. No. 13 902 662 filed May 24 2013 which is a continuation of issued U.S. Pat. No. 8 452 921 filed Mar. 17 2010 which is assigned to the same assignee as the present application.

This invention relates to the field of virtualization systems and in particular to an automatic end to end pass through disk provisioning solution.

Virtualization is an abstraction that decouples the physical hardware from the operating system in a data processing system to deliver greater resource utilization and flexibility. Virtualization allows multiple virtual machines with heterogeneous operating systems e.g. Windows Linux UNIX etc. and applications to run in isolation side by side on the same physical host machine. A virtual machine is the representation of a physical machine by software. A virtual machine has its own set of virtual hardware e.g. random access memory RAM central processing unit CPU network interface card NIC hard disks etc. upon which an operating system and applications are loaded. The operating system sees a consistent normalized set of hardware regardless of the actual physical hardware components.

A conventional virtualized processing system may include a physical host machine which runs virtualization software such as a hypervisor. The hypervisor software runs on the physical host machine e.g. a computer and abstracts physical hardware e.g. processors memory storage and networking resources etc. to be provisioned to one or more virtual machines.

A guest operating system e.g. Windows Linux UNIX etc. may be installed on each of the virtual machines. The virtualization software presents the physical hardware of the host machine as virtual hardware to the guest operating system and applications running in the guest operating system. A user may access the virtual machine to perform computing tasks as if it were a physical machine. Generally the virtualization process is completely transparent to the user.

A pass through disk is a way for virtual machines to directly access a physical disk in a virtualized computing environment. The virtualization software on the host machine provides an Application Programming Interface API or other tools to assign pass through disks to the virtual machines. The user cannot through the virtual machine provision a pass through disk on the host machine if necessary and attach the pass through disk to the virtual machine. Furthermore the user cannot remove a provisioned pass through disk.

A physical host machine determines that a storage device from a network storage system is available to the host machine as a pass through disk. Virtualization software running on the host machine assigns the pass through disk to a temporary resource group on the host machine. The pass through disk is logically attached to the virtual machine running on the host machine and made available to an operating system and application running on the virtual machine.

In the following detailed description of embodiments of the invention reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims.

Embodiments are described for an end to end solution for provisioning a pass through disk from a network storage system to a virtual machine. Software components that run in a hypervisor on a physical host machine and in virtualization software running on both the physical host machine and the target virtual machine collaborate to provide the functionality described below. In one embodiment for a high availability virtual machine running in a host cluster configuration the pass through disk is provisioned as a shared cluster disk. At the request of virtualization software running on a high availability virtual machine virtualization software running on the host machine provisions a LUN from the network storage system and maps it to the host as a cluster disk. The virtualization software on the host machine creates a temporary resource group adds a disk resource representing the LUN to the temporary resource group and logically attaches the LUN to the requesting virtual machine. The hypervisors on the host machine and other host machines in the cluster are updated to show that the LUN is serving as a pass through disk for the virtual machine. The disk resource is moved from the temporary resource group to a virtual machine resource group and the temporary resource group is deleted. The pass through disk is made available to an operating system and applications running on the virtual machine. In response to a request from the virtual machine to remove the pass through disk reverse operations are performed.

Host machine may be a physical computer having a processor to execute instructions stored in memory. Host machine may run a hypervisor such as for example Hyper V in the Microsoft Windows Server 2008 R2 operating system. Hypervisor enables host machine to host one or more virtual machines each running its own operating system. In one embodiment host machine is also running virtualization software which may enable the provisioning of storage from the network storage system through the virtual machines. In one embodiment virtualization software includes NetApp SnapDrive for Windows developed by NetApp Inc. of Sunnyvale Calif. One of the virtual machines may be virtual machine . In one embodiment virtual machine runs the same operating system as host machine . In other embodiments virtual machine may run a different operating system such as for example Microsoft Windows Server 2003 2008 or another operating system. Virtual machine may also be running virtualization software .

In network environment a user may desire to provision a pass through disk. A pass through disk is a mass storage device logically attached to the host machine but assigned directly to virtual machine and formatted with an abstraction layer such as a file system recognizable by the operating system of virtual machine . In one embodiment the user through virtualization software running on virtual machine makes a request for a pass through disk. The virtualization software on the virtual machine forwards the request to the virtualization software running on the host machine . In one embodiment virtualization software on the host machine determines if the host machine is hypervisor enabled and whether the operating system on the host machine supports certain features such as for example hot add delete features. This information may be determined by querying the operating system of host machine .

If host machine has the proper features the virtualization software on the host machine sends the request for the pass through disk to the network storage system to provision a Logical Unit Number LUN . An operating system such as operating system as shown in running on the storage server of network storage system creates the LUN and maps it to the host machine . Mapping a LUN to host machine may include writing an identifier of LUN into a data structure representing the storage resources associated with host machine . In one embodiment virtualization software on the host machine determines if there is a virtual small computer system interface SCSI controller for virtual machine resident on the host machine and whether the controller has a free slot e.g. the capability to manage an additional storage device . If so virtualization software calls the hypervisor APIs to logically attach the LUN to the virtual machine . These actions may be performed using an operating system interface such as for example Windows Management Interface WMI . The virtualization software on the virtual machine uses an API to discover the logically attached LUN mount a volume assign a drive path name to the LUN and present it to the user .

As discussed above a user of virtual machine may wish to provision a pass through disk. Virtualization software on host machine may provision a LUN in response to the request as discussed with respect to . However because virtual machine is a high availability virtual machine running in a cluster of host machines the LUN is provisioned as a shared LUN . When the shared LUN is provisioned through virtual machine virtualization software places virtual machine and shared LUN in a virtual machine resource group as shown in .

One feature of the cluster is that if one physical host machine fails the virtual machine may be automatically moved to another host machine in the cluster. This process is typically transparent to the user. For example virtual machine may be currently running on host machine . If host machine fails for example due to a hardware or software error a clustering system in the operating systems of the host machines in cluster can detect the failure and automatically move virtual machine to host machine . When moved the virtual machine resource group is transferred from the failed host machine to another host machine in the cluster along with all resources in the resource group.

A software module such as a cluster manager operating within the operating system of host machine may list the resources for each resource group. In one embodiment the name of the shared pass through disk may be stored with the name of the associated virtual machine and the name of the mount point on the virtual machine side. For example the name of shared pass through disk may be Disk VM vm1 MP C mt1 .

In one embodiment when a shared disk is added to a virtual machine the resource is added to a fixed resource group called Available Storage resource group. The Available Storage resource group may be a data structure listing all available storage resources that is stored on the host machine as shown in . After the LUN is logically attached to a virtual machine an API call enumerates the LUN which is identified in the Available Storage group. This Available Storage group is owned only by one host machine in the cluster at any moment. In one embodiment the virtual machine must be running on the same physical host machine as the Available Storage group to provision a pass through disk. To provision a pass through disk on a virtual machine running on a host machine that does not currently own the Available Storage group i.e. the Available Storage group is not accessible to the host machine a temporary resource group is created as illustrated in . In an alternative embodiment the virtual machine uses the temporary resource group regardless of whether or not the host machine owns the Available Storage group.

Each host machine in the cluster may be viewed as either active or passive with respect to a particular virtual machine. For example virtual machine is currently running on physical host machine so host machine is active with respect to virtual machine and its associated resource groups. In one embodiment active resources groups on host machine include cluster group Available Storage and virtual machine resource group . In one embodiment the resource groups are stored in a memory of host machine however depending on the cluster implementation the resource groups may be stored in some other location. The active attachment is illustrated in by the solid lines between host machine and the resource groups. The remaining host machines in the cluster group are considered passive with respect to virtual machine because while not currently hosting virtual machine and its resources virtual machine could be transferred to another host machine in the cluster as discussed above. The passive attachment between resource groups and host machine is illustrated by the dotted lines in .

In one embodiment virtual machine is running on host machine concurrently with virtual machine running on host machine . As shown there is no Available Storage group actively associated with physical host machine . Therefore if a user makes a request for a pass through disk through virtual machine virtualization software on host machine either locates an existing LUN or requests that the operating system of a network storage system not shown provision a LUN as discussed below with respect to . Virtualization software maps the LUN to the physical host machine and generates a temporary resource group . In certain embodiments temporary resource group may be a data structure stored in memory on host machine or a series of memory addresses reserved to store the contents of the temporary resource group. In one embodiment temporary resource group is given the name of the associated virtual machine and the name of the mount point on the virtual machine side. For example the name temporary resource group may be Disk VM vm1 MP C mt1 Group . A disk resource representing the LUN is added to temporary resource group by writing the name of the disk resource to the data structure that makes up temporary resource group . Virtualization software logically attaches the LUN to virtual machine . Once the pass through disk is logically attached to virtual machine the provisioned disk resource is moved from the temporary resource group to the virtual machine resource group that virtual machine belongs to and the temporary resource group is removed.

In one embodiment at block method receives a request for a pass through disk from a connected virtual machine such as virtual machine of . The request may be made by a user through virtualization software running on the connected virtual machine . At decision block method determines whether an existing LUN is available to serve as the pass through disk. Method may determine whether an existing LUN is available by checking the available storage resource group . If at block method determines that an existing LUN is not available at block method requests that a new LUN be provisioned by attached connected network storage system such as storage system of . Network storage system provisions a LUN and maps it to host machine .

If at block method determines that an existing LUN is available method proceeds to block . At block method sends a command to virtualization software running on all other cluster host machines to map the temporary resource group to their physical host machines. In the event of a failure of host machine the temporary resource group may be transferred to another host machine allowing virtual machine to operate on the new physical host machine. The transfer may be performed automatically by cluster manager software of the host machine operating system and may be transparent to a user of the virtual machine. In one embodiment when the cluster manager transfers the resource groups an identifier of the resource group is removed from a data structure representing the available resource groups of the first host machine and added to a data structure representing the resource groups of the new host machine. At block method creates a disk resource for the LUN. The disk resource is a representation of the LUN that is used by virtualization software to assign the LUN in network storage to the requesting virtual machine. At block method creates a temporary resource group such as temporary resource group of and adds the disk resource from block to the temporary resource group. The temporary resource group identifies the virtual machine requesting the pass through disk and the LUN that will serve as the pass through disk. In a SCSI environment at block method locates a free slot in a virtual SCSI controller and logically attaches the LUN to the free slot. In one embodiment logically attaching the LUN includes associating a disk name or a file path name with an identifier representing a free slot of the virtual SCSI controller. When a LUN on a network storage system is mapped to a host machine the operating system of the host machine may view the LUN as a local hard disk. Thus host side operations on the LUN may be referred to in terms of a disk. The virtual SCSI controller may be part of the hypervisor software running on host machine but may be associated with virtual machine . In other embodiments virtual disk controllers other than SCSI may be used at block .

Once the LUN is identified in the temporary resource group at block method puts the disk resource offline making it unavailable to the virtual machine and updates a cluster manager running on the host machine with the virtual machine configuration change i.e. the addition of a new physical disk . In one embodiment putting a disk resource offline includes marking a bit in the disk resource to signify that the disk resource is offline. In another embodiment putting the disk resource offline includes adding the disk resource to a data structure representing all storage resources that are offline. At block method puts the disk resource online and moves the disk resource from the temporary resource group to a virtual machine resource group such as moving the resource from temporary resource group to virtual machine resource group as shown in . In one embodiment moving the disk resource includes removing an identifier of the storage resource from a data structure representing the temporary resource group and adding the identifier to a data structure representing the virtual machine resource group. At block method deletes the temporary resource group. At block method reports to virtualization software running on the virtual machine that the LUN is available as a pass through disk.

At block method creates a temporary resource group such as temporary resource group of and adds the disk resource for the pass through disk to the temporary resource group. At block method puts the disk resource offline making it unavailable to the virtual machine and updates a cluster manager running on the host machine with the virtual machine configuration change i.e. the removal of the pass through disk . At block method puts the disk resource online and sends a command to virtualization software running on all other cluster host machines to un map the temporary resource group from their physical host machines. Un mapping a temporary resource group may include removing an identifier of the temporary resource group from a data structure representing the storage resources associated with host machine.

At block method un maps the LUN from physical host machine . At block method deletes the temporary resource group from host machine . At block method reports to virtualization software running on the virtual machine that the pass through disk has been removed.

In one embodiment the processor reads instructions from the memory and executes the instructions. The memory may include any of various types of memory devices such as for example random access memory RAM read only memory ROM flash memory one or more mass storage devices e.g. disks etc. The memory stores instructions of an operating system . The processor retrieves the instructions from the memory to run the operating system . The processing system interfaces with one or more storage systems via the storage adaptor which may include a small computer system interface SCSI adaptor fiber channel adaptor etc.

The processing system may have a distributed architecture for example it may include a separate N network blade and D disk blade not shown . In such an embodiment the N blade is used to communicate with clients while the D blade includes the abstraction layer functionality and is used to communicate with the storage subsystem. In one embodiment the N blade and D blade communicate with each other using an internal protocol. In one embodiment where the N blade or D blade has its own separate processor and memory separate from that of the storage server the storage device slicing method as further described below is implemented within the N blade or D blade. Alternatively the processing system may have an integrated architecture where the network and data components are all contained in a single box. The processing system further may be coupled through a switching fabric to other similar storage systems not shown which have their own local storage subsystems. In this way all of the storage subsystems can form a single storage pool to which any client of any of the storage systems has access.

The operating system also includes a storage access layer and an associated storage driver layer to allow the storage system to communicate with the storage subsystem. The storage access layer implements a higher level disk storage protocol such as RAID while the storage driver layer implements a lower level storage device access protocol such as Fibre Channel Protocol FCP or SCSI. To facilitate description it is henceforth assumed herein that the storage access layer implements a RAID protocol such as RAID 4 RAID 5 or RAID DP and therefore it is alternatively called the RAID layer .

Also shown in is the path of data flow through the operating system associated with a read or write operation. The operating system also includes a user interface to generate a command line interface CLI and or graphical user interface GUI to allow administrative control and configuration of the processing system from a local or remote terminal.

The above description sets forth numerous specific details such as examples of specific systems components methods and so forth in order to provide a good understanding of several embodiments of the present invention. It will be apparent to one skilled in the art however that at least some embodiments of the present invention may be practiced without these specific details. In other instances well known components or methods are not described in detail or are presented in simple block diagram format in order to avoid unnecessarily obscuring the present invention. Thus the specific details set forth are merely exemplary. Particular implementations may vary from these exemplary details and still be contemplated to be within the scope of the present invention.

Embodiments of the present invention include various operations which are described above. These operations may be performed by hardware components software firmware or a combination thereof. As used herein the term coupled to may mean coupled directly or indirectly through one or more intervening components. Any of the signals provided over various buses described herein may be time multiplexed with other signals and provided over one or more common buses. Additionally the interconnection between circuit components or blocks may be shown as buses or as single signal lines. Each of the buses may alternatively be one or more single signal lines and each of the single signal lines may alternatively be buses.

Certain embodiments may be implemented as a computer program product that may include instructions stored on a machine readable medium. These instructions may be used to program a general purpose or special purpose processor to perform the described operations. A machine readable medium includes any mechanism for storing or transmitting information in a form e.g. software processing application readable by a machine e.g. a computer . The machine readable medium may include but is not limited to magnetic storage medium e.g. floppy diskette optical storage medium e.g. CD ROM magneto optical storage medium read only memory ROM random access memory RAM erasable programmable memory e.g. EPROM and EEPROM flash memory or another type of medium suitable for storing electronic instructions.

Additionally some embodiments may be practiced in distributed computing environments where the machine readable medium is stored on and or executed by more than one computer system. In addition the information transferred between computer systems may either be pulled or pushed across the communication medium connecting the computer systems.

The digital processing devices described herein may include one or more general purpose processing devices such as a microprocessor or central processing unit a controller or the like. Alternatively the digital processing device may include one or more special purpose processing devices such as a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or the like. In an alternative embodiment for example the digital processing device may be a network processor having multiple processors including a core unit and multiple microengines. Additionally the digital processing device may include any combination of general purpose processing devices and special purpose processing device s .

Although the operations of the methods herein are shown and described in a particular order the order of the operations of each method may be altered so that certain operations may be performed in an inverse order or so that certain operation may be performed at least in part concurrently with other operations. In another embodiment instructions or sub operations of distinct operations may be in an intermittent and or alternating manner.

In the above descriptions embodiments have been described in terms of objects in an object oriented environment. It should be understood that the invention is not limited to embodiments in object oriented environments and that alternative embodiments may be implemented in other programming environments having characteristics similar to object oriented concepts.

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. It will however be evident that various modifications and changes may be made thereto without departing from the broader scope of the invention as set forth in the appended claims. The specification and drawings are accordingly to be regarded in an illustrative sense rather than a restrictive sense.

