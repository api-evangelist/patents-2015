---

title: State machine controlled dynamic distributed computing
abstract: A computer readable storage medium with executable instructions specifies the execution of a state machine operating across a set of computing nodes in a distributed computing system. The executable instructions execute a set of operators, where the execution of each operator is under the control of a state machine that periodically invokes pause control states to pause the execution of an operator in response to a violation of a service level agreement specifying an operating condition threshold within the distributed computing system. Partitions of input data are formed that are worked on independently within the distributed computing system. A set of data batches associated with the input data is processed. Data partition control states to process the partitions associated with the set of data batches are specified. Key control states to process a set of keys associated with a data partition of the partitions are defined.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09438485&OS=09438485&RS=09438485
owner: JOVIANDATA, INC.
number: 09438485
owner_city: San Jose
owner_country: US
publication_date: 20150213
---
This application is a continuation of U.S. patent application Ser. No. 13 721 902 filed Dec. 20 2012 and now issued as U.S. Pat. No. 8 984 327 which is a continuation of U.S. patent application Ser. No. 13 315 500 filed Dec. 9 2011 and now issued as U.S. Pat. No. 8 352 775 which is a continuation of U.S. patent application Ser. No. 12 366 958 filed Feb. 6 2009 and now U.S. Pat. No. 8 090 974 which claims priority to U.S. Provisional Patent Application Ser. No. 61 027 357 filed Feb. 8 2008 entitled Apparatus and Method for Large Scale Distributed Query Processing with Dynamic Data Operators Task Processing and Tuple Routing the contents of all of which are incorporated herein by reference.

This application is related to the commonly owned patent application entitled System for Pause and Play Dynamic Distributed Computing filed Feb. 6 2009 Ser. No. 12 366 937 now U.S. Pat. No. 8 090 741.

This invention relates generally to distributed computing. More particularly this invention relates to discontinuous operator execution and dynamic resource allocation in a distributed computing environment.

In the context of this document distributed computing refers to hardware and software systems containing multiple processing elements and concurrent processes running under loose control. In particular in distributed computing a program is split into parts that run simultaneously on multiple computers communicating over a network. In contrast parallel computing involves simultaneously running program segments on multiple processors of a single machine. Distributed computing must address heterogeneous environments network links of varying latencies and unpredictable failures within the network of computers.

A query processing task to be performed in a distributed environment is split into operators. An operator is a unit of work to complete a sub task associated with the task. The unit of work may be an operational code opcode or set of opcodes. An opcode is the portion of a machine language instruction that specifies an operation to be performed. The specification and format of an operator are defined by the instruction set architecture of the underlying processor. A collection of operators forms a data processing operation that executes in a pipelined fashion. An operator works on objects. As used herein an object refers to operands or data that are processed by an operator. In a distributed computing environment objects are commonly processed as batches partitions keys and rows. A batch is a large collection of data. Partitions define the division of data within a batch. Keys correlate a set of data within a partition. Each key has an associated set of data typically in one or more rows or tuples.

Existing distributed computing systems execute query processing tasks in accordance with a static set of resources and a static sequence of operator execution. illustrates a distributed computing workflow utilized in accordance with the prior art. A daily statistics collector produces statistics regarding source data e.g. tables in the distributed computing environment. This results in data distribution statistics . A parser parses a query e.g. a task to be computed in the distributed computing environment. The parsed or divided query is then processed by a compiler . The compiler divides the task into operators. This operation relies upon the data distribution statistics and execution statistics. In particular the compiler uses sophisticated compilation strategies to generate the best distributed processing resource utilization plan for the operators. The operators are then executed . Execution statistics are then generated and stored.

The technique illustrated in relies upon data distribution statistics characterizing past operation of the distributed computing environment. In other words the execution plan does not rely upon the current state of the distributed computing environment. The execution plan also relies upon a static resource allocation based upon past network performance. In addition a static order of operator execution is utilized. The static nature of this approach does not accommodate existing situations in the distributed computing environment.

The preceding paragraph discussed query processing in particular because query processing has the most formal model of execution. However the problem of static resource allocation applies to distributed programs in general.

It would be desirable to execute tasks in a distributed computing environment in a manner that addresses the existing state of the environment. More particularly it would be desirable to dynamically allocate resources in a distributed computing environment in response to discontinuous operator execution that surveys existing conditions in a distributed computing environment.

A computer readable storage medium with executable instructions specifies the execution of a state machine operating across a set of computing nodes in a distributed computing system. The executable instructions execute a set of operators where the execution of each operator is under the control of a state machine that periodically invokes pause control states to pause the execution of an operator in response to a violation of a service level agreement specifying an operating condition threshold within the distributed computing system. Partitions of input data are formed that are worked on independently within the distributed computing system. A set of data batches associated with the input data is processed. Data partition control states to process the partitions associated with the set of data batches are specified. Key control states to process a set of keys associated with a data partition of the partitions are defined.

While the pause and play operations of the invention introduce some delay the delay is minor in the context of the long running operators the system executes and the total execution time of a job is typically reduced through opportunistic dynamic resource allocation. This is the case because the distributed computing environment is processing large tasks and large volumes of data.

The next operation of is operator execution . This operation may be invoked by an API call to play or execute an operator. During execution additional API calls associated with the operator provide Service Level Agreement SLA reports . A Service Level Agreement SLA of the invention specifies a set of operating condition thresholds within the distributed computing environment. These operating condition thresholds may be based upon customer requirements and or expected behavior in the distributed computing environment. For example an SLA may specify that an operator must access a computing resource in a predetermined period of time. Alternately an SLA may specify that an operator must complete execution in a predetermined period of time. Another SLA may be configured to identify skew in operator execution on different partitions. Violations of SLAs are reported to an SLA data analysis module via a reporting API.

The execution of the operator of the invention is not continuous. Rather it is under the control of a state machine accessed through the API. The state machine waits for triggering events to proceed with processing as discussed below.

Any operator that implements pause and play operations of the invention executes in a paranoid world. Instead of blind optimism it continuously checks with the SLA data analysis module for any changes to its execution plan. In essence it does not assume a continuously stable resource map. The sequence of SLA data analysis checks and reactions are non trivial since they have to be executed in the right points in the lifetime of the operator. If done at random times these checks will lead to physical and logical inconsistencies. The state machine organizes various states of the pause and play operation. The state machine guarantees correct results even if the operator has to adapt to changes in the data distribution and resource map.

The SLA data analysis module is a form of back office for pause and play operators of the invention. As the operators continuously report their current characteristics to the back office it collects all the data and analyzes it alongside data from other operators. After this analysis if a specific operator is in violation of pre set SLAs the operator is paused to execute an adaptability action. Thus the SLA data analysis module independently and asynchronously analyzes system wide information to impose SLAs for pause and play operators.

Based upon an analysis of an operator s SLA reports adaptability instructions are sent to operators in accordance with protocols . The protocols react to reports from the SLA data analysis module and issue instructions back to an executing operator. In particular the protocols interface with the state machines associated with operators. The protocols marry the adaptive actions specified by the SLA data analysis module with the states in the pause and play API. Thus as shown in pause and play reaction state management may pause operator execution.

The processing operations of are typically deployed on various nodes of a distributed computing environment. For example a parser may be on one node a compiler may be on another node a pause and play execution module may be on another node while the SLA data analysis module is on still another node. Modules may be combined on given nodes or may be fully distributed. It is the operations of the invention that are significant not the precise manner or location of operation execution.

As previously indicated most data processing query compilers generate operator plans based upon some prior data such as statistics or access paths. Based upon this information the operators have a static order of execution as well as near static resource allocation. Any mistakes made during compilation are not corrected during execution. At most there are attempts to learn from previous mistakes after execution and correct those mistakes with subsequent query processing. The invention redefines operators to implement in flight correction and adaptability. With the invention there is no static access path no pre set resource map and no static location in the cluster. The operator implements a series of states that allows it to adapt to a universe of transient resources.

It should be appreciated that the operations of are performed among many nodes processing resources within a distributed computing environment. The state transitions forming the state machine of may be implemented as different processing modules on different nodes. For example the batch open state may be a module of executable instructions performed on a first node while the partition iterator state may be another module of executable instructions performed on a second node. Control of state transitions may be implemented in the different modules or a separate module not shown . It should be noted that once a batch is opened partitions within a batch can be processed concurrently. Similarly once a partition is opened keys within a partition can be processed concurrently.

The problem with the system of is that if there are too many values for a specific batch partition or key then processing can be severely degraded. In addition some resources can get flooded during operator execution. This leads to operators running for extended periods of time and then possibly failing in the midst of processing data. For a complex operator conventional systems need to discard work and restart processing for the entire batch of data. Finally for operators that take hours or longer to run execution on a large distributed system increases the possibility of physical failures during operation execution. In this case restarting from scratch is a very prohibitive operation both because of the cost of rolling back any work that was done up to the point of failure and because of the cost of re running the operator.

The invention addresses these problems by making no assumption about a resource s CPU disk network memory characteristics or the behavior of data distribution skew . Instead the invention relies upon execution visibility. Execution dynamics are analyzed throughout the processing of an operator. For example every action by the operator on behalf of a batch partition or key may be reported back to the data analysis module . This may be thought of as adding state conditions to the processing of .

As previously indicated the invention is implemented to assume various data distribution skews and resource disruptions in the distributed computing environment. Therefore it is desirable to record partial progress in the execution of an operator. This partial progress represents success indicators. illustrates the state machine of but with additional states of partition success and key success . In this embodiment to prepare for unforeseen events the operator logs the successful completion of a partition and a key. In particular the boundaries for status logging are defined as the closing point of each key and each partition. The successful completion of a specific batch partition and key is used by the data analysis module during restart processing in the event of an adaptability cycle. In particular the stored state is used to recommence operator processing from the point of paused processing.

Since an operator can be invoked many times for the same data the operator may need to first verify whether it has seen a particular data item. Therefore a replay status state is introduced. For example illustrates partition replay status and key replay status . In this state the operator first checks the status of a given key within the context of a particular batch and partition. Similarly for a partition the operator checks the status of a partition before processing it. If the partition or key has already been processed it will not be processed at all and the iterators will move to the next partition or key respectively. If the partition or key has been processed partially the operator will execute for the remainder of the data. This represents partial re play.

In summary the state machine is implemented such that states and their transitions are implemented to achieve visibility graceful pause partial replay and fidelity across replay. A typical data processing system might contain as many as 20 operators. If each operator has to implement this complex state machine the implementation may become overwhelming and impractical. Thus in one embodiment the state machine operations are abstracted and accessed via an API. The operator implementation has no awareness of the state machine or its transitions. But implementing the API guarantees that the operator is compatible with the state machine.

The following is a discussion of various methods that an operator needs to support so that the operator is executed in accordance with the state machine. The API and its implementation is the building block for a system that is flexible and ready to adapt itself to the vagaries of a loosely coupled distributed system.

Any data processing engine works by pipe lining a sequence of operators. Each operator does some logical algebraic computation work to contribute to the answer to the user query. In conventional systems Operator foo asks for data from Operator bar. Operator bar waits until Operator foo returns data. In the case of serial programs located in the same memory space foo and bar have prior contracts on which memory locations will be used for doing the data transfer. foo calls bar and returns with an answer. With the invention once initiated operators work in their own domain and fill up an intermediate area with data. In the case of a cluster of machines once an operator starts on a certain machine node as a thread or a process it remains at that location for the life of the operator. It works on preset partitions of data and fills up the intermediate area. The operator that produces the data is called the producer and the operator that consumes the data is called consumer.

Another generic operation in the parallel world is to create clones of the operator. Each clone does the same operation but on a different part of the data. Clones can be distinguished from each other by their physical manifestation i.e. process ID or thread ID . Another identity function is the portion of the data that a particular clone is working on. An operator clone may start off working on a certain set of keys but might change its key range if the system decides that the operator is either overwhelmed or underwhelmed. In a distributed system an operator clone may start off working on Machine N1 migrate to N2 and end up at some machine N6. Location change is an important part of utilizing the cluster as a whole. The location is dictated by the partition of data on which the clone is working.

The following is an exemplary list of generic APIs that are the starting point for a pause and play API.

Each operator gets input data in batches. The source of this batch of data is either another operator or customer data pushed into the system. A sample Java implementation of an operator interface for processing batches is shown below 

In one embodiment operators do their work through clones. Copies of logical operators are instantiated as clones on any compute resource in a distributed system. Incoming data is partitioned by predefined attributes of each tuple. The clones are mapped to work on a specific partition of data. Each clone executes the same logical algorithm on a different partition of data. Clones and partitions are very similar to a conventional data processing operator. The conventional operator API is enhanced in pause and play with additional state saving parameters passed to the operator. A sample Java implementation of an operator interface for processing partitions is shown below.

In the table below we show the framework code that implements the call to PartitionOpen. This code snippet is responsible for handling calls to PartitionRestart if this happens to be a re run of the job on a particular partition. The code snippet is also responsible for calling PartitionOpen both with and without saved restart state.

When multiple keys within a partition are targeted by a query the framework iterates over these input keys and invokes operator specific KeyOpen and KeyClose functions with each key. If multiple values are specified with each key then the KeyValueNext interface on the operator is invoked for each of these values. Both the operator interface and the framework implementation for per key iteration are similar to the corresponding per partition sections specified above.

Key selection is also important for the compute location of an operator s clones. The key is used to map to a partition via a distributed index defined on the key. The partitions of data are moved periodically to load balance the system. The Clones are moved along with the data. Movement of live operators i.e. clones with the data without stopping the query is an innovative feature associated with an aspect of the invention.

A clone of an operator is defined by the partition of data that it is processing. Clones should not be confused with threads. Even though a clone is executed as a separate thread on some node the clone s defining property is not really the thread. In its lifetime a clone could move across many nodes. This movement means that there is no 1 to 1 mapping between a thread and a clone.

An operator s data is partitioned across the cluster using some data boundaries. These boundaries are not permanent. A partition could merge with a neighboring partition or might get sub divided into multiple partitions. This could lead to migration of a partition of data from one machine to another machine in the cluster. Clones of an operator are tied to partitions of data using the key identity. In conventional systems operators assume that their data is available on a static location throughout their lifetime. With the invention intermediate data access is through partition location methods. With these methods an operator s code does not make an assumption about the location of intermediate data.

With the APIs shown above an operator implementation has flexibility to repair itself relocate or restart at anytime. The operators work with a lightweight state. This state enables operator clones to migrate at any point in their lifetime. This migration is important as operator clones adopt to any data characteristics. If and when the data distribution changes each operator clone has the capability to reinstall the state on a new node when it is migrated from one node to another.

Attention now returns to the SLA data analysis module which processes SLA violations and issues control information to state machines associated with operators. At times the SLA data analysis module is referred to herein as a controller.

Referring to a work queue is submitted for batch processing . An individual batch is executed . If there are no SLA violations No then processing is done . If there are SLA violations Yes then it is determined whether a harvest or yield threshold is met . If so processing is done . If not the repetition count of SLA violations is compared to a threshold . If there are too many repetitions of a particular violation notice is given . If there are an acceptable number of repetitions for SLA violations then an SLA failure is isolated and analyzed . A fix is issued notice is given and the uncompleted work is returned to the work queue . In addition data availability is analyzed . If necessary a copy of the data is secured. Data status is evaluated and notice is given . The incomplete work is then placed in the work queue .

If violations prevent meeting a harvest requirement an attempt is made to change the properties of the underlying data in order to meet SLAs on a subsequent operator run. This modification of properties is based on a detailed analysis of the violation that occurred. The data used for analysis may include clock time CPU cycles I O latency throughput rates and data size analyses of both the data that is operated upon and the data that the operator brings with it. These statistics are preferably recorded at the granularity of partitions keys and key values.

The analysis of SLA partition failures may be implemented in a similar manner as shown in . A partition is analyzed . If there is no partition SLA violation No then a batch is analyzed . Otherwise it is determined if a partition store optimization would be helpful . If so the partition store is optimized and then a batch is analyzed . If a partition store optimization will not be helpful then it is determined whether an increase in partition resources would be helpful . If so movement to a different partition is considered . If this would be helpful the key is moved to a different partition . Otherwise a partition split is considered . If this would be helpful the partition is split . Otherwise a new node is considered . If this would be helpful then the key is moved to a new node . If the foregoing resource considerations will not solve the problem then an administrator is notified .

Batch SLA failures may also be analyzed by the SLA data analysis module as shown in . A batch is analyzed . If there is no batch SLA violation No then control is returned to the work queue . Otherwise a volume store optimization is considered . If a volume store optimization would be helpful then the volume store is optimized and the batch is added to the work queue . If a volume store optimization will not be helpful additional resources are considered . If additional resources will be helpful additional resources are allocated and work is rebalanced . Otherwise an administrator is notified .

As discussed in connection with protocols of the invention implement operations specified by the SLA data analysis module . Before discussing protocols of the invention please consider the following terms. A key processing flood occurs when processing on a single key is overwhelmed by either the amount of data in the key or the characteristics of the data in the key. Similarly a partition processing flood occurs when processing on a single partition is overwhelmed by either the amount of data in the partition or the characteristics of the data in the partition. An embodiment of the invention utilizes the following mechanisms to implement the following protocols.

Key processing flood restart occurs as a result of action taken by the system when a key processing flood is detected. The system detects this state as an SLA violation and pauses processing of the key. After taking repair actions the system restarts key processing. Key processing flood repair also occurs as a result of action taken by the system when a key processing flood is detected. The job is paused and the system carries out automatic repair actions based on the SLA violation that occurred. Data stored with the key can be restructured to handle future queries better the data can be split up into multiple storage and processing silos or other such actions can be taken.

Key processing flood relocation occurs as a result of action taken by the system when a key processing flood is detected. The job is paused and the system decides whether to relocate the key to a node with different characteristics such as a node with better compute or input output capabilities.

Partition processing flood restart occurs as a result of action taken by the system when a partition processing flood is detected. The system detects this as an SLA violation and pauses processing of the partition. After taking repair actions the system restarts partition processing.

Partition processing flood key repair occurs as a result of action taken by the system when a partition processing flood is detected. The job is paused and the system carries out automatic repair actions based on the SLA violation that occurred. Data stored with the partition can be restructured to handle future queries better. For example the data can be split up into multiple storage and processing silos.

Partition processing flood relocation occurs as a result of action taken by the system when a partition processing flood is detected. The job is paused. The system may decide to relocate the partition with different characteristics such as better compute or JO capabilities.

The invention allows operators to easily pause and restart from the point they left off with the resolution of up to a single key. Operators may also store their own restart state to get even finer restart control. There are a number of benefits associated with this adaptability. Instead of relying on stale statistics collected many hours ago the system uses real time data to make better inline decisions during runtime. When decisions are made about keys or partitions being overused these decisions are based on real runtime data not on imprecise statistics. This means that any adjustments made during query run are much more likely to help. If there is a single overburdened key or partition one can isolate and repair or improve the performance of the problem data. This means that any impact on query performance can be limited to the problem key or partition alone. Advantageously repair operations only impact a small amount of data and can be targeted precisely. If the query that has some paused components is part of a larger query workflow then downstream queries can start processing the data that has been produced already. Depending on how long downstream queries take there may not be any overall slowdown in the eventual user response. Installations can start very small and grow as needed to use more compute and storage resources. This helps installations to be need based and adjust as opposed to the current situation where installation sizes are planned based on forecasts that are very hard to get right. Bursts in query traffic can be handled by adapting the system to the new traffic pattern. A later reduction in the volume or resource needs of queries will make the system adapt by reducing the number of resources available to it. All of this helps system cost by only using resources as necessary.

SLA violations may be characterized in three broad categories availability SLAs absolute SLAs and relative SLAs. Availability SLAs relate to a scenario where part of the input data or working data is not available because of hardware component failures or software failures that impact parts of the data store. Absolute SLAs relate to the fact that every key partition and batch is given a maximum amount of time to run. Each of these objects is also given a maximum in terms of other resources such as disk storage used. Any violation is reported. Relative SLAs define a threshold multiplier that specifies the maximum differential permitted between the processing time of a specific object and the average processing time for all objects of that type. This helps keep track of less than optimal resource distribution even when all resources are meeting their absolute SLA requirements. This in turn helps the system optimize these resources early before any problems show up.

For availability SLAs the repair operation is fairly simple notify the analysis module that the data is no longer available and wait for recovery attempts to complete. For absolute and relative SLA violations possible repair operations are more complicated to undertake. If a relatively small percentage of objects is seeing problems then an analysis of the statistics reported by all objects may find that relocating these objects to parts of the system that are underutilized may help. If a large percentage of objects is seeing problems then this is indicative of a modification to the input data or to the kind of queries being run that can only be improved by the addition of new resources to the system followed by the redistribution of existing objects across the newly available set of resources. The system has access to some reserve nodes that can be deployed in this situation. If this is not sufficient to handle the new load then administrative intervention may be required to substantially increase the provisioning of resources to the system.

The following operations characterize the execution of a batch where there are no reported SLA violations. This details the normal execution path and sets the stage for later descriptions of execution paths where problems are detected and corrected.

Key Flood Restart Repair Relocation in this scenario a detailed look at the flow of the system is considered in the event that a single key sees an SLA violation. Only those parts of the workflow that are different than the common case are listed.

Partition Flood Restart Repair and Relocation in this scenario we take a detailed look at the flow of the system when a single partition sees an SLA violation. We only enumerate those parts of the workflow that are different than the common case.

As each partition is scheduled it goes through the following stages. In the steps below the partition being worked on is referred to as P and the key being worked on in the partition is referred to as PK .

In the cases where the batch has been re queued into the work queue the work scheduler picks up this job from the work queue and starts processing it.

The general strategy of pausing operators on particular objects does not normally impact the concurrent usage of those objects by other batches that are being processed at the same time. The data store uses an online replication strategy that permits objects to change their physical locations without impacting object access.

When there is an availability issue for sections of the store all future operators will also need to pause at similar places in their execution. The disclosed techniques of retrying the batches that don t meet harvest requirements a few times and reporting success on others with the appropriate reporting on harvest numbers works well. When considering a workflow of jobs that needs to be chained together two different approaches may be used depending on the operators in question. In one approach as batches execute and complete partially data is published for operators down the chain to consume. If a batch needs to get retried then as those retries publish new bits of data that also will be passed down the line for antecedent operators to consume. This works well if the operators don t have any requirements of needing to get all the data at once. Other operators require a stricter chaining and execute only on success of the preceding operators in the workflow. In these cases only after a batch has completed successfully is the following operator scheduled. These scheduling decisions are taken by the work scheduler based on the configured properties of the operators and the workflow.

An embodiment of the present invention relates to a computer storage product with a computer readable medium having computer code thereon for performing various computer implemented operations. The media and computer code may be those specially designed and constructed for the purposes of the present invention or they may be of the kind well known and available to those having skill in the computer software arts. Examples of computer readable media include but are not limited to magnetic media such as hard disks floppy disks and magnetic tape optical media such as CD ROMs DVDs and holographic devices magneto optical media and hardware devices that are specially configured to store and execute program code such as application specific integrated circuits ASICs programmable logic devices PLDs and ROM and RAM devices. Examples of computer code include machine code such as produced by a compiler and files containing higher level code that are executed by a computer using an interpreter. For example an embodiment of the invention may be implemented using Java C or other object oriented programming language and development tools. Another embodiment of the invention may be implemented in hardwired circuitry in place of or in combination with machine executable software instructions.

The foregoing description for purposes of explanation used specific nomenclature to provide a thorough understanding of the invention. However it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. Thus the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed obviously many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications they thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. It is intended that the following claims and their equivalents define the scope of the invention.

