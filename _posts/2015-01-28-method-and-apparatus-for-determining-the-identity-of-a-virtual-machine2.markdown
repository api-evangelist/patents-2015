---

title: Method and apparatus for determining the identity of a virtual machine
abstract: A hypervisor preferably provides VM (virtual machine) identification, priority and LUN/LBA range information to the HBA (host bus adapter) when a VM is created. Alternatively, the HBA can determine that a LUN/LBA range is new and request VM identity, priority and LUN/LBA range from the hypervisor. The HBA creates a table containing the VM identification, priority and LUN/LBA range. The HBA then detects operations directed to the LUN/LBA range and does a lookup to determine VM identification and priority. VM identification and priority are then mapped into a field in a frame using a unique identifier. The unique identifier can be placed using reserved bits on the existing Fiber Channel (FC) header or can use bits in an additional header, such as a modified IFR header or an optional device header. The VM identification aware HBAs register with the NS.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582310&OS=09582310&RS=09582310
owner: Brocade Communications Systems, Inc.
number: 09582310
owner_city: San Jose
owner_country: US
publication_date: 20150128
---
This is related to U.S. patent application Ser. No. 12 838 624 entitled Method and Apparatus for Providing Virtual Machine Information to a Network Interface filed Jul. 19 2010 which application claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Patent Application Ser. No. 61 228 127 entitled Virtual Machine Identification in Packets Transmitted over a Network filed Jul. 23 2009 both of which are hereby incorporated by reference.

This application is also related to U.S. patent application Ser. No. 12 838 627 entitled Method and Apparatus for Determining the Identity of a Virtual Machine filed Jul. 19 2010 which is hereby incorporated by reference.

This application is also related to U.S. patent application Ser. No. 14 608 001 entitled Method and Apparatus for Providing Virtual Machine Information to a Network Interface and Ser. No. 14 608 013 entitled Method and Apparatus for Registering and Storing Virtual Machine Unique Information Capabilities both filed on the same day as this application and both hereby incorporated by reference.

The present invention relates generally to storage area networks. Particularly the present invention relates to operation of storage area networks with attached hosts running virtualization software and having a plurality of virtual machines.

Virtual machines VMs are being used in increasing numbers in networks. They are advantageous because they maximize the use of the hardware resources in the network particularly the host or server hardware. However the use of virtual machines presents problems when the host machine is connected to a storage area network SAN . For a number of reasons it is desirable to have visibility of the particular virtual machines in the various hosts on the SAN. These reasons include simplified management through the use of a single management tool cost back charging relating to resource use service level agreement enforcement isolation and improved quality of service QoS or prioritization of the communications for given VMs.

Current VM hypervisors do not readily provide this capability. For example in VMware the VMs can be separately identified on the SAN if they use the NPIV features provided by the host bus adaptors HBAs . But to use NPIV the VM must be setup to use raw device mapping RDM of the hypervisor. This results in management difficulties in both the hypervisor and on the SAN. On the SAN zoning becomes very complicated as each VM must be operated on individually. Similarly SAN QoS is also more difficult to manage because of the individual nature of the VMs and their NPIV addresses. In addition as the environment scales up the problems increase at a greater rate.

VMware ESX the prevalent hypervisor provides an alternate technique referred to as VMFS or virtual machine file system. It is much easier to administer VMs when VMFS is used so the majority of server administrators would prefer to utilize VMFS. But VMFS does not allow identification on the SAN of the individual VMs. Currently NPIV cannot be used even with its attendant SAN management issues. So the inability to manage charge back and so on has limited the use of hypervisors using VMFS operation on the SAN.

Similar issues are present with Hyper V from Microsoft and its clustered shared volume CSV file system and XenServer from Citrix with the Control Domain and Storage Repositories.

As VMFS or CSV depending on the hypervisor is the greatly preferred technique for providing storage resources in a hypervisor it would be desirable to be able to better operate with VMFS or CSV based systems on a SAN.

According the embodiments of the present invention the hypervisor preferably provides VM identification priority and LUN LBA range information to the HBA or network interface when a VM is created and provides VM identification at the beginning of each new command. Alternatively the HBA or network interface can determine that a VM or LUN LBA range is new and request VM identity priority and LUN LBA range from the hypervisor. The HBA creates a table containing the VM identification priority and LUN LBA range. The HBA then detects operations directed to the VM or LUN LBA range and does a lookup to determine priority. VM identification and priority are then mapped into a field in a frame using a unique identifier. The unique identifier can either be placed using reserved bits on the existing Fibre Channel FC header or can use bits in an additional header such as a modified IFR header or an optional device header. With the unique identifier in the frame fabric wide handling of the frames for QoS is greatly simplified as the unique identifier can be directly mapped to SLAs and priority levels. Additionally statistics based on the frames can also readily be developed based on particular VMs to allow greatly improved chargeback mechanisms and the like. Further the presence of the unique identifier allows improved management of the SAN as operations can be traced back directly to individual VMs not just physical hosts for operations such as zoning and access control.

The unique identifier can also be used in the storage devices. One particular use is to incorporate the VM instance into the caching algorithm with per VM caching not just per host caching.

The VM identification capability of the HBA can be registered with the name server to allow querying to determine the presence of other VM identification capable HBAs. The optional device header then can be added if the target is also VM identification aware.

The host includes a hypervisor which executes a virtual machine file system VMFS . A series of virtual machines in VM VM execute on top of the VMFS . Similarly the host includes a hypervisor a VMFS and virtual machines VM VM .

Illustrated in is a path from VM to storage unit . This path indicated by the long dashed line traverses the VMFS to the hypervisor the link the switch the link the switch and the link . VM has a similar path to storage unit . VM has a path which traverses the VMFS the hypervisor the link the switch the link the switch and the link to reach storage unit . The VM has a path which traverses the VMFS the hypervisor the link the switch and the link to the storage unit . A path connects VM to VM by traversing the VMFS the hypervisor HBA A not shown in host the link the switch the link the switch the link an HBA B not shown in host hypervisor and VMFS .

Packets or frames the terms being used synonymously in this description of VM and VM travel identical routes to the storage unit so it is very difficult to determine which packets were related to which path and therefore it is very difficult to prioritize the two sets of packets differently. VM in the host and VM in the host use different routes to contact storage unit and would have different source addresses but if VM were transferred to host using VMotion then the paths would align and the same difficulty would appear as with VM and VM.

In a host B executing a hypervisor which is operating with an SCSI network interface card NIC hardware which in turn is connected to an iSCSI SAN which is connected to the storage unit . Elements and items similar to those shown in receive the same numbering with the addition of the letter B. Thus VMFS B is connected to an iSCSI driver . The iSCSI driver includes an iSCSI API which operates similarly to the HBA API .

In a host C executing a hypervisor is connected to converged network adapter CNA hardware which is connected to a Fibre Channel over Ethernet FCoE SAN which is connected to the storage unit . Similar elements in to those of are numbered similarly and end with the letter C. The VMFS layer C is connected to an FCP driver C with the FCP driver C connected to a CNA driver . The CNA driver is connected to the CNA hardware . The CNA driver includes an FCoE API which operates similarly to the HBA API .

In at step the VMFS A receives a SCSI command provided from the guest operating system A through the SCSI driver A and the SCSI virtualization layer A. In step the VMFS A determines if virtual machine identification is needed to be provided in the SCSI command. If so control proceeds to step where the VMFS A places VM identification values in either the SCSI CDB or an extended CDB. If no identification is needed or used as done in selected embodiments or after the VM identification has been provided in step control proceeds to step where the VMFS A provides the SCSI CDB to the FCP driver A.

Return frames from the storage unit can be developed at least two different ways. First the storage unit can include an HBA similar to HBA in that it can provide the unique identifier in any return frames. The storage unit HBA stores the unique identifier information in its context tables and builds the proper frame structure to allow the inclusion of the unique identifier. Second if the storage unit cannot provide the unique identifier the switches that form the FC SAN can monitor for return frames having a D ID and OXID that match the S ID and OXID of the frames that included the unique identifier. Upon detecting the D ID and OXID match for a frame that does not include the unique identifier the switch can then reformat the frame to include the unique identifier. This allows the various operations to be done on both flow directions.

An alternative to the HBA doing the command snooping and the placement of the unique identifier in the frame is to have the snooping and unique identifier insertion done by the switch connected to the HBA . The switch needs to receive the VM identification priority and LUN LBA range to allow the snooping of received frames. The snooping is much like that done by the HBA in step except that it is done on the normal frames provided by the HBA . In one variation the VM identification priority and LUN LBA range are provided from the HBA to the switch in command packets so that the HBA retains the interface with the VM. In this case the switch will also communicate with the HBA to request the VM identification priority and LUN LBA range for frames that miss the table in the switch. The HBA will do the query described above and provide the information to the switch. This variation minimizes the work being performed in the HBA to just the simple interfaces with the VM and leaves the snooping and frame development to the more powerful switch. A second variation has the hypervisor providing the VM identification priority and LUN LBA range directly to the switch. In this variation the APIs are effectively between the switch and the hypervisor not the HBA and the VMFS. This is less desirable as new commands and the like have to be developed for both the hypervisor and the switches. A third variation has the hypervisor and the switch cooperating with a management entity which effectively has the APIs shown in the HBA of . This is simpler than the second variation as the interfaces will be more easily developed but will require the constant operation of the management entity.

The frame provided to the fabric includes the unique identifier of the VM. The various devices in the fabric can examine the frame to determine the unique identifier and use that as an entry into tables which define the priority and handling of the frame. This information is provided across the fabric using a management tool which can select a VM from the information present in the HBA and then propagate necessary priority and handling information appropriate for each device in the fabric to those devices. Thus the user or administrator need only use one management tool to track the VM through the SAN and then obtain desired information such as traffic information used for charging back to the proper department. The management tool will also be able to simply define the SLA of the VM and set the priority and handling of the frames across the fabric accordingly. And it is noted that all of this is done with the hypervisor using a file system such as VMFS which does not readily provide information about the VMs to the HBA. It is also noted that no changes need to be made to modules such as VMFS. The minimal operation uses an API from the HBA driver back into the hypervisor via the hypervisor storage API with the preferred operation also including the hypervisor proactively providing VM information to the HBA driver on VM creation or modification.

While the above description has focused on operations using the FC HBA similar operations occur with iSCSI and FCoE variations with the iSCSI driver and iSCSI NIC hardware or CNA driver and CNA hardware being substituted for the HBA driver and HBA hardware . Similarly switch operations for the embodiments would be done by the Ethernet switches forming the iSCSI SAN or FCoE SAN . In iSCSI frames the unique identifier can be placed in a new tag similar to a VLAN tag as shown in or at some possible location in the frame. In FCoE frames the unique identifier can be placed in the FC frame as described above as shown in .

Various fabric level operations can be performed using the unique identification value representing the VM provided in the frames. These include quality of service QoS encryption and or compression by VM zoning access control migration of VMs between hosts in the same or different data centers fabrics or network clouds and other VMotion aspects improved statistics by VM and federated management of the SAN.

The following U.S. patents or applications are incorporated by reference to provide further details relating to QoS usage of the VMs U.S. Pat. No. 7 239 641 entitled QUALITY OF SERVICE USING VIRTUAL CHANNEL TRANSLATION U.S. Pat. No. 7 426 561 entitled CONFIGURABLE ASSIGNMENTS OF WEIGHTS FOR EFFICIENT NETWORK ROUTING Ser. No. 11 782 894 filed Jul. 25 2007 entitled METHOD AND APPARATUS FOR DETERMINING BANDWIDTH CONSUMING FRAME FLOWS IN A NETWORK Ser. No. 11 674 637 filed Feb. 13 2007 entitled QUALITY OF SERVICE USING VIRTUAL CHANNEL TRANSLATION Ser. No. 12 119 440 filed May 12 2008 entitled AUTOMATIC ADJUSTMENT OF LOGICAL CHANNELS IN A FIBRE CHANNEL NETWORK Ser. No. 12 119 436 filed May 12 2008 entitled METHOD AND SYSTEM FOR FACILITATING APPLICATION ORIENTED QUALITY OF SERVICE IN A FIBRE CHANNEL NETWORK Ser. No. 12 119 448 filed May 12 2008 entitled METHOD AND SYSTEM FOR CONGESTION MANAGEMENT IN A FIBRE CHANNEL NETWORK Ser. No. 12 119 457 filed May 12 2008 entitled WORKLOAD MANAGEMENT WITH NETWORK DYNAMICS and Ser. No. 12 119 430 filed May 12 2008 entitled METHOD AND SYSTEM FOR FACILITATING QUALITY OF SERVICE IN EDGE DEVICES IN A FIBRE CHANNEL NETWORK. 

The following U.S. patent is incorporated by reference to provide further details relating to encryption and or compression usage of the VMs U.S. Pat. No. 7 533 256 entitled METHOD AND APPARATUS FOR ENCRYPTION OF DATA ON STORAGE UNITS USING DEVICES INSIDE A STORAGE AREA NETWORK FABRIC. 

The following U.S. patents or applications are incorporated by reference to provide further details relating to zoning usage of the VMs U.S. Pat. No. 7 366 194 entitled FIBRE CHANNEL ZONING BY LOGICAL UNIT NUMBER IN HARDWARE and U.S. Pat. No. 7 352 740 entitled EXTENT BASED FIBRE CHANNEL ZONING IN HARDWARE. 

The following U.S. application is incorporated by reference to provide further details relating to migration and VMotion usage of the VMs Ser. No. 10 356 659 filed Jan. 31 2003 entitled METHOD AND APPARATUS FOR PROVIDING VIRTUAL PORTS WITH ATTACHED VIRTUAL DEVICES IN A STORAGE AREA NETWORK. 

The knowledge of the VMs provided in the frames can also be used by the storage devices connected to the fabric. One common operation in a storage device is caching of data. By detecting the VMs based on the unique identifier in the frames the caching algorithm employed in the storage unit can be improved by breaking down to the VM level rather than the S ID or host address level as done today. A combination of caching algorithms could be used some by address and some by VM. The details of the caching could also be varied between VMs based on priority values.

As discussed VMware ESX is used as the described embodiment but various other hypervisors can be used such as Microsoft s Hyper V with CSV other variations of VMware products and other vendor products. Further the preferred embodiment was discussed based on a FC SAN environment. Other SANs such as iSCSI and FCoE can also be used alone or in combinations as illustrated in with appropriate changes to .

The basic alternate embodiments and operations have been described above. The following description provides an additional embodiment where the VMs are identified in an optional FC header with additional general and detailed operations to provide more context for the embodiments of the present invention.

Referring to operations prior to commencing I O transfers are shown. In operation the HBA registers with VMFS and provides its capabilities. This can be done using the hypervisor storage API . In operation the HBA queries VMFS to determine the capabilities of VMFS as compared to the HBA. In operation VMFS provides the information on a newly created VM as in step of . In operation the HBA port goes online and performs initial fabric operations. In operation the HBA provides the fabric capabilities to VMFS and in operation indicates that the HBA is ready for operation. In operation VMFS begins I O operations. In operation the HBA queries for the priority and UUID of the VM involved in the I O operation similar to step of . It is understood that operation may not be needed if the UUID of the VM has previously been provided to the HBA in operation . Then I O operations commence.

In I O operations with a VMID VM identification capable target are illustrated. One example where this operation could occur is path between VMin host and VMin host . In this discussion operations between two hosts are used for description purposes but it is understood that operations would be similar between any two VMID aware devices such as a storage unit and a host or two storage units. In operation I O operations begin. In operation the initiator HBA obtains the priority and UUID if needed as in operation . The initiator HBA processes the frame and places VMID addresses in the optional header of the frame. In operation the outgoing frame which includes a priority value as indicated in operation and source and destination VMID addresses is provided to the target HBA. The target HBA processes the frame and provides a response in operation the frame including the priority and the source and destination VMIDs.

This operation is to be contrasted to that shown in where the target HBA is not VMID capable. I O operations begin in step and the initiator HBA determines the priority and UUID in operation . However as the target HBA is not VMID capable the optional header is not included and the frame is simply provided to the target HBA in operation .

In this additional embodiment the VMID is a 32 bit value with an exemplary assignment as shown in Table 1.

As shown in the preferred alternate embodiment the host receives a VMID of 0x0000h with the values 0x0001h 0x07FFh being used for the various VMs on the host. A number of values are reserved for future use a number of values are reserved for protocol use as is common in FC and the 0xFFFFh value indicates an unknown VMID usually used in initial operations until the HBA knows the relevant VMID.

Fibre Channel provides the capability to include optional headers. Optional headers are defined and described in Fibre Channel Framing and Signaling 3 FC FS 3 Rev. 0.90 dated Aug. 6 2009 from T11 particularly Section 13. This alternate embodiment places the relevant VMID values and select other values in an optional header as opposed to the CS CTL bits in or in the fabric ID fields of an IFR header in . The use of the optional header is more compatible with existing devices. The preferred optional header is a device header.

As shown in the optional header is divided into certain fields. Table 2 provided here also indicates the fields in the preferred embodiment. A TAG field should have a value of ooh. In the preferred embodiment the TAG field value of ooh indicates this is a VMID header with 01h FFh being reserved values for future use. A source VMID field and a destination VMID field are provided to hold the relevant VMIDs the values of the VMIDs being discussed above. The final field is an I O Service ID field . The value in the I O Service ID field indicates the services enabled for the source and destination VMID pair such as encryption compression and Quality of Service QoS such as high medium and low.

As Fibre Channel operations are between initiators and targets operations of VMID capable initiators and targets will be slightly different. If an initiator is VMID capable or aware the HBA will send packets with a VMID Header if the destination is VMID Target Capable register the HBA capabilities with VMFS obtain VMID feature capabilities from the VMFS obtain priority and UUID from the VMFS obtain UUID creation deletion messages from the VMFS register VMID Capabilities with the fabric request VMID allocation from the fabric and query VMID feature capabilities of HBAs from fabric. The HBA cannot explicitly un register VMID initiator capability but the port going offline will un register VMID Capability.

In a target that is VMID capable the HBA will receive VMID headers in the received packets from VMID capable initiators send back the VMID header with source and destination VMID values in response and register VMID capabilities to the fabric. The HBA cannot explicitly un register the VMID target capability but the port going offline will un register VMID Capability. It is assumed that a VMID capable target will also be a VMID capable initiator to obtain the VMIDs for the various VMS and the like.

When a port is offline the HBA on the port becomes VMID NOT Capable VMID allocations are removed for the HBA on the port and switch device RSCNs will continue to occur to notify the switches and other HBAs. When the HBA port goes online the HBA re registers VMID capability features and the HBA re requests VMID allocations. The switch preferably tries to assign the same VMID to the UUID.

In operation the HBA A registers with the Name Server NS that it is VMID initiator capable and aware. As the device types of VMID Initiator and VMID Target capable and aware are not standardized FC 4 TYPE Codes vendor specific TYPE codes can be used if desired or new standardized FC 4 TYPE codes can be allocated. In operation the NS responds with an acceptance after placing the HBA A VMID initiator capability in the NS database. In normal FC operation this capability will be replicated to all switches in the fabric based on normal NS replication. In operation the HBA A requests a VMID for UUID from the switch management server MS responsible for providing fabric addresses. Just as the fabric provides FC PIDs or addresses the fabric also provides the VMIDs. In operation the MS allocates a VMID 0x0123h in the illustration and maps that VMID to UUID. The MS coordinates with the NS so that the NS database is current and has the VMIDs and UUIDs. In operation the allocated VMID is returned to HBA A. In the preferred embodiment multiple UUIDs can be registered in a single request with the NS then returning a bulk allocation of VMIDs. When the VMID is returned from the MS the HBA A stores the VMID and UUID mapping in a table for later use in inserting VMID device headers.

In operation the HBA A queries the NS for all VMID capable devices much as an HBA queries the NS for all accessible devices during fabric login. In operation the NS returns a list of all VMID capable devices. At this time the HBA A knows the capabilities of all of the other HBAs it can access. In the illustration this means that HBA A knows that HBA B is VMID target capable but not aware and HBA C is not VMID target capable. In operation the HBA A informs VMFS that the interface is fully operational and I O operations can begin.

Having obtained the VMID the priority and I O frames the HBA A now develops the VMID device header as discussed above by using the UUID to reference the previously developed table mapping UUID to VMID and other data such as priority and LUN LBA and places the VMID device header into each frame going to the designated address if the target HBA is VMID capable as it is in the illustrated case. In operation the frames are transmitted according to the indicated priority and including the source and destination VMID values. In this case as the destination VMID is not known the value 0xFFFFh is used as the destination VMID. The HBA A transmits the frame at the desired priority according to the various methods that are available. In the preferred embodiment priority is managed by proper selection of virtual channels. The priority level may be indicated in the frame by using the CS CTL field may be determined by frame address and VMID value detection and then lookup in a table or other methods depending on the particular method employed in the fabric. If the VM begins transmitting I O operations before operations and are completed to avoid delaying the frames the HBA A can utilize the unknown VMID value for both source and destination. Then when operations and complete or the source VMID value is otherwise provided the propose source VMID value can be used.

The frame of operation is transmitted through the fabric illustrated as switch at the designated priority level. Eventually the frame arrives at HBA B the target and HBA B operates on the frame as appropriate. The HBA B develops the appropriate response frame and then develops the VMID device header the header now including a VMID value of the target VM in the host containing HBA B. In the illustration this is a VMID value of 0xFFFFh. This unknown value is used as the HBA B is VMID capable but not VMID aware that is the HBA B can handle VMID headers but does not understand or contain VMs. If the HBA B were VMID aware meaning that it understood and contained VMs then the destination VMID value would be returned such as 0x0765h. The response frame is transmitted with the proper priority information and the VMID device header in operation . When HBA A receives the frame it can store the source VMID value for use in the next frame in the I O sequence.

If the I O operation of operation was instead directed to an HBA C that is not VMID target capable then in operation HBA A transmits the frame without the VMID header or the regular frame header values set to indicate an optional header.

The above discussion has included the interactions of a VMID capable HBA with its connected switch and the designated target nodes. However there are additional operations that occur at a fabric level. illustrates at least some of those operations. Operations and VMID target capable registration and acceptance occur. The registration triggers a switch registered state change notification RSCN to be sent in operation from switch to switch. Switch responds in operation with an acceptance. Switch sends a device RSCN to HBA B in operation . HBA B responds with an acceptance in operation . As typically an RSCN contains only minimal information in operation HBA B queries the NS for all VMID capable devices. The NS responds in operation . After operation the HBA B can utilize the device RSCN of operation in conjunction with the list from operation to determine the presence of new HBA A that is VMID target capable and aware.

In operation switch also sends a device RSCN to HBA C which is not VMID capable. HBA C just accepts the RSCN in operation . As HBA C is not VMID capable it does not perform the query of operation .

While the alternate embodiment is described using Fibre Channel frames and an HBA as the exemplary format it is understood that the same optional device header could be sued in FCoE with a CNA.

The use of the optional device header in this alternate embodiment provides greater compatibility and requires fewer proprietary changes as compared to the embodiments of while also providing additional flexibility.

The ASIC comprises four major subsystems at the top level as shown in A Fibre Channel Protocol Group Subsystem a Frame Storage Subsystem a Control Subsystem and a Host System Interface . Some features of the ASIC that are not relevant to the current discussion have been omitted for clarity of the drawing.

The Fibre Channel Protocol Group FPG Subsystem comprises 5 FPG blocks each of which contains 8 port and SERDES logic blocks to a total of 40 E F and FL ports.

The Frame Data Storage FDS Subsystem contains the centralized frame buffer memory and associated data path and control logic for the ASIC . The frame memory is separated into two physical memory interfaces a header memory to hold the frame header and a frame memory to hold the payload. In addition the FDS includes a sequencer a receive FIFO buffer and a transmit buffer .

The Control Subsystem comprises a Buffer Allocation unit BAL a Header Processor Unit HPU a Table Lookup Unit Table LU a Filter and a Transmit Queue TXQ . The Control Subsystem contains the switch control path functional blocks. All arriving frame descriptors are sequenced and passed through a pipeline of the HPU filtering blocks until they reach their destination TXQ . The Control Subsystem carries out L2 switching FCR LUN Zoning LUN redirection Link Table Statistics VSAN routing and Hard Zoning.

The Host System Interface provides the host processor subsystem with a programming interface to the ASIC . It includes a Peripheral Component Interconnect Express PCIe Core a DMA engine to deliver frames and statistics to and from the host and a top level register interface block . As illustrated in the ASIC is connected to the Host Processor Subsystem via a PCIe link controlled by the PCIe Core but other architectures for connecting the ASIC to the Host Processor Subsystem can be used.

Some functionality described above can be implemented as software modules in an operating system or application running on a processor of the host processor subsystem and stored in a memory or other storage medium of the host processor subsystem . This software may be provided during manufacture of the ASIC or provided on any desired computer readable medium such as an optical disc and loaded into the ASIC at any desired time thereafter. In one embodiment the control subsystem is configured by operating system software of the network switch executing in the processor of the host processor subsystem .

Serial data is recovered by the SERDES of an FPG block and packed into ten 10 bit words that enter the FPG subsystem which is responsible for performing 8b 10b decoding CRC checking min and max length checks disparity checks etc. The FPG subsystem sends the frame to the FDS subsystem which transfers the payload of the frame into frame memory and the header portion of the frame into header memory. The location where the frame is stored is passed to the control subsystem and is used as the handle of the frame through the ASIC . The Control subsystem reads the frame header out of header memory and performs routing classification and queuing functions on the frame. Frames are queued on transmit ports based on their routing filtering and QoS. Transmit queues de queue frames for transmit when credits are available to transmit frames. When a frame is ready for transmission the Control subsystem de queues the frame from the TXQ for sending through the transmit FIFO back out through the FPG .

The Header Processor Unit HPU performs header HPU processing with a variety of applications through a programmable interface to software including a Layer2 switching b Layer3 routing FCR with complex topology c Logical Unit Number LUN remapping d LUN zoning e Hard zoning f VSAN routing g Selective egress port for QoS and g End to end statistics.

The HPU provides hardware capable of encapsulating and routing frames across inter switch links that are connected to the ports of the ASIC . The HPU performs frame header processing and Layer 3 routing table lookup functions using routing tables where routing is required encapsulating the frames based on the routing tables and routing encapsulated frames. The HPU can also bypass routing functions where normal Layer2 switching is sufficient.

Proceeding then to a general block diagram of the switch hardware and software is shown. Block indicates the hardware as previously described. Block is the basic software architecture of the switch . Generally think of this as the switch fabric operating system and all of the particular modules or drivers that are operating within that embodiment. Modules operating on the operating system are Fibre Channel switch and diagnostic drivers port modules if appropriate a driver to work with the Fibre Channel ASIC and a system module . Other switch modules include a fabric module a configuration module a phantom module to handle private public address translations an FSPF or Fibre Shortest Path First routing module an AS or alias server module an MS or management server module a name server module and a security module . Additionally the normal switch management interface is shown including web server SNMP telnet and API modules. A virtual node port module performs the node port virtualization function. This module is included in the drivers in the preferred embodiment.

General operation of the management server and the name server are well known to those skilled in the art and are extended as discussed above. Further explanation of the functions of the two servers is provided in specifications Fibre Channel Generic Service 6 FC GS 6 dated Aug. 30 2007 Fibre Channel Link Services FC LS 2 dated Jun. 26 2008 Fibre Channel Switch Fabric 5 FC SW 5 dated Jun. 3 2009 al from T11 and all of which are hereby incorporated by reference.

The above description is illustrative and not restrictive. Many variations of the invention will become apparent to those skilled in the art upon review of this disclosure. The scope of the invention should therefore be determined not with reference to the above description but instead with reference to the appended claims along with their full scope of equivalents.

