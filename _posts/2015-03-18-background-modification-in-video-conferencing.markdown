---

title: Background modification in video conferencing
abstract: Methods and systems for real-time video processing can be used in video conferencing to modify image quality of background. One example method includes the steps of receiving a video including a sequence of images, identifying at least one object of interest (e.g., a face) in one or more of the images, detecting feature reference points of the at least one object of interest, and tracking the at least one object of interest in the video. The tracking may comprise aligning a virtual face mesh to the at least one object of interest in one or more of the images. Further, a background is identified in the images by separating the at least one object of interest from each image based on the virtual face mesh. The background is then modified in each of the images by blurring, changing a resolution, colors, or other parameters.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09232189&OS=09232189&RS=09232189
owner: AVATAR MERGER SUB II, LLC.
number: 09232189
owner_city: Minneapolis
owner_country: US
publication_date: 20150318
---
This disclosure relates generally to video conferencing and more particularly to systems and methods for modifying a scene background in a video stream based on identifying and tracking participants in the video.

Today video conferencing and videophone calls are popular tools for conducting two way video and audio communications over long distances. This technology has been developing rapidly due to the emergence of high speed networking solutions inexpensive hardware components and deployment of cellular networks. Typically video conferencing allows two or more individuals to communicate with each other using a variety of software applications such as video chat applications where the participants can view each other while talking. Video chats can be available on general purpose computers mobile devices and television systems as downloadable software applications or web services. Traditional hardware requirements for video conferencing include on each side an input audio module e.g. a microphone input video module e.g. a video camera output audio module e.g. speakers output video module e.g. a display or projector and a computing device that ties together input and output modules compresses and decompresses audio and video streams and initiates and maintains the data linkage via a communications network.

Although video conferencing solutions have existed for many years there can be issues with video streaming especially in the case of congested networks. When quality of service QoS in a particular network significantly drops down the video conference can experience difficulties with delivering video in a timely fashion which may cause unwanted interruptions or significant degradation of audio and video quality. Accordingly there is still a need in the art to improve video conferencing technology.

In general this disclosure relates to the technology for video conferencing which tracks faces of individuals and transmits a video stream having the image portions associated with the faces in a higher quality than the remaining video image. In various embodiments the technology allows modifying a scene background for example by blurring and keeping a foreground associated with the faces in an original quality. Ultimately this leads to the reduction of network requirements needed for video conferencing because the modified video has a lower data rate. Depending on network congestion conditions this technology allows improving video conferencing solutions reducing the number of interruptions in video streaming and preventing degradations of video streaming.

According to one aspect of the technology a computer implemented method for real time video processing is provided. The method may comprise receiving a video including a sequence of images identifying at least one object of interest in one or more of the images detecting feature reference points of the at least one object of interest and tracking the at least one object of interest in the video. The tracking may comprise creating a virtual face mesh also referred herein to as mesh for simplicity and or aligning the mesh to the at least one object of interest in one or more of the images based on the feature reference points. Further the method proceeds to identifying a background in one or more of the images by separating the at least one object of interest from each image based on the mesh modifying the background in each of the images to generate a modified background and generating a modified video which includes the at least one object of interest and the modified background.

In some embodiments the modified background has a first image quality in the modified video and the at least one object of interest has a second image quality in the modified video where the first image quality is lower than the second image quality.

In certain embodiments the step of identifying the background may include selecting an image portion which excludes pixels associated with the mesh. The modification of the background may include one or more of the following blurring changing one or more background colors changing a background resolution changing a video dot density changing posterizaion and changing pixelization of the background. In some embodiments the modification of the background may include replacement of the background or its portion with a predetermined image.

In some embodiments the at least one object of interest includes at least a portion of an individual other than a human face. In other embodiments the at least one object of interest includes a human face.

In certain embodiments the feature reference points can include facial landmarks. In certain embodiments the feature reference points are at least one of points indicating the following eyebrows vertical position eyes vertical position eyes width eyes height eyes separation distance nose s vertical position nose pointing up mouth s vertical position mouth s width chin s width upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser.

According to yet additional embodiments the method may further include the step of compressing the background. The method may further include the step of transmitting the modified video over a communications network. In yet other embodiments the method may further include the step of receiving a request to blur the background of the video.

In some embodiments the method may further comprise monitoring QoS associated with a communications network and based on the monitoring generating a request to blur the background of the video. In other embodiments the method may further comprise dynamically monitoring a network parameter associated with transferring of the video over a communications network and generating a request to blur the background of the video if the network parameter is below a predetermined threshold value or if the network parameter is above the predetermined threshold value generating a request to transmit the video without blurring. The network parameter may include a bit rate or a network bandwidth.

In certain embodiments the modifying of the background includes gradual blurring of the background where a degree of the gradual blurring depends on the network parameter. In certain embodiments the step of identifying the at least one objects of interest may include applying a Viola Jones algorithm to the images. The step of detecting the feature reference points may include applying an Active Shape Model ASM algorithm to areas of the images associated with the at least one object of interest.

In certain embodiments the method may comprise the steps of dynamically determining a value related to QoS associated with a communications network based on the determining if the value associated with the QoS is within a first predetermined range generating a first request to blur only the background of the video if the value associated with the QoS is within a second predetermined range generating a second request to blur the background of the video and other parts of the video excluding a user face and if the value associated with the QoS is within a third predetermined range not generating a request to blur the background. Here the first range differs from the second range and the third range and the second range differs from the third range and the first range.

In yet more embodiments the step of identifying the background may comprise forming a binary mask associated with the at least one object of interest aligning the binary mask to the mesh on each image and creating an inverted binary mask by inverting the binary mask. The forming of the binary mask may comprise determining a gray value intensity of a plurality of image sections in each of the images where the plurality of image sections are associated with the mesh determining object pixels associated with the object of interest by comparing the gray value intensity of each of the image sections with a reference value applying a binary morphological closing algorithm to the object pixels and removing unwanted pixel conglomerates from the mesh. The aligning of the binary mask to the mesh may comprise making a projection of the mesh to a reference grid thereby separating the mesh into a plurality of reference grid cells associating mesh elements which correspond to reference grid cells and determining pixels of each of the images which correspond to the mesh elements.

In some embodiments the method may further comprise modifying image portions associated with the at least one object of interest in one or more of the images. The modifying of the image portions associated with the at least one object of interest can be based on the feature reference points. The modifying of the image portions associated with the at least one object of interest may include changing at least one of a color a color tone a proportion and a resolution.

In some embodiments the method may comprise the steps of determining a position of a head based on the identifying of the at least one object of interest and the reference feature points determining a position of a body based on the position of the head and tracking the position of the body over the sequence of images. The background blurring or modification can be based on the position of the body. For example if tracking of body is not feasible based on the images but tracking of user face is feasible based on the images background blurring can be based on approximation of body position.

According to another aspect of the technology a system is provided. An example system comprises a computing device including at least one processor and a memory storing processor executable codes which when implemented by the least one processor cause to perform the method steps described above.

According to another aspect of the technology a non transitory processor readable medium having instructions stored thereon which when executed by one or more processors cause the one or more processors to implement the method steps described above.

Additional objects advantages and novel features will be set forth in part in the detailed description which follows and in part will become apparent to those skilled in the art upon examination of the following detailed description and the accompanying drawings or may be learned by production or operation of the example embodiments. The objects and advantages of the concepts may be realized and attained by means of the methodologies instrumentalities and combinations particularly pointed out in the appended claims.

The following detailed description includes references to the accompanying drawings which form a part of the detailed description. The drawings show illustrations in accordance with example embodiments. These example embodiments which are also referred to herein as examples are described in enough detail to enable those skilled in the art to practice the present subject matter.

The embodiments can be combined other embodiments can be utilized or structural logical and operational changes can be made without departing from the scope of what is claimed. The following detailed description is therefore not to be taken in a limiting sense and the scope is defined by the appended claims and their equivalents.

Present teachings may be implemented using a variety of technologies. For example the methods described herein may be implemented in software executing on a computer system or in hardware utilizing either a combination of microprocessors or other specially designed application specific integrated circuits ASICs programmable logic devices or various combinations thereof. In particular the methods described herein may be implemented by a series of computer executable instructions residing on a transitory or non transitory storage medium such as a disk drive or computer readable medium. It should be noted that methods disclosed herein can be implemented by a server network device general purpose computer e.g. a desktop computer tablet computer laptop computer game console handheld gaming device cellular phone smart phone television system in vehicle computing device and so forth.

The present technology provides for methods and systems for video conferencing which allow for identifying and tracking faces of individuals presented in captured video images and modifying the video such that the portions of the video images other than the faces have lower quality. This can be accomplished by blurring a scene background although other processes can be also used such as decreasing background resolution or compressing the background.

The term video conferencing as used herein refers to a telecommunication technology which allows two or more people to communicate by simultaneous two way video and audio transmissions. The video transmissions include communicating a plurality of video images also known as video frames. In this disclosure the term video conferencing covers other similar terms including videophone calling videotelephony video teleconferencing and video chatting among others.

The present technology ultimately helps to improve video conferencing experience in congested network environments especially when network QoS is reduced temporary or permanently. The technology also allows for reducing the number of interruptions in video conferencing as well as preserving privacy by obscuring a scene background.

As discussed below in details the core element of this technology is locating and tracking a background in video images and further modifying the background by changing it either graphically e.g. by blurring or by changing its quality by reducing its resolution video dot density color banding or by selectively compressing changing posterizaion changing pixelization smoothing and so forth. In some embodiments the background can be completely replaced with a predetermined image which can be stored in a local memory or selected by a video conference participant. During a teleconference the scene background typically changes from one video frame to another due to the movements of the individual. Therefore accurate identifying of the background for each video frame is one of the crucial elements in this technology.

According to various embodiments of this disclosure scene backgrounds can be identified for each video frame through a identifying individuals in video images and b considering the entire image area other than the identified individuals. The individuals can be identified and tracked using a variety of video processing algorithms. For example individual faces can be identified using the combination of a Viola Jones algorithm which is targeted to locate a face in video images and an ASM algorithm which is designed to detect feature reference points associated with the face. Once faces are located a mesh based on the feature reference points can be created and aligned to the individuals in the video images. Further selecting the entire video image area excepting the mesh constitutes a scene background. Further the background can be modified in any intended way such as by blurring smoothing changing resolution and reducing video dot density i.e. dots per inch DPI so that the image quality of the scene background is reduced compared to the faces which ultimately leads to data rate decrease. A background can be also replaced with a predetermined image. In some embodiments the located foreground or faces of individuals can be also graphically modified. For example the foreground or faces of individuals can be smoothed or sharpened their colors can be changed or any other modifications can be made.

This video processing as described herein can be implemented to a video stream in real time or it can be applied to a previously stored video file including progressive download solutions . Moreover in some embodiments the video processing is applied to each video image individually while in other embodiments the video processing can be applied to a video as a whole. It should be also noted that the video processing steps can be implemented on either a client side or a server side or both depending on a particular system architecture.

According to various embodiments of this disclosure the background modification can be initiated in response to a user request or in response to detection of a predetermined event. For example this technology may dynamically monitor one or more network parameters such as a QoS bit rate or network bandwidth. When one of these parameters drops below a predetermined threshold value the background modification can be initiated in order to reduce data rate associated with the video streaming.

In some embodiments the degree of background modification can depend on current network parameters. For example the worse the network s condition the lower the quality of the background and vice versa. In other words the degree of background blurring smoothing resolution and compression may depend on the current network parameters. Notably in this scenario when the network conditions improve the degree of background modification can be lowered or the background could be kept totally unmodified. In additional embodiments the degree of foreground modification when needed can also depend on current network parameters.

In yet more embodiments of this disclosure the modification of a background can include multiple steps. For example in addition to background blurring a background resolution can be changed. Alternatively after a background is blurred it can be also compressed. In other examples after the blurring a background can also be pixelated or its color can be changed among other processes. It should be appreciated that any combination of background modification procedures can include two three or more separate processes.

It should be also noted that the present technology can also modify portions of the video images that relate to the identified individuals. For example color parameters shape or proportions of the individual faces can be modified in any desired way. In yet another example individual faces can be replaced with predetermined images or masks. In yet other examples portions of video images related to individual faces can be smoothed.

In general video conferencing can be implemented using one or more software applications running on a client side server side or both. In some embodiments the video conferencing can be implemented as a web service or as a cloud solution meaning it is available to conference participants via a website or web interface.

Each of client devices has a video chat application . The video chat applications are generally configured to enable video conferencing between a first and second user and provide video processing as described herein. For these ends each video chat application includes a video processing module which is configured to modify a background scene in each of the video images in order to reduce a data rate of the video. The modification can include blurring compressing changing resolution pixilation video dot density color banding posterizaion or pixelization and so forth. The degree of modification can optionally depend on current network parameters. Video chat applications can be implemented as software middleware or firmware can be separate applications or can constitute a part of larger software applications.

As shown in the figure client devices are connected into a peer to peer P2P network allowing their direct video teleconferencing with each other. Data between nodes can be exchanged directly using for example TCP IP Transmission Control Protocol Internet Protocol network communication standards. In some embodiments the P2P network can include more than two client devices .

In some embodiments the video streaming between the client devices can occur via server such that the client devices are responsible for audio and video capture audio and video delivery and data transfer. In other embodiments server provides background modification only while client devices implement the remaining communication tasks.

As shown in this figure system includes the following hardware components at least one processor memory at least one storage device at least one input module at least one output module and network interface . System also includes optional operating system and video chat application .

In various embodiments processor is configured to implement functionality and or process instructions for execution within the system . For example processor may process instructions stored in memory and or instructions stored on storage devices . Such instructions may include components of operating system and video chat application . System may include multiple processors such as a central processing unit CPU and graphic processing unit GPU which can share operational tasks with each other.

Memory is configured to store information within system during operation. Memory in some example embodiments refers to a non transitory computer readable storage medium or a computer readable storage device. In some examples memory is a temporary memory meaning that a primary purpose of memory may not be long term storage. Memory may also refer to a volatile memory meaning that memory does not maintain stored contents when memory is not receiving power. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples memory is used to store program instructions for execution by the processor . Memory may also be used to temporarily store information during program execution.

Storage device can include one or more transitory or non transitory computer readable storage media and or computer readable storage devices. In some embodiments storage device may be configured to store greater amounts of information than memory . Storage device may further be configured for long term storage of information. In some examples storage device includes non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs optical discs solid state discs flash memories forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM and other forms of non volatile memories known in the art.

Still referencing to system includes one or more input modules for receiving user inputs and one or more output modules for delivering data to a user. Input modules may include a keyboard trackball touchscreen microphone video camera or web camera and the like. Output modules may include any appropriate device to deliver data through visual or audio channels including displays monitors printers touchscreens speakers and so forth.

System further includes network interface which is configured to communicate with external devices servers and network systems via one or more communications networks . Network interface may be a network interface card such as an Ethernet card optical transceiver radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G Third Generation 4G Fourth Generation LTE Lon Term Evolution and WiFi radios. In some embodiments network interface can also be configured to measure various network parameters such as QoS bit rate network bandwidth among others.

Operating system may control one or more functionalities of system or components thereof. For example operating system may interact with video chat application and may further facilitate interactions between video chat application and processor memory storage device input modules output modules and or network interface . Video chat application is configured to provide video conferencing services by implementing two way audio and video communications with another client device. Video chat application is also configured to implement video processing methods such as background blurring as described herein.

Accordingly shows a typical system for video conferencing which may operate as follows. For example video images are input into video chat application from a camera input module and then to video processing module for blurring scene background. Further the video images having a blurred background i.e. modified video can be encoded by encoder decoder and transmitted to other video conference participants via network interface . The receiving participants using similar systems receive the modified video through network interface decode it with encoder decoder to recover and display the video with the background blurred on their displays output module .

As provided above the present video processing methods enable modification of a video image background such as background blurring processing. However backgrounds shall be identified before they are graphically modified. For these ends the present technology focuses on identification of individuals and more specifically on individual faces presented in video images. Once individual faces are identified the video background can easily be determined based on selection of image regions that exclude the image portion associated with identified individual faces. Therefore the process for facial identification is one of the most important steps in the present technology.

According to various embodiments of this disclosure a face in an image can be identified by application of a Viola Jones algorithm and ASM algorithm. In particular a Viola Jones algorithm is a fast and quite accurate method for detecting a face region on an image. An ASM algorithm is then applied to the face region to locate reference feature points associated with the face. These feature reference points can include one or more facial landmarks such as ala philtrum vermilion zonem vermilion border nasolabial sulcus labial commissures lip tubercle nasion outer canthos of eye inner canthos of eye and tragus of ear. Moreover the feature reference points can include one or more of the following facial points indicating eyebrows vertical position eyes vertical position eyes width eyes height eye separation distance nose s vertical position nose s pointing up mouth s vertical position mouth s width chin s width upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser. shows an example image of a face where multiple reference feature points are illustrated.

In some embodiments locating reference feature points includes locating one or more predetermined facial landmarks. For example a predetermined facial landmark may refer to a left eye pupil. A set of landmarks can define a facial shape as a set of vectors.

Further an ASM algorithm starts searching for landmarks on a mean facial shape which is aligned to the position and size of the face presented in the input image. An ASM algorithm then repeats the following two steps until convergence i suggest a tentative shape by adjusting the locations of shape points by template matching of image texture around each point ii conform the tentative shape to a global shape model. The shape model pools the results of weak template matchers to form a stronger overall classifier. The entire search is repeated at each level in an image pyramid from coarse to fine resolution. Thus two sub model types make up the ASM namely a profile model and a shape model.

The profile models one for each landmark at each pyramid level are used to locate an approximate position of each feature reference point by template matching. Any template matcher can be used but the classical ASM forms a fixed length normalized gradient vector also known as a profile by sampling the image along a line also known as a whisker orthogonal to the shape boundary at a landmark. While training on manually landmarked faces at each landmark feature reference point the mean profile vector and the profile covariance matrix Sare calculated. While searching the landmark feature reference point along the whisker is displaced to the pixel whose profile g has lowest Mahalanobis distance from the mean profile where MahalanobisDistance . 1 

Further the shape model specifies allowable constellations of landmarks. A shape of individual can be given by its shape vector x x where xis i th facial landmark. The shape model generates the shape circumflex over x with 2 

where is the mean shape b is a parameter vector and is a matrix of selected eigenvectors of profile covariance matrix Sof the points of the aligned training shapes. Using a standard principal components approach the model has as many variations as is desired by ordering the eigenvalues i and keeping an appropriate number of corresponding eigenvectors in . In this process a single shape model for the entire ASM is used but it may be scaled for each pyramid level Further Equation 2 is used to generate various shapes by varying the vector parameter b. By keeping the elements of b within limits determined during model building it is possible to ensure that generated face shapes are lifelike.

Conversely given a suggested shape x the method can calculate the parameter b that allows Equation 2 to better approximate x with a model shape circumflex over x . The method can further use an iterative algorithm to minimize distance 3 where T is a similarity transform that maps the model space into the image space.

In one or more embodiments CANDIDE 3 shape and initial state can be estimated based on a mapping of CANDIDE 3 vertices to weighted combinations of reference feature points located by ASM. CANDIDE 3 is a parameterized three dimensional face mesh specifically developed for model based coding of human faces. It includes a small number of polygons approximately 100 and allows fast reconstruction. CANDIDE 3 is controlled by Shape Units SUs Action Units AUs and a position vector. The SUs control mesh shape so as different face shapes can be obtained. The AUs control facial mimics so as different expressions can be obtained. The position vector corresponds to rotations around three orthogonal axes and translations along the axes.

Assuming that the observed face is frontal viewed in the image only yaw estimation is needed among the three rotation parameters. It can be found as an angle from the positive direction of the x axis to a vector joining the right eye center feature point with the left one. The following equation system can be created assuming that the observed face is neutral and frontal viewed in the image and the mesh points are projected on the image plane by scaled orthographic projection 

In some embodiments a Viola Jones algorithm and ASM algorithm can be used to improve tracking quality. Face tracking processes can lose face position under some circumstances such as fast movements and or illumination variations. In order to re initialize tracking algorithm in this technology ASM algorithm is applied in such cases.

According to various embodiments of this disclosure tracking of identified faces is an important step after faces are identified in the video images. Because individuals can move in each of the video images the background also changes with the movement of the individuals. Face tracking allows tracking background modifications for each video image frame to enable its modification later.

CANDIDE 3 model can be used for face tracking. See J rgen Ahlberg Candide 3 an updated parameterized face Technical report Linkoping University Sweden 2001. shows an exemplary mesh corresponding to CANDIDE 3 model created with respect to the face shown in .

In one or more embodiments a state of CANDIDE 3 model can be described by intensity vector of SUs intensity vector of AUs and a position vector. SUs refer to various parameters of head and face. For example the following SUs can be used vertical position of eyebrows vertical position of eyes eyes width eyes height eye separation distance nose vertical position nose pointing up mouth vertical position mouth width and chin width. AUs refer to face parameters that correspond to various face mimics. For example the following AUs can be used upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser.

The state of mesh such as one shown in can be described using six coordinates yaw pitch roll x y and z scale . Following Dornaika et al. approach a mesh state can be determined by observing the region most likely to be a face. See Dornaika F. Davoine F. . IEEE Trans. Circuits Syst. Video Technol. 16 9 1107 1124 2006 . For each mesh position observation errors can be calculated. Observation errors refer to a value indicating the difference between an image under a current mesh state and a mean face. shows an example mean face. shows an example warped towards standard CANDIDE 3 state observation under a current state of the mesh illustrated in . More specifically shows an exemplary image having a face and a mesh aligned to the face.

In one or more embodiments a face modelled as a picture with a fixed size e.g. width 40px height 46px is referred to as a mean face. In one or more embodiments the observation process can be implemented as a warping process from the current CANDIDE 3 state towards its standard state and denoted by 8 where x denotes the observed image with the same size as of mean face y denotes the input image and b denotes the CANDIDE 3 AUs intensities and state parameters. Gaussian distribution proposed in original algorithms has shown worse results compared to a static image. Thus the difference between the current observation and mean face can be calculated as follows log 1 log 1 9 

Logarithm function can make the tracking more stable and reliable. In one or more embodiments a Taylor series can be used to minimize error. The gradient matrix is given by

Here giis an element of matrix G. This matrix has size m n where m is larger than n e.g. m is about 1600 and n is about 14 . In case of straight forward calculating there n m operations of division have to be completed. To reduce the number of divisions this matrix can be rewritten as a product of two matrices G A B. Here matrix A has the same size as G. Each element of matrix A can be represented as 12 

Yet another optimization can be used in this method. If matrix G is created and then multiplied by b it leads to nm operations but if the first Aand b are multiplied and then multiplied by B AA there will be only n m noperations which is much better because n

Thus face tracking in the video comprises CANDIDE 3 shape and initial state estimating that is based on located reference feature points associated with a particular face and aligning the mesh to the face in each video image. Notably this process can be applied not only to a face but also to other individual parts. In other words this process of localization and tracking of a video conferencing participant may include localization and tracking one or more of the participant s face his body limbs and or other parts. In some embodiments gesture detection and tracking processes can be also applied. In this case the method may create a virtual skeleton and a mesh aligned to these body parts.

It should be also noted that ARM advanced SIMD Single Instruction Multiple Data extensions also known as NEON provided by ARM Limited can be used for multiplication of matrices in order to increase tracking performance. Also a GPU Graphics Processing Unit can be used in addition to or instead of CPU Central Processing Unit whenever possible. To get high performance of GPU operations can be arranged in a particular ways.

According to some embodiments of the disclosure the face tracking process can include the following features. First a logarithm can be applied to grayscale the value of each pixel to track it. This transformation has a great impact to tracking performance. Second in the procedure of gradient matrix creation the step of each parameter can be based on the mesh scale.

In order to automatically re initialize tracking algorithm in failure cases the following failure criterion can be used 14 where is Euclidean norm y bare indexed by an image number t.

As outlined above once faces or other parts of video conference participants are detected identified the present technology identifies a background in each video image. There can be used various procedures for background identification including selection of the entire image area and excluding those portions that relate to identified faces based on created meshes. Another procedure can include forming a binary mask aligned to a face and then inverting the binary mask so as to select image areas not associated with the face. Identification of background in each video image allows modifying the background in any intended way. For example modification can include blurring although other modification procedures can be also applied such as changing background resolution video dot density color banding or compressing encoding changing posterizaion changing pixelization and so forth. Background modification can depend on user instructions or current network conditions. These and other embodiments for background identification and modification are described below with reference to exemplary flow charts.

Method for video processing commences at operation with establishing a videoconference between at least two users and receiving a video by a computing device such as the client device or server . The video can be captured by a video or web camera operatively coupled to the computing device. As a general matter the video includes a sequence of video images also known as video frames and the video can be received as a video stream meaning it can be continually supplied to the computing device e.g. as progressive downloading or it can be stored in a memory of the computing device. The video can be captured for video conferencing purposes but not necessarily.

At optional operation the computing device receives a request to blur or modify in other way a background in the video so as to change a data rate or file size. In one example the request can be generated manually by a user such as one of video conference participants. In another example the request can be generated automatically in response to changing networking conditions. For example the computing device may dynamically monitor QoS or other parameters associated with one or more communications networks and based on the results of monitoring a request to start background blurring or a request to stop background blurring can be generated. In one example when it is determined that the network condition becomes worse meaning that a data transmission rate bandwidth or bit rate is reduced a number of errors is increased or another parameter is changed the request for background blurring is generated in order to decrease the size of the video file or decrease data rate and prevent video interruptions or degradations.

At operation the computing device identifies or detects at least one object of interest in one or more video images. As discussed above the object of interest may refer to a face of a user or body parts of the user including limbs neck arms chest and so forth. The identification can be based on a Viola Jones algorithm although other algorithms can be also used such as Kanade Lucas Tomasi KLT algorithm CAMShift algorithm or any other computer vision method.

In some other embodiments the identification of the at least one object of interest in one or more of the images can be based on a user input. For example the user input can include data associated with an image area related to the at least one object of interest.

At operation the computing device detects feature reference points of at least one object of interest e.g. a face . Feature reference points can include various facial landmarks such as but not limited to as ala philtrum vermilion zonem vermilion border nasolabial sulcus labial commissures lip tubercle nasion outer canthos of eye inner canthos of eye tragus of ear eyebrows vertical position eyes vertical position eyes width eyes height eye separation distance nose vertical position nose pointing up mouth vertical position mouth width chin width upper lip raiser jaw drop lip stretcher left brow lowerer right brow lowerer lip corner depressor and outer brow raiser. The feature reference points can be determined using ASM or extended ASM algorithms as explained above. However other procedures of facial landmark localization can also be used including but not limited to exemplar based graph matching EGM algorithm consensus of exemplars algorithm and so forth.

At operation the computing device optionally creates a virtual facial mesh referred to as the mesh for simplicity or uses a predetermined mesh and aligns the mesh to the least one object of interest e.g. a face based at least in part on the feature reference points. This procedure is performed for some of the images or each of the video images separately which ultimately allows dynamically tracking faces in the video. As discussed above CANDIDE 3 model can be applied for creating and aligning the mesh. CANDIDE 3 is a procedure for generating a parameterized face mesh mask based on calculation of global and local AUs.

At operation the computing device identifies or detects a background in each video image. In general a background can be identified using a variety of processes. In one example embodiment a background is identified by separating the at least one object of interest from each image based on the mesh. In another example embodiment a background is identified by selecting a portion of a video image which is located outside of the mesh. In other words the background is identified by selecting an image portion for each video image which excludes pixels associated with the mesh.

In yet another example embodiment a background is identified by the process including a forming a binary mask associated with the at least one object of interest b aligning the binary mask to the mesh on each image and c creating an inverted binary mask by inverting the binary mask.

The binary mask can be formed as follows. First the computing device determines a gray value intensity or a mean gray value intensity of a plurality of image sections in each of the images where the plurality of image sections are associated with the mesh. Second the computing device determines object pixels associated with the object of interest by comparing the gray value intensity of each of the image sections with a reference value. Third the computing device applies a binary morphological closing algorithm to the object pixels. Forth the computing device removes unwanted pixel conglomerates from the mesh.

The binary mask can be aligned to the mesh for example as follows. First the computing device makes a projection of the mesh to a reference grid. thereby separating the mesh into a plurality of reference grid cells. Second the computing device associates elements of the mesh which correspond to reference grid cells. Third the computing device determines pixels of each of the images which correspond to the elements of the mesh. This determination can be made by applying a breadth first search BFS algorithm.

Still referencing to at operation the computing device modifies the identified background in each video image. The modification can include blurring such as Gaussian smoothing or a lens blurring algorithm. However other modifications can also be used such as changing background resolution changing video dot density changing colors changing color banding compressing encoding changing posterizaion and changing pixelization. In some embodiments background can be replaced with a predetermined image for privacy of video conference participants . The degree of modification can depend on current network parameters. For example the smaller the network bit rate the higher degree of blurring or other modification method is applied to the identified background and vice versa.

As discussed above the background modification is targeted to decrease image quality associated with a background while preserving high image quality of the participants. In other words the modified background has a first image quality in the modified video and the at least one object of interest has a second image quality in the modified video and the first image quality is lower than the second image quality. Difference between the first image quality and second image quality may depend on current network conditions or network parameters which can be measured by the computing device.

At optional operation the computing device may compress or encode the background or modified background. Compression may include applying one or more codecs to the background. For example codec H264 can be used for compression of the background. Notably in some embodiments two codecs can be used where one codec is applied to the background while another one to identified objects of interest e.g. faces .

At operation the computing device generates a modified video by combining the modified background with the image of the object of interest. At optional operation the computing device may transmit the modified video over communications network .

In yet additional embodiments method may further comprise optional operations of modifying those image portions that are associated with the at least one object of interest in each of the images. The modifying of the image portions associated with the at least one object of interest can be based on the feature reference points or the mesh. For example the modifying of the image portions associated with the at least one object of interest includes changing at least one of a color a color tone a proportion and a resolution. In some embodiments the at least one object of interest can be replaced with a predetermined image.

In yet more embodiments method may comprise an additional step of determining a position of the user head based on the identifying of the at least one object of interest and the reference feature points an additional step of determining a position of a body based on the position of the head and an additional step of tracking the position of the body over the sequence of images. The background modification at operation can be based on the position of the body. For example if tracking the body is not feasible based on the images but tracking of the user face is feasible based on the images background modification can be based on approximation of body position such that the user face and body remain unmodified but the remaining portions of video images are modified.

Method commences at operation with receiving a video by a computing device such as the client device or server . The video can be captured by a video or web camera operatively coupled to the computing device.

At operation the computing device dynamically monitors a network parameter e.g. QoS bit rate bandwidth associated with transferring the video over one or more communications networks . At block the computing device determines whether a current value of the network parameter is below a predetermined threshold value. If the current value of the network parameter is below the predetermined threshold value method proceeds to operation where the computing device generates a request to modify a background in the video. The method then proceeds to operation as shown in .

Otherwise if the current value of the network parameter is above the predetermined threshold value method proceeds to operation where the computing device generates a request instruction to transmit the video without modifications. In this case the method proceeds to operation as shown in .

At operation the computing device identifies at least one object of interest detects feature reference points of the at least one object of interest c aligns a mesh to the least one object of interest identifies a background in one or more of video images based on the mesh and modifies the background in each video image. These procedures can replicate those that are described above with reference to operations through .

More specifically the identification of the object of interest can be based on a Viola Jones algorithm although other algorithms can be also used such as a KLT algorithm CAMShift algorithm or any other computer vision method. In some other embodiments the identification of the at least one object of interest in each of the images can be based on a user input. The feature reference points can be determined using ASM or extended ASM algorithms as well as EGM algorithms consensus of exemplars algorithms and so forth. The mesh can be created based on CANDIDE 3 model. The background can be identified using a variety of processes such as by separating the at least one object of interest from each image based on the mesh by selecting an image portion which excludes pixels associated with the mesh or by the process including a forming a binary mask associated with the at least one object of interest b aligning the binary mask to the mesh on each image and c creating an inverted binary mask by inverting the binary mask.

The background modification can include blurring changing background resolution changing video dot density changing colors changing color banding compressing encoding changing posterizaion and changing pixelization. The degree of modification can depend on current network parameters. In some embodiments background modification can include replacement substituting or covering of the background with a predetermined image or video. The predetermined image can be selected by a user or a default image can be used which is stored in a memory of computing device.

In some embodiments the foreground can be also modified in addition to modification of the background. In some embodiments the foreground can be smoothed sharpened or its colors can be changed. The foreground may include images of individuals and possibly other elements not present in the background. In yet more embodiments only identified faces or objects of interest can be modified by smoothing sharpening changing colors and so forth.

At operation the computing device generates a modified video by combining the modified background with the image of the object of interest. At optional operation the computing device transmits the original or modified video over communications network .

Experiments show that the methods for video processing described herein allow reducing video data rate or video file size up to about 53 percent if everything but the individual faces is blurred and up to about 21 percent if everything other than the foreground is blurred.

In yet additional embodiments operations can be replaced with other ones. More specifically the present technology can modify the background based on a particular value of a network parameter. For example if at operation it is determined that the network parameter associated with the QoS is within a first predetermined range then at operation a first request is generated to blur only the background of the video keeping the face and body unmodified . When at operation it is determined that the network parameter associated with the QoS is within a second predetermined range then at operation a second request is generated to blur the background of the video and other parts of the video excluding a user face. Further when at operation it is determined that the network parameter associated with the QoS is within a third predetermined range then at operation either no request to blur is generated or a third request is generated to transmit the video without modifying. Note that the first second and third ranges differ from each other although can optionally overlap.

Thus methods and systems for real time video processing have been described. Although embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes can be made to these example embodiments without departing from the broader spirit and scope of the present application. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

