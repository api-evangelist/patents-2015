---

title: Remote-direct-memory-access-based virtual machine live migration
abstract: The current document is directed to methods and systems for moving executing virtual machines between host systems in a virtual data center. In described implementations, remote-direct memory access is used for transferring memory contents and, in certain implementations, additional data between the host systems to facilitate live migration of virtual machines. To provide increased efficiency, transfer of the contents of a shared memory page from a source host system to target host system during migration of a virtual machine is deferred until the relocated virtual machine attempts to write to the shared memory page.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09619270&OS=09619270&RS=09619270
owner: VMware, Inc.
number: 09619270
owner_city: Palo Alto
owner_country: US
publication_date: 20150826
---
Benefit is claimed under 35 U.S.C. 119 a d to Foreign application Serial No. 3248 CHE 2015 filed in India entitled REMOTE DIRECT MEMORY ACCESS BASED VIRTUAL MACHINE LIVE MIGRATION on Jun. 27 2015 by VMware Inc. which is herein incorporated in its entirety by reference for all purposes.

The current document is directed to management of virtualized distributed computer systems and in particular to methods and systems that employ remote direct memory access to facilitate movement of executing virtual machines between host systems.

Early computer systems were monolithic single processor systems that executed only a single task at each point in time. The early computer systems lacked operating systems and basic programming facilities such as assemblers and compilers. During the first several decades of the computer revolution many of the basic single system components of computer systems were developed and evolved to produce capable operating system controlled multi tasking computer systems. For another decade rapid evolution of processor technology data storage technologies memory and communications technologies led to dramatic increases in the capabilities and capacities of single processor computer systems. Ultimately however in order to achieve even greater capabilities and capacities computer system designers turned to multi processor systems and then to more complex distributed computing systems comprising aggregations of many intercommunicating computer systems. This turn towards distributed computing was facilitated by the development of distributed locking methods for controlling access to distributed computing resources distributed operating systems and high bandwidth electronic communications. During the past few decades extremely large cloud computing facilities have been developed and commercialized to the point that computational bandwidth and data storage capacity are provided to customers of large cloud computing providers much as electrical power and water are provided to customers of utility companies. Cloud computing facilities often employ hundreds thousands or more networked and often geographically distributed multi processor servers that are controlled by virtualization technology to produce hierarchical layers of virtualized computing facilities.

Virtual data centers and other virtual data structures are generally implemented on large physical distributed computing systems including computing systems that employ a management sever to manage a large number of remote host systems. Management servers in cooperation with host systems have been designed to move executing virtual machines between host systems with minimal interruption in virtual machine execution on the order of less than one second to a few seconds in order to facilitate high availability fault tolerance load balancing and other features of robust and efficient virtual data centers. However copying memory and swap files between host systems during virtual machine migration may be an expensive temporary overhead consuming significant processor bandwidth. Designers and developers of distributed computer systems continue to seek methods and systems to more efficiently implement live migration of virtual machines.

The current document is directed to methods and systems for moving executing virtual machines between host systems in a virtual data center. In described implementations remote direct memory access is used for transferring memory contents and in certain implementations additional data between the host systems to facilitate live migration of virtual machines. To provide increased efficiency transfer of the contents of a shared memory page from a source host system to target host system during migration of a virtual machine is deferred until the relocated virtual machine attempts to write to the shared memory page.

The current document is directed to methods and systems for moving executing virtual machines between host systems in a virtual data center. In a first subsection below a detailed description of computer hardware complex computational systems and virtualization is provided with reference to . In a second subsection remote direct memory access RDMA is discussed with reference to . Implementations of the currently disclosed methods and systems are discussed in a third subsection with reference to .

The term abstraction is not in any way intended to mean or suggest an abstract idea or concept. Computational abstractions are tangible physical interfaces that are implemented ultimately using physical computer hardware data storage devices and communications systems. Instead the term abstraction refers in the current discussion to a logical level of functionality encapsulated within one or more concrete tangible physically implemented computer systems with defined interfaces through which electronically encoded data is exchanged process execution launched and electronic services are provided. Interfaces may include graphical and textual data displayed on physical display devices as well as computer programs and routines that control physical computer processors to carry out various tasks and operations and that are invoked through electronically implemented application programming interfaces APIs and other electronically implemented interfaces. There is a tendency among those unfamiliar with modern technology and science to misinterpret the terms abstract and abstraction when used to describe certain aspects of modern computing. For example one frequently encounters assertions that because a computational system is described in terms of abstractions functional layers and interfaces the computational system is somehow different from a physical machine or device. Such allegations are unfounded. One only needs to disconnect a computer system or group of computer systems from their respective power supplies to appreciate the physical machine nature of complex computer technologies. One also frequently encounters statements that characterize a computational technology as being only software and thus not a machine or device. Software is essentially a sequence of encoded symbols such as a printout of a computer program or digitally encoded computer instructions sequentially stored in a file on an optical disk or within an electromechanical mass storage device. Software alone can do nothing. It is only when encoded computer instructions are loaded into an electronic memory within a computer system and executed on a physical processor that so called software implemented functionality is provided. The digitally encoded computer instructions are an essential and physical control component of processor controlled machines and devices no less essential and physical than a cam shaft control system in an internal combustion engine. Multi cloud aggregations cloud computing services virtual machine containers and virtual machines communications interfaces and many of the other topics discussed below are tangible physical components of physical electro optical mechanical computer systems.

Of course there are many different types of computer system architectures that differ from one another in the number of different memories including different types of hierarchical cache memories the number of processors and the connectivity of the processors with other system components the number of internal communications busses and serial links and in many other ways. However computer systems generally execute stored programs by fetching instructions from memory and executing the instructions in one or more processors. Computer systems include general purpose computer systems such as personal computers PCs various types of servers and workstations and higher end mainframe computers but may also include a plethora of various types of special purpose computing devices including data storage systems communications routers network nodes tablet computers and mobile telephones.

Until recently computational services were generally provided by computer systems and data centers purchased configured managed and maintained by service provider organizations. For example an e commerce retailer generally purchased configured managed and maintained a data center including numerous web servers back end computer systems and data storage systems for serving web pages to remote customers receiving orders through the web page interface processing the orders tracking completed orders and other myriad different tasks associated with an e commerce enterprise.

Cloud computing facilities are intended to provide computational bandwidth and data storage services much as utility companies provide electrical power and water to consumers. Cloud computing provides enormous advantages to small organizations without the resources to purchase manage and maintain in house data centers. Such organizations can dynamically add and delete virtual computer systems from their virtual data centers within public clouds in order to track computational bandwidth and data storage needs rather than purchasing sufficient computer systems within a physical data center to handle peak computational bandwidth and data storage demands. Moreover small organizations can completely avoid the overhead of maintaining and managing physical computer systems including hiring and periodically retraining information technology specialists and continuously paying for operating system and database management system upgrades. Furthermore cloud computing interfaces allow for easy and straightforward configuration of virtual computing facilities flexibility in the types of applications and operating systems that can be configured and other functionalities that are useful even for owners and administrators of private cloud computing facilities used by a single organization.

While the execution environments provided by operating systems have proved to be an enormously successful level of abstraction within computer systems the operating system provided level of abstraction is nonetheless associated with difficulties and challenges for developers and users of application programs and other higher level computational entities. One difficulty arises from the fact that there are many different operating systems that run within various different types of computer hardware. In many cases popular application programs and computational systems are developed to run on only a subset of the available operating systems and can therefore be executed within only a subset of the various different types of computer systems on which the operating systems are designed to run. Often even when an application program or other computational system is ported to additional operating systems the application program or other computational system can nonetheless run more efficiently on the operating systems for which the application program or other computational system was originally targeted. Another difficulty arises from the increasingly distributed nature of computer systems. Although distributed operating systems are the subject of considerable research and development efforts many of the popular operating systems are designed primarily for execution on a single computer system. In many cases it is difficult to move application programs in real time between the different computer systems of a distributed computer system for high availability fault tolerance and load balancing purposes. The problems are even greater in heterogeneous distributed computer systems which include different types of hardware and devices running different types of operating systems. Operating systems continue to evolve as a result of which certain older application programs and other computational entities may be incompatible with more recent versions of operating systems for which they are targeted creating compatibility issues that are particularly difficult to manage in large distributed systems.

For all of these reasons a higher level of abstraction referred to as the virtual machine has been developed and evolved to further abstract computer hardware in order to address many difficulties and challenges associated with traditional computing systems including the compatibility issues discussed above. illustrate two types of virtual machine and virtual machine execution environments. use the same illustration conventions as used in . shows a first type of virtualization. The computer system in includes the same hardware layer as the hardware layer shown in . However rather than providing an operating system layer directly above the hardware layer as in the virtualized computing environment illustrated in features a virtualization layer that interfaces through a virtualization layer hardware layer interface equivalent to interface in to the hardware. The virtualization layer provides a hardware like interface to a number of virtual machines such as virtual machine executing above the virtualization layer in a virtual machine layer . Each virtual machine includes one or more application programs or other higher level computational entities packaged together with an operating system referred to as a guest operating system. such as application and guest operating system packaged together within virtual machine . Each virtual machine is thus equivalent to the operating system layer and application program layer in the general purpose computer system shown in . Each guest operating system within a virtual machine interfaces to the virtualization layer interface rather than to the actual hardware interface . The virtualization layer partitions hardware resources into abstract virtual hardware layers to which each guest operating system within a virtual machine interfaces. The guest operating systems within the virtual machines in general are unaware of the virtualization layer and operate as if they were directly accessing a true hardware interface. The virtualization layer ensures that each of the virtual machines currently executing within the virtual environment receive a fair allocation of underlying hardware resources and that all virtual machines receive sufficient resources to progress in execution. The virtualization layer interface may differ for different guest operating systems. For example the virtualization layer is generally able to provide virtual hardware interfaces for a variety of different types of computer hardware. This allows as one example a virtual machine that includes a guest operating system designed for a particular computer architecture to run on hardware of a different architecture. The number of virtual machines need not be equal to the number of physical processors or even a multiple of the number of processors.

The virtualization layer includes a virtual machine monitor module VMM that virtualizes physical processors in the hardware layer to create virtual processors on which each of the virtual machines executes. For execution efficiency the virtualization layer attempts to allow virtual machines to directly execute non privileged instructions and to directly access non privileged registers and memory. However when the guest operating system within a virtual machine accesses virtual privileged instructions virtual privileged registers and virtual privileged memory through the virtualization layer interface the accesses result in execution of virtualization layer code to simulate or emulate the privileged resources. The virtualization layer additionally includes a kernel module that manages memory communications and data storage machine resources on behalf of executing virtual machines VM kernel . The VM kernel for example maintains shadow page tables on each virtual machine so that hardware level virtual memory facilities can be used to process memory accesses. The VM kernel additionally includes routines that implement virtual communications and data storage devices as well as device drivers that directly control the operation of underlying hardware communications and data storage devices. Similarly the VM kernel virtualizes various other types of I O devices including keyboards optical disk drives and other such devices. The virtualization layer essentially schedules execution of virtual machines much like an operating system schedules execution of application programs so that the virtual machines each execute within a complete and fully functional virtual hardware layer.

In the layers are somewhat simplified for clarity of illustration. For example portions of the virtualization layer may reside within the host operating system kernel such as a specialized driver incorporated into the host operating system to facilitate hardware access by the virtualization layer.

It should be noted that virtual hardware layers virtualization layers and guest operating systems are all physical entities that are implemented by computer instructions stored in physical data storage devices including electronic memories mass storage devices optical disks magnetic disks and other such devices. The term virtual does not in any way imply that virtual hardware layers virtualization layers and guest operating systems are abstract or intangible. Virtual hardware layers virtualization layers and guest operating systems execute on physical processors of physical computer systems and control operation of the physical computer systems including operations that alter the physical states of physical devices including electronic memories and mass storage devices. They are as physical and tangible as any other component of a computer since such as power supplies controllers processors busses and data storage devices.

A virtual machine or virtual application described below is encapsulated within a data package for transmission distribution and loading into a virtual execution environment. One public standard for virtual machine encapsulation is referred to as the open virtualization format OVF . The OVF standard specifies a format for digitally encoding a virtual machine within one or more data files. illustrates an OVF package. An OVF package includes an OVF descriptor an OVF manifest an OVF certificate one or more disk image files and one or more resource files . The OVF package can be encoded and stored as a single file or as a set of files. The OVF descriptor is an XML document that includes a hierarchical set of elements each demarcated by a beginning tag and an ending tag. The outermost or highest level element is the envelope element demarcated by tags and . The next level element includes a reference element that includes references to all files that are part of the OVF package a disk section that contains meta information about all of the virtual disks included in the OVF package a networks section that includes meta information about all of the logical networks included in the OVF package and a collection of virtual machine configurations which further includes hardware descriptions of each virtual machine . There are many additional hierarchical levels and elements within a typical OVF descriptor. The OVF descriptor is thus a self describing XML file that describes the contents of an OVF package. The OVF manifest is a list of cryptographic hash function generated digests of the entire OVF package and of the various components of the OVF package. The OVF certificate is an authentication certificate that includes a digest of the manifest and that is cryptographically signed. Disk image files such as disk image file are digital encodings of the contents of virtual disks and resource files are digitally encoded content such as operating system images. A virtual machine or a collection of virtual machines encapsulated together within a virtual application can thus be digitally encoded as one or more files within an OVF package that can be transmitted distributed and loaded using well known tools for transmitting distributing and loading files. A virtual appliance is a software service that is delivered as a complete software stack installed within one or more virtual machines that is encoded within an OVF package.

The advent of virtual machines and virtual environments has alleviated many of the difficulties and challenges associated with traditional general purpose computing. Machine and operating system dependencies can be significantly reduced or entirely eliminated by packaging applications and operating systems together as virtual machines and virtual appliances that execute within virtual environments provided by virtualization layers running on many different types of computer hardware. A next level of abstraction referred to as virtual data centers which are one example of a broader virtual infrastructure category provide a data center interface to virtual data centers computationally constructed within physical data centers. illustrates virtual data centers provided as an abstraction of underlying physical data center hardware components. In a physical data center is shown below a virtual interface plane . The physical data center consists of a virtual infrastructure management server VI management server and any of various different computers such as PCs on which a virtual data center management interface may be displayed to system administrators and other users. The physical data center additionally includes generally large numbers of server computers such as server computer that are coupled together by local area networks such as local area network that directly interconnects server computer and and a mass storage array . The physical data center shown in includes three local area networks and that each directly interconnects a bank of eight servers and a mass storage array. The individual server computers such as server computer each includes a virtualization layer and runs multiple virtual machines. Different physical data centers may include many different types of computers networks data storage systems and devices connected according to many different types of connection topologies. The virtual data center abstraction layer a logical abstraction layer shown by a plane in abstracts the physical data center to a virtual data center comprising one or more resource pools such as resource pools one or more virtual data stores such as virtual data stores and one or more virtual networks. In certain implementations the resource pools abstract banks of physical servers directly interconnected by a local area network.

The virtual data center management interface allows provisioning and launching of virtual machines with respect to resource pools virtual data stores and virtual networks so that virtual data center administrators need not be concerned with the identities of physical data center components used to execute particular virtual machines. Furthermore the VI management server includes functionality to migrate running virtual machines from one physical server to another in order to optimally or near optimally manage resource allocation provide fault tolerance and high availability by migrating virtual machines to most effectively utilize underlying physical hardware resources to replace virtual machines disabled by physical hardware problems and failures and to ensure that multiple virtual machines supporting a high availability virtual appliance are executing on multiple physical computer systems so that the services provided by the virtual appliance are continuously accessible even when one of the multiple virtual appliances becomes compute bound data access bound suspends execution or fails. Thus the virtual data center layer of abstraction provides a virtual data center abstraction of physical data centers to simplify provisioning launching and maintenance of virtual machines and virtual appliances as well as to provide high level distributed functionalities that involve pooling the resources of individual physical servers and migrating virtual machines among physical servers to achieve load balancing fault tolerance and high availability.

The distributed services include a distributed resource scheduler that assigns virtual machines to execute within particular physical servers and that migrates virtual machines in order to most effectively make use of computational bandwidths data storage capacities and network capacities of the physical data center. The distributed services further include a high availability service that replicates and migrates virtual machines in order to ensure that virtual machines continue to execute despite problems and failures experienced by physical hardware components. The distributed services also include a live virtual machine migration service that temporarily halts execution of a virtual machine transmits the stored data and memory associated with the virtual machine to a different physical server and restarts the virtual machine on the different physical server from a virtual machine state recorded when execution of the virtual machine was halted. The distributed services also include a distributed backup service that provides centralized virtual machine backup and restore.

The core services provided by the VI management server include host configuration virtual machine configuration virtual machine provisioning generation of virtual data center alarms and events ongoing event logging and statistics collection a task scheduler and a resource management module. Each physical server also includes a host agent virtual machine through which the virtualization layer can be accessed via a virtual infrastructure application programming interface API . This interface allows a remote administrator or user to manage an individual server through the infrastructure API. The virtual data center agents access virtualization layer server information through the host agents. The virtual data center agents are primarily responsible for offloading certain of the virtual data center management server functions specific to a particular physical server to that physical server. The virtual data center agents relay and enforce resource allocations made by the VI management server relay virtual machine provisioning and configuration change commands to host agents monitor and collect performance statistics alarms and events communicated to the virtual data center agents by the local host agents through the interface API and to carry out other similar virtual data management tasks.

The virtual data center abstraction provides a convenient and efficient level of abstraction for exposing the computational resources of a cloud computing facility to cloud computing infrastructure users. A cloud director management server exposes virtual resources of a cloud computing facility to cloud computing infrastructure users. In addition the cloud director introduces a multi tenancy layer of abstraction which partitions virtual data centers VDCs into tenant associated VDCs that can each be allocated to a particular individual tenant or tenant organization both referred to as a tenant. A given tenant can be provided one or more tenant associated VDCs by a cloud director managing the multi tenancy layer of abstraction within a cloud computing facility. The cloud services interface in exposes a virtual data center management interface that abstracts the physical data center.

Considering the VI management server and cloud director layers of abstraction can be seen as discussed above to facilitate employment of the virtual data center concept within private and public clouds. However this level of abstraction does not fully facilitate aggregation of single tenant and multi tenant virtual data centers into heterogeneous or homogeneous aggregations of cloud computing facilities.

Processors such as Intel Itanium processors are the fundamental computational component of a modern computer hardware platform that supports a virtualization layer that in turn supports multiple guest operating systems in part by providing a virtual physical memory and virtual address translation facilities to each guest operating system. Various different types of processors have different types of architectures and support for virtual memory. The Intel Itanium processor architecture is used as an example in the current discussion of a modern processor architecture.

The process status register PSR is a 64 bit register that contains control information for the currently executing process. The PSR comprises many bit fields including a 2 bit field that contains the current privilege level CPL at which the currently executing process is executing. There are four privilege levels 0 1 2 and 3. The most privileged privilege level is privilege level 0. The least privileged privilege level is privilege level 3. Only processes executing at privilege level 0 are allowed to access and manipulate certain machine resources including the subset of registers known as the system register set shown in within the lower rectangle . One control register the interruption processor status register IPSR stores the value of the PSR for the most recently interrupted process. The interruption status register ISR contains a number of fields that indicate the nature of the interruption that most recently occurred to an interruption handler when the PSR.ic field flips from 1 at the time of a fault or interrupt to 0 as the interruption handler is invoked. Other control registers store information related to other events such as virtual memory address translation information related to a virtual address translation fault pointers to the last successfully executed instruction bundle and other such information. Sets of external interrupt control registers are used in part to set interrupt vectors. The IHA register stores an indication of a virtual hash page table location at which the virtual address translation corresponding to a faulting virtual address should be found.

The registers shown in in the upper rectangular region are known as the application register set. These registers include a set of general registers sixteen of which are banked in order to provide immediate registers for interruption handling code. At least 96 general registers form a general register stack portions of which may be automatically stored and retrieved from backing memory to facilitate linkages among calling and called software routines. The application register set also includes floating point registers predicate registers branch registers an instruction pointer a current frame marker a user mask performance monitor data registers processor identifiers an advanced load address table and a set of specific application registers .

The memory and virtual address translation architecture of the Itanium computer architecture is described below with references to . The virtual address space defined within the Intel Itanium computer architecture includes 2regions such as regions shown in each containing 2bytes that are contiguously addressed by successive virtual memory addresses. Thus the virtual memory address space can be considered to span a total address space of 2bytes of memory. An 85 bit virtual memory address can then be considered to comprise a 24 bit region field and a 61 bit address field .

In general however virtual memory addresses are encoded as 64 bit quantities. illustrates translation of a 64 bit virtual memory address into a physical memory address via information stored within region registers protection key registers and a translation look aside register buffer TLB . In the Intel Itanium architecture virtual addresses are 64 bit computer words represented in by a 64 bit quantity divided into three fields . The first two fields and have sizes that depend on the size of a memory page which can be adjusted within a range of memory page sizes. The first field is referred to as the offset. The offset is an integer designating a byte within a memory page. When for example a memory page contains 4096 bytes then the offset needs to contain 12 bits to represent the values 0 4095. The second field contains a virtual page address. The virtual page address designates a memory page within a virtual address space that is mapped to physical memory and further backed up by memory pages stored on mass storage devices such as disks. The third field is a three bit field that designates a region register containing the identifier of a region of virtual memory in which the virtual memory page specified by the virtual page address is contained.

One possible virtual address translation implementation consistent with the Itanium architecture is next discussed. Translation of the virtual memory address to a physical memory address that includes the same offset as the offset in the virtual memory address as well as a physical page number that references a page in the physical memory components of the computer system is carried out by the processor at times in combination with operating system provided services. When a translation from a virtual memory address to a physical memory address is contained within the TLB then the virtual memory address to physical memory address translation can be entirely carried out by the processor without operating system intervention. The processor employs the region register selector field to select a register within a set of region registers . The selected region register contains a 24 bit region identifier. The processor uses the region identifier contained in the selected region register and the virtual page address together in a hardware function to select a TLB entry containing a region identifier and virtual memory address that match the region identifier contained in the selected region register and the virtual page address . Each TLB entry such as TLB entry contains fields that include a region identifier a protection key associated with the memory page described by the TLB entry a virtual page address privilege and access mode fields that together compose an access rights field a dirty bit to indicate that the page has been modified since read from backing store and a physical memory page address .

When a valid entry in the TLB with present bit 1 can be found that contains the region identifier contained within the region register specified by the region register selector field of the virtual memory address and that entry contains the virtual page address specified within the virtual memory address then the processor determines whether the virtual memory page described by the virtual memory address can be accessed by the currently executing process. The currently executing process may access the memory page when the access rights within the TLB entry allow the memory page to be accessed by the currently executing process and if the protection key within the TLB entry can be found within the protection key registers in association with an access mode that allows the currently executing process access to the memory page. Protection key matching is required only when the PSR.pk field of the PSR register is set. The access rights contained within a TLB entry include a 3 bit access mode field that indicates one or a combination of read write and execute privileges and a 2 bit privilege level field that specifies the privilege level needed by an accessing process. Each protection key register contains a protection key of up to 24 bits in length associated with an access mode field specifying allowed read write and execute access modes and a valid bit indicating whether or not the protection key register is currently valid. Thus in order to access a memory page described by a TLB entry. The accessing process needs to access the page in a manner compatible with the access mode associated with a valid protection key within the protection key registers and associated with the memory page in the TLB entry and needs to be executing at a privilege level compatible with the privilege level associated with the memory page within the TLB entry.

When an entry is not found within the TLB with a region identifier and a virtual page address equal to the virtual page address within the virtual memory address and a region identifier selected by the region register selection field of a virtual memory address then a TLB miss occurs and hardware may attempt to locate the correct TLB entry from an architected mapping control table called the virtual hash page table VHPT located in protected memory using a hardware provided VHPT walker. When the hardware is unable to locate the correct TLB entry from the VHPT a TLB miss fault occurs and a kernel or operating system is invoked in order to find the specified memory page within physical memory or if necessary load the specified memory page from an external device into physical memory and then insert the proper translation as an entry into the VHPT and TLB. When upon attempting to translate a virtual memory address to a physical memory address the kernel or operating system does not find a valid protection key within the protection key registers when the attempted access by the currently executing process is not compatible with the access mode in the TLB entry or the read write execute bits within the protection key in the protection key register or when the privilege level at which the currently executing process executes is less privileged than the privilege level needed by the TLB entry then a fault occurs that is handled by a processor dispatch of execution to operating system code.

In a first step the first application writes data into a memory buffer allocated on behalf of the application by the operating system as a result of a previous system call made by the application program. The application then makes a system call to the operating system to initiate transfer of the data from the memory buffer on the first computer system to the second computer system . In the call to the operating system the application provides various addresses and identifiers to the operating system that identify the remote second computer system and or a memory buffer allocated to the second application program running on the remote second computer. The operating system copies data from the applications memory buffer to an operating system buffer allocated for data transfers to the NIC. The operating system then issues a command to the NIC to transfer data in the memory buffer to the remote computer system. In certain cases the operating system may generate multiple calls to transfer packet sized blocks of data. In other cases the NIC is responsible for packaging the data into packets. In either case the NIC encapsulates the data into one or more data packets together with address and identifier information and transmits the data through the communications media and to the NIC within the second remote computer . When the NIC in the second computer system receives the data packets the NIC transfers the data to an operating system buffer and notifies the operating system in the remote computer system generally through an interrupt of the arrival of the data and of the intended recipient. The operating system responds to the interrupt by using the supplied information to determine a memory buffer previously allocated to the remote application to which the data is directed and copies data from the operating system memory buffer to the application memory buffer . The operating system then notifies the second application running on the remote computer system that data has been sent to it.

The above description is a simplification of the many steps and activities that transpire during transfer of data from a first application program to the second application program . However even this simplified illustration reveals that the data transfer involves multiple context switches between the application programs and operating systems memory copies from application buffers to operating system buffers and from operating system buffers to application buffers and multiple system calls and asynchronous notifications. All of these activities add significant latency and computational overhead to the time needed for transfer of the data from the first NIC to the second NIC .

RDMA facilitated data transfer involves various additional interfaces and connection setup overheads. However as can be seen by comparing to RDMA facilitated data transfer does not involve copying of data between operating system buffers and application buffers and does not involve context switches between application programs and operating systems as a result of system calls and asynchronous notifications. However at least in certain types of RDMA facilitated data transfer the communicating application programs need to poll memory buffers in order to detect arrival of data and in most types of RDMA facilitated data transfer the application memory buffers must be pinned in physical memory so that they are not inadvertently paged out by the operating system virtual memory subsystem. Both polling and page pinning may introduce their own latencies and inefficiencies. However various types RDMA facilitated data transfer employ a variety of techniques to ameliorate these potential latencies and inefficiencies.

RDMA facilitated data transfer discussed above with reference to is particularly popular in high performance computing HPC contexts including highly parallel distributed computing systems in which distributed shared memory and inter process messaging are implemented based on RDMA data transfers. RDMA is also popular in various high end financial computing systems that monitor and respond to real time financial transaction data. While RDMA is discussed above in terms of application program initiated data transfers RDMA may also be used by virtualization layers to transfer the contents of memory between host systems by doing so offloading memory copy related CPU activity from host processors to the processor or processors within NIC devices.

In addition to allowing the VM kernel to avoid emulating processor hardware support for virtual memory the mapping between guest physical memory and host physical memory maintained by the VM kernel provides for a variety of additional virtualization features. One feature is referred to as transparent page sharing TPS . illustrate transparent page sharing. In a virtualized host supports two VMs VM1 and VM2 with the guest physical memory for VM1 the guest physical memory for VM2 and the host physical memory represented by arrays of memory pages. As discussed above guest physical memory is virtualized by the VM kernel using the Pmap in . It turns out that in many cases two different concurrently executing VMs may access pages that contain identical stored memory values. For example two different virtual machines may run the same application in which case the two virtual machines may share many identical application program memory pages. Other examples include virtual machines that run the same or different applications which access common data. When two or more VMs access memory pages with identical contents the VM kernel can economize host physical memory allocation as next described.

In the guest physical memory page at page address a and the guest physical memory page at page address b in VM1 guest physical memory and VM2 guest physical memory respectively both have identical contents. As a result the VM kernel maps both of these pages to a single host physical memory page at host physical memory page address c. The VM kernel continuously monitors the contents of physical memory and the guest to host mappings in order to detect opportunities for shared pages such as shared host physical memory page shown in . The VM kernel does this by computing hashes for pages and comparing page hashes in order to identify candidate shared pages before undertaking a more expensive word by word comparison of the contents of potential shared pages to determine whether or not their contents are identical. The virtualization layer maintains lists of shared pages each shared page represented by a data structure that includes a reference count that indicates the number of VMs currently mapping the shared page to guest memory. The VM kernel also traps or faults writes to shared pages in order to undertake a copy on write COW operation for divergent shared memory pages. illustrates the copy on write operation. shows the same guest and host physical memories shown in . However in between the memory state shown in and the memory state shown in VM1 has written a value to the memory page at page address a in VM1 s guest physical memory as indicated by the wand curved arrow in . As a result of this attempted write the VM kernel carries out a COW operation in which shared page at address c in host physical memory is copied to a newly allocated host physical memory page at page address d and the Pmap for VM1 is altered so that VM1 guest physical memory page address a is now mapped to host physical memory page address d.

Executing virtual machines may be moved from one host computer to another in an operation referred to as VMotion another term for live migration of a VM. The relocation of an executing VM is carried out by the virtualization layers of the host computers and a VI management server in such a way that there is almost no down time in VM execution. illustrate the VMotion operation. all use the same illustration conventions additionally used in that are next described with reference to . As shown in a VM is executing in a first host computer that includes a virtualization layer and a hardware layer as discussed in detail in previous subsections. In addition the host computer is connected to one or more local mass storage devices shown as a single storage device in that stores the swap files that represent the mass storage backing for the VMs virtual memory. The first host computer is interconnected with a second host computer via one or more physical and virtual networks represented by network path in . Like the first host computer the second host computer contains a virtualization layer and a hardware layer and shares mass storage device . The VMotion operation is initiated as shown in discussed below to move VM from the first host computer to the second host computer .

There are many different steps in the VMotion operation which may involve various concurrently executing processes and various types of ordering and synchronization techniques to ensure that the movement of various components of the VM occur prior to particular suboperations. The VMotion operation involves cooperative interaction between a management server and two host systems each running a management server agent. are meant to illustrate the general approach to implementing the VMotion operation but are not intended to provide fine grained details or to indicate the particular ordering of steps and sub operations.

As shown in the stored data and instructions associated with the VM are aggregated into a VM that is transferred from the first host to the second host . Execution of the VM is subsequently resumed on the second host following restoration of all VM data and instructions on the second host.

As shown in the memory contents for the guest OS physical pages currently mapped to and residing in host physical memory on the first host computer represented in by rectangle are transferred to the memory within the second host computer. This transfer may involved transferring lists of guest physical pages and copying the contents of dirty pages to physical memory of the second host computer. During this process the VM kernel monitors using write protection or other techniques modifications to memory pages by the VM which continues to execute as the swap files and memory are being transferred from the first host computer to the second host computer. As shown in execution of the VM is halted on the first host computer and any memory pages modified during the VMotion process are then transferred from memory on the first host computer to memory on the second host computer as represented by curved arrows such as curved arrow and small rectangles such as small rectangle representing modified pages. Finally as shown in execution of the VM is renewed on the second host computer and all of the resources allocated for the VM on the first host computer are deallocated.

The current document is directed to a new approach to the VMotion operation that takes advantage of RDMA data transfer facilities within host computers. illustrate the RDMA based approach to VMotion. is similar to with the exception that the RDMA interface and data transfer machinery within each host computer are represented by a small RDMA block in the virtualization layers of the two host computers and a small RDMA block in the two hardware layers of the host computers. is similar to with the exception that when possible transfer of the VM from the first host computer to the second computer may be undertaken via RDMA based file transfer as indicated by curved arrows that represent an RDMA based file transfer path. When RDMA file transfer is not available then a traditional file transfer between the first and second hosts is carried out as represented by curved arrows .

Finally as shown in execution of the original VM is terminated any memory pages that have been written during the VMotion operation that were not transferred to the new host system are transferred and execution of the relocated VM is launched. Resources allocated to the original VM in the first host system are additionally reclaimed.

As shown in during a VMotion operation in which VM1 is moved from the first host computer to the second host computer a guest physical memory for the transferred VM1 is established within the second host computer and dirty or modified non shared pages are transferred by the RDMA data transfer facility from the first host to the second host as indicated by arrows in .

As shown in after the transferred VM is restarted on the second host and VM1 has been completely removed from the first host shared pages and remain shared with the VM kernel on the second host mapping the shared pages from VM1 guest physical memory via RDMA data transfer back to the shared pages on the first host computer as indicated by arrows and in .

As shown in the VM kernel on the second host computer may during monitoring for potential shared ages identify a page shared between VM3 and VM1 and remap the mapping between VM1 guest physical memory to the shared page as represented by arrow in . Alternatively the mapping for VM3 may be altered. However in either case local page sharing continues to be carried out on the second host computer.

As shown in when VM2 which continues to execute on the first host computer attempts to write to shared page then a COW operation is carried out in which shared page is copied to a new page in host physical memory and the mapping from guest physical memory to host physical memory for VM2 has changed as indicated by arrow . However the shared page continues to be mapped for VM1 back to the shared page in the first host computer as indicated by arrow .

As shown in when VM1 attempts to access the second shared page via a write operation a RDMA transfer is used to copy the second shared page to a location in the host physical memory of the second host computer and the pages remapped from VM1 guest physical memory to host physical memory as indicated by arrow in . Thus sharing over RDMA from the second host computer to the first host computer is discontinued by a remote COW like operation. In certain implementations any access by VM1 to a remote shared page may result in a remote COW like operation while in other implementations the remote COW like operation is carried out for any write access by VM1 to the shared page resulting in copying over RDMA of the contents of the page to the physical memory of the second host computer. In yet other implementations the remotely shared pages are copied to the new host over time. The remote COW like operation may be carried out using normal network operations or RDMA data transfer and may involve in addition to the first and second host systems the management server.

It should be noted that for both memory page transfers and remote access to shared memory pages by a relocated VM the RDMA transfer may be initiated through the RDMA interface of either or both of the host systems between which a VM is moved. For example transfer of dirty memory pages are likely to be initiated through the RDMA interface to the network controller of the source host system and access of a shared memory page is likely to be initiated through the RDMA interface to the network controller of the host system on which a relocated VM executes. However because both host systems have management server agents the management server can direct data transfer commands to either agent.

Although the present invention has been described in terms of particular embodiments it is not intended that the invention be limited to these embodiments. Modifications within the spirit of the invention will be apparent to those skilled in the art. For example any of various different implementations of RDMA based VMotion can be obtained by varying any of many different design and implementation parameters including programming language control structures data structures hardware platforms virtualization layers and other such design and implementation parameters. In different implementations the COW like breaking of remote page sharing can be carried out upon any access to the page by moved VM or alternatively when the moved VM attempts to write to the page. Any of many different techniques for transferring modified memory pages and other information through the RDMA data transfer facility can be used so that on the new host computer the transferred memory pages can be copied into host physical memory pages and the guest page tables and virtualization page tables properly constructed and initialized.

It is appreciated that the previous description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the present disclosure. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the disclosure. Thus the present disclosure is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

