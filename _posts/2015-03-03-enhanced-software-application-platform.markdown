---

title: Enhanced software application platform
abstract: A system includes information for generating a first appliance based on first appliance information, information for generating a second appliance based on second appliance information, and information for configuring communication between the first appliance and the second appliance. The system further includes at least one processor configured to generate a first volume by copying the first appliance information using a first set of parameters associated with an environment in which the first volume is situated, and generate a second volume by copying the second appliance information using a second set of parameters associated with an environment in which the second volume is situated. The system also includes at least one processor configured to initiate a first appliance instance by executing code from the first volume using information in the third volume, and initiate a second appliance instance by executing code in the second volume using information in a fourth volume.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09389791&OS=09389791&RS=09389791
owner: CA, Inc.
number: 09389791
owner_city: New York
owner_country: US
publication_date: 20150303
---
The present application is a continuation under 35 U.S.C. 120 of U.S. Non Provisional patent application Ser. No. 13 676 090 filed Nov. 13 2012 which claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Patent Application No. 61 559 631 filed Nov. 14 2011 all of which are hereby incorporated by reference in their entirety.

The present disclosure relates generally to network based applications and more specifically to an enhanced software application platform. Applications or services are increasingly being provided using networks. For example applications have been delivered as a service in Software as a Service SaaS architectures. Problems have arisen in this area. For example provisioning and or upgrading such applications is labor intensive. As another example access to items such as log files and audit information is difficult. As yet another example using more than one application that is delivered using networks as described above has many seams that may hinder productivity and integration. As yet another example multiple applications may require multiple authentications.

According to particular embodiments of the present disclosure a system includes information for generating a first appliance based on first appliance information information for generating a second appliance based on second appliance information and information for configuring communication between the first appliance and the second appliance. The system further includes at least one processor configured to generate a first volume by copying the first appliance information using a first set of parameters associated with an environment in which the first volume is situated and generate a second volume by copying the second appliance information using a second set of parameters associated with an environment in which the second volume is situated. The system also includes at least one processor configured to initiate a first appliance instance by executing code from the first volume using information in the third volume and initiate a second appliance instance by executing code in the second volume using information in a fourth volume.

According to particular embodiments of the present disclosure the system further includes information for generating a third appliance based on the third appliance information and information for configuring communication between the first appliance and the third appliance. The system also includes at least one processor is further configured to generate a fifth volume by copying the first appliance information into the fifth volume using a third set of parameters associated with an environment in which the fifth volume is situated and generate a sixth volume by copying the third appliance information into the sixth volume using a fourth set of parameters associated with an environment in which the sixth volume is situated. The at least one processor is further configured to initiate a third appliance instance by executing code from the fifth volume using information in the seventh volume and initiate a fourth appliance instance by executing code in the fifth volume using information in the eighth volume.

According to particular embodiments of the present disclosure a method includes accessing application information comprising information for generating a first appliance based on first appliance information information for generating a second appliance based on second appliance information different from the first appliance information information for configuring communication between the first appliance and the second appliance a first identifier associated with a third volume and a second identifier associated with a fourth volume. The method further includes generating a first volume using the application information and a first set of parameters associated with an environment in which the first volume is situated and generating a second volume using the second application information and a second set of parameters associated with an environment in which the second volume is situated. The method also includes initiating a first appliance instance by executing code from the first volume using information in the third volume and initiating a second appliance instance by executing code in the second volume using information in a fourth volume.

As will be appreciated by one skilled in the art aspects of the present disclosure may be illustrated and described herein in any of a number of patentable classes or contexts including any new and useful process machine manufacture or composition of matter or any new and useful improvement thereof. Accordingly aspects of the present disclosure may be implemented as entirely hardware entirely software including firmware resident software micro code or other suitable types of software or combining software and hardware implementation that may all generally be referred to herein as a circuit module component or system. Furthermore aspects of the present disclosure may take the form of a computer program product embodied in one or more computer readable media e.g. tangible non transitory computer readable media having computer readable program code embodied thereon.

Any combination of one or more computer readable media may be utilized. The computer readable media may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an appropriate optical fiber with a repeater a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device. Program code embodied on a computer readable signal medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF or any suitable combination of the foregoing.

Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages including an object oriented programming language such as Java Scala Smalltalk Eiffel JADE Emerald C C VB.NET Python or the like conventional procedural programming languages such as the C programming language Visual Basic Fortran 2003 Perl COBOL 2002 PHP ABAP dynamic programming languages such as Python Ruby and Groovy or other programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider or in a cloud computing environment or offered as a service such as a Software as a Service SaaS .

Aspects of the present disclosure are described herein with reference to flowchart illustrations and or block diagrams of methods apparatuses systems and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable instruction execution apparatus create a mechanism for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that when executed can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions when stored in the computer readable medium produce an article of manufacture including instructions which when executed cause a computer to implement the function act specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer other programmable instruction execution apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatuses or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

SaaS system may allow the separation and independence of an application s operational and functional lifecycles. Thus integrations of applications are made independent. Each application may be accompanied by sufficient descriptive information e.g. metadata such that an infrastructure management layer can create instances of the application expose information about it and manage its lifecycle. One aspect of the present disclosure is that all such applications may conform to and implement a common set of standards specifying the means and semantics for their interaction and communication with the infrastructure management machine and or other applications. The infrastructure management machine then requires no knowledge of the internal details of applications and manages them all in a standard uniform way.

In this way users of one or more applications are unaware of the physical location of the underlying machines instances and may interact with applications via the provided logical connections and standardized protocols and semantics. As a consequence of this mechanism applications interact opaquely and moreover securely in that they cannot directly address one another. Integrations between applications then are unable to become dependent on the details or implementation of the underlying infrastructure or even other applications.

The explicit separation of operational protocols interfaces and semantics from application functional protocols interfaces and semantics then decouples operational and application functional lifecycles. For example a constant version of the operational components can support rapidly evolving versions of applications. In particular embodiments all tier 1 operational communication is via a standardized uniform protocol and semantics. The protocol standardizes object type and base attribute semantics and uses metadata to describe their behavior and values. This provides a significant degree of version resilience to the system since all applications will never upgrade synchronously. Additional version resilience and loose coupling is achieved by programming elements to be more metadata aware using metadata to discover what can be communicated.

The infrastructure management layer in some embodiments may be unaware of the functional interactions integrations between applications and users it is responsible only for enabling the execution of the applications their management and facilitating logical connections to them. Changes to application behavior implementation interaction and or integration are opaque to it. Management and operational integration is invariant to the underlying integrations or solutions.

The disclosed mechanism then decouples functional integration from operational integration permitting each application instance to have its own set of lifecycles independently from the other. Operational functions are robust to changes in applications including versioning and applications are robust to changes in operational machinery including versioning. This independence eliminates a large equivalence class of often cyclic dependencies that emerge in integrated systems such as the typically slower operational lifecycle holding back functional application level changes.

The disclosed mechanism also enables functional application decoupling. Each application s tightly coupled functionality is encapsulated in a single operational unit. By convention loosely coupled components are encapsulated separately. The disclosed mechanism may use metadata to implement loose coupling among applications at the user functionality level which is much more version change resilient than with standard interfaces used by Java or SOAP. Some embodiments may use the OData standard to implement RESTful interfaces with metadata to define the objects attributes and describe their values and behaviors for every interface If encapsulation is complete e.g. application integration is only through standard OData interfaces then operational upgrades can be independent of integration induced coupling.

In some embodiments a virtual machine manager may run a virtual machine environment on shared hardware and may embody aspects of the present disclosure. In such embodiments system may utilize hardware virtualization techniques to run one or more virtual machines inside one or more virtual machine environments that are controlled by one or more virtual machine managers. Hardware virtualization may refer to the abstraction of computer hardware resources allowing for the sharing of the underlying resources between the one or more virtual machines. A virtual machine manager may refer to virtualization software operable to manage and or control the execution of one or more virtual machines. In some embodiments virtual machine manager may refer to a bare metal or native type hypervisor running directly on shared hardware to control the hardware and manage the virtual machines running thereon. In other embodiments virtual machine manager may refer to a hosted type hypervisor running within a conventional operating system environment. Examples of virtual machine manager software include VMware vSphere VMware ESXi Citrix XenServer and Microsoft Hyper V. In other embodiments virtual machine manager may refer to a purely software based virtual machine or byte code interpreter such as a Java Virtual Machine or NET or in an application container framework such as OSGI containers. In certain embodiments the virtual machine manager may run on a plurality of computer systems in shared hardware . In other embodiments the virtual machine manager may run on a single computer system in shared hardware .

In embodiments with virtualization the virtual machine manager may be operable to create and control a virtual machine environment. The virtual machine environment may be operable to run one or more virtual machines. A virtual machine may refer to the software implementation of a physical machine wherein the virtual machine may provide computer functionality. Thus a virtual machine may provide an environment that emulates a physical computer platform and may execute an operating system and or one or more software programs or modules. For example a virtual machine may run operating systems such as Microsoft Windows Linux and Mac OS. Furthermore a virtual machine may run one or more software applications and or other logic that may be encoded in one or more tangible computer readable media and may perform operations when executed by a processor.

Infrastructure management layer of system may communicate with the one or more application instances through one or more interfaces associated with the application instance. For instance the application instance may expose one or more interfaces to infrastructure management layer through an application API. In particular embodiments the APIs exposed by application instances to infrastructure management layer may conform to a uniform format and may include a particular set of APIs that are common to all applications. Through these APIs infrastructure management layer may communicate with application instances similar to objects in object oriented programming. In other words application instances may essentially be black boxes with the interfaces being the only way for infrastructure management layer to communicate with them.

To improve the version resilience of these interfaces particular embodiments may include interfaces defined in terms of RESTful semantics POST PUT DELETE GET where the object s attributes are fully described using metadata. This may require that both ends of each communication agree on the object naming and metadata semantics as well as the REST protocol and from that make assumptions that there is agreement on functional semantics. In certain embodiments the OData protocol may be used.

Infrastructure management layer may be operable to facilitate the entire lifecycle i.e. the creation management and destruction of applications . For instance infrastructure management layer may be operable to instantiate application instances in execution environment based on metadata and an application templates . Once instantiated in execution environment certain details of applications may be stored in managed resource data . In addition certain resource allocation requests made by system administrators may be handled by infrastructure management layer . These requests may be stored and or managed through task module . In performing this role infrastructure management layer may become the source of record for almost all information related to that tenant s resource assignments for each provisioned capability. This information may be stored at tenant data .

Sometimes requests to infrastructure management layer and the actions performed on their behalf may often take a long period of time to complete such as in the case where a new application instance must be created on which to home the tenant. It is also possible that infrastructure management layer may be too busy to immediately fulfill a given request or that certain resources are not yet available. Therefore in some embodiments the interaction model for resource allocation de allocation and related operations performed by infrastructure management layer may be asynchronous.

In some embodiments tenants for application instances may be created and managed by some external management service. As one example this function may be managed by a Liferay or other Portlet Container component running on hardware not shown . Organizations tenants and users in such embodiments are all created and managed by this external service. Infrastructure management layer would therefore be concerned solely with tenants and capabilities which are identified by a globally unique identifier UUID preferably accompanied by a human readable display name. When infrastructure management layer becomes aware of a new tenant capability mapping it creates a first class object called a Tenant Application Context TAC . A TAC becomes the container so to speak for all information relating to that capability for that tenant. TACs may be stored in tenant data on infrastructure management layer .

Requests submitted to infrastructure management layer may be in connection with some end user administrator transaction. They also may be part of a larger set of transactions such as for example provisioning three new capabilities for a tenant. In such embodiments infrastructure management layer may guarantee that once it has accepted a request that it will eventually complete the request or provide the client with notification of its failure. Some implications of this may include a the task module may require a durable backing store b tasks may never deleted until they have completed and such completion is acknowledged by the client or some very long timeout expires c workers may logically acquire tasks by atomically acquiring a task and marking it as being processed and as such workers may need to periodically update their status e.g. a timestamp on the task and d if a task claims to be in progress but has not been updated within a defined time interval it may be deemed that the given instance e.g. application instance failed and the task may then either be taken up by another instance e.g. application instance marked for a cleanup operation or as applicable moved into a failed status.

Sometimes certain types of requests may have precedence over others. In such cases task module may implement a priority system such as a set of parallel queues each with its own scheduling semantics. Sometimes other scheduling disciplines or algorithms may be used to select the order in which tasks are processed. In some embodiments it may be possible to administratively pause the task module i.e. prevent further tasks from being serviced so that the system can be quiesced e.g. for maintenance activities. In addition in some embodiments infrastructure management layer may provide an interface for clients to query the task module for example to determine the existence of tasks related to a tenant. As appropriate infrastructure management layer may also allow clients to abort or cancel tasks presuming that such cancellation can be done cleanly and safely.

Execution environment may be considered in some embodiments to be the union of instances of common components and provisioned components constituting in part or in whole a particular SaaS deployment. In certain embodiments each execution environment has exactly one infrastructure management layer instance associated therewith. Infrastructure management layer may have access to and credentials for the execution container API on each execution container .

Each execution container may be the authoritative source of record for any execution container specific objects such as application instances or data volumes to which the execution container assigns a UUID. Examples of execution containers may include the CA APPLOGIC grid controller and VMWARE VSPHERE. Infrastructure management layer fetches and monitors available resources e.g. CPU memory network bandwidth disk consumption for each of its execution container . Such information is fetched dynamically via execution container API and may be stored in managed resource data . Examples of execution container API include the API exposed by a hypervisor such as VMWARE ESX or ESXi. Infrastructure management layer may make resource allocation decisions in part based on available execution container resources. In some embodiments each execution container may be the authoritative source of record for inter alia present application and component state volume storage locations appliance catalog contents master volume images actual available resources as opposed to unreserved resources and execution container status. Infrastructure management layer may associate other attributes with execution containers such as an execution container with special properties e.g. high performance I O or low availability SLAs . In some embodiments these properties may be used to make resource allocation decisions for example in deciding which execution container a given application instance should be instantiated.

In particular embodiments components may be assigned network addresses. Network addresses may include IP addresses including version 4 IPv4 addresses and or version 6 IPv6 addresses or any other suitable addresses based on network layer Layer 3 protocols. Infrastructure management layer in particular embodiments may assign IP addresses to application instances . These IP addresses are required to be from networks of specific types and execution container association. The set of addresses of a specific type for a specific execution container may be referred to as a pool of addresses. A network in execution environment may be uniquely identified by its UUID and by its network address subnet mask and execution container ID. In some embodiments execution environment may include one or more DNS servers and or default gateways. In particular embodiments each network may also have a property specifying which if any VLAN must be used. Networks may be added and managed as an administrative function of infrastructure management layer .

The IP addresses assigned by infrastructure management layer may be associated with exactly one network and an IP address may be uniquely identified by its UUID and by the combination of its host address and network UUID. Note that in some embodiments the same Host Address e.g. 192.168.55.24 may exist in multiple networks such as if infrastructure management layer manages execution containers in multiple data centers . Thus the network UUID may be required for uniqueness in some embodiments. Further each IP address may be associated with zero or one application instance this relation is by UUID and may be stored at infrastructure management layer for both the application instance and the IP address. Once provisioned in infrastructure management layer the IP addresses in a given network are managed by infrastructure management layer and may not be used for other purposes. Infrastructure management layer then becomes the authoritative source of record for IP address availability assignment or any other suitable network address information and stores this information in managed resource data . In particular embodiments infrastructure management layer thus assumes responsibility for managing the IP address spaces assigned to it including inter alia the prevention of duplicate IP address assignments.

Management of application instances management may also be handled by infrastructure management layer . Moreover infrastructure management layer may be the authoritative source of record for the set of application instances available in its execution environment as well as all associated metadata . Infrastructure management layer may create application instance on demand based on tenant provisioning requests or administrative requests. In one embodiment application instances are created only when a request for a capability for a new tenant is received and based on capacity and other factors that tenant cannot be provisioned on an existing application instance .

To create an instance infrastructure management layer may combine metadata associated with an application with an application template . Metadata may refer to all of the requisite information to instantiate configure operate and manage an application and its lifecycle in a completely data driven manner. In this way metadata may define the generic executable portion of an application instance. Metadata may be stored in a single object at infrastructure management layer . This object may be the complete set of information needed to create and use an instance of an application. Metadata may be expressed in any suitable computer readable data format and may include for example an XML or JSON document. In particular embodiments metadata is expressed in a format that is both human and machine readable. Examples of metadata include for example machine capability identifiers version identifiers human readable descriptions or other text the source of the generic executable portion of the machine instance specific configuration parameters requisite resource assignments such as network addresses license keys host names credentials and or key material definitions of all network communications interfaces or other services exposed by the machine and how to access them capacity nameplate ratings resource requirements e.g. disk space memory CPU power performance capacity and other metrics exposed by the machine descriptions of the machine s ability to perform backup and restore operations of various sorts regulatory compliance standards implemented or that can be activated in the machine the types of high availability fault tolerance et seq. mechanisms supported by the machine machine specific requirements for provisioning top level objects object owners data partitions e.g. tenants information regarding the machine s support of one or more authorization and or role management schemes the machine s ability to be upgraded and as applicable what prior upgrade from versions are supported digital signatures checksums or other suitable cryptographic data used to establish integrity origin and authenticity of an instance of the machine and or the metadata itself.

To be created an application instance may be assigned and allocated a set of resources within execution environment . In some embodiments some resources may be specific to a particular execution container . Instantiation of the application instance may not occur unless and until all requisite resources have been acquired. In certain embodiments the remaining capacity of each execution container managed by infrastructure management layer may be analyzed and considered when provisioning a new application instance. In some embodiments infrastructure management layer may have configurable thresholds for limited resources such as IP pools and may generate alarms when these thresholds are not met. A failure to atomically acquire the requisite resources for an application instance may result in failed instantiation rollback of any changes made on its behalf freeing of any resources already assigned to it and the failure or suspension and subsequent retry as appropriate of any user transaction s depending thereon.

Infrastructure management layer may also control the deletion of application instances when appropriate. In some embodiments application instances may be deemed vacant when they are no longer associated with any tenant application contexts or tenant data . In some embodiments deletion of vacant application instances may occur automatically if excess spare capacity is detected by infrastructure management layer . Alternatively deletion may be requested or forced via a request to infrastructure management layer .

Furthermore application instances may be pre allocated by infrastructure management layer . In some embodiments the instantiation and startup of a new application instance may take a long time for example longer than is desirable for user interaction. Free or unused applications instances for a given capability may be instantiated ahead of time to speed onboarding or assignment of new tenants. In some embodiments infrastructure management layer may be empowered to automatically manage a pool of pre allocated application instances. In some embodiments infrastructure management layer may choose whether pre allocated application instances are maintained in a stopped or running state which functionality may be useful for capacity management.

Allocation of resources by infrastructure management layer when instantiating application instances may be done according to one or more capacity metrics. In some embodiments these metrics are defined and declared in one or both of metadata application template . Examples of capacity metrics include a number of tenants a number of users a number of user data objects such as tickets or images transactions per second and a number of concurrent users. Each application instance may also have its own resource requirements as well. Examples of these include CPU e.g. number of cores RAM IOPs and network bandwidth. Furthermore there may also be commercial requirements that imply capacities such as a number of seat licenses within an environment OS Licenses or limitations due to trial licensing agreements for example. In embodiments including commercial capacity limitations such commercial capacities may or may not be owned or managed directly by infrastructure management layer . However infrastructure management layer may still be responsible for reserving them from e.g. a third party license server or registering them with an auditing tracking system.

In some embodiments tenant application creation requests may include capacity reservation information. For example a given request may specify a requirement for 100 user seat licenses and 10 GB of document storage. In some embodiments infrastructure management layer may ensure that the created or existing application instance to which the tenant application is assigned has sufficient capacity to satisfy the request.

Subsequent provisioning operations for an existing tenant on an application instance may require increasing that tenant s reservation for one or more capacity resources such as adding more users or increasing the tenant s allotted storage. However because instances may serve multiple tenants infrastructure management layer may keep track of both the tenant s own reservations and the total reservations for that instance over all tenants thereon . Such information may be kept in managed resource data . In particular embodiments a user transaction requiring or requesting the allocation or deallocation of capacity related resources may reserve such resources via infrastructure management layer and the transaction may not complete successfully unless such reservation is successfully made.

Infrastructure management layer itself may or may not have exclusive control over the resources it s reserving and allocating to capabilities and tenants. For example the literal allocation of RAM to an instance may be performed by the hypervisor in a virtualization environment. As another example a licensing server owns or manages the number of remaining licenses for a given product. Infrastructure management layer does own however the mapping of that reservation to the tenant and capability for which it was reserved and is the authoritative source of record for the same. Thus infrastructure management layer is responsible for tracking each reservation s lifecycle. For example if a tenant is moved to a different application instance e.g. from instance to the resource reservations remain charged to assigned to that TAC in tenant data the old instance gets credited for applicable resources and the new instance gets charged for them. If the TAC is later destroyed then all of its reserved allocated resources may be freed up. In the case of commercially implied capacities this could mean returning the licenses to the license server.

In addition to resource management in some embodiments infrastructure management layer may also handle metering of resources in execution environment . In such embodiments each application will expose a metering interface via infrastructure API . The metering interface permits infrastructure management layer or other entities to query the present or past values for the capacity metrics it exposes. Infrastructure management layer may expose via an API read only access to metering data for the applications it manages. In some embodiments metering data for non capacity related metrics may be exposed in like manner. For example an application may expose metrics relating to transaction response times in some cases such metrics may not relate to capacities or reservations but may be of interest for example for monitoring the application s health or performance.

In the event that metering data indicates that a tenant or application instance is exceeding its reserved capacity for example too many concurrent user sessions or transactions per second such information may be used to adjust the actual available capacity for that instance. System level capacity monitoring such as CPU utilization memory utilization or free disk space are not the infrastructure management layer s responsibility. However should the monitoring system or administrators determine that a given instance is being over utilized e.g. response times are slow due to CPU consumption infrastructure management layer may be able to respond accordingly e.g. by marking the offending resource e.g. CPU as exhausted and denying the allocation of more resources on the instance.

When deciding where to home a given application instance or tenant infrastructure management layer may consider several factors including each execution container s remaining resource capacity the locations of other instances for the same tenant which might make co location on the same execution container more or less preferable each execution container s remaining capacity in terms of total running applications and components and or the availability and or maintenance status of a given execution container.

In particular embodiments when considering a provisioning request the present in use capacity may be calculated as the maximum of the present reserved capacity and the actual in use capacity as determined for example by metering data. Moreover in some embodiments when deciding where to home a new tenant infrastructure management layer may leave some headroom or unused capacity on existing instances. For example the addition of a given tenant should not exhaust the user capacity of an instance. The amount of headroom on each metric may be determined heuristically or empirically and so may be configurable via infrastructure management layer API.

In some embodiments infrastructure management layer may support a feature wherein it maintains a pool of pre created application instances for a given capability which will be used to fulfill new requests for that capability. The size of the pool how many instances if any are powered on or any other suitable metric of the pool may be configurable via infrastructure management layer s API. Some configurable default settings may also be applied to newly added applications. Infrastructure management layer may but need not commit resources such as IP addresses to these vacant instances. This is to say that an instance can exist but not be started or necessarily have tenant assignments without having all of its runtime resources committed or charged. Of course for this instance to be used to fulfill a tenant provisioning request infrastructure management layer may be able to reserve and assign all other applicable resources the process is identical except that the instance already exists. These pools will be created and replenished automatically by infrastructure management layer at its convenience based on system activity and configuration. In some embodiments infrastructure management layer may further provide a method through an API for operators to request that additional instances be pre provisioned. This might be done for example in anticipation of a large batch of new tenant requests at the end of a fiscal quarter. As with all functions pool management related and user requested instance creation is managed via task module and each action requires the acquisition of the appropriate resources.

Infrastructure management layer may also delete tenants over time. In some cases this will free up resources on multi tenant applications. In other cases this will leave empty application instances i.e. instances which are not associated with any tenant. If pre allocated pools are enabled these vacant instances may be reused by those pools. In other cases there may be an excess of vacant instances resulting in wasted system resources. If pooling is disabled pool size zero then upon becoming vacant an instance may be immediately shut down and destroyed and any resources consumed by it may be freed. In embodiments having pooling is enabled a high water mark for each pool size may exist if the size of the pool grows and stays above this threshold for some period of time then one or more of the excess instances may be destroyed. In some embodiments infrastructure management layer may also through an API permit administrators to request the destruction of vacant application instances.

Some embodiments of infrastructure management layer may also provide power state monitoring. Other facilities may be in place for the runtime monitoring of application e.g. application availability or liveness and these facilities may be empowered to restart or power on instances based on failure conditions. In some embodiments infrastructure management layer may coordinate or help to coordinate maintenance activities requiring the shutdown or restart of a given application instance. In such instances infrastructure management layer should be capable of fetching understanding and manipulating application power state. In some embodiments it may be desirable for infrastructure management layer to periodically query power state for its instances e.g. if infrastructure management layer is recovering from a cold restart and as applicable power on instances that are expected to be up and serving customers. In like manner infrastructure management layer may expose an API enabling administrators to explicitly shut down a given instance e.g. for maintenance. In particular embodiments when infrastructure management layer has brought an instance down in this manner requests towards this instance e.g. routing requests from the dispatcher may receive responses that explicitly state that the instance is down for maintenance. This permits disambiguation between truly failed instances and instances that are down for a reason.

Additionally infrastructure management layer may manage backup restore operations associated with application instances as a whole or for individual tenants assigned to the application instance. As user data is backed up infrastructure management layer may be informed of the backup s location recording this as a user data resource for a given tenant or tenants in managed resource data and or tenant data . The process of restoring an instance could then take several shapes. In one embodiment user data may be restored directly into an instance e.g. a database restore . In particular embodiments infrastructure management layer may need to coordinate with respect to scheduling of tasks ensuring that the restored data matches the current e.g. schema version s of the present application instance partially shutting down or quiescing the application or tenant within the application during the restoration procedure or restarting or reactivating the application or tenant post restoration.

In one embodiment user data for an entire application instance may be restored by swapping in a backup copy of a user data volume. The role of infrastructure management layer would then include scheduling the modification s to the respective instance checking that the inbound volume is current enough e.g. it is not missing any new tenants that have been added since the backup was created checking that the volume is of a compatible schema version with the running application and providing operations with the scope of impact of the maintenance operation with respect to the tenants involved. Restoration by infrastructure management layer may then include shutting down a running application instance moving exporting existing user data volume s importing backup volume s restarting the application instance and performing some kind of sanity validation that the instance is alive. In the event of a failure of the restoration infrastructure management layer can reverse the above process to restore the original volume. This action is not tenant aware.

In some embodiments infrastructure management layer may also work to import data into a new application instance. In such embodiments infrastructure management layer creates a new application instance. However instead of creating the user data volumes from scratch it uses backups. Either new resources are assigned to the new instance or resources are transferred from the existing instance and the new instance is brought up and validated. The old instance is then disposed of appropriately by infrastructure management layer . In this discussion we see the level of interaction required between infrastructure management layer and external components. It should be noted that some functions described as being done by infrastructure management layer may not be proper features of infrastructure management layer but rather processes built on top of an API associated with infrastructure management layer . Insofar as volumes application instances power states or other application components or attributes are being manipulated these functions are performed by infrastructure management layer but driven by the API associated with infrastructure management layer .

Infrastructure management layer may additionally be involved in disaster recovery activities. Disaster recovery may refer to the process and implementation for re establishing services that have been disabled disconnected or destroyed by a catastrophic event. The context is not one of a server going down but rather of the failure unavailability of an entire datacenter or more. To handle such events backups or replicas of infrastructure management layer itself and its databases and each tenant s component s data will be located at other datacenters. It is assumed in aggregate the remaining set of datacenters has sufficient capacity to service these additional instances. 

For example an instance of a capability providing the implementation for a first TAC T is destroyed or is otherwise rendered permanently unavailable at a first datacenter DC managed by a first infrastructure management layer AIM . It may then be administratively or automatically determined that the instance must be resurrected at a second data center DC managed by a second infrastructure management layer AIM . The second infrastructure management layer is instructed either through some specific feature or through composition of existing APIs to take ownership of the first TAC T and materialize it on an instance in the second datacenter DC. The AIM server presumptively has a read only replica of AIM s data store. AIM fetches all of T s data from AIM s database which it can safely do because all identifiers are UUIDs including those for e.g. application descriptors which are assigned when the descriptor is created. . AIM is now in possession of all information required to reconstruct an instance for T namely its application descriptor the locations of its non user data volumes the sizes and shapes of its non user data volumes the values of any instance specific configuration properties non site specific non environment specific properties references via the application descriptor for the values of e.g. credentials or other value types reservations for resources not managed by infrastructure management layer such as licenses as applicable and resource requirements based on reservations AND actual metering data . AIM can now begin the process of reserving whatever other resources capacities are required including locating and obtaining user data files volumes. Once reservations allocations are obtained AIM assigns T s context to a new or existing Application Instance moves data in and re starts the instance as appropriate. Another step in this process might be for AIM to enqueue a job whose purpose is to notify AIM should it ever come back up that AIM now has ownership of T AIM would then be responsible for cleaning itself up as appropriate.

As explained above and shown in each application instance may be composed of one or more appliance instances . Appliance instances may have several common characteristics. For instance each appliance may have a set of appliance boundary properties which may include volume definitions resource requirements and other metadata of the appliance. In addition appliances may be shipped as images in catalogs and subsequently stored in an appliance image repository e.g. appliance image repository of . Appliances may be instantiated within application instances e.g. application instances of . Every instance of a particular appliance or class is identical at initial startup. In other words if customer X and customer Y are both using version V of some appliance then they are running exact copies of the same appliance and they differ only in boundary configuration and user data contents.

Appliances come in two different forms mainly host appliances and cluster appliances or assemblies . Standards for each form differ and will be discussed below. Host appliances are literally virtual machine VM images. They consist of one or more volumes and metadata. Cluster appliances or assemblies on the other hand are assemblies of one or more other appliances that are encapsulated so as to look act and be managed just like host appliances. Cluster appliances are run powered on but are not real hosts the assembly itself has no Operating System and consumes no resources note however that the appliances the assembly encapsulates do consume resources . Much like an application template is a recipe for creating an instance of a particular application a cluster appliance is a pre defined set of appliances and configuration encapsulated in a neat black box that is accessed via interfaces.

Catalog appliances e.g. those located in a repository such as appliance image repository of are host images. The configuration and disk contents of a catalog appliance are the same every time it starts. Booting up a catalog appliance is analogous to booting off of a Live CD the exact same code and configuration are used. All instances of a particular catalog appliance are identical if the catalog appliance is updated all instances will see the change upon restart.

Another form of appliance is the singleton appliance. A singleton exists exclusively within the context of a specific Application Instance. Unlike a catalog appliance changes to singletons are persistent. For example every singleton host has its own unique history persisting configuration data and or software patches. Whereas a catalog appliance is analogous to a Live CD a singleton is analogous to having installed an OS and application onto a server s hard disk i.e. changes are persisted and the system no longer is tied to the installation media. The contents of a singleton will naturally tend to diverge from its initial state. Traditional servers are essentially singletons. Thus a singleton comes along with all of the operational challenges and headaches associated with managing traditional servers. Every instance of a particular catalog appliance is identical among other things this means that all such instances can be changed via a single change to the respective catalog appliance. In contrast every singleton is unique and regardless of its origin has no relationship to any version controlled objects. Singleton hosts allow configuration drift and require traditional upgrades and whole application backups practices inconsistent with efficient mass deployment. They can however be managed and maintained using traditional enterprise approaches.

In some situations such as when there is a persistent license associated with an appliance instance the appliance instance can run as a singleton retaining new state. However the appliance may otherwise conform to the restriction that such state is completely independent of how the appliance instance behaves. That the singleton is considered disposable replaceable as long as it starts stops backs up and restores user data with no difference in functionality independent of which singleton the data is associated with.

In particular embodiments application may include boundary properties which may store for example information about what the application connects to in order to be properly configured. Some parameters may be defined so that these properties can be properly built into the appliance s configuration at start time. In some embodiments the standard is that all application instance specific configuration parameters are externalized. Once part of an application assembly or appliance cluster these parameter values will be chained together and eventually externalized. There are also generic externalized boundary properties that the provisioning system sets for all applications. These properties can be imported from application and used in the startup process for each appliance .

Data volumes e.g. user volume and application user volume may include certain user data written durably to disk by application i.e. persistent data. Data written to appliance instance volumes e.g. appliance volume or the appliance volumes off of which a particular appliance instance runs is not persistent it is never backed up and can be lost any time the Appliance restarts. Application data that requires persistence may be referred to as user data. In particular embodiments user data may be written to user volumes e.g. user volume and application user volume volumes which are explicitly created assigned to specific appliances via the overall application s assembly and are managed by a specific application instance. User volumes may also be used to store log files monitoring data paging files swap space or other persistent information about an appliance or the application e.g. at log volume and application log volume . These volumes don t technically contain user data but function in the same manner the volumes themselves are created within and managed by an application instance. In some embodiments application user data may be a virtual identification of or reference to individual user volumes within application . Likewise in some embodiments application log data may be a virtual identification of or reference to individual log volumes within application .

In some embodiments application descriptors may specify what user volumes an application requires and what types of volume they are whether true User Data or other such as logs. The backup security auditing and other operational requirements for different types of volumes may vary. User data proper may be backed up using a common backup mechanism. In certain embodiments backups may occur at the application level by default in other embodiments backups may occur at a volume level. It should be noted that every single piece of user data should be associated to exactly one tenant on the system. This is because there is no notion of persistent system data or configuration data except to the extent that such data is specific to a particular tenant.

Configured as such application can be distributed from a central control point as a set of version controlled components. Such components may include application templates appliances and application descriptors. Creation of user data volumes may be separate from application distribution. Further application may have a boundary separating external configuration from internal configuration. For instance a version number may reflect an internal configuration. However in some embodiments the version presented by the external boundary may reflect exactly one collection of internal component versions.

In some embodiments resources in or used by application are provisioned using a parametric description language applicable to multiple types of applications and not particular to any one type of application. Application instances can be instantiated from a single application template. Application may use external services such as a collaboration service or a transaction database service. Tenants of application can be allocated as a separate step from resource provisioning.

In some embodiments application becomes a host to at least one tenant. Application exposes a common interface via which it is provisioned for new tenants. Tenants may be associated with tenant IDs that can be globally unique. Provisioning a tenant can involve initializing data in application and removing a tenant can involve deleting tenant data from application . Tenant data can be stored in application user volume and or user volumes . Resources consumed by application can be reported on a per tenant basis as well as a per application instance basis. Application can expose at least one capacity metric. A common API is used to fetch values of these metrics. Tenant provisioning can be automated through a standard OData interface on the management network.

In some embodiments application can include an interface for provisioning users to support single sign on SSO . As an example an OData interface on the management network can be used to add and delete users. Users can be added at two levels admin all powerful and basic user minimal access . In some embodiments application can expose additional roles. In other embodiments when SSO is not available application can include user database and authentication methods.

In some embodiments application can be deployable and maintainable in an execution container . Application can be deployed in a cloud network such as one deployed by CA Technologies as well as in other public and private cloud networks. In private cloud networks application may be able to communicate within private networks. In some embodiments application may use agents to access remote private networks. Such agents can be centrally configured and maintained. Application can be accessible from a network routable location. References to external services can be done by names e.g. resolved by DNS .

In some embodiments application can include internal monitoring. For example application can include a monitor appliance e.g. appliance which provides monitoring for application . For example this monitoring appliance can be implemented using the APPLOGIC MON appliance provided by CA Technologies. The MON appliance aggregates all internal monitoring so that the application can be monitored as a single entity. The MON appliance in this example can have built in connectivity to monitoring systems a third party probe such as NIMSOFT probes or exposing SNMP MIBs. At runtime each of appliances can stream generic monitoring metrics and application specific metrics and critical events on a single connection to a monitor appliance. This does not disable use of additional specialized monitoring probes such as Oracle database probes on each of two redundant appliance instances. Instead the mon acts as a single point that can differentiate monitoring results by whole application information versus information that exposes internal architecture of the application. In particular embodiments all monitoring activity within the application is funneled through the mon appliance such that all applications may be monitored in an identical manner.

In some embodiments appliances may be further partitioned such that read only volumes containing proprietary code may be distributed separately from the pre configured operating system and other components of each appliance .

In some embodiments appliances may be further partitioned such that read only volumes containing Open Source or third party licensed code may be distributed separately from the pre configured operating system and other components of each appliance . In yet other embodiments appliances may be distributed such that certain volumes can be built by end customers themselves for example to comply with certain licensing or regulatory restrictions.

In some embodiments application exposes two ingress network interfaces interface on a DMZ network that provides ingress for user sessions interface on a management network that provides ingress for management and operational functions and interface that provides egress onto the management network. Networking within application may be private. Addresses internal to application may be assigned dynamically and connections between appliances may be made explicit via definition in the respective application metadata. Such inter appliance connections can be protocol restricted e.g. NFS or HTTP as appropriate. In some embodiments appliances may not have external network interfaces. In particular embodiments only specialized gateway appliances have external network interfaces and thus access to extra application networks in such embodiments all intra application communication is via private networks. Access to appliances by external resources may be through a firewall e.g. provided by CA APPLOGIC appliance and the firewall may be specifically configured to protect this application using whitelist only policies. For example the application may not create connections to the internet. Access to certain internal components of application can be via the management network and can use standardized ports.

In some embodiments application may be upgraded. Such upgrades may be systematic automated and driven via a common interface. In some embodiments upgrades may be accomplished by stopping application altering its metadata such that it references newer versions of one or more of its constituent appliances and then restarted with the new version with pre existing user data e.g. stored in user volumes boundary properties and log volumes . If data changes such as database schema modifications are required the logic to perform these changes may be included in the new versions of the appliances in application e.g. new versions of appliances . In some embodiments an individual appliance may be upgraded without shutting down application . An upgrade can be implemented as a time controlled backup and restore process resulting in a newer version of application . New versions of appliances can detect version changes and update user data e.g. schemas . In some embodiments appliances can detect when the version of user data to which they are attached is higher or lower than the software on the appliance is compatible with in such embodiments appliances in this situation may automatically abort startup to avoid for example data corruption or loss.

In some embodiments patches to application may be performed by copying patch files to the volume cache for a given appliance. In some embodiments such patches may remain on every restart of application . In other embodiments such patches may be lost upon restart of application .

In some embodiments developmental versions of appliances and or application function similarly or the same as in production except that certain version control performance and security standards can be relaxed to enable developer productivity. For example during development proprietary code can be dynamically downloaded by an appliance as a result of a new build. To do this during development appliances may have network egress rights that may not be allowed in a secure production environment. The common bootstrap scripts can be configured to do this automatically using boundary parameters. Entire deployments can be upgraded and re tested with reduced human intervention in some cases without human intervention in a development environment. In a release environment boundary parameters inform the bootstrap script to skip the download step. In some embodiments appliances may be altered during the release process such that the download functionality is permanently disabled. As a second example during development application configurations that are neither redundant nor resource intensive may be used thereby reducing the hardware resource requirements for development execution containers. During release assembly new versions of appliances are placed into production grade application configurations e.g. templates and assemblies . In particular embodiments such changes are accomplished purely via changes to the application metadata. For example if multiple application servers fronted by a load balancer are used in production then a development application might use only a single application server still behind the same load balancer . Similarly production firewalls may be configured to block network egress that developers may need for easy debugging.

In some embodiments application may include internal redundancy. Appliances can be designed to support redundancy. In some embodiments appliances can be built such that the same appliance can be configured for redundant or non redundant operation. In yet other embodiments whole applications can be built that support inter instance replication. In some embodiments disaster recovery can be based on modified backup and restore procedures. In some embodiments the restoration procedure can be altered to implement certain features for example redacting PII filtering out obsolete data or restoring only specific tenants or users. Backups can be saved to remote resources and application templates catalogue appliances and application descriptors are available at the respective remote site s .

In some embodiments application can include an appliance that performs logging. Such data may be included in log volumes and or application log volume . In some embodiments appliances can perform logging using a network storage protocol such as NFS or CIFS. Logging programs can have an interface via which log files can be downloaded and archived. In some embodiments logging programs or appliances may be configured to upload log files to a particular location.

In some embodiments application functionality may include backup and restoration functions for application instances and their constituent appliances. All user data is stored on user data volumes e.g. user volumes separate from the operating system of an appliance and other code associated with the functions of the appliance generically i.e. how all appliances of that type function and or execute but not specifically i.e. related to the specific instance deployed in a certain environment . In some embodiments every piece of data that can be backed up is associated with one tenant. In such embodiments a backup or restore request effectively asks the application to e.g. backup tenant X. Data backup should include a means to ensure that data is consistent across volumes or database snapshot sources.

In some embodiments the direct backup of entire user data volumes is used for restorable procedures and is the minimal form of backup that should be supported by applications. In some embodiments whole volume backups may require shutting down the instance of application first.

This type of backup may be associated with some downtime however. Backups while application is running can be invoked through a common OData interface exposed by the application instance. Within the application instance there is a common appliance e.g. appliance which coordinates the backups of individual appliances e.g. appliance . Appliances can include a backup software agent a common component and an application specific plugin. Some embodiments may support continuous data backup.

Service restoration can use backups in an automated way in various locations. Restores mirror backups. They can be specified on a per tenant basis or a per application instance basis. Restoration involves restoring tenant specific user data onto a compatible version of application . In particular embodiments restoration may be invoked through a common OData API exposed by application . Restoration can use the output of a prior backup operation and restore the specified tenant s data. In some embodiments restoration may require downtime for a tenant or an application instance but this is an application specific decision. As with backup in some embodiments restoration can be coordinated by a common appliance which manages data restoration on the application instance s constituent components.

In some embodiments licensing of all third party products can be tracked and open source distribution limitations are possible. Appliances may separate proprietary software from the operating system and other capabilities by the use of separate volumes e.g. boot volumes appliance volumes and one or more proprietary volumes . The release process for applications can include the packaging and archiving of all license related information. Each released artifact will be uniquely associated to such a bundle of license information. Reporting of use of licensed components may be via a common monitor appliance e.g. appliance that can include mechanisms used for telemetric collection.

In some embodiments metering of capacity metrics may be performed. This may be performed through a standard API exposed by application . Applications can specify at least one capacity metric such as its tenant capacity e.g. maximum number of tenants . The capacity metrics of each application type will differ examples include a number of users a number of concurrent sessions possible a transaction rate and tenant scoped file storage capacity. The capacities are generically defined as attribute value pairs for a specific time frame as part of an application descriptor. Multiple statistic types e.g. minimum maximum average count percentiles instantaneous may be supported for metering along with multiple time frame resolutions.

In particular embodiments the metering interface of application may be exposed via a standard OData interface. Metering data may be accessed via a management network. In one embodiment an OData front end is coupled with application specific plugins on the same appliance that exposes e.g. tenant provisioning. Capacity reserved and consumed by application can be reported on a per tenant basis as well as a per application instance basis. Applications may report other metrics that are not specifically capacity related such as tickets created per hour distinct user logins per day and performance related statistics e.g. transactional latencies .

In some embodiments host security and access control may be performed. Access controls are managed at the application instance level. In some embodiments all hosts are catalog appliances and can be cleaned have their volumes re imaged from the catalog at any time. In some embodiments none of the hosts will have user specific access controls. In some embodiments hosts do not communicate directly with outside components and communication may be through sanctioned firewall gateway appliances within application .

In particular embodiments appliances instances may be versioned. For applications with static configuration controls may not be directly patched. Rather operating system third party and or application patches are introduced through upgrade mechanisms. Appliances themselves run in a very private strictly controlled context making it difficult to exploit many equivalence classes of vulnerabilities. It is expected that security patches will be applied however and that such applications will be infrequent e.g. only aligned with new application version releases . If versioning of applications is dynamic upgrades are systematically deployed such that production network egress and ingress security constraints are enforced and that application version is modified transactionally. This is important to preserve predictability of operational behavior and compatibility of data backups for restoration purposes.

In particular embodiments troubleshooting access to a running application will be standardized by support tier and access given only as needed. Access to externalized data as well as to specific internals of an application instance should be fully enabled over the management network through standard interfaces. Such access is restricted in a tiered fashion to enable troubleshooting productivity while protecting user data and maintaining least privilege standard. In one embodiment for example Tier 1 support s view is restricted to specific external interfaces e.g. a black box view . Thus Tier 1 does not have access to internals nor to user data. Access to internals of application on the other hand has two levels Tier 2 which enables a Tier 2 support person to perform more detailed troubleshooting while protecting user data and Tier 3 which enables full access to the application and user data for higher escalations.

In some embodiments appliance may be a logger appliance. The logger appliance may automatically archive log data locally and expose those logs for external access to authorized and authenticated personnel systems. Log data or log files may refer to write once data to be archived that is not part of the direct function of the application but typically used for audit troubleshooting or bug fixing purposes. For instance log data may include any data that need not be restored in order to recover an application s functionality though such log data may be used for a variety of reporting and analytic purposes and may require access controls around its subsequent use.

The logger appliance may also expose an OData interface via which requests for instantaneous log snapshots can be fetched e.g. to attach to a trouble ticket. In some embodiments the ability to change run time log levels e.g. errors only errors warnings and or information on a per application or per tenant basis will be exposed as standard OData interface. In particular embodiments standard definitions for the semantics of specific log levels e.g. debug information and or warnings are defined and applications can adhere to them. Log data may be fully contained on a single logger appliance for the entire application instance. Log data whose contents conforms to logging statement labeling standards for time tenant severity and source may be filtered appropriately when requested.

In some embodiments access to application instances and or volumes is limited and tracked. For example Tier 1 support may not directly access any user data. Restrictive need to know access rights should be implemented at multiple levels including at the whole deployment level the execution container level the storage level or any other suitable level. In some embodiments facilities will be available to enable automatic logging of potential access to user data on the in each execution container. In some cases proper audit controls may require that all sensitive logging data conform to logging statement labeling standards e.g. such that logs can be redacted prior to transmission to support personnel. Application can conform to content driven audit requirements.

In some embodiments a general release process may be followed. For example application artifacts appliances application templates and application descriptors may move from a development stage to a lockdown stage. After lockdown artifacts are assigned standardized version controlled identifiers and names and are stored in a configuration management system of record. Only artifacts sourced from the system of record can receive official QA. The system of record may be the sole source of artifacts for all downstream consumers and distribution channels.

In some embodiments there may be a standard streamlined path from development to production for appliances that ensures conformance to configuration management and quality control standards. Appliances from development are made production ready by inter alias adding proprietary software components to appliances external update mechanisms if any are disabled because the development scope automatic update feature is part of the common appliance automation suite such disablement is standardized and straightforward all historical or runtime artifacts such as shell history locally stored log information staged files temporary files e.g. packages used only to assist in development are erased and instance and shared volumes are shrunk as appropriate. After the above cleanup and lockdown appliances are assigned unique version controlled identifiers names and are moved into specific catalogues as per the configuration management processes.

In some embodiments there may be a standard streamlined path from development to production for applications that ensures conformance to configuration management and quality control standards. Production application templates start out based on those provided by development. They are made standards conformant with respect to topology and naming conventions. In addition they are fitted for redundancy and capacity e.g. add additional application servers behind the load balancer and are sized according to the application s capacity and performance nameplate ratings.

In some embodiments hardware resources may be tuned during the release and testing processes to meet capacity requirements. In particular embodiments resource tuning e.g. memory CPU and JVM heap sizes is performed in production specification quality assurance execution containers. The same tuning is then implemented in the application template and possibly appliances before release to the system of record. The official application descriptors are updated assigned unique IDs and or version IDs and are delivered along with the associated application template s and catalog appliances. In some embodiments a plurality of performance or capacity configurations may be supported by the same application via boundary parameters. In yet other embodiments a plurality of application profiles may be released each supporting different performance or capacity requirements.

Referring back to in particular embodiments infrastructure management layer may create and be the logical owner of any and all user volumes associated with the application instances it manages. Infrastructure management layer explicitly creates these volumes at instantiation time and sets the application instance s properties accordingly. Infrastructure management layer can identify and can be queried for the set of volumes containing a given tenant s data for a given capability. To the extent that copies of user volumes or user data are stored elsewhere e.g. in a backup system infrastructure management layer is the authoritative source of record for the mapping of those copies to the respective tenants. In most embodiments infrastructure management layer maintains references between user data volumes and user data may also maintain a reference to the respective application descriptor for which they were most recently associated. This is to say that user data for a particular capability can exist independent of an application instance. The application descriptor that presently applies to a given application instance should be persisted by infrastructure management layer . It should be possible to query infrastructure management layer for a given instance s descriptor. In the event that the descriptor for a given capability is modified or deleted it should still be possible for infrastructure management layer to fetch the descriptor that was in effect when a given application instance was created.

As explained above communication with application may only occur through one or more protocols or APIs via specific communications interfaces e.g. interfaces such that application effectively becomes a black box to outside users applications and management elements.

In particular embodiments a variety of management APIs may be exposed such as a backup restore interface e.g. backupTenant restoreInstance checkBackupStatus a start stop pause quiesce awaken interface e.g. quiesceApplication resumeApplication stopApplicationServices a log level change interface e.g. setLogLevel disableAllDebugging downloadLogs and a monitoring interface e.g. getMetric listMetrics resetCounters tenant management createTenant modifyTenant listTenants disableTenant user management createUser modifyUser listUsers deleteUser role management listRoles addRole modifyRole and deleteRole . These interfaces may be OData interfaces according to the OData protocol. In some embodiments every application may include an adapter appliance which provides a common OData interface for management and provisioning functions. For example the adapter appliance may receive a request to perform a backup and where the backup will be stored. In turn the agent would be programmed to know which appliances within application have data to be backed up. The adapter appliance may be implemented as a Java web application housed in a Servlet Container in some embodiments. The common OData layer and other parts may be provided as part of the platform with each application being required to implement plugins based on a simple Java Interface for example which perform the details of such functions as provisioning a tenant or reporting metering data.

An example backup restore interface may include an embedded BackupAgent module. The BackupAgent module may be a piece of software running on each appliance requiring backup restore services. For example in some embodiments the BackupAgent module may be a lightweight Java web service for example housed in a Servlet Container which exposes an OData interface. In particular embodiments the adapter appliance may send commands to this interface to request the invocation of backup and restore operations. In such embodiments the adapter appliance itself may mount some external NFS filesystem. It may then re export this mount point locally within the application. Each appliance that needs to back up files then mounts the adapter s export. The mounting process may be handled by the BackupAgent s common layer. In some embodiments the adapter may re export this mount point such that the set of files accessible to each appliance is limited to the scope of the given application instance or even to each specific appliance so as to provide additional security least privilege and defense in depth. Each appliance requiring backup restore services may implement a plugin e.g. a Java plugin that implements a supplied interface e.g. a Java interface . The appliance specific details of the backup and restore procedures e.g. snapshot and back up the database are handled within this plugin. The plugin is also responsible for generating a manifest file containing among other things a listing of all the files generated by the backup their checksums e.g. MD5 or SHA 1 hashes and possibly other application specific metadata. This manifest file may be used later on during the restore process to validate the integrity of the backup artifacts.

In some embodiments a log level interface may be utilized in application to change one or more logging rules or procedures. For example applications may log all errors by default along with whatever else is necessary for that application. Being able to increase or decrease log levels at run time is optional but may be preferable. In some embodiments an application may enable changing some or all generic log levels through the log level interface. It is desirable that logging be able to be changed on a per tenant basis too and this may also be changed through log level interface as well. In further embodiments an application may have some logging semantics such as logging specific to a subset of it functionality. Some embodiments of the log level interface may enable pass through of such application specific logs.

In some embodiments a monitoring interface may be utilized in application to change or access one or more monitoring functions for an application. In some embodiments a mon appliance may be associated with the monitoring interface. This mon appliance may by default receive some amount of base appliance metrics every few seconds. The mon appliance s reporting frequency and detail is useful when debugging or validating inside the application. The mon appliance may allow in some embodiments a user to view the useful groups of data in real time. In some embodiments the mon appliance or monitoring interface may expose aggregated views of data such as moving averages or rank and order statistics. The mon appliance may expose all the details of the application too so using it may require some knowledge of application internals. In some embodiments appliances include web interfaces for additional monitoring capability. Each web interface may implement its own simple authentication based on parameter values passed into the appliance s boundary or be integrated into an environment level access control system. These interfaces are considered tier 2 support interfaces in that they are necessary for debugging internal operational issues of the application. In certain embodiments it may be desirable to include additional tier 3 i.e. support for application functionality debugging web interfaces as well. Other embodiments may expose application monitoring via other interfaces or protocols such as SNMP.

Operational encapsulation of an application means that everything needed for operations and Tier 1 support is provided through standardized external interfaces and that the internal implementation is insulated from the environment seen by operations and Tier 1 support. Operations may include distribution configuration management provisioning deployment monitoring backup restoration networking licensing metering host security and access control upgrade basic troubleshooting and repair and audit. It may also include enabling Tier 2 and Tier 3 support to troubleshoot an application instance. When the implementation includes restriction of access to internal components this distinction separates access control for the tiers of support. Importantly tier 2 and tier 3 support can now be performed without enabling access to the external multi application environment.

In current systems all data related to an application is configured in such a way that the data is intermingled. Because of this provisioning and or upgrading such applications may be labor intensive prone to human error and carry high risk. In addition access to log files and user data may be difficult. Furthermore using more than one application that is delivered according to current systems may have many seams that may hinder productivity and integration.

Thus according to particular embodiments of the present disclosure appliance may logically separate its environment instance specific properties common appliance artifacts user data and log data . In doing this user data and log data may be transferrable between instances of an application providing increased efficiency in SaaS environments due to rapid provisioning migration restoration backup or other functions related to the operation and or management of applications and their associated data. The separation of persistent user data from essentially disposable application artifacts and disposable or readily reproduced configuration information provides further benefits in kind.

Environment instance specific properties may include properties associated with appliance that relate to the specific instance in a specific execution environment. For example site specific configuration properties may include IP addresses DNS names or other network parameters that are related to other applications the infrastructure management layer other objects in the specific execution environment or execution container. Likewise site specific application configuration data may include for example location of volumes associated with the specific application instance in the specific execution environment or the fully qualified domain name FQDN of a particular instance s SSL certificate. In some embodiments site specific application configuration data may include configuration pertaining to replication high availability or disaster recovery for example IP addresses of remote replicas virtual IF addresses or credentials. It may also include site specific hosting properties such as PCI or SSAE 16 compliance SLAs and the regional designations for restrictions on backup or information transfer. The distinction is that site specific configuration data is common to all instances of a particular application in a particular site whereas site specific application configuration data is specific to individual instances of an application.

Common appliance artifacts may include properties or data associated with appliance that relate to all application instances in all environments a particular appliance may be used by many different applications . For example appliance binaries may include any binary files that are necessary for execution of an appliance regardless of the environment that the corresponding application instance is located. Similarly as an example appliance common data may include data for consumption by appliance during execution not to be confused with user data and appliance common configuration data may include data for configuring application for execution each being necessary for appliance to execute regardless of the execution environment in which the instance is located. In some embodiments common data may include initialization information for persistence mechanisms such as database seeding scripts configuration or data relating to upgrading from one or more prior versions localization and internationalization resource bundles static data such as end user license agreements or document templates or common configuration properties such as Java Virtual Machine tuning parameters heap cache or other memory sizing parameters or default debugging or logging levels. Operating system OS may refer to the software that manages the interaction between appliance and the hardware be it physical or virtual on which appliance runs. In some embodiments Operating System may also include so called user land software commonly shipped with a given OS such as shared libraries shell interpreters command line executables or services. In some embodiments common appliance artifacts may include artifacts which may include appliance runtime artifacts.

User data may include any data associated with application that is specific to a user of the application on a persistent or session basis as distinct from log data . User volume may include data generated or stored by or for a user of application during execution while user configuration data may include data associated a user specific configuration of appliance during execution. For example in some embodiments the backing stores for a database may be stored and treated as user data on the other hand for example localization settings license quantities or feature set enablement may be stored and treated as user configuration.

Log data has similar properties to user data except that log data may include data generated by application during execution that relates to a state of the application i.e. not user information . Log data differs from user data in that users of the system do not know of its existence if logs are lost otherwise malfunction users will not notice Log data may include a log volume for storing logs associated with application . Log data is not required to be restored in order to preserve user available functionality of the application. In some embodiments log data may be accessed by the appliances of application by mounting a network file system such as NFS or CIFS which in some embodiments may be served by a logging appliance within application .

In particular embodiments user data and log data may be distinguished from common application artifacts by the fact that the former is persistent while the latter is not. This allows for applications to be portable and disposable such that they may be used as services with rapid on demand creation and destruction as necessary. It also allows for more efficient management of applications in an execution environment as it prevents singleton applications from causing fragmentation that affects application behavior.

SSO server may be a central security layer that may manage security tokenuser authentication issue validate and manage expiration for s for users . In particular embodiments the communications between SSO server and browser may be encrypted for example by means of SSL or TLSIn some embodiments encrypted communication between browser and the SSO server may be terminated by a device such as a load balancer or firewall upstream of the SSO server . SSO server may be implemented for example using software such as SiteMinder provided by CA Technologies.

API dispatcher may be a layer that exposes all underlying functionality as headless functionality. This could be used by a web browser a mobile computing and or communication device e.g. iPhone iPad Android RSS Rich Site Summary or other suitable user interface. API dispatcher may be operable to route requests to the proper destination application instances via application APIs . For example API dispatcher may expose methods to query application instances list instances for a particular tenant retrieve detailed information about instances or other information related to the application instance. API framework may then be operable to intercept API calls in order to standardize calls to application instance . The authorization caching pagination sorting filtering and other functions will be used the same across all features and will be applied here. In this embodiment control of how the workspace is rendered is external to the application and the application only provides APIs to support the user interaction. This approach has significant impacts that enable runtime integration. API dispatcher can also route traditional browser non API requests to an application instance. In this application embodiment the externally defined workspace is a trivial pipe from the browser to the application where the application itself controls rendering. Thus the rendering of two applications cannot be combined nor can the data content of application APIs interact without hard wired assumptions that break encapsulation. The data that describes the application s operation indicates whether each application instance provides an externalized UI using APIs a traditional application controlled UI or some combination of the two. In some embodiments API dispatcher exposes federated query queries spanning a plurality applications searching aggregation and other functions which functions may in some embodiments be implemented by API dispatcher or in other embodiments forwarded to the implementing services by API dispatcher .

Browser may interact directly with an application where a single application provides direct instructions on how to render portals in the browser. In this case each application defines its own workspace. This is the typical case seen in industry today. The present disclosure includes another mode where the rendering instructions for multiple applications are managed by a separately deployed portlet container server which is part of the User Experience Server . In this mechanism one or more portlets that support tasks performed by the users from multiple independently developed applications. Portlets can be delivered to Browser within a single workspace. Portlets may be implemented in some embodiments using standard Java portlets and portlet containers JSR 286 and may be displayed in the user interface of browser as shown in . Portlets make API calls that are dispatched to their intended applications or to a data federation service that can interact and aggregate across multiple applications. In certain embodiments portlets may be used to display information to users regarding one or more application instances such as usage or capacity metrics.

Particular embodiments of the present disclosure uses the browser resident pub sub event service to enable integration between portlets from different application sources and accessing data from separate applications without requiring the applications to be aware of this integration. For example if one portlet is updated to display details a new object it could publish that the owner of the object another object could also be displayed. If a portlet from another application has subscribed to display this data the portlet would then receive the event search for and subsequently display the details about the original object s owner. No application side implementation is required to enable this integration.

User experience server provides a layer that customers may access via browser . This layer will have access to underlying SaaS applications and additional services that will give added value to the exposed SaaS application features. Through user experience server one or more of the following concepts services may be implemented roles workspaces tasks portlet workspace rendering containers dynamic ad hoc rendering and object views.

Roles are assigned to users which control access to various capabilities of the system. Roles will control access to workspaces tasks and portlets. A user can have multiple roles assigned and in that case would have access to the full set of capabilities associated with all the assigned roles. Some example roles may include for example Service Desk Analyst Service Desk Manager or Tenant Administrator.

A workspace may include user interface pages and may be implemented for example as portals served by portlet containers such as Liferay and may be composed of a number of portlets as shown in . Each portlet may address one or more applications or capabilities such that the workspace may be functional composition of many underlying capabilities exposed by a plurality of application instances.

Each workspace may be optimized to allow a user to perform one or more Tasks in an efficient manner. Being optimized for efficiency means that to perform a certain task the user does not need to navigate through multiple menus tabs overlays and or pages. Those tasks for which the workspace is optimized should be very easy to access from within the workspace. A user can have multiple workspaces open at the same time and can switch between them. New workspaces are generally opened by user initiated actions that open a new workspace. An example would be clicking a See More . . . link in a dashboard workspace that displays Top or Recent lists e.g. clicking on Recent Tickets would take you to a workspace that allows you to browse Tickets with advanced filtering capabilities. Another way to open a new workspace is when drilling down to see more details about an object. An example of this would be when clicking on a Contact s name could bring up a small in line summary view with basic contact information. That view would have a See More Details . . . link which will open a new workspace where you can see the more details about that Contact. Portlets may be standard JSR 286 portlets which are combined to form workspaces. Many of the portlets are used in more than one workspace. If a portlet allow the user to perform multiple tasks the portlet should check if the user has access to those tasks before making the relevant UI elements available enabled.

In one implementation of a workspace the workspace is rendered it is rendered in an IFrame. Each workspace will be initialized with a root object instance. While the portlets in the workspace will likely need to display information about additional objects all of those objects will usually be referenced from the object used to initialize the workspace. Sometimes the root object will not be intuitive at first sight. For example in the example above while the workspace is focused on Tickets it is actually initialized with a User object. The reason is that the workspace shows all of the tickets assigned to a user. From there the user can drill down into individual tickets but the overall context for workspace is the user.

In another implantation of a workspace the workspace includes all portlets available to a user session based on the access rights of the user for that session. In some situations the portlets available to a user are specific to a single tenant context or are limited based on the authentication level of the user. Independent of source these portlets interact through the event manager .

Tasks represent high level actions that are associated with a role. As an example for the Service Desk Analyst role associated tasks can include things like View Tickets Update Tickets and Escalate Tickets Service Desk Manager role would also have access to tasks like View Service Desk Reports and Create Update Ticket Categories. Tasks and workspaces have a many to many relationship. For some tasks user needs access to multiple workspaces in order to accomplish a given task e.g. a wizard like task that walks you through multiple workspaces . Likewise some workspaces allow a user to accomplish multiple tasks. This relationship between tasks and workspaces is what defines which workspaces are available for each role. As a single workspace can be associated with multiple tasks the workspace or rather portlets in a workspace might expose different actions based on which tasks the user has access to. As an example the Ticket Details workspace would have many tasks associated with it View Ticket Add Comment to Ticket Change Ticket Priority Escalate Ticket Assign Ticket to Another User or Close Ticket . Some of these tasks would be available to Level 1 Analyst role additional tasks would be available to Level 2 Analyst and all the tasks would be available to Service Desk Manager. The workspace and the portlets inside it would check which tasks the user has access to and modify disable hide user interface component accordingly.

Object views may refer to lookup tables that map Object Type View Name Tenant Id to View Type View Implementation . Each object view may be defined by one or more of the following properties object type view name tenant ID view type and view implementation. The object type used to initialize the view of the object. In our previous examples this would be User . A view name will look up views by the object type and this name. In the previous example this would be Assigned Tickets. By default seed data for Object Views will have Tenant Id set to null meaning that the value applies to all tenants. If a tenant overrides this value they will have an entry that specifies an implementation of an Object View. This can be either Portlet or Workspace. The view implementation uniquely identifies either the portlet or the workspace that implements the view.

User experience server may include a self service portal a user role admin portal a landing portal and a workspace role manager according to particular embodiments of the present disclosure. Self service portal may allow a user to manage their own profile. User role admin portal may provide a user interface for adding or managing user identities as well as associating and provisioning generic access rights to applications as well as associate and provision application idiosyncratic roles and rights. Portal may also associate and request provisioning fulfillment of application resources for tenants as well as enable users to have membership in zero one or more tenants. User roles and security groups may be managed for users on a per tenant basis. Managing user identities may include a tenant or organization with rights to manage the identity that is separate from any tenant membership that provides access rights to applications. Landing portal may provide the user an interface to connect to one or more workspaces to select from multiple workspaces. If the user is a member of more than one tenant the user may select the tenant context for the current session. Workspace role manager may determine which workspaces are available to a user based on the user s tenancy and roles. look at the collection of roles. The workspace manager may also keep track of customized workspaces on behalf of the tenant user or other group. The workspace manager may also enable multiple users to share synchronized access to a workspace so that they may remortely collaborate. In this case two users and are accessing the workspace from distinct network separated browser instances each containing separate browser resident event managers . The event managers are configured to share local events between the browsers used by and by routing them through the dispatcher .

Infrastructure management layer may have the same functionality as that described above with respect to infrastructure management layer of . In particular embodiments infrastrtucture management layer may communicate with application instances via platform APIs and API framework which may be operable to intercept API calls from platform API in order to standardize calls to application instance . In some embodiments infrastructure management layer may implement an asynchronous bulk request mechanism for use by the API Dispatcher . This may be used for example to prime the cache to enable a restarted API Dispatcher to route requests to. For example the correct IP address using request content and user tenant information. The query would return a list of all tenant id capability id DMZ IP mappings managed by infrastructure management layer . A similar Dispatch function exists on the separate management network if implemented.

Next at step resources are assigned or allocated to the instance. Resources may contain configuration parameters corresponding to a plurality of the application s boundary parameters. In some embodiments the parameters may be based on or associated with the metadata retrieved. In other embodiments some configuration parameters may be provided for example to the infrastructure management layer by an administrator initiating a request. In most embodiments all other resources are assigned by the infrastructure management layer itself. Examples of configuration parameters include an application s name the names of services the application needs to access and the environment the application is located within. Examples of other sorts of resources include IP addresses license keys volumes and physical resources within execution containers.

At step an application instance is generated based on the received application template and metadata. The application instance s boundary properties are then populated at step the application instance according to the metadata and the resources allocated at step . During this step appliances within the application inherit or self seed any necessary properties and or parameters from the configured application. At step user data volumes are created based on the configuration parameters and metadata and the volumes are associated with the application and its appliances. Finally the application is started at step .

In some embodiments this entire sequence of steps will run to completion. In other embodiments the sequence may pause for example to await the replenishment of a resource pool such as an IP address pool for some network. In some embodiments the application may be fully instantiated up to and including step but not immediately started.

Referring to as an example the bootstrap process of can alter appliance volume based on the contents in proprietary volume . User volumes may be used only at runtime and is the only location for persisting user configuration or user data when the application is temporarily stopped. All logging is externalized e.g. to log volume . Any changes to appliance volume or boot volume are non durable.

In some embodiments one or more of volumes and volumes other than volume and may be configured as read only volumes. In some embodiments read only volumes do not require cloning or duplication for each appliance instance saving storage space and time. The use of read only volumes in this context provides additional measures of security and consistency because runtime artifacts on such read only volumes cannot be compromised and any runtime artifacts derived therefrom can be reconstituted from known good sources by simply restarting the appliance.

The method begins at step where the boot process for the appliance is started and followed to a point where appliance specific code is invoked. At step the destination for runtime artifacts for the appliance is cleared. In most embodiments the clearing action is implemented by a recursive deletion of the contents of the respective volume. This guarantees that each boot cycle gets a clean copy of the runtime artifacts from an infrastructure management layer for example and eliminates any vestigial remnants from prior executions. In certain embodiments the clearing action in step may take the form of reverting the respective appliance volume back to a snapshot original blank state such reversion may be implemented external to the appliance before its runtime execution. For example if the execution container were similar to Amazon EC2 so called in memory disks which are guaranteed not to persist across boot cycles could be used affecting the clearing action automatically.

Next at steps and common appliance artifacts and application specific artifacts are loaded respectively. In some embodiments these artifacts may be loaded from an infrastructure management layer charged with managing the applications and their appliances or from an external content management facility. In other embodiments the artifacts may be loaded from a separate server such as a web server or database server charged with storing a most current version of the artifacts. In yet other embodiments the artifacts may be loaded from a separate server such as a web server the version of which artifacts being determined by the value s of one or more boundary parameters this embodiment is particularly well suited to development environments.

In certain embodiments the loaded artifacts may be stored on a read only repository volume and copied into a temporary location for use by the application. This mechanism guarantees that every instance of a particular appliance always loads exactly the same software on each boot cycle. Because the repository is read only there is no way for the appliance to modify content. In addition this mechanism also provides enhanced security as the application artifacts delivered via the repository cannot be changed. If they were changed during runtime such changes will be lost when the appliance is restarted because of application encapsulation as described above . The repository concept provides a clear mechanism to combine a generic appliance with a particular software release. Thus a common known good base appliance can be re used for many software releases. This enables greater release to release consistency and simplifies the appliance release process. Further during this step a number of environment variables are exported including among other things the locations of the appliance and application specific artifacts passed in enabling relocation of these files without modifying any downstream software.

In particular embodiments the remainder of the startup process of and as the appliance specific startup initiated in may be the same regardless of the source of the runtime artifacts. A fortunate consequence is that many artifact sources can be used during research and development without other changes to the appliance itself the only difference in all cases is the source of the artifacts.

At steps and environment variables and application variables on the appliance are configured. which in the present embodiment is the means by which certain information is provided to the appliance at runtime Appliances having application specific content may contain an application provided startup script which must service commands to configure the appliance set up persistence objects initialize or upgrade databases for example and start or stop the application. Critically any dynamic application configuration is generated during this step. The appliance is thus empowered to derive its runtime configuration from its environment from scratch on each startup. This ensures consistency and generality. Finally at step the appliance starts with the newly configured variables.

In a specific example at some point in the operating system startup process a first script may be invoked. The first script then fetches certain configuration files from the execution container via HTTP. The HTTP server itself is the same as the DHCP server and is available to the appliance only for a brief time. Using this configuration information the first script configures local storage network interfaces and other parts of the system. It also extracts the appliance s boundary parameters into canonically named files for consumption by subsequent scripts. Two daemons are launched these maintain communication with the execution container and the hypervisor.

Later a second startup script is invoked. This script is responsible for performing any configuration or initialization specific to this appliance. The details of such operations are automated in a third script. For example on the load balancer appliance configures the load balancing software during this part of startup. When the startup is complete a signal is sent to the execution container indicating successful or failed startup.

In further embodiments additional logic may be added into the bootstrap process which provides a uniform pattern for implementing application specific actions during startup. A set of common components can be used and each application can implement a series of call backs that are invoked by the bootstrap. This standardized process eliminates the need for each appliance developer to invent his own startup mechanisms and also guarantees a predictable repeatable startup sequence for all conformant appliances. The process inherently incorporates means to satisfy license restrictions on distribution as necessary as well as alternatives to enhance control over access to proprietary executables and or user data.

For instance the bootstrap process is performed as usual. The same second script is invoked. However the platform now reconfigures it to invoke a fourth bootstrap script in place of where it would usually invoke the second bootstrap script. The fourth script is therefore invoked. It copies proprietary files from a separate read only location into a runtime staging directory. All files that require configuration changes prior to application startup may be in this location. This insures that the application source files are in a known state and can be fully separated if needed for licensing purposes. In development this step can be made more flexible. Instead of copying from a configuration controlled read only source new executables can be downloaded from build servers to fully automate the development to execution container deployment process.

Continuing with our example embodiment all logs on the appliance are then diverted to a common NFS mounted logger volume which may be automated. System and application logs are systematically redirected to the log mount point. The script now performs the following it ensures environment variables are exported for use in configuration it includes environment specific host names for accessing shared external services it runs the configure setupPersistence and start procedures as defined by the appliance specific appliance local.sh script. The configure operation modifies file contents appropriate to the application context. The setupPersistence operation initializes files and tables used for persistence. For example if the database is empty this step would seed the data. Or if the bootstrap detects that an upgrade is being performed it applies any needed data changes to conform to the new version. This is the point in the startup sequence where the appliance first touches its user data volume s and may determine what action s if any to take to prepare its user data volumes for runtime. The start operation begins the execution of the application.

The method begins at step where the source application instance s user data is backed up. In some embodiments such backup may be performed by directly duplicating the application s user data volumes depending on the execution container such volume duplication may necessitate the temporary shutdown or suspension of the application. In some embodiments such backup may be performed by instructing the running application via the Platform API to back up its data to some location in such embodiments the application typically performs a backup without needing to shut down. Applications may also be designed such that additionally no application downtime or service interruption is incurred due to the backup taking place.

Next at step which may follow immediately from step or may occur sometime in the future a target application instance is selected or created. In some embodiments instance identification or creation is performed by the infrastructure management service . Note that depending on the reason for restoring or transferring the data the location of the target instance could be local with respect to the source or remote. In some embodiments backups may regularly be replicated to remote data centers to enable disaster recovery or other forms of increased availability. In some embodiments backups may be transferred between execution containers for example to relocate an application instance to a container with more free resources.

At step the data backed up in step is restored to the target instance of step . In some embodiments volume backups may be attached to or mounted by the target instance. In other embodiments the target instance may be started and instructed to restore data from a particular backup. In yet other embodiments the target instance may already be running and contain user data and may be instructed to restore the backup by effectively importing the backed up data into the running system preserving existing user data. If the data backed up in is for a particular tenant and this particular embodiment is used the net effect is the migration or transfer of the given tenant from the source instance to the destination instance.

Finally at step if needed the target instance is started up restarted or resumed. In embodiments where the restore procedure is performed online by a running application it may be the case that nothing whatever is done at step . In other embodiments restoring to a running application may require that the application be paused or the application services briefly shut down for example to ensure consistency when importing data. In still other embodiments such as when duplicate volumes are attached to a new target instance the application instance will be in a powered off state and require full startup.

Processor may be a microprocessor controller application specific integrated circuit ASIC or any other suitable computing device operable to provide either alone or in conjunction with other components e.g. memory and instructions IT infrastructure monitoring functionality. Such functionality may include providing a ranking of the most troublesome or unreliable components of an IT infrastructure as discussed herein. In particular embodiments processor may include hardware for executing instructions such as those making up a computer program or application. As an example and not by way of limitation to execute instructions processor may retrieve or fetch instructions from an internal register an internal cache memory or storage decode and execute them and then write one or more results to an internal register an internal cache memory or storage .

Memory may be any form of volatile or non volatile memory including without limitation magnetic media optical media random access memory RAM read only memory ROM flash memory removable media or any other suitable local or remote memory component or components. Memory may store any suitable data or information utilized by computer including software e.g. instructions embedded in a computer readable medium and or encoded logic incorporated in hardware or otherwise stored e.g. firmware . In particular embodiments memory may include main memory for storing instructions for processor to execute or data for processor to operate on. In particular embodiments one or more memory management units MMUs may reside between processor and memory and facilitate accesses to memory requested by processor .

Storage may include mass storage for data or instructions e.g. instructions . As an example and not by way of limitation storage may include a hard disk drive HDD a floppy disk drive flash memory an optical disc a magneto optical disc magnetic tape a Universal Serial Bus USB drive a combination of two or more of these or any suitable computer readable medium. Storage may include removable or non removable or fixed media where appropriate. Storage may be internal or external to computer and or remote transceiver where appropriate. In some embodiments instructions may be encoded in storage in addition to in lieu of memory .

Interface may include hardware encoded software or both providing one or more interfaces for communication such as for example packet based communication between computer and any other computer systems on network . As an example and not by way of limitation interface may include a network interface controller NIC or network adapter for communicating with an Ethernet or other wire based network and or a wireless NIC WNIC or wireless adapter for communicating with a wireless network. Interface may include one or more connectors for communicating traffic e.g. IP packets via a bridge card. Depending on the embodiment interface may be any type of interface suitable for any type of network in which computer is used. In some embodiments interface may include one or more interfaces for one or more I O devices. One or more of these I O devices may enable communication between a person and computer . As an example and not by way of limitation an I O device may include a keyboard keypad microphone monitor mouse printer scanner speaker still camera stylus tablet touchscreen trackball video camera another suitable I O device or a combination of two or more of these.

Bus may include any combination of hardware software embedded in a computer readable medium and or encoded logic incorporated in hardware or otherwise stored e.g. firmware to couple components of computer to each other. As an example and not by way of limitation bus may include an Accelerated Graphics Port AGP or other graphics bus an Enhanced Industry Standard Architecture EISA bus a front side bus FSB a HYPERTRANSPORT HT interconnect an Industry Standard Architecture ISA bus an INFINIBAND interconnect a low pin count LPC bus a memory bus a Micro Channel Architecture MCA bus a Peripheral Component Interconnect PCI bus a PCI Express PCI X bus a serial advanced technology attachment SATA bus a Video Electronics Standards Association local VLB bus or any other suitable bus or a combination of two or more of these. Bus may include any number type and or configuration of buses where appropriate. In particular embodiments one or more buses which may each include an address bus and a data bus may couple processor to memory . Bus may include one or more memory buses.

In addition to hardware virtualization e.g. VMware or Xen other technologies such as network virtualization software defined networks and various forms of storage virtualization presently exist and other embodiments of the same are in development. One of ordinary skill in the art will appreciate that infrastructure and resources traditionally facilitated by for example wired networks and or disk arrays can just as easily be facilitated by virtualized versions of the same. Thus embodiments of the present disclosure utilizing virtualized resources or infrastructure not explicitly discussed herein are still consistent with aspects of the present disclosure.

The flowcharts and block diagrams in illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various aspects of the present disclosure. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

The terminology used herein is for the purpose of describing particular aspects only and is not intended to be limiting of the disclosure. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents of any means or step plus function elements in the claims below are intended to include any disclosed structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present disclosure has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the disclosure in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the disclosure. The aspects of the disclosure herein were chosen and described in order to best explain the principles of the disclosure and the practical application and to enable others of ordinary skill in the art to understand the disclosure with various modifications as are suited to the particular use contemplated.

