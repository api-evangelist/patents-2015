---

title: Non-volatile storage for graphics hardware
abstract: Methods and systems may provide for an apparatus having a graphics processing unit (GPU) and a non-volatile memory dedicated to the GPU. If a request for content is detected, a determination may be made as to whether the non-volatile memory contains the content.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09530178&OS=09530178&RS=09530178
owner: Intel Corporation
number: 09530178
owner_city: Santa Clara
owner_country: US
publication_date: 20150527
---
The present application is a continuation of U.S. patent application Ser. No. 12 790 812 filed on May 29 2010.

Game engines may be highly optimized pieces of software that provide a framework for 3D three dimensional rendering user interaction scene management physics modeling artificial intelligence and other capabilities needed for a given game application. Game applications can use hardware accelerated graphics APIs application programming interfaces to leverage the capabilities of a local GPU graphics processing unit wherein this leveraging can include offloading graphical and non graphical computation to the GPU in order to maintain interactive frame rates. In particular the normal methodology may be to transfer content from the host system s disk decode decompress the content as necessary into system memory then transfer the content to the GPU using an API e.g. DirectX or OpenGL APIs . This process is typically bound by the disk IO input output capability of the host system. Accordingly there can be considerable overhead imposed by the host system hardware during memory operations e.g. the load transfer and storage of game assets to from volatile memory dedicated to the GPU . Indeed such overhead may be experienced each time the application initializes or updates during run time.

Embodiments may provide for a computer readable storage medium including a set of stored instructions which if executed by a processor cause a computer to detect a request for content and determine whether a non volatile memory dedicated to a graphics processing unit GPU contains the content.

Embodiments can also provide for an apparatus including a graphics processing unit GPU a non volatile memory dedicated to the GPU and a computer readable storage medium. The computer readable storage medium may include a set of stored instructions which if executed by a processor cause the apparatus to detect a request for content and determine whether the non volatile memory contains the content. The computer readable storage medium may be part of or separate from the non volatile memory.

Other embodiments may include a system having a host device with a central processing unit CPU to execute a graphics application. The system can also include a graphics processing unit GPU a non volatile memory dedicated to the GPU and a volatile memory dedicated to the GPU. In addition the system may include a computer readable storage medium including a set of stored instructions which if executed by a processor cause the system to detect a request for texture content from the graphics application and determine whether the non volatile memory contains the texture content. The instructions can also cause the system to allocate a buffer resource to the texture content if the non volatile memory contains the texture content and return an identifier of the buffer resource to the host device. The instructions may also cause the system to transfer the texture content to the buffer resource if a graphics optimization criterion is met and transfer the texture content from the buffer resource to the volatile memory via a direct memory access DMA request.

Turning now to an architecture includes a host device with a CPU volatile memory such as system memory and non volatile memory such as disk . The illustrated architecture also includes a GPU with a dedicated non volatile memory NVM and a dedicated volatile memory such as a graphics double data rate GDDR memory . As will be discussed in greater detail the GPU and NVM may reside on the same or separate cards substrates. The NVM could be a NAND based SSD solid state disk a USB Universal Serial Bus e.g. USB Specification 2.0 USB Implementers Forum flash storage device a hard disk drive HDD etc. The GPU and its dedicated memories may communicate with the CPU over a graphics bus such as a PCI Express Graphics PEG e.g. Peripheral Components Interconnect PCI Express x16 Graphics 150W ATX Specification 1.0 PCI Special Interest Group bus or Accelerated Graphics Port e.g. AGP V3.0 Interface Specification September 2002 bus.

By providing the GPU with a dedicated NVM the illustrated architecture represents a more balanced platform in which advanced graphics rendering and compute capabilities can be facilitated. As will be discussed in greater detail the ability to quickly transfer graphical content from the NVM to the GDDR can enable content to bypass the traditional GPU loading mechanism and may therefore offer significantly improved load times for graphics applications. For example the content assets loaded into the 3D graphics pipeline can be stored in the NVM for later use wherein the stored data may be loaded at a later time and reconstituted within the 3D graphics pipeline in the same state in which the data existed when it was stored. As a result load performance may be improved.

Other load time performance improvements may involve the use of procedurally generated content. Such content might result from a variety of algorithmic techniques which can be computational expensive. Instead of generating procedural content at load time procedural assets may be generated once then stored in the NVM for later recovery. This approach can reduce the amount of potentially redundant content generation which may further improve load times. In particular such an approach could be used in dynamic level of detail LOD systems where computationally expensive mesh subdivision or simplification might only be applied once per unique object. Different versions of each mesh at various LODs could be pre computed in this manner and cached on the NVM for later use. Accordingly LOD levels may be computed only once for an application and allowed to persist across multiple load cycles. The architecture can enable other advantages to be obtained as will be discussed in greater detail.

With continuing reference to processing block provides for detecting a request e.g. read request for content. In one example the read request is received and processed by a driver stack that functions as an interface between a host device system domain and a GPU domain . The content might include texture content vertex content and or procedurally generated content as already discussed wherein the read request could be received from a graphics application executing on a CPU in the host system domain . If it is determined at block that an NVM such as an SSD which is dedicated to the GPU domain contains the requested content a buffer resource may be allocated to the content at block . Block provides for returning an identifier of the buffer resource e.g. resource handle to the host system domain .

A determination may be made at block as to whether one or more optimization criteria are met. The optimization criteria might be used by the driver stack to reorder consolidate and otherwise optimize requests. For example advanced implementations may involve the optimization of the file system for streaming loads the optimization of spatial locality of accesses and or the batching of read and write requests and so on. Moreover some form of data compression on the SSD file system may be advantageous to increase the speed of data loads. Simply put the read of the actual contents into the allocated buffer resource can be deferred until it is performant urgent or otherwise appropriate that reads be completed. If the optimization criteria are met illustrated block provides for transferring the content to the allocated buffer resource. The content can be transferred from the buffer resource to a dedicated volatile memory such as DDR memory at block . In one example the transfer to DDR memory is conducted via one or more DMA direct memory access requests.

With continuing reference to if it is determined at block that the requested content does not reside in the SSD block provides for obtaining the content from the memory of the host system domain . For example in the illustrated example a read decode is conducted on the disk and the results are transferred into the system memory . The content can then be submitted to the driver stack which returns a handle for a buffer resource associated with the DDR memory . In addition a write from the DDR memory to the SSD may be performed so that the next time the content is requested it may be retrieved from the SSD in the GPU domain .

Turning now to a hardware implementation and its respective driver stack are shown. In particular a host device system may be part of a mobile platform such as a laptop personal digital assistant PDA wireless smart phone media player imaging device mobile Internet device MID etc. or any combination thereof. The system may also be part of a fixed platform such as a personal computer PC server workstation etc. The illustrated system includes a plurality of CPUs and system memory which could include DDR synchronous dynamic random access memory SDRAM e.g. DDR3 SDRAM JEDEC Standard JESD79 3C April 2008 modules. The modules of the system memory may be incorporated into a single inline memory module SIMM dual inline memory module DIMM small outline DIMM SODIMM and so on. The illustrated CPUs communicate with the system memory via a North Bridge .

The CPUs may also communicate with the disk via the North Bridge and a South Bridge . In the illustrated example the South Bridge and disk communicate over a Serial ATA SATA e.g. SATA Rev. 3.0 Specification May 27 2009 SATA International Organization SATA IO bus . The host system may include a graphics bus e.g. PCI e slot wherein a graphics card containing the GPU DDR memory SSD and a bridge could be coupled e.g. plugged into the graphics bus . Thus in the illustrated the on board model SSD access can be achieved by the GPU through a direct physical connection on the graphics card which may either be mounted directly on the board of the host system or mounted elsewhere in the system chassis and connected with a cable. The graphics card may be modified to include an onboard dedicated bus and or disk controller to support the SSD.

In particular the CPUs may have processor cores not shown to execute a host OS operating system application software and a driver stack where each core may be fully functional with instruction fetch units instruction decoders level one L1 cache execution units and so on. Generally the driver stack may include at the lowest level kernel mode driver KMD that provides ring 0 access to the hardware on the graphics card . A user mode driver UMD can sit atop the illustrated KMD and expose particular features of the graphics hardware on a per application basis. Thus failures in the UMD may present as application level exceptions whereas KMD failures could be system wide. In the illustrated example an OpenGL e.g. OpenGL 4.0 the Khronos Group Mar. 10 2010 API and a DirectX e.g. Direct 11 Microsoft Corporation Redmond Wash. API are extended at the driver level to include hardware specific extensions to each API. In addition a native API may be used to extend DirectX for GPU SSD operations whereas OpenGL could be augmented though a customary extensions interface. Specific APIs are described herein to facilitate discussion only. Finally sitting atop the illustrated DirectX API OpenGL API and Native API is application software which may leverage the capabilities of each.

In the on board example shown the driver stack takes into consideration the specific components employed in the physical implementation of the graphics card . The host driver stack may be structured in multiple ways the software stack for both graphics adaptor and the SSD could be implemented in the host OS the SSD drivers could be implemented on a OS micro OS running independently on the graphics card and so on. In the first case the GPU may be seen as a passive device that is subordinate to the host OS and the drivers for the SSD and GPU could be combined as a single entity made up of driver instructions to be executed by the CPUs . In the second case the GPU may be seen as autonomous from the host OS relying on its own OS to service IO requests. In this case the SSD driver may be implemented as an SSD KMD specifically for the OS wherein the GPU executes the instructions for SSD KMD and the host driver stack simply forwards IO requests to be dispatched on the graphics card .

Turning now to an alternative hardware configuration and corresponding driver implementation are shown in which a cooperative model is used. In the illustrated example a graphics card contains a GPU and the DDR memory whereas an SSD is coupled to a separate memory card . Thus the graphics bus could include a first bus coupled to the graphics card and a second bus coupled to the memory card . In the cooperative model the graphics hardware may be unmodified and combined with a commercially available PCI e based SSD e.g. SSD device from vendors such as Addonics DDRdrive and Dolphin Fuji Xerox Fusion io Intel Micron and SanDisk . Rather than attaching the SSD to a conventional HDD and maintaining HDD mounting form factors the SSD could be implemented entirely on a PCI e expansion board which may place an upper bound on disk throughput to that of PCI e e.g. 32 Gb s . When compared to the SATA throughputs e.g. 4.8 Gb s such a solution could offer greater throughput for a lower attachment cost.

Thus the integration of the GPU and SSD may involve simply plugging each corresponding card into adjacent memory slots. Moreover the mechanics of the integration could be handled by a graphics driver stack on the host system . Generally the graphics driver stack could have low level access to a SSD driver to facilitate direct memory accesses between the DDR memory and the SSD . This approach may increase the traffic over the graphics bus which could result in lower performance than the on board solution. Although the traffic generated between the GPU and the SSD may compete with traffic between the GPU and host system a variety of driver level optimizations e.g. including data caching and I O scheduling could mitigate this contention.

In particular in the illustrated example the SSD and the graphics adaptor for the GPU could cooperate in a number of different ways. One scenario may involve an SSD card KMD and a graphics card KMD that expose a low level interface that allows the SSD to DMA the DDR memory contained on the graphics card . Another possible scenario might involve the graphics card exposing SSD hooks that can be lined into via a published interface. There are many other configurations but a generalization is that the SSD card KMD and graphics card KMD may be independent pieces of software possibly from different vendors with some low level interface that enables interoperability between the two devices.

The above techniques may be designed for interoperability with existing graphics pipelines such as DirectX pipelines. While portions of this discussion may refer to DirectX the approaches described herein may be readily extended to other scenarios such as OpenGL pipelines. The basic structure of SSD reads and writes can be consistent with the conventional DirectX command buffer the set of commands typically used to dispatch draw and state change requests may be augmented with a small number of IO commands to ensure that IO requests preserve in order execution.

The fundamental unit of storage in this software architecture could be a DirectX buffer resource containing texture vertex or other content used during rendering. When buffers are written to the SSD all of the run time state information associated with the buffer can be written in the file system along with its respective data. Similarly when buffer data is reconstituted from the SSD the run time state of that resource may be reconstituted in the same state the buffer was in when it was saved. Each IO request can contain a string that corresponds to a specific file path in a hierarchical file system.

The way the command buffer is utilized in read and write requests may differ slightly. For example a read request can issue a SSD READ QUERY command that is inserted in a queue and host blocks. When the SSD READ QUERY request is serviced the path can be extracted from the command and the local file system may ensure that the file exists. If the file is valid a file descriptor may be used to allocate a buffer with an identical configuration as was used when the file was originally stored. As soon as the buffer is allocated the caller could be returned a valid resource handle and an SSD READ DEFFERED command may be inserted into the command buffer. Up until this point all that may have been obtained from the file system is the file descriptor. The read of the contents into the allocated buffer can be deferred until an optimization criterion is met. The driver may guarantee that the contents of a buffer will be loaded from storage prior to its first access but prior to this the driver can have the latitude to reorder consolidate and otherwise optimize requests as appropriate.

Write requests could be much simpler than read requests as there may be no need to defer access because the caller is not waiting for a valid resource handle to return. When a write is issued a SSD WRITE command may be inserted into the command buffer. When this command is dispatched the path can be extracted and the buffer resource and its descriptor may be written to the SSD. There are many opportunities for optimization though deferral here but these may be highly specific to the driver implementation.

The aforementioned software architecture can culminate into the essential read and write operations necessary for utilizing a GPU side file system. Since this functionality could fall outside the DirectX or other API specification a native API may be created that provides abstraction around the augmented driver. The native API can contain additional features for manipulating and interrogating the file system. These features might include functions for directory management including creation deletion copy move and listing. The native API may also contain support for reading writing renaming and deleting individual files as well as page files late binding mechanisms and file security. Below are examples of some function specifications of the native API that may be provided.

This function can write the configuration and content of in buffer to the GPU side file system at the path specified in in pFileName . The buffer contents are written to disk including a header that contains information on how the APIBuffer resource is to be allocated and configured when reconstituted by the API0FileLoadBuffer. The buffer can be guaranteed to be written to disk prior to any modification of the specified buffer.

This function can load a file previously stored using the API0FileLoadBuffer function. It loads the file specified in in pFileName reads the files header creates an appropriate buffer resource and loads the file contents accordingly. Upon success the handled pointed to by out buffer will contain the same state and content as it did when it was stored. This buffer can be allocated according to its configuration and may be freed using the API0FileDestroyBuffer function which ensures that it is free in a manner appropriate to its creation. This function may return with success when the specified file is found and an appropriate buffer is allocated. The actual reading from disk to buffer memory may be defered but will be complete prior to the buffers first access.

This function may free a buffer obtained using the API0FileLoadBuffer function. This function can ensure that the buffer is deleted in a manner consistent with its creation. Since APIFile can handle both Native and DirectX shared surfaces the underlying deallocation process may differ for each case.

This function can delete an existing file stored using the API0FileStoreBuffer function. If pFileName refers to a directory the directory must be empty for the deletion to succeed.

This function frees an existing API FILE DIR LIST structure obtained from a prior call to API0FileGetDirectoryList.

Thus the techniques described herein can offer a number of improvements for graphics applications such as combining texturing techniques with a large SSD storage capacity. An example is gigapixel image manipulation where the working set of pixels can far exceed the amount of GDDR memory available on the graphics hardware. With non volatile storage closer to the GPU the response time and size of the virtual texture maps supported can be improved. In addition real time mega textures could leverage this capability adding considerable depth and realism to 3D games.

An additional advantage may involve the expansion of the types of algorithms applicable to non graphical GPU programmability e.g. general purpose computation on graphics processing units GPGPU techniques. These techniques can use a GPU as high performance offload engine levering the data parallel capabilities of its micro architecture. While this technique can be powerful it may conventionally only work for a very narrow set of algorithms because there may be a costly transfer from GPU to host system memory via the graphics bus potentially for every computational pass . Adding a local SSD to the GPU pipeline however may enable the applications to hide this latency. Through a combination of storing partial results locally consolidating transfers and other techniques a major limiting factor of GPGPU based approaches can be mitigated using the techniques described herein.

Embodiments of the present invention are applicable for use with all types of semiconductor integrated circuit IC chips. Examples of these IC chips include but are not limited to processors controllers chipset components programmable logic arrays PLA memory chips network chips and the like. In addition in some of the drawings signal conductor lines are represented with lines. Some may be thicker to indicate more constituent signal paths have a number label to indicate a number of constituent signal paths and or have arrows at one or more ends to indicate primary information flow direction. This however should not be construed in a limiting manner. Rather such added detail may be used in connection with one or more exemplary embodiments to facilitate easier understanding of a circuit. Any represented signal lines whether or not having additional information may actually comprise one or more signals that may travel in multiple directions and may be implemented with any suitable type of signal scheme e.g. digital or analog lines implemented with differential pairs optical fiber lines and or single ended lines.

Example sizes models values ranges may have been given although embodiments of the present invention are not limited to the same. As manufacturing techniques e.g. photolithography mature over time it is expected that devices of smaller size could be manufactured. In addition well known power ground connections to IC chips and other components may or may not be shown within the figures for simplicity of illustration and discussion and so as not to obscure certain aspects of the embodiments of the invention. Further arrangements may be shown in block diagram form in order to avoid obscuring embodiments of the invention and also in view of the fact that specifics with respect to implementation of such block diagram arrangements are highly dependent upon the platform within which the embodiment is to be implemented i.e. such specifics should be well within purview of one skilled in the art. Where specific details e.g. circuits are set forth in order to describe example embodiments of the invention it should be apparent to one skilled in the art that embodiments of the invention can be practiced without or with variation of these specific details. The description is thus to be regarded as illustrative instead of limiting.

Some embodiments may be implemented for example using a machine or tangible computer readable medium or article which may store an instruction or a set of instructions that if executed by a machine may cause the machine to perform a method and or operations in accordance with the embodiments. Such a machine may include for example any suitable processing platform computing platform computing device processing device computing system processing system computer processor or the like and may be implemented using any suitable combination of hardware and or software. The machine readable medium or article may include for example any suitable type of memory unit memory device memory article memory medium storage device storage article storage medium and or storage unit for example memory removable or non removable media erasable or non erasable media writeable or re writeable media digital or analog media hard disk floppy disk Compact Disk Read Only Memory CD ROM Compact Disk Recordable CD R Compact Disk Rewriteable CD RW optical disk magnetic media magneto optical media removable memory cards or disks various types of Digital Versatile Disk DVD a tape a cassette or the like. The instructions may include any suitable type of code such as source code compiled code interpreted code executable code static code dynamic code encrypted code and the like implemented using any suitable high level low level object oriented visual compiled and or interpreted programming language.

Unless specifically stated otherwise it may be appreciated that terms such as processing computing calculating determining or the like refer to the action and or processes of a computer or computing system or similar electronic computing device that manipulates and or transforms data represented as physical quantities e.g. electronic within the computing system s registers and or memories into other data similarly represented as physical quantities within the computing system s memories registers or other such information storage transmission or display devices. The embodiments are not limited in this context.

The term coupled is used herein to refer to any type of relationship direct or indirect between the components in question and may apply to electrical mechanical fluid optical electromagnetic electromechanical or other connections. In addition the terms first second etc. are used herein only to facilitate discussion and carry no particular temporal or chronological significance unless otherwise indicated.

Those skilled in the art will appreciate from the foregoing description that the broad techniques of the embodiments of the present invention can be implemented in a variety of forms. Therefore while the embodiments of this invention have been described in connection with particular examples thereof the true scope of the embodiments of the invention should not be so limited since other modifications will become apparent to the skilled practitioner upon a study of the drawings specification and following claims.

