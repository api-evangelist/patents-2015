---

title: Generation of API call graphs from static disassembly
abstract: Data is received that includes at least a portion of a program. Thereafter, entry point locations and execution-relevant metadata of the program are identified and retrieved. Regions of code within the program are then identified using static disassembly and based on the identified entry point locations and metadata. In addition, entry points are determined for each of a plurality of functions. Thereafter, a set of possible call sequences are generated for each function based on the identified regions of code and the determined entry points for each of the plurality of functions. Related apparatus, systems, techniques and articles are also described.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09378012&OS=09378012&RS=09378012
owner: Cylance Inc.
number: 09378012
owner_city: Irvine
owner_country: US
publication_date: 20150106
---
This application is a continuation of and claims the benefit of priority under 35 U.S.C. 120 of U.S. patent application Ser. No. 14 169 841 filed Jan. 31 2014 issuing under U.S. Pat. No. 8 930 916 on Jan. 6 2015 entitled Generation of API Call Graphs From Static Disassembly the disclosure of which is incorporated herein by reference.

The subject matter described herein relates to generation of application programming interface API call graphs from static disassembly of program code.

In modern execution environments a program generally only has control of its own private virtual memory and the unprivileged state of the central processing unit CPU executing the program. To alter the state of the system at large the program must request that the operating system OS perform some operation on its behalf almost always by calling a well defined API function provided by the OS. Capturing the API calls performed by a program and additionally the parameters supplied in each call and the result of each call is a simple concise and effective means of profiling the behavior of a program.

Most API call profiling systems execute a program of interest in a virtual machine featuring a complete OS installation and some amount of user or kernel mode instrumentation for intercepting and recording API calls. However such a dynamic analysis approach consumes a significant amount of resources required for each virtual machine thereby dramatically limiting the number of virtual machines that can concurrently run on a physical computer. In addition programs are typically executed in real time and so they might wait for some event to occur or time interval to elapse before exhibiting any significant activity meaning the system might be forced to run each program of interest for minutes or risk aborting before the program has performed substantive API calls.

Lighter weight implementations that emulate programs of interest have been adopted. However emulators often suffer from incomplete implementations of the API and detectable divergences from the behavior of a real execution environment but they generally offer size and speed benefits. Both emulators and virtual machines face the significant drawback that they can only profile a single path of execution generally they do not consider the other paths which execution potentially could have followed through the code.

In a first aspect data is received that includes at least a portion of a program. Thereafter entry point locations and execution relevant metadata of the program are identified and retrieved. Regions of code within the program are then identified using static disassembly and based on the identified entry point locations and metadata. In addition entry points are determined for each of a plurality of functions. Thereafter a set of possible call sequences are generated for each function based on the identified regions of code and the determined entry points for each of the plurality of functions.

The call sequences can include application programming interface API calls. The call sequences can also include calling subfunctions implemented by the program. An API call graph can be generated that characterizes the generated set of possible call sequences.

At least one of the calls in the call sequence can be decorated with parameter information affecting a behavior of the corresponding function.

The entry point locations can correspond to places within the program at which an operating system or other program initiates execution of the program.

The identifying and retrieving entry point locations can include as one example scanning the program for pre defined byte sequences.

In an interrelated aspect data is received that includes at least a portion of a program comprising machine code. Thereafter the machine code is disassembled into instructions. These instructions are then organized into functions comprising code blocks. A control flow graph is then constructed that characterizes the functions. Application programming interface API call sequences are next extracted by traversing some or all possible paths through the control flow graph. Subsequently a relative order of API calls and child function calls is determined so that an API call graph can be generated that is based on the extracted API call sequences according to the determined relative order.

Non transitory computer program products i.e. physically embodied computer program products are also described that store instructions which when executed on one or more data processors of one or more computing systems causes at least one data processor to perform operations herein. Similarly computer systems are also described that may include one or more data processors and memory coupled to the one or more data processors. The memory may temporarily or permanently store instructions that cause at least one processor to perform one or more of the operations described herein. In addition methods can be implemented by one or more data processors either within a single computing system or distributed among two or more computing systems. Such computing systems can be connected and can exchange data and or commands or other instructions or the like via one or more connections including but not limited to a connection over a network e.g. the Internet a wireless wide area network a local area network a wide area network a wired network or the like via a direct connection between one or more of the multiple computing systems etc.

The subject matter described herein provides many advantages. For example the current subject matter provides an approach to extracting API call sequences from a program that achieves far greater code coverage than conventional techniques in many cases producing much more representative API call sequences. In particular the current subject matter uses static disassembly aided by program metadata when available to construct control flow graphs of the possible paths that execution can take at both the basic block and function levels. The end result of this process is a compressed representation of every possible sequence of API calls that could occur starting from a point where execution enters the program with parameter values captured as desired and to the extent possible.

The details of one or more variations of the subject matter described herein are set forth in the accompanying drawings and the description below. Other features and advantages of the subject matter described herein will be apparent from the description and drawings and from the claims.

Initially code e.g. all code within a given program can be discovered through static disassembly. Static disassembly in this regard refers to a process of iteratively traversing the instructions constituting a program s code by alternately decoding an instruction and predicting the effect or possible effects that the instruction would have on the flow of execution if it were executed. A program has one or more well defined entry points where the OS or another program may initiate execution therefore it is generally expected that code will exist at these entry points. In some cases code discovery can be improved by scanning the program for certain byte sequences typically indicative of code such as a common prolog or a well known function. In situations in which a portion of code is only reached via an execution transfer determined from complicated runtime state scanning for common code byte sequences offers a simple and fairly effective alternative to static disassembly for discovering that code. As an example consider a virtual function or callback that is supplied by the program to the OS for later execution. Even if static disassembly encounters a pointer to the function it might not recognize that the pointer will be used by the OS as a function pointer and therefore it might not disassemble the referenced function. However if the function begins with a common prolog such as PUSH EBP MOV EBP ESP SUB ESP imm scanning the program for that prolog sequence will result in the discovery and subsequent disassembly of that function and likely other functions as well. In many cases programs also contain metadata that describe where code or execution relevant data reside in the program. Examples include the runtime function information in 64 bit native WINDOWS programs relocations exception handler structures and MICROSOFT Interface Definition Language MIDL structures.

Because programs often contain data and padding interspersed with chunks of code disassembly commences at the entry point s and other well defined locations known to contain code and it discovers subsequent code one instruction at a time. For instance a Portable Executable header may specify a program entry point to be executed by the OS after the program has loaded an export directory describing functions or program elements which the program intends to make available to other programs a Thread Local Storage directory describing callback functions to be executed by the OS a delay import directory referencing delay import helper functions exception data referencing exception handling functions and so on. Other structures referencing code might also exist such as Remote Procedure Call RPC related structures referencing RPC server side functions to be executed by the RPC runtime. For the most part each instruction implicitly flows into the instruction immediately following it although branch call and return instructions which explicitly redirect execution also exist.

With reference again to diagram of a generic instruction as used in disassembly can be defined as an instruction that implicitly passes execution to the instruction immediately following it i.e. the program counter is incremented by the length of the instruction . The possibility of an execution transfer due to an exception such as a processor trap or fault or other interrupt is not depicted in . An unconditional relative branch instruction on the other hand explicitly can transfer execution to a destination which is relative to the relative branch instruction and therefore does not implicitly pass execution to a following instruction. An indirect branch instruction can also explicitly transfer execution but the destination is retrieved from the system s state rather than explicitly encoded in the branch instruction which generally makes the destination nontrivial to determine during disassembly. A conditional relative branch instruction can transfer execution explicitly to a destination relative to itself or it may implicitly pass execution to the following instruction. The state of the CPU flags register can control which of the two possibilities is realized and therefore it can be nontrivial or even impossible to determine during static disassembly unless the relative conditional branch instruction s destination is equal to the following instruction. A relative call instruction like a relative branch instruction can explicitly transfer execution to a destination relative to itself but it can also push the address immediately following itself that is the address of the instruction plus the length of the instruction onto the stack. It can generally be assumed for the sake of disassembly that the call instruction s destination is the entry point of a function which will return to the address following the call but this is not strictly guaranteed. For example a no return function will not return execution and therefore the compiler might place data or unrelated code immediately after a call to such a function.

An indirect call instruction can explicitly transfer execution in the same manner as an indirect branch instruction while pushing the address immediately following itself in the same manner as a relative call instruction and it is therefore subject to the caveats of both. For the current purposes a software interrupt instruction not shown is conceptually equivalent to an indirect call instruction. A return instruction can transfer execution to an address popped from the top of the stack this is generally the address of an instruction immediately following a call instruction but like the destination of an indirect branch instruction this address is dependent on the state of the system and may be nontrivial to determine during disassembly. Finally a halt instruction does not transfer execution at all it remains at the halt instruction until an interrupt occurs.

With this basic knowledge for each type of instruction static disassembly can generally produce a control flow graph covering the extent of program code reachable from a program entry point or any other identifiable instruction excluding those transitions in execution flow that depend on run time state and therefore cannot be easily predicted during static disassembly. A control flow graph in this regard refers to a directed graph in which each instruction is a node and each possible execution transfer is an edge. For efficiency each node may instead represent a block of one or more instructions through which there is only a single path of execution. As suggested above however static disassembly may not always succeed in constructing a complete control flow graph as there may be transitions in execution that it fails to fully traverse. One difficult example of such a transition is a traversal of indirect branches indirect calls and returns. Special case logic can handle some such situations The possible destinations of branches and calls making use of indexed static tables e.g. JMP table EAX 4 can often be elicited by checking for nearby instructions that confine the index and by examining table memory for valid pointers especially those marked with relocations .

Calls present a more subtle challenge. Although it seems intuitive to continue disassembly after a call instruction not every callee will return execution to the return address supplied to it. Here no return functions and calls that are meant to push the instruction pointer for subsequent retrieval can sometimes be identified which allows for avoiding disassembling likely non code after a relative call instruction in the simplest cases.

To better handle situations in which execution is influenced by information outside the scope of a single instruction a disassembler can be augmented with an emulation like understanding of instructions and API calls. In order to avoid the drawbacks of emulation this capability can be mostly used to propagate information to where there would otherwise be none. For example disassembly without emulation would not know the target of a CALL EBX instruction but with emulation it might be able to track that the EBX register was previously loaded with a specific function pointer. In general emulation should not be used to restrict which paths of execution are explored although it can be helpful in avoiding the disassembly of non code.

With the current emulation considered are both a ternary representation of integers in which the state of each bit is maintained as a 0 1 or X if indeterminate with special symbolic consideration for pointers see Advanced Return Address Discovery Using Context Aware Machine Code Emulation Soeder et al. July 2004 the contents of which are hereby fully incorporated by reference and fully symbolic emulation in which values are maintained as arithmetic expressions capable of simplification by a Satisfiability Modulo Theory SMT solver. Such a form of emulation provides more comprehensive code coverage better tracking of API function calls and better tracking of the parameters supplied to API calls all three of which are facilitated by the more powerful propagation of information especially function pointers .

For illustration purposes an example function is rendered in four forms its original higher level language source code diagram of a disassembly of the compiled function into x86 instructions listed below a control flow graph representing the compiled code s constituent blocks diagram of and the API call sequences that the function has the potential to perform listed below . In particular diagram of illustrates a function written in C which makes use of the following WINDOWS API calls CreateFileW CloseHandle GetFileSize DeleteFileW LocalAlloc LocalFree and ReadFile.

Below is an assembly language listing of the function of diagram of when it is compiled to x86 instructions. The disassembly can be divided into basic code blocks after branch instructions and at branch destinations with each instruction being annotated with a letter in brackets representing the code block to which it belongs.

Below are all API call sequences possible in the example function of the diagram of . The ReadFile API call is marked with an asterisk because it may occur repeatedly during execution of the loop.

Given a program containing machine code the current approach disassembles the code into instructions which it then organizes into functions comprising basic code blocks. At the intra function level construction of a control flow graph enables the extraction of API call sequences by exhaustively traversing all possible paths through the graph noting which API calls occur in a loop but never visiting any block more than twice per path in order to avoid indefinite traversal. The approach can then generate a call graph a directed graph in which nodes represent functions and edges represent one function calling another or itself for the entire program.

Although the call graph depicts calls between functions this information alone is not sufficient to allow accurate reconstruction of all inter function API call sequences the relative order of child function calls and API calls needs to be captured. For instance if a function g calls API A function and API B and if function calls API C the call sequence A f B for function g and C for function can be maintained so that the former sequence can be properly expanded into A C B . If non API function calls are not included in the call sequence one would be left with a call graph telling only that function g has the API call sequence A B and may call function which in turn has the API call sequence C making it unclear which sequences among A B A B C A B C C A C B A C B C C A B etc. are actually possible.

Although all function call sequences could be expanded into an exhaustive set of API call sequences the number of sequences to be considered would likely become infeasibly large. Instead this compressed representation of API call sequences can be maintained and algorithms can be considered to search for API call n grams within or between the per function API call sequences.

To produce even more descriptive API call sequences the API calls can be decorated with parameter information when available. Returning to the earlier example the API calls listed above can be decorated as follows 

The parameter information retained should be specific only insofar as it affects the behavior of the API function or serves as a reasonably generic fingerprint. For instance noting that an API function is called with a reference to a certain stack location is overly specific as it is subject to change across platforms and even among recompilations of the same code. Meanwhile other parameter values such as the access mask passed to CreateFileW or the function name passed to GetProcAddress are valuable as differentiators among the possible intentions a program might have for calling a given API function.

One or more aspects or features of the subject matter described herein may be realized in digital electronic circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations may include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device e.g. mouse touch screen etc. and at least one output device.

These computer programs which can also be referred to as programs software software applications applications components or code include machine instructions for a programmable processor and can be implemented in a high level procedural language an object oriented programming language a functional programming language a logical programming language and or in assembly machine language. As used herein the term machine readable medium sometimes referred to as a computer program product refers to physically embodied apparatus and or device such as for example magnetic discs optical disks memory and Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable data processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable data processor. The machine readable medium can store such machine instructions non transitorily such as for example as would a non transient solid state memory or a magnetic hard drive or any equivalent storage medium. The machine readable medium can alternatively or additionally store such machine instructions in a transient manner such as for example as would a processor cache or other random access memory associated with one or more physical processor cores.

To provide for interaction with a user the subject matter described herein can be implemented on a computer having a display device such as for example a cathode ray tube CRT or a liquid crystal display LCD monitor for displaying information to the user and a keyboard and a pointing device such as for example a mouse or a trackball by which the user may provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well. For example feedback provided to the user can be any form of sensory feedback such as for example visual feedback auditory feedback or tactile feedback and input from the user may be received in any form including but not limited to acoustic speech or tactile input. Other possible input devices include but are not limited to touch screens or other touch sensitive devices such as single or multi point resistive or capacitive trackpads voice recognition hardware and software optical scanners optical pointers digital image capture devices and associated interpretation software and the like.

The subject matter described herein may be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the subject matter described herein or any combination of such back end middleware or front end components. The components of the system may be interconnected by any form or medium of digital data communication e.g. a communication network . Examples of communication networks include a local area network LAN a wide area network WAN and the Internet.

The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

The subject matter described herein can be embodied in systems apparatus methods and or articles depending on the desired configuration. The implementations set forth in the foregoing description do not represent all implementations consistent with the subject matter described herein. Instead they are merely some examples consistent with aspects related to the described subject matter. Although a few variations have been described in detail above other modifications or additions are possible. In particular further features and or variations can be provided in addition to those set forth herein. For example the implementations described above can be directed to various combinations and subcombinations of the disclosed features and or combinations and subcombinations of several further features disclosed above. In addition the logic flow s depicted in the accompanying figures and or described herein do not necessarily require the particular order shown or sequential order to achieve desirable results. Other implementations may be within the scope of the following claims.

