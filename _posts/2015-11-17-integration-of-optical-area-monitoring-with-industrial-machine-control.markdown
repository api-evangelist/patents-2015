---

title: Integration of optical area monitoring with industrial machine control
abstract: An industrial safety system is provided that integrates optical safety monitoring with machine control. The safety system includes an imaging sensor device supporting pixel array processing functions that allow time-of-flight (TOF) analysis to be performed on selected portions of the pixel array, while two-dimensional imaging analysis is performed on the remaining portions of the array, reducing processing load and response time relative to performing TOF analysis for all pixels of the array. The portion of the pixel array designated for TOF analysis can be pre-defined through configuration of the imaging sensor device, or can be dynamically selected based on object detection and classification by the two-dimensional imaging analysis. The imaging sensor device can also implement a number of safety and redundancy functions to achieve a high degree of safety integrity.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09477907&OS=09477907&RS=09477907
owner: Rockwell Automation Technologies, Inc.
number: 09477907
owner_city: Mayfield Heights
owner_country: US
publication_date: 20151117
---
This application is a continuation of and claims priority to U.S. patent application Ser. No. 14 553 431 filed on Nov. 25 2014 and entitled INTEGRATION OF OPTICAL AREA MONITORING WITH INDUSTRIAL MACHINE CONTROL which claims priority to U.S. Provisional Application Ser. No. 62 000 487 filed on May 19 2014 entitled INTEGRATION OF OPTICAL AREA MONITORING WITH INDUSTRIAL MACHINE CONTROL. The entireties of these related applications are incorporated herein by reference.

The subject matter disclosed herein relates generally to industrial safety and more particularly to integration of industrial control with optical area monitoring using an imaging sensor capable of performing selective time of flight TOF analysis on specified portions of a pixel array

The following presents a simplified summary in order to provide a basic understanding of some aspects described herein. This summary is not an extensive overview nor is it intended to identify key critical elements or to delineate the scope of the various aspects described herein. Its sole purpose is to present some concepts in a simplified form as a prelude to the more detailed description that is presented later.

In one or more embodiments an optical safety system is provided comprising a pixel array component configured to for one or more pixel arrays of one or more images of a hazardous industrial area captured by one or more imaging sensor devices group pixels of the one or more pixel arrays to yield a subset of the pixels on which two dimensional 2D analysis is to be performed an image analysis component configured to perform 2D analysis on the subset of the pixels a distance determination component configured to perform 3D analysis on point cloud data received from the one or more imaging sensor devices and a hazard analysis and decision component configured to classify a first object in the image as motorized industrial equipment and a second object in the images as a human based on results of the 2D analysis and the 3D analysis to correlate the results of the 2D analysis and the 3D analysis to yield correlated information relating to a relationship between the human and the motorized industrial equipment and to generate a control output that controls the motorized equipment based on the correlated information.

Also one or more embodiments provide a method for controlling motorized industrial equipment comprising collecting by a safety system comprising at least one processor image data from one or more imaging sensor devices that monitor a hazardous industrial zone performing two dimensional 2D imaging analysis on a set of pixels of the image data performing three dimensional 3D analysis on point cloud data comprising distance information for selected pixels of the image data calculated by the one or more imaging sensor devices classifying a first object in the image data as a human based on at least one of the 2D imaging analysis or the 3D analysis classifying a second object in the images data as motorized industrial equipment based on at least one of the 2D imaging analysis or the 3D analysis correlating results of the 2D imaging analysis and the 3D analysis to yield correlated information relating to a relationship between the human and the motorized equipment within the hazardous industrial zone and sending an instruction to alter operation of the motorized industrial equipment based on the correlated information.

Also according to one or more embodiments a non transitory computer readable medium is provided having stored thereon instructions that in response to execution cause a system comprising a processor to perform operations the operations comprising collecting image data from one or more imaging sensor devices monitoring a hazardous industrial area performing two dimensional 2D imaging analysis on one or more groups of pixels comprising the image data performing 3D 3D analysis on point cloud data comprising distance information generated for selected pixels of the image data received from the one or more imaging sensor devices classifying a first object in the image data as a human based on at least one of the 2D imaging analysis or the 3D analysis classifying a second object in the image data as motorized industrial equipment based on at least one of the 2D imaging analysis or the 3D analysis correlating results of the 2D imaging analysis and the 3D analysis to yield correlated information relating to a relationship between the human and the motorized equipment within the hazardous industrial zone sending an instruction to alter operation of the motorized industrial equipment based on the correlated information.

To the accomplishment of the foregoing and related ends certain illustrative aspects are described herein in connection with the following description and the annexed drawings. These aspects are indicative of various ways which can be practiced all of which are intended to be covered herein. Other advantages and novel features may become apparent from the following detailed description when considered in conjunction with the drawings.

The subject disclosure is now described with reference to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding thereof. It may be evident however that the subject disclosure can be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to facilitate a description thereof.

As used in this application the terms component system platform layer controller terminal station node interface are intended to refer to a computer related entity or an entity related to or that is part of an operational apparatus with one or more specific functionalities wherein such entities can be either hardware a combination of hardware and software software or software in execution. For example a component can be but is not limited to being a process running on a processor a processor a hard disk drive multiple storage drives of optical or magnetic storage medium including affixed e.g. screwed or bolted or removable affixed solid state storage drives an object an executable a thread of execution a computer executable program and or a computer. By way of illustration both an application running on a server and the server can be a component. One or more components can reside within a process and or thread of execution and a component can be localized on one computer and or distributed between two or more computers. Also components as described herein can execute from various computer readable storage media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal . As another example a component can be an apparatus with specific functionality provided by mechanical parts operated by electric or electronic circuitry which is operated by a software or a firmware application executed by a processor wherein the processor can be internal or external to the apparatus and executes at least a part of the software or firmware application. As yet another example a component can be an apparatus that provides specific functionality through electronic components without mechanical parts the electronic components can include a processor therein to execute software or firmware that provides at least in part the functionality of the electronic components. As further yet another example interface s can include input output I O components as well as associated processor application or Application Programming Interface API components. While the foregoing examples are directed to aspects of a component the exemplified aspects or features also apply to a system platform interface layer controller terminal and the like.

As used herein the terms to infer and inference refer generally to the process of reasoning about or inferring states of the system environment and or user from a set of observations as captured via events and or data. Inference can be employed to identify a specific context or action or can generate a probability distribution over states for example. The inference can be probabilistic that is the computation of a probability distribution over states of interest based on a consideration of data and events. Inference can also refer to techniques employed for composing higher level events from a set of events and or data. Such inference results in the construction of new events or actions from a set of observed events and or stored event data whether or not the events are correlated in close temporal proximity and whether the events and data come from one or several event and data sources.

In addition the term or is intended to mean an inclusive or rather than an exclusive or. That is unless specified otherwise or clear from the context the phrase X employs A or B is intended to mean any of the natural inclusive permutations. That is the phrase X employs A or B is satisfied by any of the following instances X employs A X employs B or X employs both A and B. In addition the articles a and an as used in this application and the appended claims should generally be construed to mean one or more unless specified otherwise or clear from the context to be directed to a singular form.

Furthermore the term set as employed herein excludes the empty set e.g. the set with no elements therein. Thus a set in the subject disclosure includes one or more elements or entities. As an illustration a set of controllers includes one or more controllers a set of data resources includes one or more data resources etc. Likewise the term group as utilized herein refers to a collection of one or more entities e.g. a group of nodes refers to one or more nodes.

Various aspects or features will be presented in terms of systems that may include a number of devices components modules and the like. It is to be understood and appreciated that the various systems may include additional devices components modules etc. and or may not include all of the devices components modules etc. discussed in connection with the figures. A combination of these approaches also can be used.

Two dimensional 2D imaging sensors are generally used to detect and identify shape and or surface characteristics of objects within a viewing field of the sensor. illustrates identification of an object using a 2D imaging sensor . Some types of 2D imaging sensors e.g. imaging cameras operate by projecting a wide light beam toward an area to be monitored and collecting the reflected light reflected from the surfaces and objects e.g. object within the viewing area at a receiver. Some sensors may sweep the light beam across the viewing area in an oscillatory manner to collect line wise image data which is analyzed to identify object edges and surfaces surface patterns or other such information. Alternatively the sensor may project a stationary substantially planar beam of light across an area of interest and collect data on objects that pass through the beam. In general 2D image sensors perform grayscale or red green blue RGB analysis on the pixel data generated based on the reflected light to yield two dimensional image data for the viewing field which can be analyzed to identify object edges object surface patterns or contours or other such information. is a schematic illustrating 2D image analysis of an image using a 2D image sensor . 2D image analysis yields object and surface information in the x y plane. Depending on the particular application in which the imaging sensor is being used the sensor will generate suitable outputs based on the objects and or patterns detected within the viewing area.

Three dimensional 3D image sensors also known as time of flight TOF sensors are designed to generate distance information as well as two dimensional shape information for objects and surfaces within the sensor s viewing field. Some types of TOF sensors determine a distance of an object using phase shift monitoring techniques whereby a beam of light is emitted to the viewing field and the measured phase shift of light reflected from the object relative to the emitted light is translated to a distance value. Other types of TOF sensors that employ pulsed light illumination measure the elapsed time between emission of a light pulse to the viewing field and receipt of a reflected light pulse at the sensor s photo receiver. Since this time of flight information is a function of the distance of the object or surface from the sensor the sensor is able to leverage the TOF information to determine the distance of the object or surface point from the sensor. a schematic illustrating 3D image analysis of an image using a 3D image sensor . As shown in this figure 3D analysis yields distance or depth information in the z direction that is the distance of objects and surfaces from the sensor as well as imaging information in the x y plane.

Three dimensional image analysis which entails measurement of time of flight information and subsequent calculation of distance information is generally more processing intensive than 2D image analysis. The additional processing time and power required for 3D analysis may render 3D image sensors unsuitable for certain types of applications that require fast reliable response times. However there are certain types of applications that could benefit from 3D image analysis but which require fast and reliable decision making and response times. For example industrial safety monitoring applications must be able to reliably detect the presence of human beings within a potentially hazardous area and to respond with appropriate safety control outputs e.g. commands to stop or slow a running machine to remove power from hazardous machinery etc. with minimal delay to prevent injury.

To address these and other issues one or more embodiments of the present disclosure provide an industrial safety monitoring system that include an imaging sensor capable of performing 3D image analysis on selected subsets or portions of the sensor s pixel array. In one or more embodiments the imaging sensor allows one or more specified portions of the pixel array to be selected for 3D time of flight analysis in order to obtain distance information for pixels in that portion of the pixel array while the remaining pixel array areas will be processed using 2D image analysis. For example after the imaging sensor is trained on the area of interest a user may select a horizontal stripe of pixels across a middle section or an upper or lower edge of the pixel array for 3D analysis so that distance information as well as object identification information can be obtained and managed for the area corresponding to the selected stripe of pixels. The imaging sensor will apply 2D analysis e.g. grayscale or RGB analysis to the remaining non selected areas of the pixel array in order to detect identify classify and or correlate objects within the viewing area. Since 2D imaging processes more quickly than 3D processing processing load is reduced and sensor response time is improved by limiting 3D analysis to only those areas of the scene for which distance information is required. The imaging sensor can also be configured to correlate results of the 2D and 3D analysis so that the identity speed distance and trajectory of an object within the viewing space can be obtained with a high level of safety integrity. The imaging sensor is configured to interface with one or more pieces of industrial equipment e.g. an industrial automation controller a motion device a robot or other hazardous machine etc. and can send control instructions to the industrial equipment over a networked or hardwired interface based on results of the 2D and 3D image processing. In this way control of an industrial system is supplemented by the imaging sensor which provides a layer of vision facilitating additional control of the industrial system based on objects movements and events within the monitored area or volume.

In some embodiments the imaging sensor may be configured to dynamically select or modify the portion of the pixel array to which 3D analysis is to be applied e.g. based on detection of an object within the viewing area that satisfies one or more criteria. For example during normal operation the imaging sensor may be configured to perform continuous 2D analysis on the entire pixel array until an object or collection of objects having a certain defined classification e.g. a person a trolley etc. is detected. When such an object is detected e.g. when a person enters the viewing area of the sensor the sensor may define a portion of the pixel array corresponding to an area around the object for 3D analysis so that TOF distance information for the object can be tracked. The imaging sensor may dynamically change this defined pixel area to move with object so that distance and speed information can be monitored for the object as long as the object remains within the viewing area.

In one or more embodiments the imaging sensor may also be configured to for a given image identify non contiguous groups of pixels that belong to a single object of a defined classification. This can allow the imaging sensor to identify the presence of a person within the viewing area even if the person is partially obscured within the image. For example the imaging sensor may be trained to identify the presence of two separate visible objects corresponding to human legs and to correlate these two objects within the image as belonging to a human being who is within the viewing area but partially obscured. The sensor can track these correlated objects as necessary e.g. by performing 3D analysis on the pixel areas corresponding to the two objects so that appropriate safety output can be delivered to a controlled industrial system based on the location and speed of the person within the area.

Imaging sensor device can include an illumination component a pixel array component a distance determination component an image analysis component a hazard analysis and decision component a safety component an interface component one or more processors and memory . In various embodiments one or more of the illumination component pixel array component distance determination component image analysis component hazard analysis and decision component safety component interface component the one or more processors and memory can be electrically and or communicatively coupled to one another to perform one or more of the functions of the imaging sensor device . In some embodiments components and can comprise software instructions stored on memory and executed by processor s . Imaging sensor device may also interact with other hardware and or software components not depicted in . For example processor s may interact with one or more external user interface devices such as a keyboard a mouse a display monitor a touchscreen or other such interface devices. Imaging sensor device may also include network communication components and associated networking ports for sending data generated by any of components and over a network either or both of a standard data network or a safety network or over a backplane.

Illumination component can be configured to control emission of light by the sensor device. Imaging sensor device may comprise a laser or light emitting diode LED light source under the control of illumination component . In some embodiments illumination component may generate pulsed light emissions directed to the viewing field so that time of flight information for the reflected light pulses can be generated by the sensor device. The pixel array component can be configured to process and analyze a pixel array corresponding to an image of the viewing field monitored by the sensor device. For example the pixel array component may control which subset of pixels will be processed using 3D analysis. The subset of pixels to which 3D analysis is to be applied may be fixed e.g. preconfigured via user input alternatively the pixel array component may select the subset of pixels for 3D analysis dynamically according to one or more defined criteria e.g. human or facial recognition object classification etc. .

Distance determination component can be configured to derive distance information by performing 3D analysis on all or selected portions of the pixel array data. Any suitable analysis technique can be implemented by distance determination component including but not limited to phase shift monitoring or pulsed time of flight analysis.

The image analysis component can be configured to perform 2D analysis on portions of the pixel array that have not been selected for 3D analysis. The hazard analysis and decision component can be configured to analyze and control one or more sensor outputs based on results generated by the pixel array component distance determination component image analysis component and the safety component . This can include for example sending a control signal to a control or supervisory device e.g. an industrial controller an on board computer mounted in a mobile vehicle etc. to perform a control action initiating a safety action e.g. removing power from a hazardous machine switching an industrial system to a safe operating mode etc. sending a feedback message to one or more plant personnel via a human machine interface HMI or a personal mobile device sending data over a safety network or other such output.

Safety component can be configured to implement one or more safety and or redundancy features within the imaging sensor device to render the sensor device suitable for use in safety applications e.g. industrial safety applications designed to monitor a hazardous area and reliably perform automated control actions to mitigate risk of injury in response to detection of a potentially unsafe human presence or action automobile safety applications in which one or more imaging sensors mounted on a vehicle control breaking of the vehicle based on detected risk conditions etc. . By implementing such safety and redundancy functions the imaging sensor device can monitor a two dimensional plane and a three dimensional volume and respond to detected conditions with a high safety integrity level e.g. SIL or ASIL making the sensor device suitable for use in some safety application as an alternative to light curtains or other such sensors. Interface component can be configured to exchange data between imaging sensor device and one or more other pieces of industrial equipment e.g. an industrial controller a safety controller a robot controller an industrial machine etc. . For example interface component may be configured to interface with a plant network e.g. a control and information protocol network and Ethernet IP network a safety network etc. and exchange data with other devices over the network connection.

The one or more processors can perform one or more of the functions described herein with reference to the systems and or methods disclosed. Memory can be a computer readable storage medium storing computer executable instructions and or information for performing the functions described herein with reference to the systems and or methods disclosed.

Although components are depicted in as integrated components of an imaging sensor device it is to be appreciated that in some implementations some of the components may reside on a separate hardware component of a larger safety system of which imaging sensor device is a part. For example imaging sensor device may be one of multiple imaging devices that monitor a hazardous zone from various angles and provide imaging data to a central safety controller for analysis decision making and control. In such implementations one or more of the components may reside on the safety controller while other components may reside on one or more imaging sensor devices.

Perception allows the system to make sense of the environment given the data collected by the sensing functions. To this end information from multiple different sensors e.g. imaging sensors or other types of sensing devices can be fused and correlated in order to detect objects within the environment with a high degree of confidence. The system can also classify objects based on the sensed data e.g. distinguish between humans vehicles machines etc. determine current or anticipated behaviors of the objects and track detected objects as they move through the viewing space. The system may use multiple sensing technologies in addition to 2D and 3D sensing in order to accurately classify objects including but not limited to passive infrared sensing localization object tagging temperature sensing etc. The system can then identify hazardous conditions based on analysis of the perceived information. For example the location of certain objects or humans relative to a machine together with the machine s current behavior e.g. running stopped high speed low speed etc. may indicate a potential hazard requiring action by the system to mitigate risk of injury or damage. Accordingly the system can make a decision regarding a suitable response to the condition based on a defined decision strategy. In another example the behavior of a motorized industrial system e.g. running stopped high speed low speed movement direction etc. such as a robot can be monitored by feedback from the robot controller and also from the 2D 3D imaging sensor s . The system can thus diagnose anomalies by cross monitoring data relating to robot behavior from both sources. A detected anomaly may indicate a potential hazard due to a fault requiring action by the system to mitigate risk of injury or damage. Accordingly the system can make a decision regarding a suitable response to the condition based on a defined decision strategy. This provides a system level layer of fault diagnosis in addition to any diagnostics capability embedded in the robot controller and 2D 3D imaging sensor s and could thereby serve to increase the safety integrity level e.g. PL SIL or ASIL of the whole or parts of the system.

Depending on the scenario the system may generate a standard control action that allows the industrial system to continue running but in an altered state that mitigates the risks associated with the identified hazard. If the hazard has an associated risk of injury to a human operator the system may decide to initiate a safety action to stop the machine. In addition to controlling the machine s behavior based on analysis of the environment the system may also attempt to control human behavior in an effort to further mitigate risk by issuing feedback or instructions to people in the environment.

It is to be appreciated that the implementations described above are only intended to be exemplary and that other configurations are within the scope of one or more embodiments of this disclosure. For example in some implementations the analysis and decision functions can be performed on the one or more of the imaging sensor devices themselves which then provide instructions to other industrial components e.g. industrial controllers or other devices based on the decision strategy implemented by the sensor. In another example implementation the central safety controller can receive information from one or more imaging sensors using additional diverse technologies e.g. separate CMOS grey scale imaging radar ultra sonic etc. regarding the presence location classification and behavior of objects within the environment and perform hazard identification analysis and decision making locally based on the gathered sensor information.

Upon receipt of reflected light at the photo receivers of the imaging sensor device pixel data is generated based on the light intensity measured at each photo receiver and pixel array block performs processing on the resulting pixel array data comprising the image. This can include for example identifying a first subset of pixels in the array on which 3D processing is to be performed and designating a remaining second subset of pixels for 2D imaging analysis. Subsequent processing of each pixel depends upon the type of analysis 2D or 3D to be performed on that pixel.

For a pixel selected for 3D distance or depth analysis 3D distance analysis determines a distance of an object or surface in the viewing field corresponding to the pixel e.g. using phase shift time of flight analysis on a light beam reflected by the object or using pulsed time of flight analysis on a light pulse reflected from the object. Performing distance calculations for each pixel of the 3D analysis portion s of the pixel array yields a 3D point cloud for the selected areas of the viewing field.

2D imaging block performs 2D image analysis on the portion s of the pixel array for which 3D analysis is not performed. 2D image analysis can comprise RGB or grayscale analysis of the image portions corresponding to the non 3D pixels including but not limited to edge detection contour analysis image sharpening contrast adjustment difference and additive imaging etc. The imaging sensor device can employ 2D image analysis to identify objects within the viewing area and determine whether the identified objects correspond to one or more defined object classifications e.g. a human being a forklift or trolley a machined part on a conveyor a pallet containing packaged products etc. . In some embodiments the imaging sensor device may also be configured to perform facial recognition using 2D image analysis which is useful for applications in which a control decision or operator feedback output is dependent upon an identity of the person detected within the viewing field.

Imaging sensor device can correlate results of the 2D and 3D analysis to yield object data at object data block . Object data can include for example a location speed an acceleration and or trajectory of an identified object within the three dimensional viewing space. Depending on the type of application a hazard analysis and decision block can generate suitable outputs or operator feedback based on the correlated object data. In some embodiments imaging sensor device can interface with an industrial control or safety system a vehicle safety system or other such system to implement control features based on object detection. Accordingly outputs generated by the sensor device can include control instructions to an associated control or safety system e.g. a programmable logic controller or other safety automation controller an engine control unit of a mobile vehicle etc. to alter operation of a machine or system based on the object data safety outputs to an associated safety system e.g. a safety relay that place an industrial system in a safe state based on the presence and movements of a human being within the viewing field or other such outputs. Imaging sensor device can also include a safety block that monitors and diagnoses internal components and faults of the sensor device including but not limited to power monitoring vibration monitoring and temperature monitoring. Accordingly control outputs and messages generated by the hazard analysis and decision block can additionally be a function of the diagnosis results generated by the safety block .

In some embodiments illumination component may uniformly illuminate the viewing field using a laser LED or remote phosphor light source. Alternatively some embodiments of illumination component may employ a patterned illumination technique whereby rather than uniformly illuminating the viewing area the illumination component concentrates light in spots spaced with a certain distance over the viewing area. This technique can improve reliability of detection of small objects and of objects with low reflectivity. In such embodiments the size of each spot of light can be defined based on the effective size of the pixels and the lens characteristics of the receiving lens element of the sensor device. The receiving lens element is sized relative to the spot size such that the image of a spot on the receiving lens element covers at least the light sensitive area of one pixel. In a variation of this technique the illumination component or the lens design can also be configured to modulate the illumination intensity of the emitted spots such that high brightness spots and low brightness spots are interlaced across the viewing area simultaneously. This technique can facilitate reliable detection of bright and dark objects within a single image frame. In an example implementation the focused spots of illumination can be achieved by placing a squared lenslet comprising square or rectangular apertures in front of the LED laser or remote phosphor light source. The locations of the apertures on the lenslet define the spot pattern. To ensure accurate detection with small object sizes the spot pattern can be defined such that at least two horizontal spots and two vertical spots cover the minimum size of object at the given distance from the receiving lens element .

Lens element receives light reflected from the viewing field and pixel array component performs processing on the pixels of the resulting image data. As noted above imaging sensor device allows portions of the resulting pixel array to be selected for 3D distance or depth processing and analysis while the remaining portions of the pixel array are processed using 2D imaging analysis. In the example depicted in a horizontal band across a middle section of the pixel array has been selected for 3D analysis while the remaining portions of the pixel array above and below the selected band will be processed using 2D analysis. In some embodiments pixel array component identifies and groups the pixels into 2D and 3D sections based on a predefined configuration profile specifying one or more areas of the pixel array for which 3D analysis is to be performed. Alternatively pixel array component may be configured to dynamically select the areas of the pixel array on which 3D analysis is to be performed as will be described in more detail below.

Although depicts the area of 3D processing as a single horizontal band across the middle of the pixel array it is to be appreciated that substantially any manner of pixel grouping can be managed by pixel array component . illustrates other example pixel groupings. In addition to the single horizontal band depicted in pixel array pixels may also be grouped into multiple 3D bands either horizontal or vertical as shown in example pixel array . Pixel array depicts a split screen type of pixel grouping in which a left side portion of the pixel array is selected for 3D analysis while 2D analysis is performed on the right side portion. Pixels may also be grouped into non contiguous pixel clusters of various sizes as shown in example pixel array .

In an example scenario wherein the imaging sensor device is used to monitor an area of an industrial facility it may be known that certain areas of the viewing field correspond to potentially hazardous zones while other areas of the viewing field correspond to safe zones that pose little or no risk to operators. Accordingly a system designer may define a section of the pixel array that encompasses the known hazardous areas for 3D analysis. These pixel area definitions can be stored in configuration profile and leveraged by pixel array component to group pixels of the pixel array accordingly for group analysis. Portions of the pixel array that are not selected for 3D analysis will be processed using 2D analysis which is less computationally intensive than 3D analysis. By limiting 3D analysis to crucial subsets of the pixel array and performing 2D analysis on the remaining portions of the array overall processing time can be reduced relative to performing 3D analysis on the entire image.

In another example a ceiling mounted imaging sensor device may be oriented to face downward with the line of site substantially perpendicular to the floor in order to monitor traffic through an entrance gate to a room or zone of interest. In this example it may only be necessary to perform 3D analysis on a middle band of the pixel array corresponding to the pathway to the entrance gate. Accordingly a system designer can define this area of the pixel array and save these settings in the configuration profile .

Imaging sensor device can support any suitable technique for allowing a user to define 3D zones one the pixel array . For example an interface application executable on a personal computing device e.g. tablet computer laptop computer desktop computer mobile phone etc. may be used to facilitate data exchange between the computing device and the imaging sensor device . The interface application can generate and render configuration display screens capable of receiving input data from a user that set configuration parameters and definitions for the sensor. One or more configuration display screens may allow a user to define the areas of 3D analysis by entering x y coordinates that define the sections of the pixel array for which 3D analysis is to be performed. Alternatively the configuration display screens may allow the user to draw using a mouse or stylus boundary lines either linear or curved that define the areas of 3D analysis. If the imaging sensor device has been trained on the viewing area the configuration screens can display a live image or a screenshot of the viewing area and allow the user to draw the 3D analysis boundary lines as an overlay on the image or screenshot.

Upon receipt of live pixel array data and after the pixel array component has grouped the pixels into respective 3D and 2D zones image analysis component performs 2D imaging analysis on those portions of pixel array that were not designated by pixel array component for 3D analysis. As noted above imaging sensor device can employ 2D imaging analysis to identify and classify objects within the image frame. Classification of objects can be based on pre defined classes of objects that the imaging sensor device has been trained to identify including but not limited to human beings particular types of vehicles e.g. forklifts trolleys etc. a manufactured part a pallet or other such object classifications.

In some embodiments one or both of the pixel array component or the image analysis component can be configured recognize instances in which two or more non contiguous groups of pixels of the pixel array belong to a common object or person that may be partially obscured within the image. In an example scenario an operator may enter the image frame but may be partially obscured by another object within the frame such that only portions of the operator s legs or feet are directly visible to the sensor device. The pixels of the pixel array corresponding to the operator s left and right legs or feet may comprise separate non contiguous pixel groups since the operator is obscured above the knees. The image analysis component may be trained recognize lower body human features and therefore recognizes that two separate detected objects identified as human legs which are oriented a certain way with respect to one another within the frame belong to a common person and are indicative of a human presence within the image. Accordingly image analysis component can identify and classify the two objects as human legs and instruct the pixel array component to associate the two detected objects for collective analysis under the assumption that the two objects correspond to a human being.

Concurrently or in coordination with the 2D image analysis distance determination component can perform 3D analysis on the pixels comprising the defined 3D portion of the pixel array to determine a distance value associated with each of those pixels. The distance value measured for a pixel represents the distance of the object or surface from the imaging sensor device with respect to the pixel of the sensor device. Collectively the distance values for a set of pixels comprising an image are referred to as a point cloud.

The analysis technique employed by the distance determination component to measure the distance values depends on the type of illumination and 3D analysis supported by the device. For example for imaging sensor devices that employ phase shift analysis the distance determination component can monitor the phase shift of a reflected light beam received at a photo receiver and compare this phase shift with the phase of the light beam emitted by the illumination component . The distance is then determined as a function of the relative phase shift between the emitted and received light.

Other types of imaging sensor that employ pulsed light illumination measure the time duration between emission of a light pulse by the illumination component and receipt of a reflected light pulse at the photo receiver for each pixel and determining the distance as a function of this duration. In such embodiments the distance determination component may monitor the electrical output of the photo receiver which is a function of the intensity of light incident on the surface of the photo receiver and generate a waveform representing the reflected light pulse. The front edge of the returned light pulse can then be identified based on analysis of the waveform data which represents the time at which the light pulse was received at the lens element . The distance determination component can then compare this time with the time at which the emitted light pulse was sent by the illumination component . The difference between the two times represents the time of flight for the pulse from which the distance information for the pixel corresponding to the photo receiver can be derived. By performing waveform reconstruction and distance determination for each pixel in the 3D analysis portions of the pixel array a 3D point cloud can be derived for the selected areas of the pixel array .

Some embodiments of imaging sensor device may support dynamic definition of 3D analysis zones based on object detection and classification by the 2D image analysis component. For example during normal operation the imaging sensor device may perform 2D analysis on the entire pixel array until an object of a specified classification is detected within the viewing field. In response to detection of such an object e.g. a person a vehicle etc. within the viewing field the image analysis component may provide information to pixel array component identifying the object and its location within the pixel array . Pixel array component can then define one or more pixel groups corresponding to the identified object and instruct distance determination component to begin performing 3D analysis on those groups of pixels so that both the location and distance of the object can be tracked. In some embodiments the pixel array component and image analysis component can operate in conjunction to move the defined 3D analysis portion of the pixel array to track with the detected object as long as the object remains within the frame. Thus embodiments of the imaging sensor device can use 2D imaging analysis to recognize objects of interest within the frame and instruct pixel array component where 3D analysis should be performed. In this way the imaging sensor device can continuously collect TOF information for objects of interest while substantially minimizing the areas of the pixel array on which 3D analysis is performed optimizing processing and response times.

The imaging sensor device or a separate safety controller can correlate results of the 2D and 3D analyses and determine suitable control or messaging outputs based on object classification location velocity and or trajectory. illustrates correlation of 2D imaging and 3D distance information by the imaging sensor device or safety controller. As described above image analysis component can generate 2D analysis results including but not limited to object recognition or classification x y location of objects correlation of pixel groups determined to belong to a common object human and or facial recognition based on image analysis and other such data. Distance determination component generates 3D analysis results time of flight distance information for each pixel yielding 3D point cloud data for areas of interest areas of the pixel array specified for selective 3D analysis either manually by a system designer or dynamically by the sensor based on information provided by image analysis component as described in previous examples . The imaging sensor device or safety controller can correlate all or selected portions of these data sets to yield correlated results . These correlated results can include but are not limited to object location velocity and trajectory within the three dimensional space a predicted future location of an object of interest based on the three dimensional location velocity and trajectory or other such information.

In a non limiting example of 2D and 3D result correlation image analysis component may identify objects within the image frame that correspond to a class of objects for which 3D analysis is required. In response the imaging sensor device can apply 3D analysis to the region of the pixel array corresponding to the detected object to obtain distance information for the object over time while 2D analysis can track the x y location of the object within the frame. By correlating these results the object s instantaneous position velocity acceleration and trajectory within the three dimensional viewing space can be determined. For embodiments in which the imaging sensor device or safety controller supports prediction of future object position the sensor may also determine whether the object is predicted to be within a particular subspace of the three dimensional viewing field based on the current location speed and trajectory and generate a control or feedback output based on risk analysis using this prediction.

In another example the imaging sensor device or safety controller may coordinate object classification and edge detection 2D analysis results with depth analysis a 3D analysis result in order to obtain depth information for all pixels enclosed within the edges of an identified object. For example when an object enters the viewing field the imaging sensor device may leverage 2D imaging analysis to identify and classify the object as corresponding to a defined object class requiring 3D analysis. The 2D analysis may further include edge detection which identifies the visible edges or boundaries of the object. The imaging sensor can then perform selective 3D analysis on all pixels within the object boundaries identified via 2D analysis.

As noted above one or more embodiments of the imaging sensor device or associated safety controller can determine a location of a first identified object relative to another detected object for example the location of a human operator relative to a hazardous machine. This includes determining the distance of the first object from the second object within the three dimensional space. In order to reduce the complexity and computational overhead of this three dimensional distance calculation some embodiments of the imaging sensor device or safety controller can estimate distances between objects by projecting three dimensional object data onto two dimensional planes. is a graph illustrating distance measurement using projection of three dimensional objects onto a two dimensional plane. In this example two objects and are detected in the viewing field based analysis of point cloud data generated by 3D analysis of image information collected by one or more imaging sensor devices. In order to simplify the calculation of distances between the detected objects the one or more imaging sensor devices or the safety controller that collects and aggregates the point cloud data from the one or more imaging sensor devices project the three dimensional objects onto the X Y plane resulting in two dimensional projections and . The system then calculates the minimum distance between the two dimensional projections and within the respective two dimensional planes. As described in previous examples the hazard analysis and decision component can then perform decision making and generate safety action outputs based on the classifications of the respective objects and and the calculated minimum distance .

In some embodiments minimum distance calculation can be further simplified by encapsulating the projected two dimensional shapes within known geometric shapes and determining the minimum distances between the shapes. is a graph illustrating distance measurement between the two detected objects using projection and encapsulation. omits the three dimensional objects and for clarity and only shows the two dimensional projections of those objects on the X Y plane. In this example the one or more imaging sensor devices or the safety controller selects for each of the two dimensional projections and a known geometric shape capable of closely encapsulating the two dimensional projection. In the illustrated example the two dimensional projections and are encapsulated within ellipses. However in some embodiments the one or more imaging sensor devices or the safety controller can select a known geometric shape from a library of known shapes e.g. a rectangle a triangle a circle etc. determined to best fit the two dimensional projection. The minimum distance between the respective geometric shapes and is then calculated. Encapsulating the two dimensional projections within known geometric shapes transforms the irregularly shaped projections and into regular geometric shapes thereby reducing the complexity involved in determining the minimum distance between irregularly shaped projections within the two dimensional planes. Since the edges of the geometric shapes will always lay outside of the edges of the projections the minimum distance between the geometric shapes will never be greater than the minimum distance that would be calculated between edges of the actual projections that is the distances between the geometric shapes will always be equal to or less than the distance between the projections. Consequently although the calculated minimum distances between the encapsulating shapes may not be as accurate as the distances between the projections themselves the inaccuracy will always trigger an earlier safety response by virtue of the closer calculated distance than would be the case if the distances between the projections are used.

In some embodiments the encapsulation technique described above can also be implemented in the three dimensional space without first projecting the shapes onto the two dimensional planes. is a conceptual diagram illustrating three dimensional encapsulation according to one or more embodiments. In the illustrated example two objects an operator and a robot are detected and classified based on analysis of point cloud data generated by one or more imaging sensor devices e.g. based on distance analysis performed by the distance determination component . In order to determine the minimum distance between the two objects without the need to determine the nearest points between two irregularly shaped three dimensional objects which would render the computation more complex the system encapsulates both objects and within known three dimensional geometric shapes and . In this case three dimensional ellipses are used. However the system can select suitable shapes from a library of known shapes determined to most closely encapsulate each object e.g. cylinders cubes spheres egg shapes etc. . Once encapsulated the system can calculate the minimum distance between the two geometric shapes.

The examples illustrated in for determining distances between objects are only intended to be exemplary and it is to be appreciated that any suitable computational technique for calculating distances between objects are within the scope of one or more embodiments of this disclosure.

Returning now to based on the particular application being executed by the sensor device hazard analysis and decision component can be instructed to generate a suitable control safety or feedback output when the object classification position speed acceleration and or trajectory satisfy a defined criterion. In some embodiments interface component may interface with a control device e.g. an industrial controller a safety relay etc over a hardwired or networked connection and hazard analysis and decision component can issue control instructions to the control device based on identity position and behavior of objects observed in the viewing field. In an example scenario based on correlation of analysis results generated by the distance determination component and the image analysis component the imaging sensor device may identify that a plant employee has entered the viewing field and that the employee s current location speed acceleration and trajectory may place the employee within a potentially hazardous area near a controlled industrial machine. In response the hazard analysis and decision component is instructed to issue a command to the industrial controller to place the machine in a safe mode e.g. by placing the machine in an idle mode or a slowed operation mode or by instructing a safety relay to remove power from certain movable components of the machine . In another example scenario the hazard analysis and decision component may be configured to generate feedback information to be rendered on a display device based on object identification and behavior. This can include for example customized warning messages recommending that a user follow an alternate path or relocate to a safe area within the monitoring area. For embodiments of the imaging sensor device that support facial recognition feedback messages generated by hazard analysis and decision component may also be further customized based on an identity of the employee detected within the viewing field. Hazard analysis and decision component may interface with a display device mounted within the monitored area or may be targeted to a personal device associated with the identified employee.

The object detection and tracking features described above together with the reduced processing load and commensurate improvement in decision making and response time that results from minimizing the amount of 3D processing required render the imaging sensor devices described herein suitable for safety applications which require a high degree of safety integrity and fast response times in order to mitigate risk of injuries. is an example architecture of an integrated control and safety system that utilizes one or more embodiments of the imaging sensor device described above. In this example scenario an industrial robot or other type of industrial machine or system operates under the control and supervision of industrial controller e.g. a programmable logic controller or other type if industrial automation controller . Although the industrial system is depicted in as comprising a robot it is to be appreciated that other types of controlled industrial systems may also be used within this architecture including but not limited to a motion system a palletizing system a stamping press or other types of hazardous industrial systems. Robot may operate under direct control of robot controller which controls the robot in accordance with instructions and information provided by industrial controller . Alternatively industrial controller may interface directly with the robot s I O and control the robot directly.

The architecture may also include a human machine interface HMI 1106 that visualizes status operational and production data to an operator via one or more graphical screens. HMI may also allow the operator to issue limited control instructions to the control system or set values for one or more parameters e.g. setpoint values via the graphical screens.

One or more imaging sensor devices are configured to monitor the environment surrounding the robot the hazardous area . For configurations in which the imaging sensor devices act as stand alone safety control devices the imaging sensor devices are communicatively interfaced with one or both of the industrial controller and the robot or its associated robot controller to provided integrated vision based safety control. Imaging sensor devices may also interface with the HMI in order to deliver feedback messages or image data to the operator. Alternatively the imaging sensor devices may interface with a separate safety controller that aggregates imaging and point cloud data from the imaging sensor devices and performs one or more of object detection object classification object tracking decision making and safety response based on the aggregated data. The industrial controller robot controller HMI imaging sensor devices and safety controller may all reside on a plant network or safety network e.g. via one or more network switches . Alternatively imaging sensor devices and or safety controller may interface directly with one or both of the industrial controller or the robot controller via the controllers I O.

Using the techniques described above imaging sensor devices and or safety controller can identify and classify objects within the space defined by viewing fields and . For example imaging sensor devices and can be trained to identify product as a first object class product and to identify human operator as a second object class human . When product enters the viewing field e.g. when a conveyor system another downstream robot or a transportation vehicle delivers the product to be processed by robot imaging sensor devices and can perform 2D analysis on the image of the safety zone to identify the object determine that the object belongs to the product classification and allow robot and associated systems to continue cycling normally.

If an operator enters the viewing field the imaging sensor devices and identify the operator as belonging to the human classification. A determination is also made regarding the current operating mode of robot . This determination can be made by imaging sensor devices and or safety controller based on robot status information provided by industrial controller . The current operating status of the robot determines whether the viewing field is safe to enter. That is if the robot is currently stopped or disabled or is performing a cycle that does not pose a risk of injury to operators within the monitored viewing field detection of a human within the viewing field may not require a safety action to be performed by the imaging sensor device. If the robot is running a cycle that poses a potential risk to the operator within the viewing field detection of an object classified as human within the viewing field causes one or more of the imaging sensor devices and or safety controller to generate a control output placing the robot in a safe mode. This may comprise for example sending a control output to industrial controller instructing the controller to disable the robot and any other sub systems e.g. conveyors associated with the robot . In another example rather than disabling the robot the imaging sensor devices and may instruct the industrial controller to initiate a different robot operating cycle that poses no risk of injury to operator .

In some embodiments initiation of the control output and the type of control output generated may be based on a correlation between the operator s location and behavior and that of the robot . For example the imaging sensor devices or safety controller may determine the robot s current position speed and trajectory based on 2D analysis 3D analysis or a correlation of both types of analysis on the portion of image frame corresponding to the robot . Similar data can be obtained for the human operator . Imaging sensor devices and can then identify a level of risk based on such factors as the human operator s location relative to the machine the trajectories of the machine and the operator relative to one another or other such information. When the level of risk based on these factors exceeds a threshold one or more the imaging sensor devices and or an associated controller can initiate a suitable safety output to change the operation of robot in a manner that mitigates the risk or to disable the robot entirely. Additionally if the determined level of risk becomes elevated given the monitored factors but has not yet reached the threshold for initiation of a safety action imaging sensor devices and may generate and deliver a feedback message warning the operator of the potential risk and recommending a behavior e.g. relocating to a different area that will mitigate the risk. The imaging sensor devices and can deliver this message to HMI to a personal device carried by the operator to a display board mounted in the monitored area or other such devices.

To improve reliability of human detection while minimizing processing load and maintaining acceptable response times imaging sensor devices and can be configured to perform selective 3D analysis on certain pixels of the image frame of the viewing field while performing faster 2D analysis on the remaining portions of the image frame. For example one or both of the imaging sensor devices and may be configured to perform 3D analysis on portions of the image immediately surrounding the robot while performing 2D analysis on the outer perimeter of the image frame. In another scenario one or both of the imaging sensor devices and may be configured to perform 2D analysis over the entire pixel array of the image during normal operation until an object that does not conform to the product classification enters the viewing field. In response to detection of a non product object entering the viewing field the pixel array component of the sensor can designate an area of the pixel array corresponding to the object for 3D analysis in order to obtain height information and profile over time for the new object which can be used by the sensor to assist in determining whether the new object corresponds to the human being classification. The pixel array component can dynamically move the portion of the image designated for 3D analysis to track with the object as it moves through the viewing field e.g. based on object detection information provided by the image analysis component .

In another example of dynamic hazard analysis one or more of the imaging sensor devices and may be configured to adjust the size of the 3D analysis pixel area e.g. the hazard zone based on a current hazard level determined via 2D image analysis. For example an area of the pixel array designated for 3D analysis may correspond to an area surrounding the robot . One or more of the imaging sensor device and may adjust a size of the 3D analysis area surrounding the robot based on detected conditions. For example based on 2D analysis of the pixel array an imaging sensor device may determine a speed at which the robot is currently operating. If this speed is determined to exceed a threshold implying a higher risk of injury given expected human response times the portion of the pixel array for which 3D analysis is performed may be increased to a larger area surrounding the robot . When the imaging sensor device determines that the robot is operating at a slower speed the risk level is assumed to be lessened and the 3D analysis portion of the pixel array is made smaller to allow operators freedom to approach the robot more closely. In some such embodiments the size of the 3D portion of the pixel array may be a continuous function of the determined speed of the robot.

For system configurations in which 3D analysis and safety control is performed by the safety controller the imaging sensor devices and may provide point cloud information pixel level distance information calculated by the respective sensor devices to the safety controller for centralized analysis. The point cloud information provided by the sensor devices represents 3D distance data for multiple angular views of the monitored area. The safety controller can aggregate this multi angular point cloud data to yield a composite three dimensional representation of the monitored area and perform object detection classification and analysis on selected portions of the composite three dimensional representation. This analysis can include but is not limited to detection and classification of objects within the three dimensional representation determination of object location speed acceleration and or trajectory calculation of minimum distances between selected objects e.g. using one or more of the techniques described above in connection with or other such analysis.

The configuration depicted in has a number of advantages over a light curtain solution. For example light curtain transmitters and receivers are typically mounted vertically on either side of an entryway exposing those components to possible damage by passing objects. Mounting the imaging sensor devices and on the ceiling mitigates the risk of damage by placing the monitoring device outside the reach of passing objects. Moreover light curtains are often muted to allow a product to pass through the entryway at particular defined durations during the operating cycle that is durations during which a product is expected to pass through the light curtain and enabled during the remaining portions of the cycle. Since muting of the light curtain in such scenarios is a function of the particular portion of the operating cycle being executed this method opens the possibility that a human may pass through the light curtain undetected during those times when the light curtain is muted. By contrast since imaging sensor devices and and or their associated safety controller are able to classify detected objects as corresponding to a human classification and alter control based on this object classification the imaging sensor devices and are able to perform more direct and intelligent muting based on object detection and classification rather than being cycle dependent.

To ensure safety integrity of the imaging sensor device one or more embodiments may include a safety component that implements one or more features for ensuring reliability and accuracy of the sensor in a range of operating conditions improving the safety integrity of the sensor device. In general safety component is configured to perform fault monitoring and diagnostic analysis on a range of conditions that may impact the integrity of the sensor operation and trigger actions designed to mitigate hazards that may arise when a monitored deviates from a safe state e.g. instruct the hazard analysis and decision component to switch a machine to a safe state output a warning message etc. illustrates an example safety component that can be integrated in one or more embodiments of imaging sensor device . Safety component can comprise one or more sub components that perform various types of diagnostics fault monitoring and compensation for temperature power vibration and internal component faults. However it is to be appreciated that other types of fault monitoring and diagnostic capabilities may be supported by various embodiments of safety component and are within the scope of this disclosure.

Temperatures within the sensor device may have an impact on the distance values generated by the sensor components. Accordingly safety component can include a temperature control component configured to adjust the distance values generated by distance determination component to compensate for measured deviations in temperature. Some embodiments of temperature control component can also include mechanisms to regulate the sensor s internal temperature to maintain a specified optimal operating temperature as well as redundant fault detection mechanisms to ensure that the temperature compensation meets or exceeds a defined minimum safety integrity level e.g. SIL SIL ASIL ASIL etc. .

Safety component can also include a power monitoring component configured to monitor the internal rails that provide power to crucial components and perform compensation actions in response to detected voltage deviations from rated tolerances. In this regard some embodiments of imaging sensor device may include a redundant power supply to ensure that a failure of the main supply does not prevent continued operation of the sensor device. Vibration compensation component can be configured perform appropriate compensation actions in response to monitored vibrations induced on the sensor.

Fault detection component can be configured to monitor and diagnose internal sensor faults and to generate information or instructions to the hazard analysis and decision component based on the fault information. Also to further comply with safety integrity level requirements processor s can be specified as a SIL or ASIL rated processor to ensure that the imaging sensor conforms to required safety standards.

At three dimensional 3D analysis is performed on a second subset of the pixel array to determine distance information for spaces within the viewing area corresponding to the second subset of the pixel array. In some embodiments the second subset of the pixel array on which 3D analysis is to be performed can be defined by a system designer prior to operation and recorded in a configuration profile which can be read by a pixel array component of the imaging sensor device in order to group the first and second subsets of the pixel array for respective 2D and 3D analysis. Alternatively the imaging sensor device can dynamically select the second subset of the pixel array for 3D analysis based on results of the 2D analysis performed at step . For example if the 2D analysis determines that an object of a certain classification has entered the viewing field the imaging sensor device may define an area of the pixel array corresponding to the newly identified object and begin performing 3D analysis on the object in order to obtain spatial information for the object.

At a control output is generated by the imaging sensor device based on correlation of information generated by the 2D analysis and the 3D analysis and sent to an industrial controller associated with the industrial system. For example the imaging sensor device may correlate the 2D and 3D analysis results to yield an identity position speed acceleration orientation and or trajectory for the object and generate the control output based on one or more of these measured factors. The control output may comprise for example an instruction to an industrial controller to transition the industrial system to a safe mode to disable the industrial system to alter the current operating cycle of the industrial system or other such instructions.

At a determination is made regarding whether the classification of the object determined at step requires 3D distance analysis. For example the sensor device may be trained to identify when a human has entered the viewing area. Accordingly the sensor can determine that an object having a human classification has entered the viewing area based on the 2D analysis and object classification performed at steps and .

If the object classification does not require 3D analysis the methodology returns to step and continues monitoring received image data. Alternatively if the object classification is determined to require 3D analysis the methodology moves to step where a subset of the pixel array corresponding to an area of the image surrounding the object is identified. At 3D analysis is performed on the subset of the pixel array identified at step in order to determine distance information for the object. At a control output is generated by the imaging sensor device based on correlation of information generated by the 2D analysis of step and the 3D analysis of step and the control output is sent to an industrial controller associated with the industrial system. This may include for example identifying a potentially hazardous condition or risk based on correlation of the 2D and 3D results and sending an instruction to the industrial controller which is communicatively connected to the imaging sensor to perform an action designed to mitigate the detected hazard. The action may comprise for example switching an industrial machine or system to a safe state e.g. stopping the machine switching the machine to a slow operation mode returning the machine to the home position etc. or other such action.

At two or more objects within the three dimensional representation generated at step are identified and classified based on analysis of the aggregated point cloud data. For example an analysis component may be configured to recognize objects corresponding to human operators hazardous machines trolleys or other predefined object classifications. In some configurations information from other sensing technologies may also be used in conjunction with the point cloud data in order to accurately classify the two or more objects e.g. passive infrared sensing localization tagging objects etc. . At three dimensional representations of the identified images are projected onto a two dimensional plane to yield one or more projections. At as an optional step the two or more projections are encapsulated in respective two or more known geometric shapes.

At a minimum distance is estimated between at least two of the two or more projections or the two or more geometric shapes if optional step is implemented . At a control output is sent to an industrial controller to alter the operation of an industrial system based on the object classification determined at step and the minimum distance determined at step .

Embodiments systems and components described herein as well as control systems and automation environments in which various aspects set forth in the subject specification can be carried out can include computer or network components such as servers clients programmable logic controllers PLCs automation controllers communications modules mobile computers on board computers for mobile vehicles wireless components control components and so forth which are capable of interacting across a network. Computers and servers include one or more processors electronic integrated circuits that perform logic operations employing electric signals configured to execute instructions stored in media such as random access memory RAM read only memory ROM a hard drives as well as removable memory devices which can include memory sticks memory cards flash drives external hard drives and so on.

Similarly the term PLC or automation controller as used herein can include functionality that can be shared across multiple components systems and or networks. As an example one or more PLCs or automation controllers can communicate and cooperate with various network devices across the network. This can include substantially any type of control communications module computer Input Output I O device sensor actuator and human machine interface HMI that communicate via the network which includes control automation and or public networks. The PLC or automation controller can also communicate to and control various other devices such as standard or safety rated I O modules including analog digital programmed intelligent I O modules other programmable controllers communications modules sensors actuators output devices and the like.

The network can include public networks such as the internet intranets and automation networks such as control and information protocol CIP networks including DeviceNet ControlNet safety networks and Ethernet IP. Other networks include Ethernet DH DH Remote I O Fieldbus Modbus Profibus CAN wireless networks serial protocols and so forth. In addition the network devices can include various possibilities hardware and or software components . These include components such as switches with virtual local area network VLAN capability LANs WANs proxies gateways routers firewalls virtual private network VPN devices servers clients computers configuration tools monitoring tools and or other devices.

In order to provide a context for the various aspects of the disclosed subject matter as well as the following discussion are intended to provide a brief general description of a suitable environment in which the various aspects of the disclosed subject matter may be implemented.

With reference to an example environment for implementing various aspects of the aforementioned subject matter includes a computer . The computer includes a processing unit a system memory and a system bus . The system bus couples system components including but not limited to the system memory to the processing unit . The processing unit can be any of various available processors. Multi core microprocessors and other multiprocessor architectures also can be employed as the processing unit .

The system bus can be any of several types of bus structure s including the memory bus or memory controller a peripheral bus or external bus and or a local bus using any variety of available bus architectures including but not limited to 8 bit bus Industrial Standard Architecture ISA Micro Channel Architecture MSA Extended ISA EISA Intelligent Drive Electronics IDE VESA Local Bus VLB Peripheral Component Interconnect PCI Universal Serial Bus USB Advanced Graphics Port AGP Personal Computer Memory Card International Association bus PCMCIA and Small Computer Systems Interface SCSI .

The system memory includes volatile memory and nonvolatile memory . The basic input output system BIOS containing the basic routines to transfer information between elements within the computer such as during start up is stored in nonvolatile memory . By way of illustration and not limitation nonvolatile memory can include read only memory ROM programmable ROM PROM electrically programmable ROM EPROM electrically erasable PROM EEPROM or flash memory. Volatile memory includes random access memory RAM which acts as external cache memory. By way of illustration and not limitation RAM is available in many forms such as synchronous RAM SRAM dynamic RAM DRAM synchronous DRAM SDRAM double data rate SDRAM DDR SDRAM enhanced SDRAM ESDRAM Synchlink DRAM SLDRAM and direct Rambus RAM DRRAM .

Computer also includes removable non removable volatile nonvolatile computer storage media. illustrates for example a disk storage . Disk storage includes but is not limited to devices like a magnetic disk drive floppy disk drive tape drive Jaz drive Zip drive LS 100 drive flash memory card or memory stick. In addition disk storage can include storage media separately or in combination with other storage media including but not limited to an optical disk drive such as a compact disk ROM device CD ROM CD recordable drive CD R Drive CD rewritable drive CD RW Drive or a digital versatile disk ROM drive DVD ROM . To facilitate connection of the disk storage to the system bus a removable or non removable interface is typically used such as interface .

It is to be appreciated that describes software that acts as an intermediary between users and the basic computer resources described in suitable operating environment . Such software includes an operating system . Operating system which can be stored on disk storage acts to control and allocate resources of the computer . System applications take advantage of the management of resources by operating system through program modules and program data stored either in system memory or on disk storage . It is to be appreciated that one or more embodiments of the subject disclosure can be implemented with various operating systems or combinations of operating systems.

A user enters commands or information into the computer through input device s . Input devices include but are not limited to a pointing device such as a mouse trackball stylus touch pad keyboard microphone joystick game pad satellite dish scanner TV tuner card digital camera digital video camera web camera and the like. These and other input devices connect to the processing unit through the system bus via interface port s . Interface port s include for example a serial port a parallel port a game port and a universal serial bus USB . Output device s use some of the same type of ports as input device s . Thus for example a USB port may be used to provide input to computer and to output information from computer to an output device . Output adapters are provided to illustrate that there are some output devices like monitors speakers and printers among other output devices which require special adapters. The output adapters include by way of illustration and not limitation video and sound cards that provide a means of connection between the output device and the system bus . It should be noted that other devices and or systems of devices provide both input and output capabilities such as remote computer s .

Computer can operate in a networked environment using logical connections to one or more remote computers such as remote computer s . The remote computer s can be a personal computer a server a router a network PC a workstation a microprocessor based appliance a peer device or other common network node and the like and typically includes many or all of the elements described relative to computer . For purposes of brevity only a memory storage device is illustrated with remote computer s . Remote computer s is logically connected to computer through a network interface and then physically connected via communication connection . Network interface encompasses communication networks such as local area networks LAN and wide area networks WAN . LAN technologies include Fiber Distributed Data Interface FDDI Copper Distributed Data Interface CDDI Ethernet IEEE 802.3 Token Ring IEEE 802.5 and the like. WAN technologies include but are not limited to point to point links circuit switching networks like Integrated Services Digital Networks ISDN and variations thereon packet switching networks and Digital Subscriber Lines DSL .

Communication connection s refers to the hardware software employed to connect the network interface to the system bus . While communication connection is shown for illustrative clarity inside computer it can also be external to computer . The hardware software necessary for connection to the network interface includes for exemplary purposes only internal and external technologies such as modems including regular telephone grade modems cable modems and DSL modems ISDN adapters and Ethernet cards.

What has been described above includes examples of the subject innovation. It is of course not possible to describe every conceivable combination of components or methodologies for purposes of describing the disclosed subject matter but one of ordinary skill in the art may recognize that many further combinations and permutations of the subject innovation are possible. Accordingly the disclosed subject matter is intended to embrace all such alterations modifications and variations that fall within the spirit and scope of the appended claims.

In particular and in regard to the various functions performed by the above described components devices circuits systems and the like the terms including a reference to a means used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. a functional equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary aspects of the disclosed subject matter. In this regard it will also be recognized that the disclosed subject matter includes a system as well as a computer readable medium having computer executable instructions for performing the acts and or events of the various methods of the disclosed subject matter.

In addition while a particular feature of the disclosed subject matter may have been disclosed with respect to only one of several implementations such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore to the extent that the terms includes and including and variants thereof are used in either the detailed description or the claims these terms are intended to be inclusive in a manner similar to the term comprising. 

In this application the word exemplary is used to mean serving as an example instance or illustration. Any aspect or design described herein as exemplary is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather use of the word exemplary is intended to present concepts in a concrete fashion.

Various aspects or features described herein may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device carrier or media. For example computer readable media can include but are not limited to magnetic storage devices e.g. hard disk floppy disk magnetic strips . . . optical disks e.g. compact disk CD digital versatile disk DVD . . . smart cards and flash memory devices e.g. card stick key drive . . . .

