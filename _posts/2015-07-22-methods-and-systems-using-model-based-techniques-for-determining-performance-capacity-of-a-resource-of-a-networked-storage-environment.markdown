---

title: Methods and systems using model based techniques for determining performance capacity of a resource of a networked storage environment
abstract: Methods and systems for managing resources in a networked storage environment are provided. One method includes using a queuing model for a resource that processes a plurality of requests at a networked storage environment for predicting a relationship between latency and utilization of the resource. The queueing model uses inter-arrival time and service time to determine latency, where inter-arrival time is a duration that tracks when requests arrive at the resource and the service time tracks a duration for servicing the requests by the resource. The method further includes identifying optimum utilization of the resource using the predicted relationship between latency and utilization, where the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency; and determining available performance capacity for the resource using the optimum utilization and actual utilization of the resource.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09465548&OS=09465548&RS=09465548
owner: NETAPP, INC.
number: 09465548
owner_city: Sunnyvale
owner_country: US
publication_date: 20150722
---
Various forms of storage systems are used today. These forms include direct attached storage DAS network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple clients with access to shared data backing up data and others.

A storage system typically includes at least a computing system executing a storage operating system for storing and retrieving data on behalf of one or more client computing systems may just be referred to as client or clients . The storage operating system stores and manages shared data containers in a set of mass storage devices.

Quality of Service QOS is a metric used in a storage environment to provide certain throughput for processing input output I O requests for reading or writing data a response time goal within which I O requests are processed and a number of I O requests processed within a given time for example in a second IOPS . Throughput means amount of data transferred within a given time for example in megabytes per second Mb s .

To process an I O request to read and or write data various resources are used within a storage system for example processors at storage system nodes storage devices and others. The different resources perform various functions in processing the I O requests and have finite capacity to process requests. As storage systems continue to expand in size complexity and operating speeds it is desirable to efficiently monitor and manage resource usage and know what capacity of a resource may be available at any given time. Continuous efforts are being made to better manage resources of networked storage environments.

In one aspect a machine implemented method is provided. The method includes using a queuing model for a resource that processes a plurality of requests at a networked storage environment for predicting a relationship between latency and utilization of the resource. The queueing model uses inter arrival time and service time to determine latency where inter arrival time is a duration that tracks when requests arrive at the resource and the service time tracks a duration for servicing the requests by the resource. The method further includes identifying optimum utilization of the resource using the predicted relationship between latency and utilization where the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency and determining available performance capacity for the resource using the optimum utilization and actual utilization of the resource.

In another aspect a non transitory machine readable storage medium having stored thereon instructions for performing a method comprising machine executable code is provided. The code when executed by at least one machine causes the machine to use a queuing model for a resource that processes a plurality of requests at a networked storage environment for predicting a relationship between latency and utilization of the resource. The queueing model uses inter arrival time and service time to determine latency where inter arrival time is a duration that tracks when requests arrive at the resource and the service time tracks a duration for servicing the requests by the resource. The code further causes the machine to identify optimum utilization of the resource using the predicted relationship between latency and utilization where the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency and determine available performance capacity for the resource using the optimum utilization and actual utilization of the resource.

In yet another aspect a system having a memory containing machine readable medium comprising machine executable code with stored instructions is provided. A processor module coupled to the memory is configured to execute the machine executable code to use a queuing model for a resource that processes a plurality of requests at a networked storage environment for predicting a relationship between latency and utilization of the resource. The queueing model uses inter arrival time and service time to determine latency where inter arrival time is a duration that tracks when requests arrive at the resource and the service time tracks a duration for servicing the requests by the resource. The code further causes the machine to identify optimum utilization of the resource using the predicted relationship between latency and utilization where the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency and determine available performance capacity for the resource using the optimum utilization and actual utilization of the resource.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various thereof in connection with the attached drawings.

As a preliminary note the terms component module system and the like as used herein are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a hardware processor a hardware based processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example at non transitory computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one aspect a performance manager module is provided that interfaces with a storage operating system to collect quality of service QOS data or performance data for various resources. QOS provides a certain throughput i.e. amount of data that is transferred within a given time interval for example megabytes per seconds MBS latency and or a number of input output operations that can be processed within a time interval for example in a second referred to as IOPS . Latency means a delay in completing the processing of an I O request and may be measured using different metrics for example a response time in processing I O requests.

In one aspect methods and systems for managing resources in a networked storage environment is provided. The resources may be managed based on remaining or useful performance capacity at any given time that is available for a resource relative to a peak optimal performance capacity without violating any performance expectations. The available performance capacity may be referred to as headroom that is discussed in detail below. The resource maybe any resource in the networked storage environment including processing nodes and aggregates that are described below in detail. Peak performance capacity of a resource may be determined according to performance limits that may be set by policies for example QoS or service level objectives SLOs as described below .

In one aspect the remaining or available performance capacity is determined from a relationship between latency and utilization of a resource. shows an example of one such curve. Latency for a given resource that is used to process workloads is shown on the vertical Y axis while the utilization of the resource is shown on the X axis.

The latency v utilization curve shows an optimal point after which latency shows a rapid increase. Optimal point represents maximum or optimum utilization of a resource beyond which an increase in workload are associated with higher throughput gains than latency increase. Beyond the optimal point if the workload increases at a resource the throughput gains or utilization increase is smaller than the increase in latency. An optimal point may be determined by a plurality of techniques defines below. The optimal point may also be customized based on a service level that guarantees certain latency utilization for a user. The use of optimal points are described below in detail.

An operational point shows current utilization of the resource. The available performance capacity is shown as . In one aspect the operational point may be determined based on current utilization of a resource. The operational point may also be determined based on the effect of internal workloads for example when a storage volume is moved when storage node is configured as a high availability failover node or when there are workloads that can be throttled or delayed because they may not be very critical.

Headroom may be based on current utilization and a current optimal point that is ascertained based on collected and observed data. This is referred to sampled headroom. The sampled headroom may be modified by updating the current utilization of the resource to reflect any high availability node pair load defined below or any work that can be throttled or defined as not being critical to a workload mix. The term workload mix represents user workloads at a resource. Details for computing sampled and actual headroom are provided below.

In one aspect methods and systems for managing resources in a networked storage environment are provided. One method includes using a queuing model for a resource that processes a plurality of requests at a networked storage environment for predicting a relationship between latency and utilization of the resource. The queueing model uses inter arrival time and service time to determine latency where inter arrival time is a duration that tracks when requests arrive at the resource and the service time tracks a duration for servicing the requests by the resource. The method further includes identifying optimum utilization of the resource using the predicted relationship between latency and utilization where the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency and determining available performance capacity for the resource using the optimum utilization and actual utilization of the resource.

Before describing the processes for generating the latency v utilization may also be referred to as LvU the following provides a description of the overall networked storage environment and the resources used in the operating environment for storing data.

The performance manager obtains the QOS data and stores it at a data structure . In one aspect performance manager analyzes the QOS data for determining headroom for a given resource. Headroom related information may be stored at data structure A that is described below in detail. Details regarding the various operations performed by the performance manager for determining headroom are provided below.

In one aspect storage system has access to a set of mass storage devices A N may be referred to as storage devices or simply as storage device within at least one storage subsystem . The storage devices may include writable storage device media such as magnetic disks video tape optical DVD magnetic tape non volatile memory devices for example solid state drives SSDs including self encrypting drives flash memory devices and any other similar media adapted to store information. The storage devices may be organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID . The aspects disclosed are not limited to any particular storage device type or storage device configuration.

In one aspect the storage system provides a set of logical storage volumes may be interchangeably referred to as volume or storage volume for providing physical storage space to clients A N or virtual machines VMs A N . A storage volume is a logical storage object and typically includes a file system in a NAS environment or a logical unit number LUN in a SAN environment. The various aspects described herein are not limited to any specific format in which physical storage is presented as logical storage volume LUNs and others 

Each storage volume may be configured to store data files or data containers or data objects scripts word processing documents executable programs and any other type of structured or unstructured data. From the perspective of one of the client systems each storage volume can appear to be a single drive. However each storage volume can represent storage space in at one storage device an aggregate of some or all of the storage space in multiple storage devices a RAID group or any other suitable set of storage space.

A storage volume is identified by a unique identifier Volume ID and is allocated certain storage space during a configuration process. When the storage volume is created a QOS policy may be associated with the storage volume such that requests associated with the storage volume can be managed appropriately. The QOS policy may be a part of a QOS policy group referred to as Policy Group that is used to manage QOS for several different storage volumes as a single unit. The QOS policy information may be stored at a QOS data structure maintained by a QOS module . QOS at the storage system level may be implemented by the QOS module . QOS module maintains various QOS data types that are monitored and analyzed by the performance manager as described below in detail.

The storage operating system organizes physical storage space at storage devices as one or more aggregate where each aggregate is a logical grouping of physical storage identified by a unique identifier and a location. The aggregate includes a certain amount of storage space that can be expanded. Within each aggregate one or more storage volumes are created whose size can be varied. A qtree sub volume unit may also be created within the storage volumes. For QOS management each aggregate and the storage devices within the aggregates are considered as resources that are used by storage volumes.

The storage system may be used to store and manage information at storage devices based on an I O request. The request may be based on file based access protocols for example the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP . Alternatively the request may use block based access protocols for example the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP .

In a typical mode of operation a client or a VM transmits one or more I O request such as a CFS or NFS read or write request over a connection system to the storage system . Storage operating system receives the request issues one or more I O commands to storage devices to read or write the data on behalf of the client system and issues a CIFS or NFS response containing the requested data over the network to the respective client system.

System may also include a virtual machine environment where a physical resource is time shared among a plurality of independently operating processor executable VMs. Each VM may function as a self contained platform running its own operating system OS and computer executable application software. The computer executable instructions running in a VM may be collectively referred to herein as guest software. In addition resources available within the VM may be referred to herein as guest resources. 

The guest software expects to operate as if it were running on a dedicated computer rather than in a VM. That is the guest software expects to control various events and have access to hardware resources on a physical computing system may also be referred to as a host platform or host system which maybe referred to herein as host hardware resources . The host hardware resource may include one or more processors resources resident on the processors e.g. control registers caches and others memory instructions residing in memory e.g. descriptor tables and other resources e.g. input output devices host attached storage network attached storage or other like storage that reside in a physical machine or are coupled to the host system.

In one aspect system may include a plurality of computing systems A N may also be referred to individually as host platform system or simply as server communicably coupled to the storage system via the connection system such as a local area network LAN wide area network WAN the Internet or any other interconnect type. As described herein the term communicably coupled may refer to a direct connection a network connection a wireless connection or other connections to enable communication between devices.

Host system A includes a processor executable virtual machine environment having a plurality of VMs A N that may be presented to client computing devices systems A N. VMs A N execute a plurality of guest OS A N may also be referred to as guest OS that share hardware resources . As described above hardware resources may include processors memory I O devices storage or any other hardware resource.

In one aspect host system interfaces with a virtual machine monitor VMM for example a processor executed Hyper V layer provided by Microsoft Corporation of Redmond Wash. a hypervisor layer provided by VMWare Inc. or any other type. VMM presents and manages the plurality of guest OS A N executed by the host system . The VMM may include or interface with a virtualization layer VIL that provides one or more virtualized hardware resource to each OS A N.

In one aspect VMM is executed by host system A with VMs A N. In another aspect VMM may be executed by an independent stand alone computing system often referred to as a hypervisor server or VMM server and VMs A N are presented at one or more computing systems.

It is noteworthy that different vendors provide different virtualization environments for example VMware Corporation Microsoft Corporation and others. The generic virtualization environment described above with respect to may be customized to implement the aspects of the present disclosure. Furthermore VMM or VIL may execute other modules for example a storage driver network interface and others the details of which are not germane to the aspects described herein and hence have not been described in detail.

System may also include a management console that executes a processor executable management application for managing and configuring various elements of system . Application may be used to manage and configure VMs and clients as well as configure resources that are used by VMs clients according to one aspect. It is noteworthy that although a single management console is shown in system may include other management consoles performing certain functions for example managing storage systems managing network connections and other functions described below.

In one aspect application may be used to present storage space that is managed by storage system to clients A N or VMs . The clients may be grouped into different service levels also referred to as service level objectives or SLOs where a client with a higher service level may be provided with more storage space than a client with a lower service level. A client at a higher level may also be provided with a certain QOS vis vis a client at a lower level.

Although storage system is shown as a stand alone system i.e. a non cluster based system in another aspect storage system may have a distributed architecture for example a cluster based system of . Before describing the various aspects of the performance manager the following provides a description of a cluster based storage system.

The clustered storage system includes a plurality of nodes . . a cluster switching fabric and a plurality of mass storage devices . . may be referred to as and similar to storage device that are used as resources for processing I O requests.

Each of the plurality of nodes . . is configured to include a network module maybe referred to as N module a storage module maybe referred to as D module and a management module maybe referred to as M Module each of which can be implemented as a processor executable module. Specifically node . includes a network module . a storage module . and a management module . node . includes a network module . a storage module . and a management module . and node . includes a network module . a storage module . and a management module ..

The network modules . . include functionality that enable the respective nodes . . to connect to one or more of the client systems . .N over the computer network while the storage modules . . connect to one or more of the storage devices . .. Accordingly each of the plurality of nodes . . in the clustered storage server arrangement provides the functionality of a storage server.

The management modules . . provide management functions for the clustered storage system . The management modules . . collect storage information regarding storage devices .

Each node may execute or interface with a QOS module shown as . . that is similar to the QOS module . The QOS module may be executed for each node or a single QOS module may be used for the entire cluster. The aspects disclosed herein are not limited to the number of instances of QOS module that may be used in a cluster.

A switched virtualization layer including a plurality of virtual interfaces VIFs is provided to interface between the respective network modules . . and the client systems . .N allowing storage . . associated with the nodes . . to be presented to the client systems . .N as a single shared storage pool.

The clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers or storage virtual machines SVM in which each SVM represents a single storage system namespace with separate network access. Each SVM has a client domain and a security domain that are separate from the client and security domains of other SVMs. Moreover each SVM is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more SVMs. Client systems can access the data on a SVM from any node of the clustered system through the VIFs associated with that SVM. It is noteworthy that the aspects described herein are not limited to the use of SVMs.

Each of the nodes . . is defined as a computing system to provide application services to one or more of the client systems . .N. The nodes . . are interconnected by the switching fabric which for example may be embodied as a Gigabit Ethernet switch or any other type of switching connecting device.

Although depicts an equal number i.e. of the network modules . . the storage modules . . and the management modules . . any other suitable number of network modules storage modules and management modules may be provided. There may also be different numbers of network modules storage modules and or management modules within the clustered storage system . For example in alternative aspects the clustered storage system may include a plurality of network modules and a plurality of storage modules interconnected in a configuration that does not reflect a one to one correspondence between the network modules and storage modules.

Each client system . .N may request the services of one of the respective nodes . . . and that node may return the results of the services requested by the client system by exchanging packets over the computer network which may be wire based optical fiber wireless or any other suitable combination thereof.

Performance manager interfaces with the various nodes and obtains QOS data for QOS data structure . Details regarding the various modules of performance manager are now described with respect to .

Performance manager collects a certain amount of data for example data for 3 hours or data samples of workload activity. After collecting the QOS data performance manager determines the headroom for a resource as described below in detail. Performance manager uses the headroom to represent available resource capacity at any given time.

Performance includes a current headroom coordinator that includes a plurality of sub modules including a filtering module an optimal point module and an analysis module . The filtering module filters collected QOS data shown as incoming data and provides the filtered data to the optimal point module . The optimal point module then determines an optimal point for a LvU curve. In one aspect the optimal point module determines the optimal point using a plurality of techniques and the technique that provides the most reliable value i.e. with the highest confidence level is selected.

The optimal point with the LvU curve is provided to the analysis module that uses the curve and determines the headroom based on one or more operational points . The headroom information may be stored in a headroom data structure A. Details of using the filtering module optimal point module and the analysis module are provided below.

In one aspect the current headroom coordinator and its components may be implemented as a processor executable application programming interface API which provides a set of routines protocols and tools for building a processor executable software application that can be executed by a computing device. When the current headroom coordinator is implemented as API then it provides software components in terms of its operations inputs outputs and underlying types. The API may be implemented as a plug in API which integrates headroom computation and analysis with other management applications.

When the current headroom coordinator is implemented as an API then various inputs may be provided for determining headroom. For example inputs may include a resource identifier that identifies a resource whose performance capacity is to be computed. The outputs may include headroom values a confidence factor and a time range for which the headroom is computed and other information.

Referring now to System A shows two clusters A and B both similar to cluster described above. Each cluster includes the QOS module for implementing QOS policies and appropriate counters for collecting information regarding various resources. Cluster A may be accessible to clients . and . while cluster B is accessible to clients . .. Both clusters have access to storage subsystems and storage devices . .N.

Clusters A and B communicate with collection module . The collection module may be a standalone computing device or integrated with performance manager . The aspects described herein are not limited to any particular configuration of collection module and performance manager .

Collection module includes one or more acquisition modules for collecting QOS data from the clusters. The data is pre processed by the pre processing module and stored as pre processed QOS data at a storage device not shown . Pre processing module formats the collected QOS data for the performance manager . Pre processed QOS data is provided to a collection module interface of the performance manager via the performance manager interface . QOS data received from collection module is stored as QOS data structure shown as incoming data and used by the filtering module before the data is used for computing the optimal point .

In one aspect the performance manager includes a GUI . Client may access headroom analysis results using GUI . Before describing the various processes involving performance manager and its components the following provides an overview of QOS in general as used by the various aspects of the present disclosure.

As shown in the network module of a cluster includes a network interface A for receiving requests from clients. Network module executes a NFS module C for handling NFS requests a CIFS module D for handling CIFS requests a SCSI module E for handling iSCSI requests and an others module F for handling other requests. A node interface G is used to communicate with QOS module storage module and or another network module . QOS management interface B is used to provide QOS data from the cluster to collection module for pre processing data.

QOS module includes a QOS controller A a QOS request classifier B and QOS policy data structure or Policy Group 111. The QOS policy data structure stores policy level details for implementing QOS for clients and storage volumes. The policy determines what latency and throughput rate is permitted for a client as well as for specific storage volumes. The policy determines how I O requests are processed for different volumes and clients.

The storage module executes a file system A a part of storage operating system described below and includes a storage layer B to interface with storage device .

NVRAM C of the storage module may be used as a cache for responding to I O requests. In one aspect for executing a write request the write data associated with the write request is first stored at a memory buffer of the storage module . The storage module acknowledges that the write request is completed after it is stored at the memory buffer. The data is then moved from the memory buffer to the NVRAM C and then flushed to the storage device referred to as consistency point CP .

An I O request arrives at network module from a client or from an internal process directly to file system A. Internal process in this context may include a de duplication module a replication engine module or any other entity that needs to perform a read and or write operation at the storage device . The request is sent to the QOS request classifier B to associate the request with a particular workload. The classifier B evaluates a request s attributes and looks for matches within QOS policy data structure . The request is assigned to a particular workload when there is a match. If there is no match then a default workload may be assigned.

Once the request is classified for a workload then the request processing can be controlled. QOS controller A determines if a rate limit i.e. a throughput rate for the request has been reached. If yes then the request is queued for later processing. If not then the request is sent to file system A for further processing with a completion deadline. The completion deadline is tagged with a message for the request.

File system A determines how queued requests should be processed based on completion deadlines. The last stage of QOS control for processing the request occurs at the physical storage device level. This could be based on latency with respect to storage device or overall node capacity utilization as described below in detail.

Various resources are used to process I O requests. As an example there are may be two types of resources a service center and a delay center resource. The service center is a resource category that can be represented by a queue with a wait time and a service time for example a processor that processes a request out of a queue . The delay center may be a logical representation for a control point where a request stalls waiting for a certain event to occur and hence the delay center represents the delay in request processing. The delay center may be represented by a queue that does not include service time and instead only represents wait time. The distinction between the two resource types is that for a service center the QOS data includes a number of visits wait time per visit and service time per visit for incident detection and analysis. For the delay center only the number of visits and the wait time per visit at the delay center are used as described below in detail.

Performance manager uses different flow types for its analysis. A flow type is a logical view for modeling request processing from a particular viewpoint. The flow types include two categories latency and utilization. A latency flow type is used for analyzing how long operations take at the service and delay centers. The latency flow type is used to identify a workload whose latency has increased beyond a certain level. A typical latency flow may involve writing data to a storage device based on a client request and there is latency involved in writing the data at the storage device. The utilization flow type is used to understand resource consumption of workloads and may be used to identify resource contention.

Referring now to delay center network is a resource queue that is used to track wait time due to external networks. Storage operating system often makes calls to external entities to wait on something before a request can proceed. Delay center tracks this wait time using a counter not shown .

Network module delay center is another resource queue where I O requests wait for protocol processing by a network module processor. This delay center is used to track the utilization capacity of the network module . Overutilization of this resource may cause latency as described below in detail.

NV RAM transfer delay center is used to track how the non volatile memory may be used by cluster nodes to store write data before the data is written to storage devices in one aspect as described below in detail.

A storage aggregate or aggregate is a resource that may include more than one storage device for reading and writing information. Aggregate is tracked to determine if the aggregate is fragmented and or over utilized as described below in detail.

Storage device delay center may be used to track the utilization of storage devices . In one aspect storage device utilization is based on how busy a storage device may be in responding to I O requests.

In one aspect storage module delay center is used for tracking node utilization. Delay center is tracked to monitor the idle time for a CPU used by the storage module the ratio of sequential and parallel operations executed by the CPU and a ratio of write duration and flushing duration for using NVRAM C or an NVRAM at the storage module not shown .

Nodes within a cluster communicate with each other. These may cause delays in processing I O requests. The cluster interconnect delay center is used to track the wait time for transfers using the cluster interconnect system. As an example a single queue maybe used to track delays due to cluster interconnects.

There may also be delay centers due to certain internal processes of storage operating system and various queues may be used to track those delays. For example a queue may be used to track the wait for I O requests that may be blocked for file system reasons. Another queue Delay Center Susp CP may be used to represent the wait time for Consistency Point CP related to the file system A. During a CP write requests are written in bulk at storage devices and this will typically cause other write requests to be blocked so that certain buffers are cleared.

A request may have a plurality of attributes for example a source a path a destination and I O properties. The source identifies the source from where a request originates for example an internal process a host or client address a user application and others.

The path defines the entry path into the storage system. For example a path may be a logical interface LIF or a protocol such as NFS CIFS iSCSI and Fibre Channel protocol. A destination is the target of a request for example storage volumes LUNs data containers and others. I O properties include operation type i.e. read write other request size and any other property.

In one aspect streams may be grouped together based on client needs. For example if a group of clients make up a department on two different subnets then two different streams with the source restrictions can be defined and grouped within the same workload. Furthermore requests that fall into a workload are tracked together by performance for efficiency. Any requests that don t match a user or system defined workload may be assigned to a default workload.

In one aspect workload streams may be defined based on the I O attributes. The attributes may be defined by clients. Based on the stream definition performance manager tracks workloads as described below.

Referring back to a workload uses one or more resources for processing I O requests shown as A N as part of a resource object . The resources include service centers and delay centers that have been described above with respect to . For each resource a counter queue is maintained for tracking different statistics or QOS data . For example a response time and a number of visits a service time for service centers a wait time and inter arrival time are tracked. Inter arrival time is used to track when an I O request for reading or writing data is received at a resource. The term QOS data as used throughout this specification includes one or more of and according to one aspect.

Performance manager may use a plurality of counter objects for resource monitoring and headroom analysis according to one aspect. Without limiting the various adaptive aspects an example of the various counter objects are shown and described in Table I below 

Without limiting the various aspects of the present disclosure Table II below provides an example of the details associated with the object counters that are monitored by the performance manager according to one aspect 

Format maybe hierarchical in nature where various objects may have parent child peer and remote peer relationships as described below. As an example format shows a cluster object that may be categorized as a root object type for tracking cluster level resources. The cluster object is associated with various child objects for example a node object QOS network object a portset object a SVM object and a policy group . The cluster object stores information regarding the cluster for example the number of nodes it may have information identifying the nodes and any other information.

The QOS network object is used to monitor network resources for example network switches and associated bandwidth used by a clustered storage system.

The cluster node object stores information regarding a node for example a node identifier and other information. Each cluster node object is associated with a pluralities of child objects for example a cache object a QOS object for a storage module a QOS object for a network module a CPU object and an aggregate object . The cache object is used to track utilization latency of a cache for example NVRAM C . The QOS storage module tracks the QOS of a storage module defined by a QOS policy data structure described above in detail with respect to . The QOS network module object tracks the QOS for a network module. The CPU object is used to track CPU performance and utilization of a node.

The aggregate object tracks the utilization latency of a storage aggregate that is managed by a cluster node. The aggregate object may have various child objects for example a flash pool object that tracks usage of a plurality of flash based storage devices shown as flash pool . The flash pool object may have a SSD disk object that tracks the actual usage of specific SSD based storage devices. The RAID group is used to track the usage of storage devices configured as RAID devices. The RAID object includes a storage device object shown as a HDD hard disk drive that tracks the actual utilization of the storage devices.

Each cluster is provided a portset having a plurality of ports that may be used to access cluster resources. A port includes logic and circuitry for processing information that is used for communication between different resources of the storage system. The portset object tracks the various members of the portset using a port object and a LIF object . The LIF object includes a logical interface for example an IP address while the port object includes a port identifier for a port for example a world wide port number WWPN . It is noteworthy that the port object is also a child object of node that may use a port for network communication with clients.

A cluster may present one or more SVMs to client systems. The SVMs are tracked by the SVM object which is a child object of cluster . Each cluster is also associated with a policy group that is tracked by a policy group object . The policy group is associated with SVM object as well as storage volumes and LUNs. The storage volume is tracked by a volume object and the LUN is tracked by a LUN object . The volume object includes an identifier identifying a volume size of the volume clients associated with the volume volume type i.e. flexible or fixed size and other information. The LUN object includes information that identifies the LUN LUNID size of the LUN LUN type read write or read and write and other information.

Aggregate utilization is tracked using counter A that tracks the duration of how busy a device may be for processing user requests. An aggregate latency counter B tracks the latency due to the storage devices within an aggregate. The latency may be based on a measured delay for each storage device in an aggregate. The use of these counters for headroom analysis is described below in detail.

The process begins in block B when the storage system is operational and data has been stored at the storage devices. In block B performance data for example latency and utilization data inter arrival times and or service times for at least the cluster nodes and aggregates has been collected. The collected data is provided to the performance manager . In one aspect current and historical QOS data may both be accessed by the performance manager for headroom analysis. The performance manager also obtains information regarding any events that may have occurred at the storage system level associated with the QOS data. Any policy information that is associated with the resource for which the QOS data is also obtained by the performance manager .

In block B the filtering module filters the collected data. In one aspect potential erroneous observations such as unreasonable large latency values variances service times or utilizations are identified. If there is any data associated with unusual events like hardware failure or network failure that may affect performance may be discarded. For example if a flash memory card used by a node fails and has to be replaced then the latency for processing I O requests with the failed card may be unreasonably high and hence data associated with that node may not be reliable for headroom computations. Any outliers in the collected and historical QOS data may also be removed for example the top 5 10 and the bottom 5 10 of the latency and utilization values may be discarded .

In one aspect filtering module may also insert missing data according to one aspect. For example service times for different resources are expected to be within a range based on collected historical service time data. If the collected data have a high coefficient of variation then the collected data may not be reliable and hence may have to be corrected.

After the data is filtered in block B one or more LvU curves are generated and an optimal point is determined by the optimal point module . In one aspect as an example different techniques for example model based observation based or any other techniques are used to generate the LvU curves and compute the optimal point. The technique that provides the most reliable optimal point is used for headroom analysis.

The model based technique uses current observations and queueing models to generate the LvU curve. The model based technique uses inter arrival times and service times for a resource. The inter arrival times track the arrival times for I O requests at a resource while the service times track the duration for servicing user based I O requests. The observation based technique uses both current and historical observations of latency and utilizations for generating LvU curves. Details regarding the various optimal point techniques are provided below with respect to . It is noteworthy that the various adaptive aspects of the present disclosure are not limited to any specific technique.

In block B the optimal point with the highest confidence level i.e. the most reliable optimal point value is selected and provided to the analysis module in block B. In another aspect the optimal point may be based on a policy based input. shows an example of a LvU curve which uses a SLO input for example from a policy . The SLO input defines a latency limit that is assigned for user resource. The custom optimal point is determined by the intersection of the SLO input and the LvU curve shown as .

In block B the analysis module determines the headroom for example using the optimal point and an operational point. In one aspect different operational points may be used for a resource based on the operating environment and how the resources are being used. For example a current total utilization may be used as an operational point with the presumption that the current total utilization may be used to process a workload mix. As described above a workload mix represents all user workloads utilizing one or more resources. This provides a sampled headroom for a resource.

In another aspect a custom operational point may be used when a volume is identified in a policy. In another aspect the analysis module may ascertain the effect of moving workloads which may affect utilization and the operational point. In yet another aspect the utilization of a node pair that are configured as high availability HA pair nodes is considered for the operational point. When nodes operate as HA pair nodes and if one of the nodes becomes unavailable then the other node takes over workload processing. In this instance latency utilization of both the nodes is used for determining the operational point and computing the headroom. This headroom analysis is referred to as the actual headroom.

The actual headroom is shown by the curve . The minimum actual headroom is shown . The minimum headroom is determined by evaluating internal workloads HA node workload and critical workload . In one aspect as described above the operational points for internal workloads HA node workloads and critical workloads are determined and then used to determine the actual headroom.

Referring back to in block B the plurality of headroom values are stored at data structure A and may also be presented to the user. Headroom information stored at headroom data structure A may include the following fields that are described in Table III below 

In one aspect headroom data structure A may be used for future analysis and historical comparison. Data structure A may also be made available to APIs that are used by third party or client systems that are monitoring a resource using a management application for example .

The process of provides a method for filtering performance data associated with a resource used in a networked storage environment for reading and writing data at a storage device and then determining available performance capacity of the resource using the filtered performance data. The available performance capacity is based on optimum utilization of the resource and actual utilization of the resource where utilization of the resource is an indicator of an extent the resource is being used at any given time the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency and latency is an indicator of delay at the resource in processing the workload.

In block B the optimal point module uses the GI G N where N is the number of cores that act as servers in the queuing model model for node resources for example a multi core CPU of a node.

In block B SSD and hard drive aggregates are queued under GI G 1 model by the optimal point module because in an aggregate the I O requests are expected to be uniformly served by all the storage devices in the aggregate and hence GI G 1 is an accurate representation.

When an aggregate is a hybrid aggregate i.e. includes both SSD and hard drives then a GI G 1 queueing model under the shortest job first SJF scheduling policy is used in block B by the optimal point module . The reason for using this model is because in hybrid aggregates the service time may be variable since some I O requests are served at a faster rate while others at a lower rate.

In block B the estimated latency is determined by the optimal point module for each resource using the inter arrival time and service time. The latency may be expressed as T. In one aspect Kingman s formula for GI G 1 queues maybe used to estimate Tof the resource based on the equation provided below 

It is noteworthy that Pis an estimation of the effective utilization in the system with n servers. The queuing system is not considered busy until all servers are busy. This is captured by expressing business as a function of the number of servers being utilized one component in Pfor each possible busy servers .

At each node resource there may be three traffic types high priority low priority and CP operations. The accuracy of the models depends how these three traffic types are interleaved to generate the final queuing model. If we assume the traffic at the storage module is managed according to priority levels of these types the latency of high priority traffic may be determined by 1 

In another aspect all types of traffic maybe combined into a single stream without any batching or priority assumptions. In that case the resulting variance when three types of traffic is mixed is given by 

Once the latency is determined by the optimal point module using the foregoing models in block B the analysis model generates the LvU curves and determines the optimal point and the confidence factor associated with the optimal point. In one aspect the confidence factor may be 10 15 of the determined optimal point.

The optimal point may also be determined based on policy settings such as SLO limits or by identifying the point of diminishing returns in the LvU curve such that increase in utilization is smaller than increase in latency.

In one aspect the model based technique described above with respect to has various advantages. The model based technique avoids the need for computational intensive curve extrapolation techniques or complex methodologies. The model based technique provides a fast and efficient way to estimate headroom.

In one aspect shows an example of a method for managing resources in a networked storage environment are provided. The method executed by the optimal point module includes using a queuing model for a resource that processes a plurality of requests at a networked storage environment for predicting a relationship between latency and utilization of the resource. The queueing model uses inter arrival time and service time to determine latency where inter arrival time is a duration that tracks when requests arrive at the resource and the service time tracks a duration for servicing the requests by the resource. The method further includes identifying optimum utilization of the resource using the predicted relationship between latency and utilization where the optimum utilization is an indicator of resource utilization beyond which throughput gains for a workload is smaller than increase in latency and determining available performance capacity for the resource using the optimum utilization and actual utilization of the resource.

Because performance capabilities of a resource are identified via observations it is desirable to identify the proper resources and processes. In block B the proper resource is identified. For example a storage and network node may be identified as the resource for monitoring. Various counters may be used to track the performance of each node as described above with respect to . Aggregates with storage devices may also be identified for monitoring. In one aspect nodes and aggregates are used by both user and storage system tasks to service I O requests.

In B latency and utilization data is collected by the performance manager for the resources identified in block B. In one aspect latency and utilization data is collected for each monitored node and aggregate. As described above counter data for counters A B A and B are collected. Counter A data is tracked by each node. In one aspect counter A may track the time a processor node is idle which indicates how busy the processor may have been over a given duration. Latency counter B collects latency data for both the storage and network modules. In one aspect the latency may be based on a total number of visits at each node number of operations per second processed by each node. This value may not include internal or system default workloads.

Aggregate utilization is tracked using counter A that tracks the duration of how busy a device may be for processing user requests. The aggregate latency counter B tracks the latency of the storage devices within an aggregate. The latency tracks the delay at each storage device. In one aspect latency at hard drives is higher that the latency at solid state storage devices.

In block B the collected data for the workload is pre processed and filtered by the optimal point module using a workload mix signature. The received data is pre processed for enhancing the accuracy and smoothness of the LvU curve.

A LvU curve captures the trend of how a resource sustains the demand of a workload mix. If the workload mix changes over time then the resulting curve may be distorted. In one aspect service time of a current workload may be used to search for stored historical latency and utilization data. The historical data for the same service time is used to augment collected data in block B. It is noteworthy that other parameters for example read write ratio and others may be used to filter the data.

In yet another aspect collected data may be filtered based on time using the assumption that in the short term the workload mix will stay the same. This means that the observations in the immediate past are more likely to have a similar workload mix and can be used to generate a curve. In one aspect for different measured latencies the optimal point module estimates a utilization latency value by removing observations that may be at a higher and lower end. For different latencies measured for the same utilization mean estimators may be used to reduce the impact of outliers.

In block B when there are missing values in a range of collected data then the missing values are interpolated between two observed utilizations by the optimal point module . One way to interpolate the data is by using historical data for similar workload mix.

In block B the optimal point module extrapolates incomplete LvU curve. The curve is extrapolated when after removing outlier values and using historical data to interpolate missing values the process still generates an incomplete curve. In such an instance the incomplete curve may be extrapolated. Different techniques may be used to extrapolate the latency v. utilization curves. For example linear extrapolation Newton Gauss geometric parametric fit and other techniques.

In block B the optimal points as described above with respect to are determined by the optimal point module . A confidence factor for each calculated optimal point is computed. The confidence factor may be based on the quality of the curve generated from observations from a single workload mix range of observed utilizations in the available data and the distance between the largest utilization value and the optimal point utilization value. The confidence factor may be computed by determining a mean distance of the observations from the fitted curve the range of utilizations such that the smaller the range higher the confidence factor or bound and farther the optimal point from the maximum utilization the wider the confidence bound. The confidence bounds are a prediction strength that quantifies the confidence in the estimated value. Prediction strength is an inverse of the width of the confidence bounds.

The process begins in block B. In block B the analysis module identifies the workload or workload set that need to be considered for an internal workload that can be throttled or delayed or are for an HA pair jointly referred to as workload signature . This information again is obtained from the various counters that are maintained by the storage operating system . The service time for the workload mix is computed and maybe referred to as workload mix signature.

In block B historical service times for the monitored resources are searched to determine if the workload mix signature is within a certain percentage X for example within 10 . This is performed by the analysis module .

In block BA if the service time of the workload mix is within a certain percentage X for example 10 of the service time of the resource for which latency utilization data has been collected then the operational point may be modified by adding or reducing the utilization value of the resource.

If the service time is beyond X then a new optimal point maybe calculated based on a modified workload mix in block BB. The modified workload mix i.e. a new actual workload mix is based on the service time of the workload mix from block B with portions of the workload that is added or removed. Historical service time values are again searched for observations to modify the workload mix. The actual headroom is the difference between the new actual optimal point and the new operational point as shown in and described above.

In one aspect the analysis module validates the operational point values and their significance. For example the analysis module validates the operational point based on neighboring values removes outliers and marks any events that may affect the validity of the operational points.

In one aspect analysis module looks at back to back consistency points for adjusting workload mix. Typically CP operations are conducted in the background and are given lower priority but if the CP becomes a high priority then the optimal point is calculated by looking at the CP traffic.

In another aspect analysis module evaluates single threaded behavior where the workloads access very few volumes. As a result the LvU curves are distorted because high latencies may be observed across multiple node processors. In such a case headroom values may be invalidated.

In one aspect the observation based technique is based on selection of observations interpolation between the observations and extrapolation beyond what is observed for a resource. The observation based techniques has various advantages for example using historical data with current data provides a smooth LvU curve. The optimal points using workload signature mirrors real operating environments and provides an effective headroom value.

In one aspect the LvU curve may be a pre measured curve called a seed curve that is constructed in a laboratory environment. Seed curves may be stored at a data structure by the performance manager . Seed curves may be used when there are not enough observations to generate a curve. In one aspect workload characteristics and the resources are matched with the resources and the workloads that were used to generate the seed curve. The prediction strength of the seed curve would depend on how well the resources workloads match the workloads and resources used in the laboratory setting.

In one aspect the foregoing systems and techniques provide a mechanism to determine a resource s available capacity at any given time. This allows a user to optimize resource utilization and also enables the storage system provider to meet contractual SLOs.

In one aspect headroom is an efficient metric to determine performance capacity of a resource. The metric can be efficiently used in systems where a plurality of resources serve complex workloads for storing data.

Processors A B may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such hardware devices. Idle time for processors A A is tracked by counters A described above in detail.

The local storage comprises one or more storage devices utilized by the node to locally store configuration information for example in a configuration data structure . The configuration information may include information regarding storage volumes and the QOS associated with each storage volume.

The cluster access adapter comprises a plurality of ports adapted to couple node . to other nodes of cluster . In the illustrative aspect Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate aspects where the network modules and storage modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the network storage module for communicating with other network storage modules in the cluster .

Each node . is illustratively embodied as a dual processor storage system executing a storage operating system similar to that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories and files at storage .. However it will be apparent to those of ordinary skill in the art that the node . may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the network module on the node while the other processor B executes the functions of the storage module.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the disclosure described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node . by inter alia invoking storage operation in support of the storage service implemented by the node.

In one aspect data that needs to be written is first stored at a buffer location of memory . Once the buffer is written the storage operating system acknowledges the write request. The written data is moved to NVRAM storage and then stored persistently.

The network adapter comprises a plurality of ports adapted to couple the node . to one or more clients . .N over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Each client . .N may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node . to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored at storage device .. The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the storage devices over an I O interconnect arrangement such as a conventional high performance Fibre Channel link topology.

In one example storage operating system may include several modules or layers executed by one or both of network module and storage module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operation i.e. executes read write operation on storage in response to client . .N requests.

Storage operating system may also include a protocol layer and an associated network access layer to allow node . to communicate over a network with other systems such as clients . .N. Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices . . or are illustrated schematically as a path which illustrates the flow of data through storage operating system .

The storage operating system may also include a storage access layer and an associated storage driver layer to allow storage module to communicate with a storage device. The storage access layer may implement a higher level storage protocol such as RAID redundant array of inexpensive disks while the storage driver layer may implement a lower level storage device access protocol such as Fibre Channel or SCSI. The storage driver layer may maintain various data structures not shown for storing information regarding storage volume aggregate and various storage devices.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node . implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the disclosure described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a storage device directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present disclosure may be utilized with any suitable file system including a write in place file system.

The processing system includes one or more processor s and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processor s are the central processing units CPUs of the processing system and thus control its overall operation. In certain aspects the processors accomplish this by executing software stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions implement the process steps of described above may reside in and executed by processors from memory .

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a Fibre Channel adapter or the like.

The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

The system and techniques described above are applicable and especially useful in the cloud computing environment where storage is presented and shared across different platforms. Cloud computing means computing capability that provides an abstraction between the computing resource and its underlying technical architecture e.g. servers storage networks enabling convenient on demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. The term cloud is intended to refer to a network for example the Internet and cloud computing allows shared resources for example software and information to be available on demand like a public utility.

Typical cloud computing providers deliver common business applications online which are accessed from another web service or software like a web browser while the software and data are stored remotely on servers. The cloud computing architecture uses a layered approach for providing application services. A first layer is an application layer that is executed at client computers. In this example the application allows a client to access storage via a cloud.

After the application layer is a cloud platform and cloud infrastructure followed by a server layer that includes hardware and computer software designed for cloud specific services. The storage systems performance manager described above can be a part of the server layer for providing storage services. Details regarding these layers are not germane to the inventive aspects.

Thus methods and apparatus for managing resources in a storage environment have been described. Note that references throughout this specification to one aspect or an aspect mean that a particular feature structure or characteristic described in connection with the aspect is included in at least one aspect of the present disclosure. Therefore it is emphasized and should be appreciated that two or more references to an aspect or one aspect or an alternative aspect in various portions of this specification are not necessarily all referring to the same aspect. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more aspects of the disclosure as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred aspects it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

