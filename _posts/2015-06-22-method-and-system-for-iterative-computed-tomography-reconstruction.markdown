---

title: Method and system for iterative computed tomography reconstruction
abstract: Methods and systems for iterative computed tomography add an auxiliary variable to the reconstruction process, while retaining all variables in the original formulation, A weighting operator or filter can be applied that causes the Hessian with respect to an image of the cost function to be well-conditioned. An auxiliary sinogram variable distinct from both a set of actual image measurements and from the set of projections computed based on an image can be applied to iteratively update during the statistical iterative image reconstruction, with applied conditions that causes the Hessian with respect to an image of the cost function to be well-conditioned.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09524567&OS=09524567&RS=09524567
owner: InstaRecon
number: 09524567
owner_city: Urbana
owner_country: US
publication_date: 20150622
---
The application claims priority under 35 U.S.C. 119 from prior provisional application Ser. No. 62 015 446 which was filed Jun. 22 2014.

This invention was made with government support under R44EB014669 awarded by National Institutes of Health National Center for Advancing Translational Sciences NCATS and National Institute of Biomedical Imaging and Bioengineering NIBIB . The government has certain rights in the invention.

A field of the invention is computed tomography. The invention is applicable to various computed tomography techniques including for example X ray Computed Tomography CT single photon emission computed tomography SPECT and positron emission tomography PET 

Computed tomography techniques including X ray Computed Tomography CT single photon emission computed tomography SPECT positron emission tomography PET are well established imaging modalities. Conventional image reconstruction in such imaging modalities and others has been performed using a method known as filtered backprojection MP . More recently iterative image reconstruction methods have been introduced with the main motivation being x ray dose reduction. One goal of the iterative techniques is to lower the dose of radiation experience by a subject being imaged. The lower doses and under sampling when used e.g. to reduce dose provide challenges to compute high contrast and clear images. Another goal of iterative techniques is to provide high quality reconstruction from data acquired by advanced detector systems such as photon counting detectors for which FBP may not provide the desired quality. Recent iterative techniques utilized an image fidelity function to encourage fidelity to the measured data and an edge preserving regularization function such as total variation or qGGIVERF that encourages smoothness in image areas with little variation while recovering sharp boundaries to preserver image resolution.

DeMan et al. U.S. Pat. No. 8 971 599 discloses methods for iterative tomographic reconstruction Three embodiments are discussed that use a ramp filter as a preconditioner. in a first embodiment the iteration applies a deconvolution filter and reevaluates to see if completion criteria have been satisfied. This first embodiment uses a change of variables that is replacing all instances of the image variable x with another variable z using the equivalence x Fz for a filter F . The approach of the first embodiment is highly similar to the classical formulation. of preconditioning. In a second embodiment a preconditioner is applied to the simultaneous image update steps of an iterative reconstruction where the preconditioner approximates the inverse Hessian of the cost function being optimized. In a variation the preconditioner is applied only to the data fidelity term called data fit term in the 599 patent and not to the regularization term. This strategy is intended to accommodate edge preserving priors which are commonly used in current iterative reconstruction methods. However by applying the preconditioner in this way it ceases to be a proper preconditioner. Instead this approach fundamentally alters the minimization process such that the fixed point of the iteration no longer corresponds to the fixed point of the original update sequence. One drawback of the techniques in the 599 patent arises in the common case that statistical weighting matrices are used in the update steps. These weighting matrices degrade the approximation of the ramp filter preconditioner as an inverse Hessian operation and therefore degrade the preconditioning effect. In particular the presence of the weighting matrix W which often has large dynamic range and therefore causes the Hesssian of the data fidelity term in the cost function to be poorly conditioned has a detrimental effect on the rate of convergence of the iterative algorithm ultimately requiring more iterations to achieve a desired accuracy.

Embodiments of the invention include methods and systems for iterative computed tomography. In preferred methods and systems of the invention an auxiliary variable is added to the reconstruction process while retaining all variables in the original formulation. Preferred embodiments apply a weighting operator or filter that causes the Hessian with respect to image of the cost function to be well conditioned. In preferred embodiments an auxiliary sinogram variable distinct from both a set of actual image measurements and from a set of projections computed based on the image is applied to iteratively update during the statistical iterative image reconstruction with applied conditions that causes the Hessian with respect to image of the cost function to be well conditioned 

Unlike the techniques in the 599 patent discussed in the background preferred methods of the invention do not degrade the approximation of the ramp filter as an inverse Hessian operation. Preferred methods instead relate the image to an auxiliary variable without statistical weighting but using a ramp filter or other norm weighting while the statistical weighting can remain in a different term relating the measured projection data derived from the CT scan data to the auxiliary variable without using the ramp filter. More generally preferred embodiments apply a weighting operator or filter that causes the Hessian with respect to image of the cost function to be well conditioned. The auxiliary variable is added to the reconstruction process while retaining all variables in the original formulation. The preferred iterative reconstruction methods use both the image variable and the auxiliary variable and specifically updates to the image variable depend on the auxiliary variable but not on the measured projection data and the updates to the image variable do not need to include the statistical weighting. Also unlike the variations of the 599 patent that alter the minimization process such that the fixed point of the iteration no longer corresponds to the fixed point of the original update sequence methods of the invention maintain mathematical equivalence with the solution of the original update process.

Preferred methods and systems provide for iterative reconstruction tomography images including for example CT images SPECT and PET images. Preferred embodiments can reduce the computation needed to produce an image of desired quality using an iterative algorithm. This can be used to obtain results more quickly using the same computing hardware or to reduce the cost and or the size and weight and or the power consumption of the computing hardware needed to produce the results in a desired time. Because iterative reconstruction can be used to reduce the x ray dose to which the subject is exposed during a CT scan and or improve image quality and reduce artifacts e.g. due to metal implants the preferred methods and systems can be used to provide the same benefits at a lower cost and or at a higher patient throughput and smaller delay for reconstruction compared to traditional CT methods and systems.

Those knowledgeable in the art will appreciate that embodiments of the present invention lend themselves well to practice in the form of computer program products. Accordingly it will be appreciated that embodiments of the present invention may comprise computer program products comprising computer executable instructions stored on a non transitory computer readable medium that when executed cause a computer to undertake methods according to the present invention or a computer configured to carry out such methods. The executable instructions may comprise computer program language instructions that have been compiled into a machine readable format. The non transitory computer readable medium may comprise by way of example a magnetic optical signal based and or circuitry medium useful for storing data. The instructions may be downloaded entirely or in part from a networked computer. Also it will be appreciated that the term computer as used herein is intended to broadly refer to any machine capable of reading and executing recorded instructions it will also be understood that results of methods of the present invention may be displayed on one or more monitors or displays e.g. as text graphics charts code etc. printed on suitable media stored in appropriate memory or storage etc.

Preferred embodiments of the invention will now be discussed with respect to the drawings. The drawings may include schematic representations which will be understood by artisans in view of the general knowledge in the art and the description that follows. Features may be exaggerated in the drawings for emphasis and features may not be to scale

A preferred iterative reconstruction module applies an iterative reconstruction with an auxiliary variable and a weighting operator or filter that causes the Hessian with respect to image of the cost function to be well conditioned. In preferred embodiments the weighting operator or filter is a ramp filter. Generally the reconstruction method of preferred embodiments is related to iterative reconstruction techniques that use the Augmented Lagrangian. Augmented Lagrangian methods of iterative reconstruction IR converge more quickly than techniques that directly minimize the objective function used in the iterative reconstruction in particular in the case of weighted or complicated discrepancy terms between the acquired sinogram and the sinogram computed based on the image.

Before discussing introductory concepts and the preferred the iterative reconstruction symbols used in the following description and the in claims will be defined. The following table defines symbols that are used.

Denote the reconstructed image by the image variable f. Image variable f can represent a 2D array of numbers in the case of 2D imaging e.g. cross sectional CT or a 3D array of numbers in the case of 3D imaging e.g. conebeam or helical conebeam CT. As known to artisans such arrays may be organized in various desired forms in computer memory. Because any data array of any dimension can be arranged into a 1D array or vector for the purposes of this disclosure it will be convenient to consider all data arrays regardless of dimension as vectors. It will also be noted as used in the disclosure and the claims that a vector approximately proportional to the gradient indicates that the vector is not perpendicular to the gradient not equal to 2 and preferably less than 2 .

A typical formulation for conventional iterative reconstruction involves minimization of a cost function to generate the reconstructed image f. This minimization problem and cost ftinction can be generally written as

where fis the final reconstructed image. The measured sinogram variable g usually corresponds to a 2D or 3D data array. For example it would correspond to a 2D data array in the case of 2D imaging or in the case of 3D helical imaging with a line detector when g is indexed by view angle and detector index. It would correspond to a 3D array for example in the case of 3D conebeam CT imaging with a multirow or area detector. In some applications such as dual or multi energy also called multispectral 3D CT the measured sinogram variable g may correspond to a 4 dimensional array. Again for the purposes of this disclosure we will consider g to be a vector.

The measured sinogram variable g is generated from CT data by conventional techniques such as those described by J. Hsieh Computed Tomography Principles Design Artifacts and Recent Advances Second Edition SPIE Wash. USA 2009 which may include negative logarithm and various corrections steps for deviations from ideal models. In the case of iterative reconstruction fewer corrections may be used in generating the measured sinogram data g and instead some of the non idealities and even the negative logarithm operation may be included if desired in the modeling incorporated into the iterative reconstruction as described below. The projection operator R Projector also known as the system matrix incorporates the model of the data acquisition process. For example it might account for the line integral measurement for the shape of the X ray beam and of the focal spot for the shape of the voxel or other basis function used to represent the image for the geometry of the CT scanner for the shape and properties of the detector etc. Given an image f the quantity Rf is the set of measurements also called sinogram or projections computed based on the image f. The function h Rf g is a measure of the discrepancy between the acquired projection data g and the computed projection data Rf generated by the current image estimate off using the projection operator R. Accordingly the first term in the cost function in 1 is sometimes called a data fidelity term. The adjoint Rof R is a backprojector operator. This backprojection operator may however be somewhat different from conventional approximate backprojection operators used in filtered back projection algorithms which may not have an exact adjoint relationship to a desired projector R.

Examples of h include a negative log likelihood term such as a Poisson likelihood a weighted sum of squared deviations or other nondecreasing and typically convex functional such as a weighted sum of generalized Huber functions of elements of the difference g Rf. With such or other choices of h it too can be used to express physical and or statistical modeling aspects of the CT data acquisition which may result in g not exactly corresponding to true projections or sinogram as described by J. Hsieh Computed Tomography Principles Design Artifacts and Recent Advances Second Edition SPIE Wash. USA 2009. The step of generating g from the CT data may be even reduced if desired to no operations i.e. using the actual CT data in unmodified form by appropriate choice of function h e.g incorporating desired transformations or corrections into h although it is usually computationally advantageous to perform at least some of these transformations and corrections in a separate step of generating g.

The f term in 1 is a regularization functional that represents prior information about the image f. Common choices for f encourage smoothness in the image by penalizing local differences that is differences between voxels in a small neighborhood about each voxel. Typically non quadratic penalties with notable examples including generalized Huber functions of the local differences the so called qGGMRF random field penalty total variation or other penalties that promote sparsity of the local differences or of some transform of the image are used to introduce an edge preserving property to the regularization or capture additional properties of the images of interest.

The iterative reconstruction algorithm involves updates to the algorithm variables to progressively perform the minimization in 1 . A stopping criterion is employed to terminate the iterative algorithm. The stopping criterion might include for example running for a fixed number of iterations examining the discrepancy e.g. some norm of the difference g Rf between the measured sinogram g and computed measurements projections Rf or detecting when the maximum absolute value of an element pixel or voxel as the case may be of an update of the image f is below some threshold.

As a specific example we consider the penalized weighted least squares PWLS formulation for h although the method described here is generally applicable to other functions. The PWLS cost function and associated optimization problem are

The weighted squared norm of a vector denotes the quantity for a positive definite symmetric operator or matrix where denotes the transpose of vector v or equivalently where denotes the Euclidean inner product between vectors and is the application of the linear operator to . Alternatively the square of the weighted norm can be implemented as where is a square root of operator and satisfies where Adenotes the adjoint of operator A. The notation for the norm simplifies to when the weight operator is the identity operator. Here too as in the rest of this disclosure vector v may represent a physical multidimensional array. In Equation 2 W is a diagonal matrix that encapsulates the statistical or other modeling information about the data g. For example the elements on the diagonal of W can be set to the inverse of an estimate of the variance of each measurement g as follows W Ie 3 

where Idenotes the number of incident photons when measured in an air scan with or without a bow tie filter as appropriate. Alternatively W can be set to some other weighting that represents the confidence about particular measurements. Another typical usage is to down weight rays passing through highly attenuating objects such as metal.

Direct minimization of the cost function in 1 can be challenging due to the large scale of the problem along with the presence of the weighting matrix W which often has large dynamic range and therefore causes the Hesssian of the data fidelity term in the cost function to be poorly conditioned or other aspects of h Rf g which make evaluation of it or of its gradients computationally demanding and or require many iterations to approach convergence. A variable splitting technique addresses these issues. The variable splitting technique converts the unconstrained minimization of 1 to the constrained minimization problem 

This is in turn converted back to an unconstrained minimization problem using the Augmented Lagrangian AL with completion of the square 

Here u is an auxiliary sinogram domain variable representing an approximation to the measurements Rf computed based on the image. Variable is another auxiliary sinogram variable serving as a Lagrange multiplier also known as dual variable . We refer to the first term in Equation 5 as the sinogram discrepancy term and to the third term as the image discrepancy term. Significantly the constant only affects the rate of convergence of the algorithm not the final resulting image at the point of complete convergence.

which is repeated for increasing values of n starting from some appropriate initial values at n 0 until some convergence criterion is met. Here symbols with superscript n denote the values of the corresponding variable in iteration number n. Typical choices for initial values for the variables are to set fas the FBP reconstruction of sinogram data g set u Rf and set 0.

Variations on the ALM include algorithms that perform the minimization in 6 approximately by block coordinate descent alternating between minimization with respect to f and u with the other variable held fixed J. Eckstein Augmented Lagrangian and Alternating Direction Methods for Convex Optimization A Tutorial and Some Illustrative Computational Results RUTCOR Research Report RRR 32 2012 December 2012 . Typically the most challenging minimization is the one with respect to f because of the presence of R.

The present inventors have recognized that the image discrepancy term i.e. the norm term in 5 can be replaced by a weighted two norm or even semi norm with a positive non negative definite operator which can provide advantages in implementation or in convergence rate. Depending on the choice of weighting operator existing theoretical convergence analyses for the resulting optimization algorithms may not exactly apply but empirical results can still be favorable. In preferred embodiments particular choices of weighting operators provide advantages. Advantageous choices for are those for which the Hessian L u f R R f of the AL with respect to f which is well defined when is twice differentiable or the Hessian R R with respect to f of the image discrepancy term last term of the AL are well conditioned e.g. approximately equal or approximately proportional to the identity operator . This will speed up convergence of the minimization with respect to f and therefore reduce the computation for the entire iterative algorithm.

Example advantageous choices of that improve the conditioning of the composite operator R R are described next. Consider data in the projection domain denoted by q t for 2D cross sectional imaging or by q t r for 3D imaging where t denotes the detector index along a detector row denotes view or x ray source angle and r denotes detector row index. Then one favorable choice for the operator is the operator which performs a 1 D filtration in the t variable along each row in the projection domain. The frequency response of the operator is the so called ramp filter response for frequency variable . Owing to the length of the impulse response of this filter in the spatial domain the operator is typically applied in the frequency domain using the FFT Fast Fourier Transform as is done with conventional filtered backprojection reconstruction. An alternative realization of the operator is to use lower order IIR Infinite Impulse Response or FIR Finite Impulse Response filters applied in the projection domain that approximate the response at lower computational cost.

This choice of weight operator in the AL produces the weighted AL L given in the following Equation 8 

This leads to the Hessian of the image discrepancy term the third term in the weighted AL 8 being equal to R R which is the identity operation for parallel beam geometries in the continuous variable formulation or an approximation of the identity operation in the discrete and divergent beam e.g. fan and cone with circular or helical geometries. Thus minimization of 8 with respect to f is approximately preconditioned by this construction.

Artisans will appreciate that the auxiliary sinogram variable u is distinc from both the measured sinogram g and from the measurements computed based on the image Rf.

The utility in this choice of weight can be demonstrated with the specific example of the PWLS Penalized Weighted Least Squares choice for h and a particular AL based scheme although the technique is applicable to other cost function formulations and other AL based schemes.

The Alternating Direction Method of Multipliers ADMM is a technique that can be interpreted as a particular variant of ALM involving alternating minimization which is used to make finding the solution more tractable. shows a preferred embodiment with the weighting applied to the image discrepancy term norm tem in Equation 5 that is with the weighted AL in 8 . Initial steps involve data acquisition and then the initial reconstruction of the image f. The initial reconstruction can be performed for example by a conventional FBP method. Each iteration of the preferred embodiment ADMM algorithm with the weighted AL involves three steps i update the image by minimizing the weighted AL L with respect to f to a desired degree of accuracy i.e. perform the minimization in Equation 9 to a desired degree of accuracy e.g using a nonlinear iterative conjugate gradient algorithm while keeping variables u and fixed at their values at the end of the previous iteration ii update the auxiliary sinogram variable u by minimizing the weighted AL L with respect to the auxiliary variable u to a desired degree of accuracy which in the case of the PWLS choice for h involves performing the minimization in Equation 10 to a desired degree of accuracy while keeping f and fixed and iii gradient update step of the Lagrangian Multiplier variable while keeping f and u fixed per Equation 11 . This sequence of steps is repeated until an appropriate convergence completion criterion is met. In variations of the embodiment the order of the update steps and is modified and one or more of the updates is performed more than once per iteration. Equations 9 11 respectively illustrate the updates of steps and in the case of the PWLS choice for h 

Here the second term in the cost functions of 9 and 10 is called the image discrepancy term and the first term in the cost function in 10 is called the sinogram discrepancy term.

The preferred updating with p weighting is labelled ADMM . The most challenging step from a computational standpoint is the image update 9 which itself must usually be solved by an iterative method. We call these iterations inner iterations and refer to the iterations for increasing n over the steps in 9 11 as outer iterations . Through the choice of a weighting the optimization problem has been implicitly preconditioned. For example approximating a solution to 9 using a conjugate gradients CG method involves calculating the gradient of the cost function in 9 with respect to f yielding 12 

which contains the term R R which as described before is a well conditioned matrix or operator approximating an identity operation. Because the convergence rate of CG is affected by the conditioning more specifically the clustering of eigenvalues of the Hessian of the cost function the convergence of the inner iterations for minimization of 9 will be accelerated by the weighting resulting in a speedup of the entire process.

Other minimization strategies can be employed in minimizing 9 . The ordered subsets OS method approximates the gradient calculation of 12 by using only a subset of views the subsets indexed by in m 1 . . . M 13 

The introduction of weighting increases somewhat the complexity of the minimization problem in 10 whose solution has the form 14 

The operation W is difficult to invert directly and similar to 9 an iterative process with a few inner iterations is also required using the gradient with respect to u of the cost function in 10 which is given by 15 

However unlike finding the solution of 9 the only operators that need to be applied in the iterative solver used to compute the update in 14 are the diagonal W and the weightings which are significantly cheaper computationally than the projector R or backprojector R especially if low order projection domain filter approximations to are used. Finallly the step in 11 is a simple update and requires negligible computation compared to the other steps.

Alternatively an additional variable split s u can be introduced to detangle the application of the W and weightings allowing for closed form updates of the auxiliary variables. The weighted AL formulation in this case is

Variable s is another singoram domain auxiliary variable which represents an approximation to the auxiliary sinogram variable u and which is distinct from all three quantities g u and Rf. The weighted AL with the indicated additional variable split in 16 is useful more generally when h s g is a relatively complicated function of s for example when h s g comprises a statistical model for g such as a negative log likelihood for a Poisson measurement model in SPECT or PET.

Again applying the ALM algorithm to the weighted augmented Lagrangian in 16 the updates for the different variables can be performed one at a time in sequence. For the ADMM applied to this AL the update sequence for one iteration consists of three minimization steps with respect to the variables f u and s respectively and two updates for the Lagrangian variables .

A similar additional variable split z f using an axillary image variable z that is distinct from f can be performed in the image domain to detangle the application of the R operator and the regularization calculations resulting in

Applying the ALM algorithm to the weighted augmented Lagrangian in 22 yields simpler updates than the original algorithm.

The stopping criteria for the iterative method can be modified to use the new variables. For example the sinogram variable u can be compared to the actual measurements g or to calculated measurements Rf. Or in the case of using the additional auxiliary sinogram variable s as in 16 it can be compared to g or u can be compared to Rf. The comparison between variables might involve using a desired function of the variables such as the function h or a norm or weighted norm. Additionally the amount of change in any of these variables in a given iteration can be calculated and compared to a threshold. For example the values of the sinogram discrepancy term and the image discrepancy term may be compared or one or both terms may be monitored for sufficiently small change from one iteration to another.

The ADMM algorithm is not the only method by which the AL can be minimized. One such alternative is the alternating minimization algorithm AMA . AMA minimizes the AL through a sequence of simpler update steps. AMA applied to the weighted AL in 8 yields the AMA procedure

Here the notation a b a b denotes the weighted inner product whereas a b ab denotes the regular Euclidean inner product. In 28 we refer to the inner product term as an image discrepancy term whereas in 29 the image discrepancy term is the third term. This sequence of steps has a simpler form in 28 for the update in the image variable f than in 9 due to the non quadratic regularizer the respective updates will require an iterative strategy but here the backprojector Rneed only be applied once during the minimization with respect to f because remains fixed whereas in 9 both R and Rare applied in every iteration in minimizing f. Updates to u and remain comparable to the ADMM algorithm.

The so called Linearized ADMM framework also known as the split inexact Uzawa method can also benefit from the weighting with . A preferred linearized ADMM L ADMM approach modifies the image update step in 9 by adding an additional penalty or so called inertia term. In the typical formulation of L ADMM without the weighting the image update is

where must be positive semi definite or positive definite. The choice of Q can make finding the solution to 31 much easier. In particular the main computational burden in minimizing in 31 is the application of the operator R and its adjoint R. The operator Q is chosen as

The coefficient is must satisfy 1 RR so that Q will be at least positive semi definite. Here the norm applied to the operator is the induced operator norm equal to the largest singular value in the case of a matrix. The minimization problem of 31 then becomes solving

A significant improvement is that the operators R and Ract only on the previous values of each variable and thus only need to be applied once each during the entire minimization in 33 to determine f. In contrast solving 9 via gradient methods requires application of each operator during each inner iteration of the minimization algorithm for solving 9 . A potential limitation of L ADMM is that typically the entire algorithm requires many more outer iterations for convergence. The objective when using L ADMM is that the increased number of outer ADMM iterations is offset by the improved speed of each outer iteration in particular solving the image minimization via 33 . The convergence of L ADMM is improved by picking as large as possible subject to the constraint with respect to the operator R mentioned previously.

Combining L ADMM with the weighting yields an algorithm with similar structure but significantly improved performance. The choice of operator Q becomes

The minimization problem of 31 is similarly modified by the inclusion of the weighting in both the second term and in Q yielding the image update equation as a solution to the minimization problem

This improvement concerns the choice of the parameter which now must satisfy 1 R R . The inclusion of the weighting allows for a larger because R R 

In this variation the weighting operator is augmented with operator F to form aggregate weighting operator E E. The operator E is applied on both sides of to maintain symmetry of the operator. The extended weighting operator E can be constructed to make the composite operator RE ER become closer to an identity operation i.e. improve the implicit preconditioning nature of the weighting.

For example E can be a rebinning operation converting from fan beam to parallel beam data. The use of the operator for parallel beam geometries makes R a more exact analytical inverse of R than when applied to fan beam geometries. Similarly in 3D the E operator can rehin from true cone beam data to cone parallel data parallel in plane but divergent beam along the axial direction . Applying the operator on this geometry is equivalent to filtering along slanted lines in the original cone beam domain which has been shown to be advantageous in FBP reconstructions. Alternatively E could simply rotate the projection data on the detector panel itself to implement filtering along slanted lines. The adjoint operation Erotates the projection data back to its original orientation. The operator E can also be adopted when using the square root of the ramp filter

In general the most effective choice of E in terms of the approximation of the identity by the composite operator RE ER would be to make the composite operator RE E approximate the inverse of R. To the extent that an FBP reconstruction algorithm provides a good approximation to the inverse of R a good choice for E would result in RE E behaving like the FBP reconstruction operator.

Alternatively the weighting operator can be chosen to correspond to filtering along slanted lines or curves in the original cone beam projection domain that is jointly in the variables t r or a subset thereof similar to the filtering used in reconstruction procedures for divergent beam CT geometries e.g. helical coneheam .

The update equations analogous to 9 and 10 replace the weighting by the extended weighting operator E E. A similar change applies to Equations 12 14 . As before the corresponding operation W is hard to invert directly and similar to the case of weighting the minimization with respect to u of the cost function corresponding to the one in 10 is performed by an iterative process with a few inner iterations using the gradient with respect to u which takes the form in 14 with replaced by E E. The same comments about applying W and at low computational cost apply except that the application of E E will now involve rebirming as well.

As before the need for an iterative algorithm to compute the update of u can be overcome by introducing another variable split. While the split u s used before can work in this situation as well the resulting update for u involves inverting I I E E which is no longer a simple shift invariant filter. Instead using the split u Es retains the linear shift invariant filtering form for the u update and the update for s involves instead inversion of W EE. For most rehinning operators E the operator EE will be very well conditioned and often structured facilitating inversion or iterative solution.

The extended weighting can be combined with the L ADMM similarly to the way the weighting was combined with the L ADMM obtaining the benefit of both features.

Another variation on the previous choices of weighting operator is directed explicitly to improve the conditioning of the Hessian with respect to f L u f R R f of the weighted AL. Typically with quadratic regularization f f where G is a weighting operator with a high pass filter characteristics. This is typically true to a good approximation even with edge preserving non quadratic regularization for small values of the image gradient. It follows that to make the Hessian L u f well conditioned or approximate the identity the weighting can be chosen so that R R has frequency response complementary to that of G. This can be achieved by replacing the RamLak ramp filter in E E by an appropriately apodized frequency weighted ramp filter with similar modifications when using the square root filter.

Artisans will appreciate that the composite operator R R is an operation that produces an output image from an input image. Hence this operation can be performed entirely in the image domain. This can lead to savings if the operation can be performed at lower cost than a projection R or backprojection R. Now with the weighting chosen to one of the forms described in this invention so that R R is well conditioned or an approximate identity R R can be approximated by a low order and inexpensive digital filtering operation in the image domain. This will further reduce the cost of the iterative algorithm.

In another preferred variation instead of incorporating the operator in the sinogram domain it can instead be introduced into the formulation using an image domain representation. Consider the minimization with respect to f of the augmented Lagrangian without the weighting in 5 . It involves the solution of the zero gradient condition expressed by the following equation n 0 41 

This equation for f is typically ill conditioned because of the ill conditioning of the Hessian RR of the image discrepancy term with respect to f. Denote the image domain representation or approximate representation of the operator as circumflex over . For example when corresponds to the ramp filter with 1 D frequency response in the projection domain the operator circumflex over may be chosen as an image domain filter with frequency response square root over where and denote the frequency variables in the transverse plane. More generally circumflex over may be chosen as an image domain filter with a high pass response in the transverse plane. Denote further a square root of tins operator as

That is appocanon of the image domain filer circumflex over to x produces f. We refer to x as a pre image. Next using the identity that holds for a linear operator A Ax A f whenever f Ax Eq. 42 reduces to circumflex over circumflex over 0 where 43 

To obtain this preconditioned form of the gradient with respect to x the AL in Eq. 5 is modified to the following form

Eq. 43 is made to be well conditioned for solving for the variable x by choosing the image domain filter circumflex over so that circumflex over RRcircumflex over is well conditioned e.g approximates or is proportional to the identity operator for example by using the square root of the image domain ramp filter as described above or its approximation using an FIR or an IIR image domain filter or using the FFT. In the case of an IIR or FIR filter it may be advantageous to use in the definition

An alternative favorable choice for circumflex over is one that makes the Hessian with respect to x of the AL L x u which is given by circumflex over x circumflex over RRcircumflex over 45 

Because the relation in Eq. 44 between the variables f and x does not appear in the expression for L x u the block coordinate optimization of L x u with respect to the variables x u and can proceed without reference to the image variable f. The image f can be computed only at the end of the iterations when the completion criterion has been satisfied or more frequently if desired e.g. for monitoring the iterations.

The steps corresponding to the ADMM algorithm in this case i.e. the block coordinate optimization of L x u are analogous to those in Eq. 9 11 and in the PWLS case that the sinogram discrepancy term has the form of a weighted two norm are given by

The only numerically difficult step is the x image variable update which will have a similar level of complexity to the f updates of the sinogram domain operator e.g. Eq. 9 and can be similarly performed to desired accuracy using an iterative minimization algorithm such as nonlinear conjugate gradients.

This is another embodiment with applied in the image domain. The same notation and meaning for circumflex over and

Following from the non weighted case in 5 the extended weighted Augmented Lagrangian for this is written

Focusing specifically on the corresponding ADMM update equations for f and for this Augmented Lagrangian we have

This embodiment shares the benefits of the above image domain embodiment defined by Equations 44 and 46 49 particularly the preconditioning operator is not directly applied to the auxiliary sinogram variable u which makes the update steps for u simpler and they may have closed form solutions. The update step for f also has a trivial closed form in this formulation. The only numerically difficult step is the x image variable update which will have a similar level of complexity to the f updates of the sinogram domain operator e.g. 9 and can be similarly performed to desired accuracy using an iterative minimization algorithm such as nonlinear conjugate gradients. However in this embodiment and Equations 51 57 above the quantities f and

The particular examples of using the different variants of weighting operators of this invention with the alternating minimization of the modified AL in the ADMM and L ADMM algorithms are not meant to be exhaustive. The different variants of weightings and iterative schemes can be combined in various ways. The same weighting operators can be used to provide similar advantages in other alternating minimization schemes known in the art for example versions studied of Gauss Seidel and of the so called diagonal quadratic approximation method J. Eckstein Augmented Lagrangian and Alternating Direction Methods for Convex Optimization A Tutorial and Some Illustrative Computational Results RUTCOR Research Report RRR 32 2012 December 2012 or DQA or related methods such as Gradient projection for sparse reconstruction GPSR M. T. Figueiredo R. D. Nowak and S. J. Wright Gradient projection for sparse reconstruction Application to compressed sensing and other inverse problems 2007 Proximal Forward backward splitting methods PFBS P. L. Combettes and V. R. Wajs Signal recovery by proximal forward backward splitting 2005 and Uzawa type iterations or as another set of examples versions with Nesterov type or Barzilai Borwein type acceleration of the inner and or the outer iterations in versions of the ALM or ADMM or L ADMM such as NESTA FISTA etc.

Artisans will also appreciate that the description and embodiments of different variations of this invention can be modified and manipulated to other mathematically equivalent forms in which not all features previously described would be apparent. For example auxiliary sinogram variables u and in Eqns. 8 12 could be represented together by a single vector say v by stacking the vectors u and as u . Eqns. 8 12 could then be rewritten in terms of the single auxiliary sinogram variable with a corresponding change in any computer code or system executing the instructions to perform the operations indicated by these equations obscuring the actual presence of more than one auxiliary variable. The distinct update steps for u and in may then also be replaced by a more elaborate single update of variable .

A brief example demonstrates the efficacy of the present approach for the PWLS formulation of the cost function. The projection data used is simulated from a thorax phantom scanned in a 2D fan beam geometry with curved detector panel. The projection data has 983 detectors and 1024 views over a full circular rotation. Poisson noise was added to the simulated data such that the standard deviation of the noise in the FBP reconstruction was around 20 HU. The baseline iterative algorithm solves 2 using the method of conjugate gradients CG . The majority of the computation will involve the application of R or R so the computational cost of the algorithm will be measured in terms of the number of such operator applications. For example in CG each iteration calculates the gradient of 2 which involves application of both R and R yielding 2 applications per iteration.

We consider the resulting image after 200 CG iterations to be the converged image. For each approach we calculate the progress of the algorithm as the distance or root mean square error RMSE from this converged image. plots this RMSE error versus computational cost to compare the effective convergence rates. For reference 200 CG iterations would require 400 operator applications. For each method we tuned the parameter for optimal convergence and for linearized ADMM algorithms set 0.95 RR or 0.95 R R as appropriate.

As demonstrates the use of ADMM by itself does not necessarily confer accelerated convergence. Indeed the convergence of ADMM is qualitatively similar to the CG algorithm for this case. However incorporation of the weighting operator ADMM provides tremendous improvement the same reduction in RMSE as achieved by CG in 120 operator applications is achieved by ADMM in as few as 45 operator applications corresponding to an almost 3 fold speedup of ADMM over CG. Similarly linearized ADMM makes the image update steps computationally easier at the expense of the convergence rate. The linearized ADMM on the other hand takes advantage of better conditioning to further improve the convergence rate the same reduction in RMSE as achieved by CG in 120 operator applications is achieved by linearized ADMM in just 25 operator applications corresponding to an almost 5 fold speedup of linearized ADMM over CG.

While specific embodiments of the present invention have been shown and described it should be understood that other modifications substitutions and alternatives are apparent to one of ordinary skill in the art. Such modifications substitutions and alternatives can be made without departing from the spirit and scope of the invention which should be determined from the appended claims.

Various features of the invention are set forth in the appended claims. In the claims when it is stated that some quantity q is a function of another quantity x this functional dependence is not exhaustive that is quantity q can be also a function of some additional quantities. Thus we say that h s g is a function of s a function of g and function of both s and g.

