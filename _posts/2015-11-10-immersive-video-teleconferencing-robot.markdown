---

title: Immersive video teleconferencing robot
abstract: A method includes receiving, at a mobile teleconferencing robot, a remote user input to alter a viewing state of a vision system of the robot. The vision system includes a forward imaging sensor arranged to capture a forward video feed, a right imaging sensor arranged to capture a right video feed, and a left imaging sensor arranged to capture a left video feed, each with respect to a forward drive direction of the mobile teleconferencing robot. The method includes altering the viewing state of the vision system by adjusting a tilt angle and/or a zoom level of the forward imaging sensor based on the remote user input and generating a combined video feed that provides an immersive peripheral view about the robot. The combined video feed is generated by combining the forward video feed with a portion of the right video feed and a portion of the left video feed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09479732&OS=09479732&RS=09479732
owner: iRobot Corporation
number: 09479732
owner_city: Bedford
owner_country: US
publication_date: 20151110
---
This disclosure relates generally to mobile robots and more specifically to providing an immersive video experience for a remote user of a mobile robot.

A robot is generally an electro mechanical machine guided by a computer or electronic programming. Mobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is an automated guided vehicle or automatic guided vehicle AGV . An AGV is generally a mobile robot that follows markers or wires in the floor or uses a vision system or lasers for navigation. Some robots use a variety of sensors to obtain data about their surrounding environments for example for navigation or obstacle detection and person following. Moreover some robots use imaging sensors to capture still images or video of objects in their surrounding environments.

High quality video conferencing using mobile devices tablets and portable computers has enabled telepresence robots to provide a better sense of remote physical presence for communication and collaboration in the office home school etc. when one cannot be there in person. There have been two primary approaches that both utilize videoconferencing on a display 1 desktop telepresence robots which typically mount a phone or tablet on a motorized desktop stand to enable the remote person to look around a remote environment by panning and tilting the display and 2 drivable telepresence robots which typically contain a display integrated or separate phone or tablet mounted on a roaming platform. These approaches for video conferencing with the use of telepresence robots have failed to provide a remote user with an experience similar to or better than an actual presence at the remote environment.

A telepresence or video collaboration robot disclosed herein can provide a remote user with freedom of movement and physical presence at remote locations while experiencing an immersive peripheral awareness about the robot using a vision system and or other sensors of the robot. The vision system may include a forward imaging sensor disposed on the robot and arranged to have a forward field of view aimed along a forward drive direction of the robot to capture a forward video feed. The vision system may include one or more peripheral imaging sensors to capture a peripheral video feed. For example the robot may include right and left imaging sensors arranged to have corresponding right and left fields of view aiming in opposite directions and perpendicular to the forward field of view. The robot can generate a combined video feed that combines the forward video feed and the peripheral video feed to provide the remote user an immersive video experience that the offers the remote user a peripheral awareness about the robot.

One aspect of the disclosure provides a method that includes receiving at a mobile teleconferencing robot maneuverable across a ground surface a remote user input to alter a viewing state of a vision system of the mobile teleconferencing robot. The vision system includes a forward imaging sensor a right imaging sensor and a left imaging sensor. The forward imaging sensor is arranged on a top portion of the mobile teleconferencing robot at a first location to have a forward field of view aimed along a forward drive direction of the mobile teleconferencing robot and configured to capture a forward video feed. The right imaging sensor is arranged on a right portion of the mobile teleconferencing robot at a second location vertically apart from the first location to have a right field of view aimed in a right direction with respect to the forward drive direction. The right imaging sensor is configured to capture a right video feed. The left imaging sensor is arranged on a left portion of the mobile teleconferencing robot at a third location vertically apart from the first location to have a left field of view aimed in a left direction with respect to the forward drive direction. The left imaging sensor is configured to capture a left video feed. The left and right imaging sensors are located along side edge portions of a head of the mobile teleconferencing robot such as a video screen head e.g. a monitor or tablet and the forward imaging sensor is located along a top edge portion of the head of the mobile robot. The left right and forward imaging sensors are not co located but disposed about the periphery of the head for unimpeded viewing of their respective portions of the environment around the robot . The method includes altering the viewing state of the vision system by adjusting a tilt angle of the forward imaging sensor with respect to a vertical axis of the mobile teleconferencing robot and or a zoom level of the forward imaging sensor based on the remote user input and generating a combined video feed that provides an immersive peripheral view about the mobile teleconferencing robot. The combined video feed is generated by combining the forward video feed with a portion of the right video feed and a portion of the left video feed. The combined video feed includes video feed dividers between the forward video feed the portion of the right video feed and the portion of the left video feed. Each video feed divider has a position and a lean angle with respect to a vertical viewing axis based on the altered viewing state of the vision system. The method also includes outputting the combined video feed from the robot to a remote computing system.

Implementations of the disclosure may include one or more of the following optional features. In some implementations the right and left imaging sensors are aimed away from each other and at least partially away from the forward drive direction and the right and left fields of view at least partially overlap the forward field of view. The lean angle of each video feed divider may be based on the tilt angle of the forward imaging sensor. When the forward imaging sensor is tilted upward the video feed dividers lean toward each other and top ends of the video feed dividers are closer to each other than bottom ends of the video feed dividers. When the forward imaging sensor is tilted downward the video feed dividers lean away from each other and the top ends of the video feed dividers are further apart from each other than the bottom ends of the video feed dividers. Moreover the lateral position of each video feed divider may be based on the zoom level of the forward imaging sensor. When the forward imaging sensor is at a zoomed in focal range the video feed dividers are further apart from each other than when the forward imaging sensor is at a zoomed out focal range.

In some implementations when the forward imaging sensor is tilted downward the video feed dividers lean toward each other and top ends of the video feed dividers are closer to each other than bottom ends of the video feed dividers. When the forward imaging sensor is tilted upward the video feed dividers lean away from each other and the top ends of the video feed dividers are further apart from each other than the bottom ends of the video feed dividers.

In some implementations the forward imaging sensor includes a wide angle lens and the forward field of view has a horizontal field of view of about 100 degrees. The right and left imaging sensors may each include a fish eye lens and the corresponding right and left fields of view each has a horizontal field of view of about 180 degrees and a vertical field of view of about 135 degrees. In some implementations the horizontal field of view presented in the combined video feed spans at least 220 degrees.

Generating the combined video feed may include selecting the portion of the right video feed and the portion of the left video feed based on at least one of the tilt angle of the forward imaging sensor the zoom level of the forward imaging sensor or a vertical field of view offset of the forward imaging sensor relative to the field of view of the right imaging sensor and or the field of view of the left imaging sensor relative to a viewing horizon. The generating of the combined video feed may also include scaling the right video feed and the left video feed to each have a similar scale of the forward video feed and arranging the portion of the right video feed and the portion of the left video feed relative to the forward video feed. In some examples the method includes at least one of correcting wide angle distortion of the video feeds color matching the video feeds blending the video feeds or scaling the video feeds.

In some implementations generating the combined video feed includes correcting wide angle distortion of the video feeds mapping the distortion corrected right and left video feeds onto a hemispherical surface cropping and or scaling the distortion corrected right and left video feeds and overlaying the distortion corrected video feeds. The distortion corrected video feeds each have a right edge and a left edge. The left edge of the right video feed is arranged relative to the right edge of the forward video feed and a right video feed divider therebetween. Similarly the right edge of the left video feed is arranged relative to the left edge of the forward video feed and a left video feed divider therebetween. Overlay here therefore does not mean matching features from the forward video feed with features from the left and or right video feeds and laying one feed over the other to create an uninterrupted panoramic view. Instead here overlay means combining portions of the forward and peripheral video feeds so that they are sized similarly and so that their horizons align where the video feeds are abutted at the video feed dividers.

Correcting the wide angle distortion of the video feeds may include mapping pixels of the forward video feed to a tangent plane fitting the pixels of the forward video feed into a corresponding grid of the tangent plane and cropping the mapped and fitted forward video feed to fit an aspect ratio. Correcting the wide angle distortion of the video feeds may also include dewarping the right and left video feeds e.g. to panoramic video feeds and texture mapping the dewarped right and left video feeds onto a spherical surface.

Another aspect of the disclosure provides a robot that includes a robot body defining a forward drive direction and a robot head supported by the robot body. The robot head has a top portion a right side portion and a left side portion. The robot includes a forward imaging sensor a right imaging sensor and a left imaging sensor. The forward imaging sensor is moveably disposed on the top portion of the robot head at a first location and aimed along the forward drive direction and is configured to capture a forward video feed. The right imaging sensor is disposed on the right side portion of the robot head at a second location vertically spaced from the first location with respect to a ground surface supporting the robot. The right imaging sensor is aimed outward from a right portion of the teleconferencing robot and arranged to have a right field of view aimed at least partially away from the forward drive direction. Moreover the right imaging sensor is configured to capture a right video feed. The left imaging sensor is aimed outward from a left portion of the teleconferencing robot and disposed on the left side portion of the robot head at a third location vertically spaced from the first location with respect to the ground surface. The left imaging sensor is arranged to have a left field of view aimed at least partially away from the forward drive direction. The left imaging sensor is configured to capture a left video feed. The left and right imaging sensors are located along side edge portions of a head of the mobile teleconferencing robot such as a video screen head e.g. a monitor or tablet and the forward imaging sensor is located along a top edge portion of the head of the mobile robot. The left right and forward imaging sensors are not co located but disposed about the periphery of the head for unimpeded viewing of their respective portions of the environment around the robot . The robot also includes data processing hardware in communication the forward imaging sensor the right imaging sensor and the left imaging sensor. The data processing hardware is configured to generate a combined video feed that provides an immersive peripheral view about the robot by combining the forward video feed with a portion of the right video feed and a portion of the left video feed. The combined video feed includes video feed dividers between the forward video feed the portion of the right video feed and the portion of the left video feed. Each video feed divider has a position and a lean angle with respect to a vertical viewing axis based on a tilt angle of the forward imaging sensor with respect to a vertical axis of the robot and or a zoom level of the forward imaging sensor.

This aspect may include one or more of the following optional features. The robot may include a drive system supporting the robot body on the ground surface and configured to maneuver the robot across the ground surface while the imaging sensors capture corresponding video feeds. The forward imaging sensor may include a wide angle lens and the forward field of view may have a horizontal field of view of about 100 degrees. Moreover the right and left imaging sensors may each include a fish eye lens and the right and left fields of view may each have a horizontal field of view of about 180 degrees and a vertical field of view of about 135 degrees. In some examples the right and left imaging sensors are aimed away from each other in opposite directions and the right and left fields of view may each capture a hemispherical field of view e.g. where combination of the right and left hemispherical fields of view form a spherical field of view . In additional examples the right and left imaging sensors are positioned at a common vertical height with respect to the ground surface. For example the right and left imaging sensors may reside in a common x y plane spaced vertically z from the forward imaging sensor. Furthermore the right and left imaging sensors may reside in a common vertical plane z plane with the forward imaging sensor.

In some implementations the video feed dividers move in response to a change in viewing state of the forward imaging sensor. Each video feed divider has a top end and a bottom end. When the forward imaging sensor is tilted upward the video feed dividers lean toward each other and the top ends of the video feed dividers are closer to each other than the bottom ends of the video feed dividers. When the forward imaging sensor is tilted downward the video feed dividers lean away from each other and the top ends of the video feed dividers are further apart from each other than the bottom ends of the video feed dividers. In some examples a position of the video feed dividers varies based on the change in viewing state of the forward imaging sensor. For example when the forward imaging sensor is at a zoomed in focal range the video feed dividers are further apart from each other than when the forward imaging sensor is at a zoomed out focal range.

Generating the combined video feed may include selecting the portion of the right video feed and the portion of the left video feed based on at least one of the tilt angle of the forward imaging sensor the zoom level of the forward imaging sensor or a vertical field of view offset of the forward imaging sensor relative to the field of view of the right imaging sensor and or the field of view of the left imaging sensor relative to a viewing horizon. The generating of the combined video feed may also include scaling the right video feed and the left video feed to each have a similar scale of the forward video feed and arranging the portion of the right video feed and the portion of the left video feed relative to the forward video feed. In some examples the generating of the combined video feed includes at least one of correcting wide angle distortion of the video feeds color matching the video feeds blending the video feeds or scaling the video feeds.

Yet another aspect of the disclosure provides a method that includes receiving at data processing hardware of a mobile teleconferencing robot a forward video feed a right video feed and a left video feed from a vision system of the mobile teleconferencing robot. The vision system includes a forward imaging sensor a right imaging sensor and a left imaging sensor. The forward imaging sensor is arranged on a top portion of the mobile teleconferencing robot at a first location to have a forward field of view aimed along a forward drive direction of the mobile teleconferencing robot and is configured to capture the forward video feed. The right imaging sensor is aimed outward from a right portion of the teleconferencing robot and arranged on a right portion of the mobile teleconferencing robot at a second location apart from the first location to have a right field of view aimed at least partially away from the forward drive direction. The right imaging sensor is configured to capture the right video feed. The left imaging sensor is aimed outward from a left portion of the teleconferencing robot and arranged on a left portion of the mobile teleconferencing robot at a third location apart from the first location to have a left field of view aimed at least partially away from the forward drive direction. The left imaging sensor is configured to capture the left video feed. The right field of view and the left field of view each have a horizontal field of view of about 180 degrees. The method further includes generating by the data processing hardware a full peripheral video feed by combining the right and left video feeds and generating by the data processing hardware an overlaid immersive video feed by correcting a wide angle distortion of the forward video feed and overlaying the distortion corrected forward video feed on the full peripheral video feed. The overlaid immersive video feed provides a forward and peripheral view about the mobile teleconferencing robot. The method also includes outputting the overlaid immersive video feed from the data processing hardware to a remote computing system. Overlay here does not mean matching features from the forward video feed with features from the left and or right video feeds and laying one feed over the other to create an uninterrupted panoramic view. Instead here overlay means combining portions of the forward and peripheral video feeds so that they are sized similarly and so that their horizons align where the video feeds are abutted at the video feed dividers.

This aspect may include one or more of the following optional features. In some implementations the forward imaging sensor includes a wide angle lens and the forward field of view has a horizontal field of view of about 100 degrees. In addition the right and left imaging sensors may each include a fish eye lens and the right and left fields of view each have a vertical field of view of about 135 degrees. In some examples the right and left imaging sensors are vertically spaced from the forward imaging sensor with respect to a ground surface supporting the mobile teleconferencing robot. The right and left imaging sensors may have about the same vertical height with respect to the ground surface and may be arranged to have the right field of view and the left field of view aiming in substantially opposite directions.

Generating the full peripheral video feed may include dewarping the right and left video feeds and texture mapping the dewarped right and left video feeds onto a spherical surface. In some examples correcting the wide angle distortion of the forward video feed includes mapping pixels of the forward video feed to a tangent plane fitting the pixels of the forward video feed into a corresponding grid of the tangent plane and cropping the mapped and fitted forward video feed to fit an aspect ratio. In additional examples correcting the wide angle distortion of the forward video feed includes adjusting a normalized radius of each pixel of the forward video feed using a polynomial equation. A placement location of the distortion corrected forward video feed on the full peripheral video feed may be based on a tilt angle of the forward imaging sensor with respect to a vertical axis of the mobile teleconferencing robot. Moreover a scale of the distortion corrected forward video feed may be based on a zoom level of the forward imaging sensor.

The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects features and advantages will be apparent from the description and drawings and from the claims.

While audio connections content sharing systems and real time video connections have allowed people to become more connected to other people and events such as conferences meetings inspections at remote locations such systems fall short of providing a remote user with a full immersive experience that simulates in person presence at the event. A telepresence or video collaboration robot referred to herein as the robot provides a user with freedom of movement and physical presence at remote locations to achieve a more personal degree of collaboration within a dispersed work environment. While video conferencing with one or more people through the robot a remote user generally wishes to experience a collaboration session that is similar to or better than an actual presence with the one or more people. This experience can be achieved by providing a remote user with a peripheral awareness such as a field of view from the robot that is similar to or better than a human s peripheral awareness. Moreover other experience enhancements may include identification of people or objects with linked access to information about those identified people or objects.

The user device includes data processing hardware which may execute a variety of different operating systems stored in memory hardware in communication with the data processing hardware . In examples where the user device is a mobile device the user device may run an operating system including but not limited to ANDROID developed by Google Inc. IOS developed by Apple Inc. or WINDOWS PHONE developed by Microsoft Corporation. Accordingly the operating system running on the user device may include but is not limited to one of ANDROID IOS or WINDOWS PHONE . In an example where the user device is a laptop or desktop computing device the user device may run an operating system including but not limited to MICROSOFT WINDOWS by Microsoft Corporation MAC OS by Apple Inc. or Linux. The user device may also access the robot and or the remote system while running operating systems other than those operating systems described above whether presently available or developed in the future.

In some implementations the user device executes one or more software applications for example a telepresence application . A software application refers to computer software that when executed by the data processing hardware causes the data processing hardware to perform a task. In some examples a software application is referred to as an application an app or a program . The functionality of the application may be accessed on the user device on which the application is installed. Additionally or alternatively the functionality of an application is accessed via the data processing hardware of the remote system e.g. via the network or the Internet . In some examples all of an application s functionality is included on the user device on which the application is installed. As such these applications may function without communication with other computing devices. In other examples an application installed on the user device accesses information from other remote hardware during operation such as the memory hardware of the remote system . In still other examples a web based application also referred to herein as a web application is partially executed by the data processing hardware of the user device and partially executed by remote data processing hardware. For example a web based application may be an application that is executed at least in part by a web server and accessed by a web browser e.g. a native application of the user device . Additionally portions of the functionality of an application may be performed by other remote data processors such as the data processing hardware of the remote system .

The sensor data transmitted by the robot through the network may include raw or processed data originating from one or more sensors of the robot which may include imaging sensors see or a sensor module see having imaging and or proximity sensors. Raw sensor output signals such as raw image data from one or more imaging sensors of the robot is streamed to a computing device such as a central processing unit CPU of the robot as illustrated in . The computing device that receives the raw image data may de warp clip crop correct overlay or otherwise transform or process the raw image data . In the example shown in the computing device utilizes a graphics processing unit and an application programming interface such as Open Graphics Library OpenGL to process or transform the raw image data . In some implementations the raw image date is processed at a rate of 30 60 Hz Alternatively the computing device utilizes other processing systems units or applications to process or transform the raw image data . From the CPU processed image data is transmitted through a codec of the robot for encoding the data and to a transceiver such as a radio transceiver of the robot for sending the processed corrected image data as part of the sensor data to the network . In some implementations the processed image data includes processed forward right and left video feeds from the corresponding forward right and left imaging sensors . Alternatively the raw image data is not be handled be the computing device . Instead the raw image data is sent directly from the one or more imaging sensors to the codec for encoding. If the raw image data is not handled by the computing device then the sensor data is include the raw image data which would be handled after transmission through the network e.g. handled by the data processing hardware of the user device or the data processing hardware of the remote system . In such examples the imaging sensors and the codec are integrated into a head see of the robot .

The torso may include a sensor module as described in U.S. Pat. No. 8 958 911 which is hereby incorporated by reference in its entirety. The torso supports an interface module which includes a neck supported by the torso and a head supported by the neck . In some examples the neck provides panning and tilting of the head with respect to the torso . In some examples the neck moves e.g. along a linear track telescopically or via articulation to alter a height of the head with respect to the floor surface . In additional examples the neck provides a range of angular movement about the x axis e.g. 90 degrees and or z axis e.g. of between about 90 degrees and about 360 degrees . Other ranges are possible as well.

The head may include a screen e.g. touch screen a speaker a microphone and a vision system that includes one or more imaging sensors . The vision system captures still images video and or 3D volumetric point clouds from an elevated vantage point of the head . In the examples shown the head includes a forward imaging sensor disposed on a top portion of the head and two peripheral awareness imaging sensors a right imaging sensor disposed on a right portion of the head and a left imaging sensor disposed on a left portion of the head . In some examples the forward imaging sensor is centrally disposed on the top portion of the head in a common vertically oriented plane with the right and left imaging sensors and the right and left imaging sensors may be disposed in a common horizontal plane vertically spaced from the forward imaging sensor . The left and right imaging sensors are fixed cameras having no pan or tilt mechanism. The forward imaging sensor is not co located with the right and left imagining sensors and the right and left imagining sensors are located along respective right and left sides of the head so that their fields of view are not obstructed by a portion of the head . For example if the right and left imaging sensors were collocated at a top central portion of the head along with the forward imaging sensor they would capture the right portion and left portion of the top edge of the head in their respective fields of view and therefore would be unable to fully view the peripheral environment along the left and right sides of the robot .

In some examples the left and right imaging sensors are located along side edge portions of the head of the mobile teleconferencing robot such as a video screen head e.g. a monitor or tablet and the forward imaging sensor is located along a top edge portion of the head of the mobile robot . The left right and forward imaging sensors are not co located but disposed about the periphery of the head for unimpeded viewing of their respective portions of the environment around the robot .

In other examples the head includes one or more imaging sensors disposed on any portion of the robot in a variety of arrangements. For example more than one imaging sensor may be disposed on the top portion of the head one or more imaging sensors may be disposed at the same or different heights on the right portion and or left portion of the head relative to the forward imaging sensor . In additional examples one or more imaging sensors are co located at some point on the head or elsewhere on the robot .

The forward imaging sensor is disposed on the top portion of head at a first location to have a forward field of view FOV aimed along the forward drive direction F of the robot and configured to capture a forward video feed . In some examples the forward imaging sensor is implemented as a high definition camera having a wide angle lens and optical and digital zoom features. The forward FOV may have a horizontal FOV of between 90 degrees and 110 degrees e.g. about 100 degrees and or a vertical FOV of between 80 degrees and 90 degrees e.g. about 90 degrees . Other fields of view are possible as well.

The right imaging sensor and the left imaging sensor which may be collectively referred to as the peripheral imaging sensors are arranged on the corresponding right and left portions of the head to aim in corresponding right and left directions R L with respect to the forward drive direction F. In the example shown the right imaging sensor is arranged on the right portion of the head at a second location vertically apart from the first location of the forward imaging sensor to have a right field of view aimed in the right direction R with respect to the forward drive direction F. The right imaging sensor is configured to capture a right video feed . Similarly the left imaging sensor is arranged on the left portion of the head at a third location vertically apart from the first location of the forward imaging sensor to have a left field of view aimed in the left direction L with respect to the forward drive direction F. The left imaging sensor is configured to capture a left video feed

In some implementations the right and left imaging sensors each have a fish eye lens and the right and left FOVs each have a horizontal FOV of about 180 degrees 10 degrees and a vertical FOV of about 135 degrees 10 degrees. In additional implementations the right imaging sensor and the left imaging sensor are each implemented as a high definition wide angle camera having a fisheye lens and may provide a virtual tilt feature such that the selection of a displayed portion of the field of view of the camera changes without physically tilting or moving the camera. While the right and left horizontal FOVs may be presented utilizing the virtual tilt feature the robot may also present a view including only a desired portion of the horizontal FOV of the right imaging sensor and or the left imaging sensor . In some implementations the horizontal FOV and the vertical FOV of the right and left imaging sensors are sized to each envelope capture a hemispherical sphere of space about the robot such that a combination of the FOVs of the right and left imaging sensors envelopes captures a sphere of space about the robot .

The power source may include one or more lithium ion batteries one or more other type of batteries or any other power source capable of powering the components of the robot . The power source which the robot body can carry is in electrical communication with each component of the robot requiring power and delivers power as necessary to these components of the robot .

Referring specifically to in some implementations the forward imaging sensor is attached to the top portion of the head of the robot in a fashion that permits an actuated tilt which is accomplished by a motor not shown that rotates the forward imaging sensor relative to a horizon. Accordingly the forward imaging sensor mechanically tilts from a first position as illustrated in having a horizontal view sightline aimed substantially parallel to the floor surface in the x y plane to a second position as illustrated in having a tilted view sightline aimed downward toward the floor surface at an angle from the x y plane or a vertical axis Z of the robot . The range of mechanical tilt of the forward imaging sensor may vary between different implementations of the robot . Referring specifically to in some implementations the forward imaging sensor is attached at the top portion of the head without mechanical tilting capabilities. In some implementations the vertical FOV of the forward imaging sensor has sufficient range such that mechanical tilting of the forward imaging sensor is not be necessary to provide an immersive video experience. The forward imaging sensor has a virtual tilt feature such that the forward imaging sensor digitally tilts the vertical FOV without mechanically moving the imaging sensor from the horizontal view sightline . In some examples the forward imaging sensor implements the digital tilting by returning a forward video feed corresponding to a portion or a sub window of the FOV . As the forward imaging sensor digitally tilts downward the forward video feed includes more of a lower portion of the vertical FOVav of the forward imaging sensor . Similarly as the forward imaging sensor digitally tilts upward the forward video feed includes more of an upper portion of the vertical FOVav of the forward imaging sensor

Other sensors associated with the sensor module e.g. presence sensors of the robot may provide a non image awareness both within and outside of the viewing area . The raw image data from the imaging sensors along with the data from other sensors of the sensor module becomes part of the sensor data as illustrated on and may enable the user device to present the remote view of the premise to the remote user and or the robot to perform tasks. For example as described in U.S. Pat. No. 8 958 911 which is incorporated herein by reference in its entirety the sensor data from both the imaging sensors and the other sensors of the sensor module may be utilized for simultaneous localization and mapping SLAM object detection and obstacle avoidance ODOA other navigation features and other autonomous operations of the robot . The controller system may process the sensor data for such utilization locally at the robot or transmit the sensor data to an external source such as the user device or the remote system as illustrated in which can utilize the sensor data for such robot operations. For example the robot transmits the sensor data from the imaging sensors and or the sensor module sensors to a cloud environment which processes the sensor data generates a usable map via SLAM and sends the map to the robot . When image data from the imaging sensors is included in the sensor data that is utilized for such purposes either the raw image data or image data processed by the computing device of the controller system of the robot for example the corrected image data shown in may be utilized by the cloud environment .

In some implementations altering a viewing state of the vision system by adjusting the tilt angle of the forward imaging sensor with respect to the vertical axis Z of the robot as shown in and or the zoom level of the forward imaging sensor based on a remote user input e.g. a command causes the controller system and or the user device to generate the remote view as a combined video feed that provides an immersive peripheral view about the robot by combining the forward video feed with a portion of the right video feed and a portion of the left video feed 

Referring to the user device may be in communication with and receive the sensor data transmitted from the robot . In some implementations the remote view on the screen of the user device includes a center portion a right portion and a left portion arranged adjacently. The center portion of the remote view displays the processed corrected real time video feed originating from the forward imaging sensor of the robot . The right portion of the remote view displays the processed corrected real time video image feed originating from the right imaging sensor of the robot . The left portion of the remote view displays the processed corrected real time video image feed originating from the left imaging sensor of the robot . Break lines also referred to as video feed dividers indicate a transition on the screen from one portion of the remote view to another portion of the remote view . Specifically a first break line indicates the transition from the center portion to the right portion and a second break line indicates the transition from the center portion to the left portion . For example the combined video feed remote view includes a first video feed divider between the processed forward video feed and a portion of the processed right video feed and a second video feed divider between a portion of the processed left video feed . Each video feed divider has a corresponding position and a lean angle with respect to a vertical viewing axis based on the altered viewing state of the vision system . In other examples such as those shown in and the remote view is shaped differently than what is shown in e.g. the remote views illustrated in the remote view appears on a larger portion a smaller portion or an otherwise different portion of the screen the remote view includes fewer than three portions the remote view includes more than three portions the portions of the remote view are not be adjacently aligned e.g. a space may separate the portions of the remote view or the remote view is configured as a picture in picture PiP view or the video feed dividers are hidden to present a seamless transition between the portions of the remote view .

With continued reference to in some implementations the lean angle of each video feed divider is based on the tilt angle of the forward imaging sensor . When the forward imaging sensor is tilted upward away from the ground surface the video feed dividers lean toward each other and top ends of the video feed dividers are closer to each other than bottom ends of the video feed dividers . Moreover when the forward imaging sensor is tilted downward toward the ground surface the video feed dividers lean away from each other and the top ends of the video feed dividers are further apart from each other than the bottom ends of the video feed dividers . In some implementations the lean angle of each video feed divider correlates to a single rotational degree measurement by which the robot turns to face a selected element within either the right portion or the left portion of the remote view . In other words regardless of where the user clicks in either the right portion or the left portion to turn the robot to face the element with the forward imaging sensor the lean angle remains the same for an unchanged tilt angle of the forward imaging sensor . In other words all points along the lines of the video dividers are the same pan degree z away from the forward view. The lean angle of the divider lines therefore corresponds to the tilted position of a longitude line on the hemisphere of each fish eye lens of the right and left imaging sensors taken at a latitude position corresponding to the tilt angle of the forward imaging sensor

In additional implementations the lateral position of each video feed divider across the combined video feed is based on the zoom level of the forward imaging sensor . When the forward imaging sensor is at a zoomed in focal range the video feed dividers are further apart from each other than when the forward imaging sensor is at a zoomed out focal range and less of each peripheral video feed is displayed. Different portions of the FOVs of each imaging sensor ranging from the full horizontal FOV to a small portion of the horizontal field of view may be presented in the portions of the remote view as illustrated in . For example illustrates a combined video feed corresponding to a zoomed in focal range where the right and left video feeds display smaller portions of the FOVs of the right and left imaging sensor and illustrates a combined video feed corresponding to a zoomed out focal range where the right and left video feeds display larger portions of the FOVs of the right and left imaging sensor . In some instances the remote view includes less than three portions . The remote view may include two portions corresponding to any two of the three imaging sensors or one portion corresponding to a single one of the three imaging sensors . For example when the vision system is fully zoomed in the combined video feed includes only the forward video feed . Additionally if the robot transmitting images to the user device includes more than or less than three imaging sensors the remote view includes portions that corresponds to any or all of the more than or less than three imaging sensors .

The portion of the FOV of the imaging sensors displayed in the remote view may vary with time. In some examples the robot receives a zoom in instruction which originates from an application of the computing device of the controller system of the robot or is relayed to the robot as a command for example from the user from an application of the user device or from the remote system . Upon receiving such a zoom in instruction the computing device of the controller system of the robot changes in time the sensor data transmitted to the user device such that the remote view on the screen increases in size magnification but decreases an area of view. As illustrated in the remote view may zoom in from a first zoomed level Zto a second zoomed level Z. At the second zoomed level Z the center portion of the remote view corresponding to the processed video feed from the forward imaging sensor enlarges while the right portion and the left portion of the remote view decrease in size. The full horizontal field of view of the forward imaging sensor is included in the center portion of the remote view at both the first zoomed level Zand the second zoomed level Z with the full horizontal field of view magnified in the second zoomed level Z. However the portion of the right field of view from the right imaging sensor included in the right portion of the remote view and the portion of the left field of view from the left imaging sensor included in the left portion of the remote view both decrease in the second zoomed level Zto accommodate the size increase of the center portion of the remote view . In other words the left and right FOVs are truncated in the horizontal direction to show only portions of the left and right FOVs closest to the forward drive direction F of the robot . Similarly a zoom out instruction causes a decrease in the size of the center portion of the remote view which results in an increase in the portion of the right field of view from the right imaging sensor that is included in an enlarged right portion of the remote view and an increase in the portion of the left field of view from the left imaging sensor that is included in an enlarged left portion of the remote view . In other words the left and right FOVs are expanded in the horizontal direction to show larger portions of the left and right FOVs closest to the forward drive direction F of the robot . In some implementations the lean angle of each video feed divider correlates to a single rotational degree measurement by which the robot turns to face a selected element within either the right portion or the left portion of the remote view . In other words regardless of where the user clicks in either the right portion or the left portion to turn the robot to face the element with the forward imaging sensor the lean angle remains the same for an unchanged tilt angle of the forward imaging sensor

In some implementations the forward imaging sensor utilizes a digital zoom feature to magnify the image data without varying the horizontal field of view . However the forward imaging sensor may also include an optical zoom feature i.e. an ability to shorten or lengthen an associated focal length which may be utilized to zoom in or zoom out to alter the horizontal field of view . In addition to the robot receiving the zoom instruction and altering the remote view at its computing device the robot transmits raw image data from each of the imaging sensors of the robot to the network and either the data processing hardware of the user device or the data processing hardware of the remote system operates a zoom feature to alter the remote view .

As illustrated in in response to a tilt command the remote view on the screen of the user device changes in time with respect to a vertical viewing angle to display a tiled forward video feed in the remote view . The robot is configured to receive a tilt instruction which originates from an application of the computing device of the controller system of the robot or is relayed to the robot as a command from the user via an application of the user device . Upon receiving such a tilt instruction the controller system effectuates a mechanical tilt of the forward imaging sensor e.g. by actuation of a motor . As a change in tilt angle of the forward imaging sensor occurs the computing device of the controller system of the robot changes the sensor data transmitted to the user device such that the relationship between the portions for example the center portion the right portion and the left portion of the remote view on the screen varies to match the tilt angle of the main imaging sensor . As the forward imaging sensor mechanically tilts upward the break lines at the edges of the center portion of the remote view rotate or lean to correspond with the tilt of forward imaging sensor moving in the remote view from a first alignment Tto a second alignment T. As the forward imaging sensor mechanically tilts downward the break lines at the edges of the center portion of the remote view rotate or lean to correspond with the tilt of the forward imaging sensor moving in the remote view from a first alignment Tto a third alignment T.

When generating the remote view a combined video feed the computing device selects a portion of the right video feed and a portion of the left video feed based on at least one of the tilt angle of the forward imaging sensor the zoom level of the forward imaging sensor or an offset of the vertical FOV of the forward imaging sensor relative to the FOV of the right imaging sensor and or the left imaging sensor relative to a viewing horizon. The computing device scales the right video feed and the left video feed to each have a similar scale of the forward video feed and arrange the portion of the right video feed and the portion of the left video feed relative to the forward video feed to form the remote view . In some implementations the computing device corrects distortion e.g. wide angle distortion matches colors blends and or scales the raw video feeds such that the combined video feed has a unified appearance much like human perception of color and size gradients across the peripheral and forward FOVs . This may be done before the selection and arrangement of the processed video feeds 

In some implementations the computing device of the robot includes a graphics processing unit and software configured to perform image calibration for distortion correction of the raw video feeds originating from the imaging sensors . Thus the sensor data transmitted from the robot as illustrated in includes processed corrected video image data transferred in real time from the mobile teleconferencing robot while moving and stationary. In other implementations however the robot transmits sensor data including the raw image data for calibration elsewhere e.g. at the remote system or the user device . Referring to the computing device of the robot performs image calibration utilizing the calibration grid to transform the raw video feed to a corrected video feed . illustrates a calibration grid overlaid onto a raw image having a barrel distorted checkered pattern. The computing device fits the checkered pattern of the raw image to the calibration grid as shown in in order to calibrate the raw image for distortion correction. The corrected image video feed which has a varying height across the image video feed results from the calibration. The distortion correction may be accomplished by identifying lens correction parameters for correcting barrel distortion caused by the lens of the imaging sensor . The computing device uses the correction parameters to accomplish a polynomial based correction. In some examples the graphics processing unit of the computing device of the robot utilizes this method and corrects the barrel distortion in real time. In some examples the corrected image video feed does not define a 16 9 aspect ratio image. Accordingly the computing device further crops the calibrated image video feed in order to size the final corrected image video feed at a 16 9 aspect W H ratio as illustrated in . In other examples without deviating from the scope of this disclosure the computing device crops the calibrated image to a different aspect ratio e.g. 4 3 or any other size to form the corrected image video feed .

Each of the peripheral imaging sensors of the robot utilizes a circular fisheye lens in some implementations in order to capture a sufficiently wide field of view as previously discussed. The raw video feed captured by such imaging sensors is shaped as a circle. For example the raw video feed from the right imaging sensor illustrated in and the raw video feed from the left imaging sensor illustrated in provide circular images video feeds of different portions of the viewing area of the premise which include heavy distortions. The graphics processing unit of the computing device of the robot corrects the distortion. First the graphics processing unit de warps the circular raw video feeds images into panoramic video feeds images . Then the graphics processing unit texture maps the panoramic video feed image onto a cylindrical or hemispherical surface to correct the majority of the distortions. The right corrected video feed image of a portion of the premise is illustrated in . The left corrected video feed image of a portion of the premise is illustrated in .

The robot transmits the corrected video feeds images to the user device for displaying the remote view on the screen of the user device . As shown in the screen of the user device displays a standard remote view of the premise . The standard remote view presents the corrected video feed in a bay window shaped display having a center portion a right portion and a left portion . The corrected video feed originating from the forward imaging sensor forms the center portion of the remote view . The right corrected video feed originating from the right imaging sensor forms the right portion of the remote view . The left corrected video feed originating from the left imaging sensor forms the left portion of the standard remote view . The standard remote view provides video feeds viewed by a remote user at a standard zoom e.g. a 1 zoom such that the screen of the user device displays images video feeds from each FOV . The one times standard zoom associated with the standard remote view may enable the screen of the user device to display the full horizontal FOV of the forward imaging sensor at the middle of the screen as the center portion . For example in implementations the center portion spans approximately sixty percent of the screen . Additionally the one times standard zoom associated with the standard remote view enables the screen of the user device to display approximately 30 degrees of the panoramic horizontal FOVs of the peripheral imaging sensors as the right portion and the left portion of the remote view . The 30 degree portions of the panoramic horizontal FOVs of the peripheral imaging sensors are near or closest to the forward FOV while not overlapping the for forward FOV . The bay window shaped display of the standard remote view includes break lines also referred to as video feed dividers to indicate the transition from one portion of the remote view to another portion of the remote view on the screen as described earlier with reference to . In some implementations portions of the screen above and below the remote view are not be utilized to display the remote view . Thus while the standard remote view provides an immersive video experience for the remote user i.e. an approximately 150 degree horizontal field of view at a reasonably detailed one times 1 zoom standard zoom level when displayed on the screen of the user device the shape of the standard remote view may leave portion of the screen available for other views or displayed information e.g. about the robot or the environment about the robot .

In the screen displays a zoomed out view for a downwardly tilted forward imaging sensor . The divider lines tilt to form a V and portions of each fixed peripheral camera the right and left imaging sensors are displayed. Those right and left video feed portions selected for display on the screen correspond with the vertical position of the horizon of the forward video feed . In some implementations as the forward imaging sensor tilts downward at an angle the center of the image shifts downward and the portions of each image or video feed captured by the right and left imaging sensors shifts to a corresponding latitude on the hemisphere of the fish eye lens of each of the right and left imaging sensors . By selecting portions of each hemispherical lens of the right and left imaging sensors that correspond with the vertical tilt of the forward imaging sensor the robot virtually tilts the affixed right and left imaging sensors to create a fully tilted view of the peripheral area about the robot as well as ahead of the robot .

Referring to the screen of the user device may display a zoomed in remote view of the premise which provides a view at any zoom greater than 1 e.g. 2 6 . The zoomed in remote view of presents the corrected images video feeds at about a 2 zoomed view. At this zoomed in level the zoomed in remote view presents only the corrected image video feed from the forward imaging sensor which is displayed across the full screen . Generally the zoomed in remote view provides video feeds viewed by a remote user such that the screen of the user device displays images video feeds ranging from the full forward horizontal FOV of the forward imaging sensor to a portion of the forward horizontal FOV . In some implementations of the system the user has an option to view the zoomed in remote view and then pan the range of the viewing area displayed on the screen such that portions of the right corrected video feed or portions of the left corrected video feed may be displayed on the screen at the zoomed in viewing level. When the range of viewing area is panned from left to right or from right to left the first break line indicating the transition from the center portion to the right portion i.e. the video feed divider between the center corrected video feed and the right corrected video feed or the second break line indicating the transition from the center portion to the left portion i.e. the video feed divider between the center corrected video feed and the left corrected video feed may be included as part of the zoomed in remote view that is displayed on the screen .

Referring to the screen of the user device may display a zoomed out remote view of the premise which provide images video feeds of a horizontal range exceeding 150 degrees e.g. the zoomed out remote view may display 180 degrees or 220 degrees of the viewing area of the premise . The zoomed out remote view presents the corrected video feed in the bay window shaped display having the center portion the right portion and the left portion . The zoomed out remote view provides video feeds viewed by a remote user at a zoom level such that the screen of the user device displays video feeds from each FOV of the imaging sensors . The zoom associated with the zoomed out remote view is set such that the full horizontal FOV of the forward imaging sensor is displayed at the middle of the screen as the center portion for example the center portion spans approximately forty percent of the screen and such that approximately fifty degrees of the panoramic horizontal FOVs of the peripheral imaging sensors are displayed in the right and left portions of the remote view . The bay window shaped display of the zoomed out remote view includes break lines video feed dividers to indicate the transition from one portion of the zoomed out remote view to another portion of the zoomed out remote view on the screen . Portions of the screen above and below the zoomed out remote view are utilized to display other information as well.

Referring to the computing device of the robot may combine and align e.g. overlay so that horizons align between video feeds but without stitching features together between the forward feed and the left and right feeds as in a continuous panoramic view the corrected video feeds in a bay window shaped display as illustrated in to form a remote view and zoom in or out on the remote view prior to transmitting the real time image data as part of the sensor data for viewing on the screen of the user device as shown in . Overlay here does not mean matching features from the forward video feed with features from the left and or right video feeds and laying one feed over the other to create an uninterrupted panoramic remote view . Instead here overlay means combining portions of the forward and peripheral video feeds so that they are sized similarly and so that their horizons align where the video feeds are abutted at the video feed dividers . Additionally the computing device of the robot also scales and calibrates the overlaid images video feeds to form an adjusted remote view . In the example shown the robot has transmitted an adjusted remote view of the zoomed out remote view . The adjusted remote view includes a seamless overlay of the corrected video feeds in a bay window shaped display on the screen . Here seamless does not mean that the three video corrected feeds are stitched so that their edge features mate to create a single uninterrupted panoramic view. Here as shown in seamless means the right video feed and the left video feed are abutted with the forward video feed to create a contiguous view of the combined video feeds despite their not having edge mated features. Even without edge mated features the views are all sized so that their horizons floor and ceiling portions of the images align at their abutment to provide an immersive experience for the user monitoring the environment of the mobile robot . While illustrates an adjusted remote view of the zoomed out remote view from the computing device of the robot may also correct the standard remote view to create a corresponding adjusted remote view or it may correct remote views at other zoomed levels. Additionally the break lines video feed dividers appear on the adjusted remote view shown in . However the computing device of the robot may hide these break lines on other adjusted remote views to provide a seamless combination of the forward right and left video feeds as the remote view .

Referring to the adjusted remote view of is shown as being displayed on the screen of the user device . In some implementations as described earlier with reference to the computing device of the robot combines the corrected video feeds to create such an adjusted remote view that the robot transmits to the network the user device receives and the screen displays to provide the user with an immersive video experience. In other implementations as illustrated in the screen displays remote views of other forms to provide the user with an immersive video experience. In a teleconferencing environment the immersive video experience allows the user navigate the robot down a hall while seeing the environment with a field of view at least equal to if not better than a human perceivable view of the environment. For example the immersive video experience provided by the robot allows the user to maneuver down a hallway while avoiding obstacles enter a conference room identify a conference table having several participants seated thereabout maneuver up to the conference table at an open available spot and allow the user to see and engage in a conversation with the participants while not being too close or too far from the participants. The immersive video experience allows a user to determine if someone is approaching the robot from behind and move off to the side if the approaching person is traveling at a speed greater than the robot . In this way the robot is more human like in its interactions maintaining preferred distances for personal space and not impeding passersby.

Referring specifically to the computing device of the robot has overlaid the corrected video feeds in a picture in picture remote view . The full width of the screen presents a corrected video feed representing the full horizontal FOV or a portion of the horizontal FOV of the forward imaging sensor . The screen also presents two windows arranged on the screen to create the picture in picture remote view of the remote view . A right window presents the corrected video feed corresponding to the right peripheral imaging sensor . A left window presents the corrected video feed corresponding to the left peripheral imaging sensor . The right window and the left window each include a corrected video feed spanning the full respective horizontal FOV or a portion of the respective horizontal FOV . Therefore the picture in picture remote view allows the user to view the full horizontal FOVs of both the right imaging sensor and the left imaging sensor without truncation. Each window may span approximately forty percent of the width of the screen . However each window may span more or less than forty percent of the width of the screen without deviating from the design of this disclosure.

Referring specifically to the computing device of the robot has overlaid the corrected video feeds in a triangular format remote view which includes three windows arranged on the screen . Across the full width or less of the screen a first window presents a corrected video feed representing the full horizontal FOV or a portion of the horizontal FOV of the forward imaging sensor . The screen also presents two additional windows arranged on the screen to create the triangular format of the remote view . A second window arranged in at the lower right of the screen presents the corrected video feed corresponding to the right peripheral imaging sensor . A third window arranged in at the lower left of the screen presents the corrected video feed corresponding to the left peripheral imaging sensor . The triangular arrangement provides a visually appealing remote view while displaying a larger horizontal FOV of both the right imaging sensor and the left imaging sensor as compared to the standard remote view . Moreover in some examples the spatially separate windows allow for separate processing by the computing device of the robot . The second window and the third window include a corrected video feed spanning the full respective horizontal FOV or a portion of the respective horizontal FOV . The second window and the third window span approximately forty percent of the width of the screen or they may span more or less than forty percent of the width of the screen without deviating from the design of this disclosure. Portions of the screen may remain unutilized when the robot transmits a triangular format for the remote view .

The immersive full peripheral remote view viewable by the remote user on the user device allows the remote user to engage in teleconferencing experiences that are not possible with a single forwardly aimed camera. For example rather than having the telepresence robot positioned away from a meeting table so that the telepresence robot can have a single field of view capturing a video feed of the entire table and or an entire group of participants. The robot of this disclosure can be maneuvered right up to the table like a regular participant and provide the remote user with an immersive full peripheral remote view that captures the table the participants forward to the sides and slightly partially to the rear and the surroundings. The robot offers the remote user with a better than human perception of the remotely viewed area the remote premise . Moreover the peripheral awareness afforded by the robot allows the remote user to teleconference with participants proximate and far away from the robot and in front of the robot and to a side of the robot . While maneuvering the robot the remote user can use the immersive full peripheral remote view to better see the environment about the robot for example to identify and avoid obstacles see corners while turning maneuver in tight spaces and more. The peripheral awareness afforded the robot via the vision system allows the robot to autonomously identify obstacles stationary or moving and maneuver around or away from the identified obstacles. For example the robot may detect a person walking straight toward it and then side step to move out of the way. The application of the user device may also allow the remote user to execute short distance maneuvers such as side stepping and or medium distance maneuvers such as local waypoint driving and preprogrammed sharp turns to navigate about a group of participants. The immersive full peripheral remote view provides a peripheral awareness about the robot that is better than that of a human about her himself. This allows the user to have eyes in the back of their head. In embodiments the robot may comprise one or more additional cameras aimed in a backward facing direction for complete 360 degree awareness of surroundings.

The application of the user device allows the remote user to send commands to the robot that facilitate an enhanced teleconferencing experience. For example the application allows the remote user to select a zero zoom command that causes the forward imaging sensor to change state to a zoom out focal range for example to see as many participants as possible. In some instances the bay window shaped standard remote view as discussed with reference to achieves the zero zoom command. When the conversation involves multiple participants the triangular format as discussed with reference to can be used to view different participants in different windows . The application also allows the remote user to select a point and look operation that commands the robot to maneuver tilt the forward imaging sensor and or alter the zoom level of the forward imaging sensor to view an area corresponding to a selected point on the immersive full peripheral remote view displayed on the screen . As described above with reference to the lean angle of the divider lines corresponds with a tilt angle of the forward facing camera such that selecting an area on either the right portion or the left portion of the combined video feed directs the robot to spin or rotate in the z direction by an amount that enables the lean angle to remain unchanged. In other words all points along the divider lines are the same pan degree z away from the forward view

Moreover the application allows the remote user to select a sit to stand operation that commands the robot to alter a height of the head to match align with or otherwise achieve a same level eye contact with a participant.

Referring to the robot when operating as an instrument of the system for providing an immersive video experience for a remote user performs a series of steps to acquire raw image data and to transform the raw image data such that it provides an immersive video experience when displayed on the screen of the user device . At step each of the imaging sensors create raw image data e.g. video feeds . The forward imaging sensor the right imaging sensor and the left imaging sensor each create unique raw image data corresponding to its particular field of view . At step the computing device of the robot creates one or more data files corresponding to the raw image data from each imaging sensor . One or more software development kits SDKs may be incorporated into the computing device in order to create the data files . For example a Blackmagic HD SDI 1080p30 capture device creates a YUV422 data file from the raw image data created by the forward imaging sensor and a libv412 device memory maps the raw image data created by the peripheral imaging sensors to create a MJPEG data file . In cases where some of the data files exist in alternate formats the graphics processing unit or another component of the controller system capable of performing the necessary decoding perform a decoding step . For example a hardware accelerated JPEG decoder transforms the MJPEG data files into YUV data files . At step the graphics processing unit and the application programming interface perform distortion correction and or calibration of the data files . Polynomial functions may be utilized to correct distortions such as barrel distortion associated with the raw image data recorded by some high definition cameras e.g. the forward imaging sensor . Three dimensional texture mapping corrects distortions associated with the raw image data recorded by some wide angle cameras utilizing fisheye lenses e.g. the right and left peripheral imaging sensors while creating a panoramic view from the fisheye image. Additionally correction of distortions associated with the raw image data recorded by some wide angle cameras utilizing fisheye lenses e.g. the right and left peripheral imaging sensors may require fisheye to hemispherical image correction. At step the graphics processing unit and the application programming interface perform color space correction conversion by converting the corrected images video feeds from YUV to an RGB color model. At step the corrected video feeds are overlaid to form a combined video feed such as the previously discussed adjusted remote view that the robot transmits for viewing by the remote user on the screen of the user device . The adjusted remote view includes the overlain corrected video feeds that provides the remote user with an immersive peripheral video experience.

At block the method includes generating a combined video feed that provides an immersive peripheral view about the mobile teleconferencing robot . The combined video feed is generated by combining the forward video feed with a portion of the right video feed and a portion of the left video feed . The combined video feed includes video feed dividers between the forward video feed the portion of the right video feed and the portion of the left video feed . Each video feed divider has a position and a lean angle with respect to a vertical viewing axis based on the altered viewing state of the vision system . The lean angle of each video feed divider may be based on the tilt angle of the forward imaging sensor . When the forward imaging sensor is tilted upward e.g. toward the ground surface the video feed dividers lean toward each other and top ends of the video feed dividers are closer to each other than bottom ends of the video feed dividers . When the forward imaging sensor is tilted downward e.g. away from the ground surface the video feed dividers lean away from each other and the top ends of the video feed dividers are further apart from each other than the bottom ends of the video feed dividers . Moreover the position of each video feed divider may be based on the zoom level of the forward imaging sensor . When the forward imaging sensor is at a zoomed in focal range the video feed dividers are further apart from each other than when the forward imaging sensor is at a zoomed out focal range. At block the method also includes outputting the combined video feed from the robot to a remote computing system .

In some implementations generating the combined video feed includes selecting the portion of the right video feed and the portion of the left video feed based on at least one of the tilt angle of the forward imaging sensor the zoom level of the forward imaging sensor or an offset of the vertical field of view of the forward imaging sensor relative to the field of view of the right imaging sensor and or the field of view of the left imaging sensor relative to a viewing horizon. The generating of the combined video feed may also include scaling the right video feed and the left video feed to each have a similar scale of the forward video feed and arranging the portion of the right video feed and the portion of the left video feed relative to the forward video feed . In some examples the method includes at least one of correcting wide angle distortion color matching blending or scaling of the video feeds 

In some implementations generating the combined video feed includes correcting wide angle distortion of the video feeds mapping the distortion corrected right and left video feeds onto a hemispherical surface cropping and or scaling the distortion corrected right and left video feeds and overlaying the distortion corrected video feeds . The distortion corrected video feeds each have a right edge and a left edge see and . The left edge of the right video feed is arranged relative to the right edge of the forward video feed and a right video feed divider therebetween. Similarly the right edge of the left video feed is arranged relative to the left edge of the forward video feed and a left video feed divider therebetween. In some examples the method includes horizontally compressing the distortion corrected distortion corrected video feeds prior to cropping and or scaling the distortion corrected distortion corrected video feeds . Correcting the wide angle distortion of the video feeds may include mapping pixels of the forward video feed to a tangent plane fitting the pixels of the forward video feed into a corresponding grid of the tangent plane as previously discussed with reference to and cropping the mapped and fitted forward video feed to fit an aspect ratio to provide a corrected forward video feed . Correcting the wide angle distortion of the video feeds may also include dewarping the right and left video feeds e.g. to panoramic video feeds and texture mapping the dewarped right and left video feeds onto a spherical surface to provide corrected right and left video feeds 

Referring to in some implementations a method for providing an immersive peripheral video feed for real time viewing such as on the screen of the user device . At block the method includes receiving at data processing hardware of a mobile teleconferencing robot a forward video feed a right video feed and a left video feed from the vision system of the mobile teleconferencing robot . At block the method further includes generating by the data processing hardware a full peripheral video feed by combining the right and left video feeds and at block generating by the data processing hardware an overlaid immersive video feed by correcting a wide angle distortion of the forward video feed and overlaying the distortion corrected forward video feed on the full peripheral video feed . The overlaid immersive video feed provides a forward and peripheral view about the mobile teleconferencing robot . At block the method includes outputting the overlaid immersive video feed from the data processing hardware to a remote computing system . Overlay here does not mean matching features from the forward video feed with features from the right video feed and or the left video feed and laying one feed over the other to create an uninterrupted panoramic view. Instead here overlay means combining portions of the forward and peripheral video feeds so that they are sized similarly and so that their horizons align where the video feeds are abutted at the video feed dividers .

In some implementations generating the full peripheral video feed includes dewarping the right and left video feeds and texture mapping the dewarped right and left video feeds onto a spherical surface to provide corrected right and left video feeds that the data processing hardware combines to form the full peripheral video feed . In some implementations a second wide angle forward looking camera not shown is affixed to look straight ahead in the forward direction of the robot and the processed video feed from the second wide angle forward looking camera is combined with the processed right and left video feeds for a stitched panoramic view in front of which is positioned the corrected forward video feed from the forward looking imaging sensor . The method in some examples includes blending the video feeds in terms of color gradients and size so that the immersive video feed does not have abrupt changes in appearance thus enhancing the immersive experience for the user . The wide angle distortion of the forward video feed can be corrected by the methods described earlier with reference to . A placement location of the distortion corrected forward video feed on the full peripheral video feed is based on the tilt angle of the forward imaging sensor with respect to the vertical axis Z of the mobile teleconferencing robot . Moreover a scale of the distortion corrected forward video feed is based on a zoom level of the forward imaging sensor . In implementations the combined video feed spans at least a 220 degree horizontal field of view that enables the user to drive the robot up to the middle of a long table and still view people seated directly adjacent the robot as well as at either end.

Various implementations of the systems and techniques described herein can be realized in digital electronic and or optical circuitry integrated circuitry specially designed ASICs application specific integrated circuits computer hardware firmware software and or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and or interpretable on a programmable system including at least one programmable processor which may be special or general purpose coupled to receive data and instructions from and to transmit data and instructions to a storage system at least one input device and at least one output device.

These computer programs also known as programs software software applications or code include machine instructions for a programmable processor and can be implemented in a high level procedural and or object oriented programming language and or in assembly machine language. As used herein the terms machine readable medium and computer readable medium refer to any computer program product non transitory computer readable medium apparatus and or device e.g. magnetic discs optical disks memory Programmable Logic Devices PLDs used to provide machine instructions and or data to a programmable processor including a machine readable medium that receives machine instructions as a machine readable signal. The term machine readable signal refers to any signal used to provide machine instructions and or data to a programmable processor.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit . Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user one or more aspects of the disclosure can be implemented on a computer having a display device e.g. a CRT cathode ray tube LCD liquid crystal display monitor or touch screen for displaying information to the user and optionally a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input. In addition a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user for example by sending web pages to a web browser on a user s client device in response to requests received from the web browser.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly other implementations are within the scope of the following claims.

