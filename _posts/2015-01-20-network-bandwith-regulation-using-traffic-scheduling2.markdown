---

title: Network bandwith regulation using traffic scheduling
abstract: Disclosed herein are systems and methods for regulating network bandwidth by means of monitoring network traffic, predicting network loads, and scheduling traffic utilizing traffic reporting and bandwidth reservation mechanisms. These systems and methods may reduce network congestion and support more efficient processing by network applications. Traffic reporting may comprise broadcasting control messages to network nodes indicating appropriate times to send and receive messages. Network nodes may use traffic reports (e.g., control messages) to proactively regulate their use of the network. Bandwidth reservation may allow network nodes to do productive processing while waiting to send and receive data, and may decrease mean wait times. Reservations may be implemented in a synchronous or asynchronous manner. In an exemplary embodiment, the reservation mechanism may emulate a traditional stream socket API. Embodiments enabling enhanced TV applications to run more effectively over cable TV out-of-band networks are described.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09577899&OS=09577899&RS=09577899
owner: FOURTHWALL MEDIA, INC.
number: 09577899
owner_city: Dulles
owner_country: US
publication_date: 20150120
---
The subject application is a Continuation Application of U.S. application Ser. No. 13 660 476 filed Oct. 25 2012 now issued as U.S. Pat. No. 8 937 866 which is hereby incorporated by reference in its entirety.

The invention relates generally to systems and methods for regulating network bandwidth. More specifically various exemplary embodiments relate to traffic reporting and bandwidth reservation mechanisms.

With the continued proliferation of communications networks from the Internet to cable TV IPTV mobile broadband WiFi and home networks and an ever growing demand for data to be sent over these networks concerns about network congestion are becoming more pervasive. As fast as larger pipes can be provided user demand threatens to fill them. Best effort mechanisms for data delivery like those underpinning the Internet often prove insufficient.

Network congestion concerns may be particularly serious for narrow asymmetrical channels such as the out of band OOB channel of a cable TV network. OOB cable channels are increasingly strained by requirements to handle traffic generated by enhanced TV applications such as those implemented using the Enhanced TV Binary Interchange Format EBIF or Open Cable Application Platform OCAP standards. Such traffic may include voting and polling data user preference and statistics traffic t commerce information and other data. Data delivery over DSL and mobile broadband networks faces similar challenges.

To address the problem of network congestion over band limited networks various approaches have been described. Some include packet dropping via tail drop or Active Queue Management TCP congestion avoidance Explicit Congestion Notification window shaping traffic shaping i.e. packet delaying Quality of Service QoS schemes and related bandwidth reservation techniques.

What is needed are systems and methods for regulating network bandwidth to reduce congestion in a way that allow nodes to continue processing while waiting to send and receive data and without requiring long messages excessive handshaking or other QoS type overhead.

In various exemplary embodiments systems and methods may be provided for regulating network bandwidth comprising monitoring network traffic predicting network loads and scheduling traffic utilizing traffic reporting and bandwidth reservation mechanisms. Traffic reporting may comprise broadcasting control messages to network nodes indicating appropriate times to send and receive messages. Network nodes may use traffic reports e.g. control messages to proactively regulate their use of the network. Reservations may be implemented in a synchronous or asynchronous manner. The reservation mechanism may emulate a traditional stream socket API.

In another exemplary embodiment in addition to the traffic report broadcast client nodes may make bandwidth reservations before sending or receiving messages or other data.

In some exemplary embodiments systems and methods may provide bandwidth regulation and lightweight Transmission Control Protocol TCP functionality over User Datagram Protocol UDP in a digital cable TV out of band OOB network wherein an Enhanced Television ETV Platform Server communicates via Hypertext Transfer Protocol HTTP payloads interchanged using UDP with multiple Set Top Boxes STBs running Enhanced TV Binary Interchange Format EBIF User Agents.

Other embodiments include networks that utilize TCP and other protocols networks such as in band cable TV DSL and cellular mobile networks and networks based on other architectures and components.

In another exemplary embodiment bandwidth regulation may be implemented by an EBIF User Agent in a way that may be transparent to EBIF applications. In other embodiments the traffic schedule may be exposed to network applications which can then use the disclosed traffic scheduling mechanisms and adjust user facing behavior appropriately.

In another exemplary embodiment a bandwidth regulation server may be responsive to external events e.g. time of day or time of year bandwidth predictions breaking news alerts indicating likely traffic storms to be appropriately managed.

It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the invention as claimed. The accompanying drawings constitute a part of the specification illustrate certain embodiments of the invention and together with the detailed description serve to explain the principles of the invention.

As illustrated in the present disclosure relates to network bandwidth regulation using traffic scheduling in the context of Traffic Management TM systems and methods. More specifically an exemplary embodiment is disclosed in the context of a digital cable TV out of band OOB network wherein an Enhanced Television ETV Platform Server communicates via Hypertext Transfer Protocol HTTP payloads interchanged using User Datagram Protocol UDP with multiple Set Top Boxes STBs running Enhanced TV Binary Interchange Format EBIF User Agents . In addition to providing bandwidth regulation the disclosed TM systems and methods may provide lightweight Transmission Control Protocol TCP functionality using UDP supporting guaranteed delivery packet sequencing flow control and data compression.

Systems and methods described herein may also be appropriate for use with traffic utilizing TCP and other protocols for other networks such as in band cable TV DSL and cellular mobile networks and for networks based on other architectures and components.

In the present exemplary embodiment the TM systems and methods take into account the following characteristics of digital cable OOB networks 

High Latency tunneling Internet Protocol IP over a Hybrid Fiber Coaxial HFC network generates delays 

Request Response Dichotomy most traffic consists of data moving in one direction or the other and the request direction tends to carry much less data 

To facilitate description of the current exemplary embodiment the following assumptions about the underlying network may be made 

The maximum UDP message size that can fit in one MAC cell along with 28 bytes of IP and UDP headers is 34 28 6 bytes. This drives the size of TM packet headers 

The total upstream bandwidth per Hybrid Fiber Coax network node HFC Node i.e. demodulator is 256 Kbps shared by 500 to 2000 STBs 

The performance of the current embodiment may be based on the following numeric constants that may be used in implementations. Representative values are noted subject to change based upon experimentation and the characteristics of deployed environments 

MTU Maximum Transmission Unit i.e. UDP packet size for TM upstream traffic may be 244 Bytes or 1952 bits. This allows one MTU to fit in exactly 6 MAC cells 272 bytes including 28 bytes of IP and UDP header. 99 of all upstream messages should fit in 1 MTU. Downstream the MTU may be larger up to a maximum of 987 bytes 

Slot ID A 6 byte unsigned integer or larger that uniquely identifies one SLOT i.e. 8 millisecond time slice since 01 01 2000 at 00 00 00. That is a Slot ID of 0 refers to the 8 millisecond time slice starting at 01 01 2000 at 00 00 00.000 and ending at 01 01 2000 at 00 00 00.008 Slot ID 2 refers to the next time slice beginning at 01 01 2000 at 00 00 00.008 and ending at 01 01 2000 at 00 00 00.016 and so on. For example the Slot ID of the 34th SLOT for the date time of 5 8 2010 3 21 04 AM is 40825508034. The Slot ID of the first SLOT representing the 100 year anniversary second of this date is 242007908000 

VPORT The actual port number 1962 used for all TM traffic. This number has been registered to BIAP Inc. 

VSEND WASTE The maximum number of milliseconds between packets that a data sending function e.g. vsend should continue to block. Initially set to 100 milliseconds 

VSEND TRIES The maximum number of packet resends that vsend should attempt before returning. Initially set to 5.

VRECV WASTE The maximum number of milliseconds that a data receiving function e.g. vrecv should continue to block waiting for a data packet. Initially set to 100 milliseconds.

The embodiment may consist of one or more ETV Platform Servers EPS located at a cable operator headend communicating with multiple STBs located at customer premises connected through a plurality of Hubs not shown and HFC Nodes in a tree and branch topology. The EPS may comprise a TM Server which may be a high performance UDP proxy server that relays incoming TM UDP traffic consisting of HTTP payloads to an Apache HTTP Server via HTTP TCP and then relays HTTP TCP responses back to the originating STB via TM UDP through the intermediate HFC Nodes and Hubs.

STBs i.e. client nodes may comprise an EBIF User Agent utilizing a TM Client to support the data networking needs of EBIF applications not shown running on the STBs . The TM Client component may be a static library that mimics a standard stream socket Application Programming Interface API for TCP networking providing for multiple simultaneous virtual stream sockets and connections while using only a single actual datagram socket port.

The indicated TM UDP protocol may use a simple packet fragmentation sequencing and acknowledgement scheme to guarantee message delivery. Bandwidth regulation may be controlled by the TM Server which may monitor upstream traffic to predict network loading and broadcast control messages to STBs that tell TM Clients how to schedule upstream messages e.g. via bandwidth reservations . Downstream bandwidth regulation may be accomplished entirely within the TM Server .

Given the architecture depicted in there may be three operational modes supported only one of which may be used for any particular cable operator. Which mode is supported may depend on a the availability of a continuous trickle of downstream bandwidth for UDP broadcast of load information to STBs and b permanent allocation of one socket port on the client system. These modes may be 

Mode 1 When both a and b are available the TM Server may broadcast info packets see below to all STBs periodically e.g. every 10 seconds and more often during heavy load conditions. This should require about 30 bps 0.03 Kbps 

Mode 2 When a is available but not b and the TM Client must wait for an info packet before sending any message these packets must be sent more frequently. This should require about 270 bps 0.27 Kbps 

Mode 3 When a is not available probe packets described below must be used to test the waters before sending every message. This may cause brief bandwidth spikes beyond the configured limits and is therefore less desirable than the other modes.

The TM Server may be a high performance UDP to TCP relay server that implements the TM guaranteed delivery protocol and OOB bandwidth regulation. Incoming connections and data may be received via a UDP Listener process and the connection packet may be placed as a task on an agenda . As depicted by the dashed lines in the agenda may be read by several processes.

A connection may be implicitly opened whenever a data or probe packet arrives with a new combination of IP address Port and ConnID see Packet Formats section below header fields. Child processes which may each implement multiple worker threads may process these packets as they arrive reassembling them in the proper order into the original messages and returning ACK packets to the client as necessary. Completed messages may then be sent to the Apache HTTP Server via a standard HTTP TCP IP connection. The HTTP response from Apache may then be returned to the corresponding client via the open virtual connection i.e. via downstream data packets .

The bandwidth regulation process may monitor incoming packets in real time. This information and its rate of change may be used to estimate loading trends. From this estimate a corresponding Send Probability may be computed. This Send Probability and clock synchronization information may be periodically broadcast via UDP to all TM Clients .

The TM Server and TM Clients may manage upstream OOB bandwidth by spreading out the time used to send messages. Every message may be broken down into fixed sized data packets and each packet may be bounded by a random delay. For example if the current global send probability i.e. variable send probability is 32767 the delay in SLOTs may be calculated to be  random 131072.0 32768 0.5  random 4

which results in a value between 0 and 3. For example if tm random returns 446 then 446 4 2. A call to a channel slot reservation function on a TM Client would return the reservation delay of 16 milliseconds to the caller i.e. 8 2 where each SLOT is 8 milliseconds . This information may also be used to update a reservation slot variable with the delay of 2 to the current Slot ID value. This is interpreted to mean that the first virtual connection in a reservation queue maintained by the TM Client has a packet to send in 16 milliseconds i.e. 2 SLOTs from now. At that time a subsequent call to the reservation function will see that the reserved time has arrived and return a 0 to the caller who will then send the message. A send probability value may be set as follows 

 2 In Modes 1 and 2 its value is received directly from info packets broadcast by the TM Server . This value supersedes any older send probability value. In Mode 3 a probe packet is sent as soon as possible to generate a corresponding ACK packet.

 3 Every ACK packet contains an update of the send probability value. This value supersedes any older send probability value.

 4 Every time a sent packet acknowledgement times out i.e. the packet is not acknowledged the send probability value is halved minimum of 1 . Every second which passes without a send probability being halved in this way send probability is caused to be increased by 100 to a maximum value of 16 387.

As described above Mode 1 generates a broadcast stream of info packets requiring about 0.03 Kbps of downstream bandwidth. Mode 2 requires 0.27 Kbps. Depending on the mode the client should start listening for info packets as soon as possible after a true socket is available.

This section presents an exemplary implementation of a random number generator i.e. tm random and tm randomize suitable for calculating message delays and other needs of various exemplary embodiments of the present invention 

tm randomize should be called exactly once on startup where is the uint32 4 byte value of the IPv4 address of the STB.

This section describes an exemplary method of client clock synchronization suitable for various exemplary embodiments of the present invention.

Info packets broadcast from the TM Server may contain timing synchronization information along with send probability updates. The timing information may come in two fields of the info packet 

SynchSecond 1 byte A value in the range 0 . . . 119 identifying which second the packet was sent according to the server clock. The value is an offset from the start of the most recent even numbered minute.

SynchPhase 1 byte The number of SLOTs 0 . . . 124 that elapsed in the synchronizing second before sending this packet.

For example if the time at which an info packet is constructed on the TM Server is 13 23 43.728 GMT including fractions of a second the value assigned to the SynchSecond field would be 103. Since the minute value 23 is odd the most recent even numbered minute is 22. The offset from 13 22 00 to 13 23 43 is 01 43 or 103 seconds.

Once the second has been identified the remaining 0.728 seconds 728 milliseconds must be accounted for. Assuming one SLOT occupies 0.008 seconds 8 milliseconds the number of slots is calculated as 728 8 91. This value e.g. 91 is assigned to the SynchPhase field in the info packet.

Every time the TM Client receives an info packet it may perform the following steps to update a time offset msec variable 

 1 Calculate the current date time GMT as seconds since some epoch in floating point variable current time.

 2 Calculate the time defined by the two fields found in the info packet GMT as seconds since the same epoch in floating point variable synch time.

 3 Calculate the floating point variable synch offset msec synch time current time 1000 the latest offset in milliseconds.

 4 Set time offset msec time offset msec 0.9 synch offset msec 0.1. This generates a slow moving average of the time offset as conditions change over time. Prior to receiving any info packets initialize time offset msec to zero 0 .

In Mode 1 it may not be necessary to update time offset msec with every info packet. One update per minute may be sufficient. In Mode 2 this time offset may preferably be calculated for every info packet.

Whenever the current time is referenced it may be assumed to be the current time of the STB GMT plus the time offset msec value scaled to the appropriate time units i.e. time offset msec 1000 if current time is reported in seconds .

As shown in these figures standard stream socket API calls may have virtual equivalents e.g. send becomes vsend enabling programmers to use stream sockets in a familiar way. A small number of additional functions may exist as indicated by the shaded blocks. According to various exemplary embodiments of the present invention all upstream traffic may preferably be scheduled based on the current load conditions of the network. When the network load is low data may be sent quickly if not immediately. When network load is higher there may be delays. The TM Client can estimate the expected delay from the return value of vreserve and decide how to proceed or not proceed and use the intervening cycles for other tasks.

The indicated API calls may be provided via a static library and prototyped in a C header file .h . Such a header file may define macros that map the traditional stream socket APIs to their Traffic Management analogues. Such macros would allow existing socket code to work with Traffic Management systems and methods requiring only minor changes for scheduling of send calls and asynchronous processing.

The following paragraphs provide detail of the virtual calls depicted in in the context of the disclosed steps. For every API call described parameter and return types would typically reference the same types used in the traditional socket APIs thus they are not described here. Names beginning with the letter v are specific to the systems and methods described herein. It is assumed that the standard i.e. non TM socket APIs are also available such as htonl ntohl getaddrinfo etc. Other traditional socket APIs may not have analogs when using Traffic Management such as poll select and shutdown .

After the process starts step the first call may be vsocket to allocate a virtual socket descriptor step . The descriptor returned by this function may be used for subsequent calls to vlisten vbind vsend and other functions. Implementation in a TM Client may include allocating a new empty virtual socket structure in a virtual socket table and returning a table index of that entry as a virtual socket descriptor. In Mode 2 this call may also cause the opening and initialization of tm socket so it can begin to listen for info packets.

In the next step the requesting node may call vconnect step to connect a virtual socket to a TM Server . Relevant parameters to pass to vconnect may include the virtual socket descriptor and the IP address and port number of the server to connect to. If the calling program wishes to specify what local port is used for the connection a call to vbind may be made not depicted in . vbind may associate a specified virtual socket descriptor with the local IP address and an explicit local port number. The virtual socket s port number may be stored in a virtual socket structure table entry indexed by the virtual socket descriptor. If vbind has not been previously called on the socket descriptor the indicated socket descriptor may be automatically bound to the local node s IP address and a random local port. This should be acceptable if the calling node is not a server since the node likely does not care which local port is used. Once the virtual socket is connected vsend and vrecv can be called as needed to send and receive data. The server s host IP address and port may be stored in a virtual socket table for a virtual socket structure indexed by sockfd.

Next a call to vreserve step may be made to make a reservation to send data upstream. vreserve may accept a virtual socket descriptor vsockfd and return the number of milliseconds to wait before sending a message using vsend . When vreserve returns zero i.e. ready to send Yes step the message can be sent with minimal probability of being blocked. While waiting for a reserved slot the calling program may perform other tasks. vreserve may be called periodically to check on reservation status.

In the next step vsend may be used to send a message to the server previously connected via the virtual socket returned by the vsocket call step . vsend may accept parameters as input indicating the connected virtual socket descriptor vsock a pointer to a buffer contain the data to be sent buf and the number of bytes to send len . It may return the number of bytes actually sent. Similar to the traditional send call depicted in step vsend is a synchronous i.e. blocking call and may return before all data has been sent i.e. sent all No step . Thus vsend may be called repeatedly in conjunction with additional calls to vreserve steps and . For large messages or when the network is heavily loaded this will likely be the case.

If vsend is called and no reservation is found for the specified virtual socket vsend may call vreserve internally and block i.e. sleep until the reservation arrives and then begin sending. If vsend is called without a reservation or before vreserve returns zero vsend may block until it can send data. In such cases the intervening time possibly several seconds will be wasted.

Upstream message scheduling may occur on the first call to vreserve for a given message or on a call to vsend when no reservation has been made for the virtual socket in question. The vsend API may not actually send data for a virtual socket until a a reservation exists in a reservation table for that virtual socket and b the Slot ID in reservation slot matches the current Slot ID. The message scheduling process may function as follows.

 1 Calculate the Slot ID of the current time where current slot id  of whole seconds since 01 01 00 00 00 00 125 fraction of current second 0.008.

 2 If vsocket descriptor is found at position X in a reservation queue set queue pos X and skip to step . Otherwise set queue pos size of reservation queue 1.

 3 If queue pos 1 i.e. the queue is empty set reservation slot current slot id tm random 131072.0 send probability 0.5 .

Referring back to after all data has been sent using vsend i.e. sent all Yes step a call may be made to vrecv to receive a message from a server via a previously connected virtual socket step . The vrecv call step may accept input parameters indicating the connected virtual socket descriptor vsock a pointer to a receive data buffer but and the size of the receive data buffer in bytes len . It may return the number of bytes actually received or zero 0 if the virtual connection to the server closes for any reason. Since the number of bytes actually received may be less than the size of the buffer vrecv may be called repeatedly until the entire message has been received i.e. received all Yes step . If all the data has not been received i.e. received all No step and the connection with the server was not closed i.e. connect closed No step vrecv may be called again step .

The implementation of vrecv may be similar to vsend except that in the vrecv case the TM Client must generate the appropriate ACK packets and send them back to the TM Server . Also because vrecv can operate asynchronously it should return to the caller after VRECV WASTE milliseconds have elapsed without receiving a data packet from the TM Server or when the provided receive buffer has been filled.

Returning to if all the data has been received i.e. received all Yes step or it is determined that the server connection was closed i.e. connect closed Yes step the client node may call vclose to close the virtual socket step and the process stops step . Since there may be a small number of available virtual sockets it is important to close them when not needed. Implementation of vclose may entail freeing a Virtual Socket structure in a Virtual Socket Table indexed by vsockfd.

The synchronous blocking HTTP transaction depicted in may be sufficient for many applications that do not need to do additional processing while waiting for a response to be received. But for applications which must perform other tasks while waiting for data an asynchronous non blocking callback mechanism may be implemented as depicted in . This asynchronous mechanism may be different from the known select API but may be much simpler to use and to implement.

Referring now to the initial steps are identical to those in . Upon starting the process step vsocket is called to allocate a virtual socket descriptor step . Then vconnect is called step to connect a virtual socket to a TM Server .

Next vasynch is called to associate a virtual socket with an asynchronous callback function step . This function may accept a virtual socket descriptor vsockfd acquired via vsocket and a reference to a callback function that references vsockfd a message code msg and any activity specific information data . All future activity on the virtual socket e.g. errors reservations received data etc. will invoke the callback function and specify an appropriate msg code identifying the activity. Msg codes may be 

Implementation of vasynch may entail storing the virtual socket s callback function in the virtual socket structure table entry indexed by the virtual socket descriptor. If a connection has been established and a message sent upstream ConnID in the packet header may be used to identify the receiving virtual socket. For alert messages the header space for ConnID and SeqNum may be co opted by the TM Server and used to store a port number. This port may be used to identify the bound socket to receive the alert see the Packet Formats section below for more information about ConnID SeqNum and packet headers .

Continuing with after vasynch is called step a call to vreserve may be made to make a traffic reservation on the indicated socket step and the process stops step .

As shown in the sending and receiving processes are asynchronous relying on a callback function i.e. my callback to start each process. The dotted arrows in the figure denote periods of time when other tasks may be performed between callback activations. Specifically the process of sending data starts step with a callback function indicated by my callback step followed by a call to vsend step and then the process stops step . The process for receiving data is similar. It starts step with a callback function i.e. my callback followed by the same data receiving steps described in . vrecv may be called step until all data is received step or the server connection is closed step then a call to vclose step may be made and processing stops step .

Two additional functions not depicted in or that may be useful include vlisten and accept . The vlisten call may be used by a node needing to act as a server to listen for incoming connections on an indicated i.e. passed in bound virtual socket vsockfd . Such functionality may be appropriate for nodes waiting for a connection from an application server e.g. for alert messages . Before calling vlisten calls to vsocket and vbind should be made to allocate the virtual socket and define the port to listen on respectively . Because synchronous listening may make no sense in many applications e.g. STBs in an OOB cable network a call to async may be needed to make the virtual socket asynchronous. The vlisten call will return immediately and the asynchronous callback function will be invoked with a VGOTCONN message when a connection arrives. A vaccept call may be defined and used to accept such new connections. Any asynchronous virtual socket bound to a port via vbind will receive VGOTCONN messages when a connection arrives. The vaccept call may accept the connection and return a new synchronous virtual socket descriptor that can be used for sending and receiving data. The original listening socket descriptor vsockfd may remain unchanged and still listening. The virtual socket descriptor returned by vaccept may be closed using vclose when no longer needed.

In another exemplary embodiment of a virtual socket implementation of HTTP according to an exemplary embodiment of the present invention the reservation functions provided by vreserve may be included in vsend and vrecv and a vreserve call not provided. In this embodiment in addition to vsend and vrecv returning the number of bytes sent or received respectively each call would transparently make a bandwidth reservation and return a negative number representing the number of milliseconds until a send or receive slot is available in cases when data cannot be immediately sent or received. This may be advantageous for use in blocking implementations of vrecv by allowing additional processing to occur on a client node while waiting to receive data in a way similar to that shown in the vreserve vsend loop depicted in . It may also simplify the API by eliminating one non standard call vreserve .

To manage multiple kinds of traffic on a single pipe an exemplary embodiment of the present invention may define four distinct message packet types as specified below in relation to . These may comprise the TM UDP protocol indicated in .

PacketType 1 byte A constant that identifies a valid TM packet and may specify which of the four types it is. All TM PacketType values may contain the high order 6 bits 0xA8 which can be used as a filter mask. The low order 2 bits vary by packet type. For Info packets these bits are 0x00 making the value for Info packet PacketType 0xA8 0x00 0xA8.

SynchSecond 1 byte A value in the range 0 . . . 119 identifying which second the packet was sent according to the server clock. The value may be an offset from the start of the most recent even numbered minute.

SynchPhase 1 byte The number of SLOTs 0 . . . 124 that elapsed in the synchronizing second before sending this packet.

SendProb 2 bytes A 16 bit number that may be used to calculate the random delay imposed on every outgoing packet.

PacketType 1 byte A constant that identifies a valid TM packet and may specify which of the four types it is. For Data packets the PacketType value is 0xA9.

ConnID 1 byte A unique connection ID corresponding to a virtual socket associated with every virtual connection.

SeqNum 1 byte The value of this field may depend on the position of the packet in the message see the Flags field as follows 

 1 IF this is the first packet in the message AND the number of packets is 1 this value is the total number of packets in the message 1 . . . 256 minus 1 this means the maximum upstream message size is 256 MT 4 or 61 440 bytes .

 2 ELSE IF this is the last packet in the message this value defines the size of this packet s payload 1 . . . MT 4 minus 1 this means the receive does not know the actual message size until receiving the last packet but an estimate based on packet count wastes at most MTU 5 bytes or 0.39 .

For server originating messages i.e. alerts an IP address may be used to identify a STB . The header space for ConnID and SeqNum are combined to provide 2 bytes where the TM Server may store a port number. The port is then used to identify which virtual connection receives the alert based on port binding. Alert messages are identified via the Alert bit in the Flags field . The entire alert message must fit within downstream MTU 5 bytes.

Regarding the minus 1 in the definitions of SeqNum an exemplary message always consists of between 1 and 256 packets but one byte represents integers between 0 and 255. So the SeqNum value is always one less than the value it represents.

PacketType 1 byte A constant that identifies a valid TM packet and may specify which of the four types it is. For ACK packets the PacketType value is 0xAA.

ACKBits 1 byte Acknowledgement of up to 8 packets after packet SoFarCt. As with all transmitted data it may be in network byte order with the first packet after SoFarCt acknowledged by the high order bit and the 8th packet after SoFarCt acknowledged by the low order bit.

SendProb 2 bytes A 16 bit number used to calculate the random delay imposed on every outgoing packet.

 1 A Virtual Socket Structure as depicted in may contain all information needed to implement a virtual stream socket. This may include state information port binding asynchronous callback information remote host IP and port and information about current messages being processed .

 2 A tm socket variable may contain an actual datagram socket descriptor bound to port VPORT used for listening for incoming messages. This socket should remain open as long as the client node agent or application e.g. EBIF User Agent is active if possible. Otherwise the socket may be opened and initialized as needed. Typically this socket is created via standard socket APIs. However in some implementations other socket acquisition APIs may be provided.

 3 A send probability variable may contain a value representing the send probability used by vreserve to schedule packet transmissions via vsend . Rather than representing the probability as a floating point number i.e. a value between 0 and 1 this value may be more conveniently represented as the product of the send probability and 65 536 rounded up to the nearest integer. The value contained in this variable may be sent out by the TM Server in both Info and ACK packets .

 4 A Virtual Socket Table as depicted in may be a table i.e. array list etc. containing S virtual socket structures where S is implementation specific based on available space but is at most 16 according to the current exemplary embodiment. A socket ID is an index e.g. 1 . . . S or 0 . . . S 1 into this table as is the ConnID i.e. connection ID field in TM packet headers. Socket ID may also be known as a Virtual Socket Descriptor.

 5 A verrno variable may be used by TM virtual sockets to report errors to programmers. This variable may be in addition to the errno variable used by standard socket APIs.

 6 A Reservation Queue as depicted in may be an array queue of reservations each identifying a virtual socket that has a packet to send. Whenever a new packet is added to the Reservation Queue or the first packet in the queue is sent the time to send the next packet in the queue may be calculated and stored in reservation slot. This calculation is given in the definition of reservation slot below where current slot ID refers to the Slot ID at the time of calculation.

 7 A reservation slot variable may be the Slot ID or equivalent in which the next packet is to be sent see Reservation Queue . Whenever a new send probability arrives at the client via either ACK or Info packets the value of reservation slot must be re calculated. reservation slot current slot ID  random 131072.0 send probability 0.5 

 8 A time offset msec variable may indicate a time offset that when added to the internal clock time GMT on a client node e.g. STB synchronizes the time with the TM Server and with other client nodes. Accurate times are needed to correctly calculate Slot IDs and for other capabilities that may be required.

The following paragraphs provide more detail about an exemplary embodiment for implementing the guaranteed delivery of messages. First details are provided about vsend in the context of Mode 1 assuming a reservation already exists for the specified virtual socket and the reserved Slot ID has arrived. Then there is a description of additions that may be necessary for the other modes.

As noted above since vsend like its BSD socket counterpart send is not guaranteed to send the entire message in one call it will block until the message or partial message is sent. This may not be an issue with send . But since various exemplary embodiments of the present invention may spread message packets out over time to avoid slot collisions an application could be blocked for many seconds waiting for vsend to return. For this reason vsend will return control back to the caller if the reservation time for a data packet is large. When the entire message is not sent vsend may create a reservation for the next vsend call based on the current send probability value.

In one exemplary implementation vsend may break the message down into packets of MTU 5 bytes or less for the last packet and send the packets as scheduled by vreserve . Processing vsend vsocket descriptor message ptr message length may occur as follows 

 6 IF the message currently in progress for vsocket descriptor is in send state and its message buffer contains message ptr message length skip to step 8.

 7 Initialize the message progress data for vsocket descriptor in the Virtual Socket Table see discussion associated with .

 8 IF all packets of the message have not yet been sent once construct the next packet to send update the virtual socket structure accordingly and skip to step .

 9 If all sent packets have not yet been acknowledged re construct the lowest numbered unacknowledged packet setting the resend flag bit update the message structure accordingly increment resend count and skip to step .

This definition of vsend assumes that the receipt of both Info and ACK packets are being handled via asynchronous signaling interrupts while vsend is processing. In particular it assumes that Info packets are updating send probability and reservation slot and that ACK packets are updating the SoFarCt and ACKBits fields in the virtual socket structure defined by ConnID . There is no need for every data packet to be acknowledged by an ACK packet . In fact the frequency of ACKs may be inversely proportional to the rate of change in server load i.e. send probability and the rate of reception of out of sequence packets. Ideally under light load an ACK may be sent for every 4th received packet. In various exemplary embodiments the only requirement may be that an ACK packet must be generated for both the first and last packet of a message.

In Mode 2 nothing in the above changes except that tm socket is not allocated and bound to port VPORT until a virtual socket is opened with vsocket and vreserve will block until an Info packet is received to define send probability .

In Mode 3 vreserve and or vsend must send a Probe packet for the message assuming send probability is the Default Send Probability value and receive an ACK packet for the probe providing an actual Send Probability value before proceeding. Failure to receive this probe s ACK before a short timeout e.g. 3 seconds causes the call to fail and set an appropriate error in verrno.

The following paragraphs further describe an exemplary embodiment of a Virtual Sockets implementation.

Traffic Management according to various exemplary embodiments of the present invention may endeavor to satisfy the following requirements using a single datagram socket i.e. the tm socket descriptor 

 3 Listen for alert messages that are unicast from application servers to the client node e.g. STB . This is true in all three modes. The only difference is how long a datagram socket is held upon the start of listening.

The TM Client APIs other than vsend and vrecv may not do anything with tm socket. Instead they may modify information in the Virtual Socket Table or Reservations Queue that are subsequently used when sending or receiving data packets. For example when an ACK packet or Data packet is received header fields in the packet may be matched with data in the Virtual Socket Table to determine which socket and thus which callback function is the actual recipient of the packet.

The assigned tm socket is bound to VPORT i.e. and listens for incoming traffic via the signaling mechanism. The packet type field of incoming packets is examined both to filter out non TM packets and to appropriately handle the various packet types that might be received. Info packets may be handled internally by the TM Server and TM Client as disclosed in the Traffic Reporting and Client Clock Synchronization sections. ACK and Data packets other than alerts may contain a ConnID field an index into the Virtual Socket Table that identify the virtual socket connection to which the packet belongs. Alert message may be identified by a Flag field bit Alert and the ConnID and SeqNum bits are combined to define a 2 byte port number the virtual socket bound to that port number is the recipient of the message.

The TM Client may rely on asynchronous I O signaling SIGIO to invoke the various TM internal routines when there is activity or errors on the tm socket socket. For example the TM asynchronous callback function associated with a virtual socket via vasyncO may be invoked by signal handlers.

Various exemplary embodiments for the sending and receiving of data are described in more detail in the Guaranteed Delivery of Messages section.

One idiosyncrasy of the cable OOB networking context is that 99 of the traffic may consist of HTTP messages. Thus a simple static compression method can generate significant savings. A static table based scheme similar to the following may be employed to compress all messages prior to transport. Decompression should be straightforward.

This compression scheme is intended to not corrupt binary data. Thus it may also be appropriate for non HTTP protocols that may not always send text data.

The interface between the TM Server which handles actual communication with TM Clients and the bandwidth regulation code may be implemented using a single API call 

Whenever the TM Server receives a packet from any client it should call got packet immediately. The size of the packet i.e. header payload may be passed in the size parameter. If the sequence number of the packet is not the expected value i.e. the previous sequence number 1 then a non zero TRUE value may be passed in isOutOfOrder otherwise a 0 FALSE value may be passed in this parameter.

The return value of got packet may be either 0 or a positive non zero integer. If the return value is 0 no further action is required. However if the value is non zero the return value should be used as the new Send Probability and be communicated to all TM Clients via ACK and Info packets .

In an exemplary embodiment the Send Probability value may be the ratio between a target bandwidth per HFC Node and the expected bandwidth per HFC Node for the immediate future. The target bandwidth is simply the configured maximum bandwidth per HFC Node multiplied by a small adjustment factor e.g. 0.9 to keep the result just below the maximum. The expected bandwidth is the number of active STBs i.e. client nodes in each HFC Node that will be sending more packets. This may be estimated as 2 P SP where P is the calculated average number of packets received per SLOT and SPis the previous value of Send Probability.

Additionally an adjustment factor K may be periodically calculated to account for inaccuracies in system configuration values and occasional transient effects.

The following paragraphs describe an exemplary method for determining the broadcast frequency of Info packets .

The frequency of Info packet generation may be driven by the return value of got packet . Whenever this function returns a non zero value a new Info packet should be broadcast. Care should be taken to prevent too frequent or spurious generation of Info packets broadcasts.

Specifically got packet may return a non zero send probability value to disseminate via another Info packet broadcast whenever 

 b At least 0.5 seconds have passed since the last Info packet broadcast AND out of order packets are being seen OR

 c At least 0.5 seconds have passed since the last Info packet broadcast AND the packets second rate is trending upward.

This section presents a description of a simulation of an exemplary embodiment of the bandwidth regulation method according to an exemplary embodiment of the present invention. A description of how the simulation was conducted is presented along with a description of the results as depicted in 

The simulation includes a modeling of a QPSK network wherein a TM Server communicates with multiple TM Clients via the network. The QPSK network model monitors the upstream and downstream throughput and throws away packets in either direction that exceed bandwidth limits.

As the throughput approaches the theoretical bandwidth limit the probability of packet loss approaches 1. Letting the theoretical maximum packet throughput upstream be 1 e 256 000 bps 94 000 bps and assuming the aggregate size of all upstream packets including TM headers and 28 bytes of UDP IP headers is S bytes or s S 8 bits then the probability of the next packet being lost and totally ignored is given by P s s 94 000 . Programmatically the following IF condition tests for this 

In the simulation results depicted in a large configurable number of STBs and HFC Nodes were simulated for all 125 SLOTs in each second of a configurable test period there set to 1 hour.

Embodiments or portions of embodiments disclosed herein may utilize any of a wide variety of technologies including a special purpose computer a computer system including a microcomputer mini computer or mainframe for example a programmed microprocessor a micro controller a peripheral integrated circuit element a CSIC Customer Specific Integrated Circuit or ASIC Application Specific Integrated Circuit or other integrated circuit a logic circuit a digital signal processor a programmable logic device such as a FPGA PLD PLA or PAL or any other device or arrangement of devices that is capable of implementing any of the steps of the processes described herein.

It is to be appreciated that a set of instructions e.g. computer software that configures a computer operating system to perform any of the operations described herein may be contained on any of a wide variety of media or medium as desired. Further any data that is processed by the set of instructions might also be contained on any of a wide variety of media or medium. That is the particular medium e.g. memory utilized to hold the set of instructions or the data used in the embodiments described herein may take on any of a variety of physical forms or transmissions for example. Illustratively the medium may be in the form of a compact disk a DVD an integrated circuit a hard disk a floppy disk an optical disk a magnetic tape a RAM a ROM a PROM a EPROM a wire a cable a fiber communications channel a satellite transmissions or other remote transmission as well as any other medium or source of data that may be read by a computer.

It is also to be appreciated that the various components described herein such as a computer running executable computer software may be located remotely and may communicate with each other via electronic transmission over one or more computer networks. As referred to herein a network may include but is not limited to a wide area network WAN a local area network LAN a global network such as the Internet a telephone network such as a public switch telephone network a wireless communication network a cellular network an intranet or the like or any combination thereof. In various exemplary embodiments a network may include one or any number of the exemplary types of networks mentioned above operating as a stand alone network or in cooperation with each other. Use of the term network herein is not intended to limit the network to a single network.

It will be readily understood by those persons skilled in the art that the present invention is susceptible to broad utility and application. Many embodiments and adaptations of the present invention other than those herein described as well as many variations modifications and equivalent arrangements will be apparent from or reasonably suggested by the present invention and foregoing description thereof without departing from the substance or scope of the invention.

While the foregoing illustrates and describes exemplary embodiments of this invention it is to be understood that the invention is not limited to the construction disclosed herein. The invention can be embodied in other specific forms without departing from its spirit or essential attributes.

