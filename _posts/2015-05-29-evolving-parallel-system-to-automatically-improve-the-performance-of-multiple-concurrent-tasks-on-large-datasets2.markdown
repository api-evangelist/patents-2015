---

title: Evolving parallel system to automatically improve the performance of multiple concurrent tasks on large datasets
abstract: We describe a high-level computational framework especially well suited to parallel operations on large datasets. In a system in accordance with this framework, there is at least one, and generally several, instances of an architecture deployment as further described. We use the term “architecture deployment” herein to mean a cooperating group of processes together with the hardware on which the processes are executed. This is not to imply a one-to-one association of any process to particular hardware. To the contrary, as detailed below, an architecture deployment may dynamically spawn another deployment as appropriate, including provisioning needed hardware. The active architecture deployments together form a system that dynamically processes jobs requested by a user-customer, in accordance with customer's monetary budget and other criteria, in a robust and automatically scalable environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09558036&OS=09558036&RS=09558036
owner: BigML, Inc.
number: 09558036
owner_city: Corvallis
owner_country: US
publication_date: 20150529
---
The present application is a division of U.S. patent application Ser. No. 13 673 502 now U.S. Pat. No. 9 098 326 issued Aug. 4 2015 which is a non provisional of and claims priority to U.S. Provisional Application No. 61 557 826 filed Nov. 9 2011 and which is a non provisional of and claims priority to U.S. provisional application 61 557 539 filed Nov. 9 2011 all of which are incorporated herein by this reference.

This invention pertains to computer implemented methods for an evolving parallel system to automatically improve the performance of multiple concurrent tasks such as decision tree model building and predictions conducted on large datasets for multiple customers.

 2011 2012 BigML Inc. A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever. 37 CFR 1.71 d .

Decision tree learning used in statistics data mining and machine learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item s target value. More descriptive names for such tree models are classification trees or regression trees. In these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels.

In machine learning building a model or decision tree based on a large data set can take a long time. Further the time and resources necessary to build a model increases as the required quality or depth of the model increases.

In the last few years virtualization has contributed to easily create auto scaling applications. Most auto scaling implementations are limited to adding up extra hardware resources and replicate exactly the same software again and again. The structure of the tasks these systems solve are the same on overload conditions. New requests just come at a higher intensity so more resources are added momentarily until the load goes back to normal and then additional resources are disabled.

The criteria used to auto scale in the past are mostly concerned with hardware monitoring parameters like CPU utilization band width consumption free memory or using metrics computed using the number of requests that are being received or pending to answer.

The following is a summary of the invention in order to provide a basic understanding of some aspects of the invention. This summary is not intended to identify key critical elements of the invention or to delineate the scope of the invention. Its sole purpose is to present some concepts of the invention in a simplified form as a prelude to the more detailed description that is presented later.

The present disclosed technology will make it easy and fast for customers to input data to form datasets create models of their datasets and generate predictions based on those models utilizing a robust scalable architecture that will comply with customer monetary budgets and other selectable criteria and automatically optimize performance across jobs and customers.

Our system in a preferred embodiment uses additional criteria to improve the overall performance of a computational system in an autonomous way taking into consideration economic performance business goals cost of additional infrastructure budget etc and quality of service.

Some portions of the detailed descriptions which follow are presented in terms of procedures logic blocks processing steps and other symbolic representations of operations on data bits within a computer memory. These descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. A procedure logic block process etc. is generally conceived to be a self consistent sequence of steps or instructions leading to a desired result. The steps require physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated in a computer system. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits bytes words values elements symbols characters terms numbers or the like.

It should be born in mind that all of the above and similar terms are to be associated with the appropriate physical quantities they represent and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussions it is appreciated that throughout the present invention discussions utilizing terms such as processing computing calculating determining displaying or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

Note that the invention can take the form of an entirely hardware embodiment an entirely software firmware embodiment or an embodiment containing both hardware and software firmware elements. In a preferred embodiment the invention is implemented in software which includes but is not limited to firmware resident software microcode etc. Furthermore the invention can take the form of a computer program product accessible from a computer usable or computer readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description a computer usable or computer readable medium can be any apparatus that can contain store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device.

Decision tree learning used in statistics data mining and machine learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item s target value. More descriptive names for such tree models are classification trees or regression trees. In these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels.

In machine learning building a model or decision tree based on a large data set can take a long time. Further the time and resources necessary to build a model increases as the required quality or depth of the model increases. Approximate histograms have been used in building decision trees. An approximate histogram was introduced by Messrs. Ben Haim and Yom Tov J. Machine Learning Research 11 2010 849 872 . The histogram is built in a streaming fashion and acts as a memory constrained approximation or compression of the entire dataset.

Tyree et al. extend the histogram so that it approximates the relationship between two numeric fields. WWW 2011 Session Ranking Mar. 28 Apr. 1 2011 Hyderabad India at 387. Of course a brute force approach of applying ever increasing resources to the problem using known parallel and distributed processing techniques can be useful. Still the need remains for more effective methods to build decision trees quickly and to better support classification problems.

Additional aspects and advantages of this invention will be apparent from the following detailed description of preferred embodiments which proceeds with reference to the accompanying drawings. The invention is not intended to be limited by the drawings. Rather the drawings merely illustrate examples of some embodiments of some aspects of this disclosure.

We describe a high level computational framework especially well suited to parallel operations on large datasets. In a system in accordance with this framework there is at least one and generally several instances of an architecture deployment as further described. We use the term architecture deployment herein to mean a cooperating group of processes together with the hardware on which the processes are executed. This is not to imply a one to one association of any process to particular hardware. To the contrary as detailed below an architecture deployment may dynamically spawn another deployment as appropriate including provisioning needed hardware. The active architecture deployments together form a system that dynamically processes jobs requested by a user customer in accordance with customer s monetary budget and other criteria in a robust and automatically scalable environment.

Referring now to the drawings is a simplified diagram illustrating a prior art scalable system that scales in response to monitoring loading on the system as mentioned in the background. is a simplified conceptual diagram illustrating a scalable system in accordance with an embodiment of the present disclosure. Here the system includes performance goals and customer budget as inputs to consider in operating and scaling the system. Additionally in a preferred embodiment the current pricing costs of additional computing resources also may be taken into account. Resources may include processors computers systems memory I O bandwidth etc. In some cases computing resources may be provisioned from one or more cloud vendors as further discussed later. The Load input to the system typically comprises one or more job requests from users or customers we use the terms interchangeably . Examples of various job requests are described in more detail below. In general a job may comprise any request from a user for a response.

Another user in called a Prediction User seeks obtain a prediction based on a specified or input dataset and corresponding model. Once a decision tree or model has been created from a given data set users can access it to make predictions either interactively or submitting batch queries. As is the case with model creation this activity can be performed via for example a webapp or a RESTful API both of them accessible only after proper authentication. Preferably accounting is also provided to collect revenue from customers for these services.

In a system in accordance with the overall architecture disclosed herein there is at least one and generally several instances of an architecture deployment running at any one time. We use the term architecture deployment herein to mean a cooperating group of processes together with the hardware on which the processes are executed. Preferably each architecture deployment is implemented as a set of very loosely coupled processes distributed and communicating via a blackboard collecting pending tasks. is a simplified graphical representation of an architecture deployment instance in accordance with an embodiment of the present disclosure including software processes and allocated hardware resources. In some of the drawings further described below we have assigned planet names to each type of process in an analogy to a planetarium or solar system in which various processes orbit around one or more supervisor processes. A supervisor may implement a central blackboard. In other embodiments some individual processes may maintain their own blackboards as alternative to or in addition to a central blackboard.

Typically we expect to have many instances of a given planet type of process up and running at any given moment. The blackboard may act as a repository of pending tasks where planets can find open problems to solve that fit their capabilities. Every planet kind again every process type behaves as a specialized agent solving a particular and usually quite narrowly defined kind of problem and can work in an autonomous way.

In that way each planet is immediately replaceable in case of failure and a completion log in the blackboard may be used as a re synchronization mechanism in case any rollback is needed. Some processes will try to complete as many jobs of a given class as possible while those serving external users e.g. file uploaders or model evaluators will be dedicated to a single task instance as long as needed. In some embodiments planets in the former group operate in a totally asynchronous manner driven by requests popping up in the blackboard while those attending external user requests need to respond synchronously to them. In an alternative arrangement a central supervisor process may implement logic to direct a user request or a specific task to directly to an appropriate process planet .

Referring again to we can roughly classify representative processes as three kinds. There may be others 

In some embodiments all services record their status periodically in Helios in the form of heartbeats. That way they can be monitored by MountTeide. In alternative embodiments a status query may be sent to each process periodically.

In an embodiment a blackboard may be implemented as a distributed MongoDB instance. It contains not only the lists of pending tasks but also completion and system logs and accounting records. In some embodiments Helios may be accessed by every other planet directly using appropriate language bindings built on top of MongoDB. Data may be stored in GridFS with pointers in the blackboard. In some embodiments there is one master blackboard per architecture deployment with a slave instance for failover. MongoDB provides a failover strategy since it allows master slave configurations with the slave taking over and configuring itself in case of master s failure. In some embodiments suggested implementation technologies may include MongoDB GridFS access libraries on top of PyMongo.

 MountTeide is the supervisor process coordinating the system and monitoring its health. More concretely MountTeide 

Thus in one embodiment there is one active MountTeide instance per architecture deployment preferably backed by a failover instance. In some embodiments suggested implementation technologies may include Python on top of Tornado. Managing processes running on a given node can be accomplished using local Supervisor instances.

In alternative embodiments since all planets have to query periodically Helios for new messages one could use the logs of this activity as heartbeats. This technique would reduce database traffic but on the other hand it conflates the two logically independent activities. Planets offer a status REST API that could be directly used by MountTeide in lieu of the blackboard heartbeats. This would again reduce database traffic perhaps at the expense of accountability. In some embodiments health monitoring may be distributed by instantiating a MountTeide per node. The node instance would use status to monitor the local planets and put overall status reports in Helios. A SuperMountTeide would then manage global allocation policies.

Data may be stored in GridFS with pointers in the blackboard. There is one master blackboard per Planetarium system with a slave instance for failover. MongoDB provides a failover strategy since it allows master slave configurations with the slave taking over and configuring itself in case of master s failure.

In some embodiments Mars may be a purely asynchronous planet reacting to upload file tasks data upload request type messages appearing in Helios Pluto being the usual generator of those requests initiated by an authorized user via Pluto s UI . Upon completing an upload the data may be stored in the blackboard for example as a model raw data message usually to be consumed by a Mercury instance or a prediction request data message usually to be consumed by Saturn . The data uploader process Reacts to data upload request. Emits model raw data prediction request data. Of course message names and formats are merely illustrative. In some embodiments suggested implementation technologies may include Python on top of Tornado via a Big Bang planet template.

In an illustrative embodiment a data analysis process is provided to react to requests such as model raw data messages analyzing the corresponding data usually downloaded by Mars and serializing it in a binary format apt for the model builder processes. The data analysis may include the following steps 

In some embodiments the data analysis results may include data type confirmed per column format defined typing and summary stats per column e.g. average range standard deviation or the like. These summary statistics may be returned to the user.

A data analysis process preferably Reacts to model raw data or similar tasks. And it Emits a model data for example small model data big model data. In some embodiments the resulting model may be conveniently stored in a JSON format.

Suggested implementation technologies for a modeling process may include Python on top of Tornado via a BigBang planet template for interaction with the blackboard. Avro library Java for serialization. Avro is a known remote procedure call and serialization framework developed within Apache s Hadoop project. It uses JSON for defining data types and protocols and serializes data in a compact binary format.

Venus is the name given in the figure to a small decision tree builder process for modeling small datasets. This separate process is optional a single model builder could be used for all datasets. Upon completion of its job it registers the model in Helios ready to be used by the prediction frontend. In this small model builder decision trees are built on a single computer preferably the one running the big data model preview process using standard machine learning libraries. Reacts to small model data. Emits prediction model Suggested implementation technologies for this process may include Python on top of Tornado via a Big Bang planet template for interaction with the blackboard. The resulting prediction model may be expressed for example in JSON PMML or any suitable format.

A large dataset model builder process is provided named Jupiter in the figure . Importantly this process orchestrates distributed module building as further discussed below. Improved model building methods especially for larger datasets are described in our commonly owned application filed concurrently herewith entitled METHODS FOR BUILDING REGRESSION TREES IN A DISTRIBUTED COMPUTING ENVIRONMENT application Ser. No. 13 673 064 now U.S. Pat. No. 9 269 054. The contents of that application are incorporated herein and described with reference to .

In some embodiments an optional previewer process may be provided to show a big data model while it is still under construction. See Juno in . The application mentioned in the preceding paragraph discusses methods for producing partial results during tree building and how these previews can be used to generate graphical screen display output. A preview may be updated each time a model is updated. The actual frequency may depend on the size of the dataset. In some embodiments a model building process may be arranged to report its progress whenever requested and a separate previewer may be unnecessary.

A predictions process see Saturn is used to make predictions once a model is available. This process reacts to messages described as prediction request data messages usually but not necessarily emitted by Mars combined with the corresponding prediction model or prediction model preview message to generate the corresponding prediction result. Reacts to prediction request data prediction model prediction request data prediction model preview. Emits prediction result which may take various forms depending in part on the type of prediction requested.

In some embodiments individual interactive predictions may be implemented by various web forms and API functions that are able to make HTTP requests to a predictions process. Prediction results may be encoded for example in JSON. JSON JavaScript Object Notation is a known lightweight data interchange format. It is a text based open standard designed for human readable data interchange. JSON is suggested here for representing simple data structures and associative arrays called objects. Despite its relationship to JavaScript it is language independent with parsers available for many languages. Other formats may be used as well consistent with the present disclosure. Further prediction models may be exported to PMML Python and C code among other things.

Pluto is the name given in the drawing to represent a web frontend for the whole system configured for receiving user requests and handling synchronous interaction with them. Thus Pluto won t usually start tasks looking for them in Helios instead in will be waiting for task completion messages appearing in there. More concretely in some embodiments Pluto 

These tasks may be broken down in different web apps accessed from a common frontend. Details of such things are known. Emits data upload request. Reads prediction model preview prediction model prediction result. Suggested implementation technologies for the web interface may include Django on top of nginx.

In some embodiments a system of calls callbacks and status messages that enable interaction with the front end may be employed. In many cases asynchronous methods may be used for example by providing simple i.e. Waiting . . . responses to queries until the results of the asynchronous request are available.

Model creation in some embodiments is illustrated in the data and control diagram of . The diagram shows the main points as follows.

When a new high level job is started Pluto needs a way to obtain a unique ID to wait on. This may be done by Helios MongoDB directly providing it. In other embodiments MountTeide may be arranged to intervene. Further appropriate data retention policies should be included. For example after a dataset is produced for model generation the original uploaded raw data may be deleted subject to customer policies.

Alternatively for small data uploads Pluto can generate directly the prediction request data message bypassing Mars.

The inputs to the system may comprise an arbitrary set of requests of arbitrary type size priorities and budgets. Each request belongs to a customer. Each customer can generate an arbitrary number of requests. Each request has associated a budget that is the value of the maximum number of resources that can be spent to compute a response.

In an embodiment customers have allocated an arbitrary budget that can individually increase or decrease at their own convenience. The arrival rate lambda of requests is unknown but the architecture may use historical data to forecast it. Arrival rate forecasts may use overall arrival rate and are also segmented on a individual customer basis and type of requests. Preferably in operation the system accumulates data that reflect overall arrival rate of requests per customer arrival rate overall job size distribution per customer job size distribution and per customer budget. The system monitors these statistics and scales as explained below to maximize satisfaction over all current customers.

In an embodiment requests are mainly data analysis tasks that require heavy computation on a big dataset that comes with the request or it is referenced by the request but can also be small tasks like generating a set of predictions based on a previously computed model by another request or generating a status report. Preferably each request is processed by a job with a unique id. Each job is decomposed in a number of tasks that depend on the type of request. A job is completed once all the tasks that compose it have been finished satisfactory. The number of tasks that compose a job is arbitrary. Tasks can use other tasks output as input. A task becomes executable only if all the inputs it needs are present. Tasks are executed using the Actions provided by one or more of the architecture deployments.

Preferably each architecture deployment or implementation deals with a family of requests for a subset of customers. Each architecture implementation comprises or implements 

1. An set of Actions. An action is executed autonomously and asynchronously as soon as a task that requires such action gets all the inputs needed for its execution.

2. A set of Reactions. Reactions are executed upon user s request. A user can also be another architecture implementation instance .

3. A number of Goals that drive the instance behavior. For example when a situation arises where an autonomous decision needs to be made. Goals help each implementation to point in the right direction.

4. A set of Requirements that collect all the hardware operating systems services libraries and tools that are needed to execute Actions and Reactions.

6. A case based History repository where cases describing previous situations are store together with the performance results achieved. For example historical data may include resource usage budget and margin model quality information.

7. A Knowledge Base that describe a number of services configurations and APIs that are available to the implementation as well as the cost associated to each one.

8. An embedded monitor that tracks the execution times and Performance for each action and reaction for all the tasks that are being executed. A Gantt chart like structure preferably is used to easily determine the dependencies and parallelism between tasks. Performance charts for previous jobs also may be stored in the history repository which may be shared across architecture implementations. Performance data may include per job performance per customer performance and per action performance.

We now summarize some of the aspects introduced above with regard to a preferred embodiment and also introduce additional related aspects as follows 

 1 We have described a high level computational framework to optimize the parallelization and performance of cooperative distributed systems each of which we referred to as an architecture deployment. Each deployment was described as a system of planets processes .

 2 Each architecture deployment instance comprises a set of reactions a set of actions a set of constraints a set of goals a set of requirements a case base of past performances and a knowledge based to know how to operate on the computational environment.

 3 Each architecture deployment instance is self aware of the resources consumed both in terms of hardware and in terms of dollars and also the return it can achieve by accelerating certain tasks or the loses the system can incur if certain goals are not achieved or certain constraints are not respected.

 4 Each architecture deployment instance also may consider Quality of Service as an additional criteria. In certain circumstances a lower quality of output or process will be acceptable to improve on the time and money constraints while in other circumstances a lower quality is absolutely not acceptable. These may be set and adjusted by each user of the system.

 5 Each architecture deployment instance uses the set of goals and constraints to determine the best strategy for the replication distribution duplication or allocation of the tasks that need to accomplished for each request. These operations are described in more detail shortly with regard to .

 6 Each architecture deployment instance is capable of reacting to a different number of messages and also proactively start its own actions.

 7 Each architecture deployment instance constantly monitors the degree of parallelization and detects bottlenecks through continuous analysis of the gantt charts generated by the tasks performed by a set of processes planets working together.

 8 Each architecture deployment instance can automatically spawn new servers and instantiate each one with a new process that is capable of performing a subset of actions and proactive actions. This is further described below.

 9 Each architecture deployment instance can communicate through a distributed blackboard but other communication mechanisms could be used.

And the same instance provides Reaction web upload data which may be sync or asynchronous. is a sample screen display showing a series of processes conducted by an example deployment instance and their dependencies in a GAANT chart representation. is a sample screen display of a dataset summary of the type generated by a process such as sniff csv and or data avro gen . These are examples of processes for inspecting and analyzing input data as discussed earlier. is a sample screen display showing a history record for a job performed by the processes shown in . Using the above tools and data each architecture deployment instance or PASADA in some of the drawings can introspect how long each of the actions or reactions it implements takes in terms of time computational resources and economic value. As noted these may be compared to applicable budgets and other constraints.

Next we discuss in more detail how a representative system may operate consistent with the present disclosure. Each job user job or request can be done by performing a set of tasks. Each task may have an arbitrary number of key value inputs that are required to be in the system before the task can be started. Each task produces a number of key value outputs that might be required by other tasks. Many tasks can be decomposed in smaller sub tasks that can be distributed to other services.

 8 Although through complexity analysis an upper bound of the time require to perform tasks on the input can be found the total amount of time to complete a job is not deterministic as it depends on the load and resources allocated.

Scaling the system in a preferred embodiment may include one or more of the following operations or primitives defined below. An architecture deployment instance combines these four primitives to distribute or replicate actions to improve the performance of the system based on criteria computed through introspecting the jobs at hand and using historical data to find the best combination of resources for the current situation.

Auto Replicate is illustrated in . It replicates the set of actions reactions goals requirements constraints history and knowledge base and performance. The resulting two instances may operate on the same hardware resources as before the replication. For example the whole instance may be duplicated here except for the central supervisor process. It continues to oversee the whole system. Again it may keep the same infrastructure as before. In this scenario the hardware may have been under utilized. After the replication that same hardware can get more work done typically at no additional monetary cost .

Auto Distribute is illustrated in it partitions the actions in two disjoint sets. For example the drawing shows five actions to be processed. These get distributed three actions to one child and two actions to the other instance in this example. For example suppose one customer does not require predictions at this time but is merely inspecting visualizations. is an example of a decision tree visualization interface. Another customer may need intensive model building. A copy of the architecture instance is created that may be optimized for the actions required.

Auto Duplicate Duplicates the infrastructure and replicates all the actions see . For example if the current hardware is pegged operating at maximum throughput say with little or no remaining free memory then the instance is duplicated and new resources brought to bear.

Auto Allocate Increases the infrastructure see . For example the system might replicate a database server a process where more traffic or hits to the database must be accommodated to meet performance goals. In this case the process is duplicated to more machines along with copies of the database.

Further with regard to the master process creates or obtains access to an initial model for example the root of a decision tree which may be based on a subset of the full dataset or the entire dataset. In the master process distributes the initial model to each of the worker processes. In each of the workers processes the data of its corresponding partition dataset through the distributed model to form local results. The local results may be gathered and expressed in the form of a histogram as illustrated which serves to compress the local results. While only a single histogram per worker is shown for illustration in practice the worker generates a separate histogram for each input variable.

In the case of categorical data i.e. to construct classification trees we apply the concept of approximate histograms in a new way. We extend the histogram so that it can also represent a relationship between a numeric field and a categorical field. Our expanded histogram is further described below.

Referring now to each worker transmits its compressed results a series of histograms to the master process which in turn merges the results which may be stored in a merged histogram . In a simple example where the bins have the same median values histograms can be merged by simply summing the counts. Importantly the illustrated structure and process may be further distributed to additional levels of worker processes. At each level a worker reports results only to its immediate parent node. Each parent node merges those results and reports the merged histograms to the next level up and so on.

Returning to our example as illustrated in the master uses the merged results from its workers to update the model. In one embodiment the model is updated by growing an additional layer of a decision tree. Then the master distributes the updated model to each of the workers . Each worker then processes its respective partition dataset once again this time using the updated model. Results are gathered and reported. This process is repeated iteratively growing the tree model until a stop condition is met. In some embodiments one model update is generated for every iteration over the data.

We prefer not to stop a model building process on reaching an arbitrary depth of the tree. Rather we prefer to stop the process when a monetary budget constraint is met. A budget may be established in advance by a user customer for a given modeling project or dataset. The dollar cost of the request can be estimated by the current costs of CPU time and I O bandwidth. CPU time and I O bandwidth and storage are now fungible and can be obtained in a competitive cloud marketplace. We can also stop the process based on a running time limit or based on performance of the model on a holdout set of data. The constraints as well can be established in advance by a user customer for a given modeling project or dataset.

In a preferred embodiment multiple model updates can be generated during a single pass of the data. Put another way a small sample of the overall data is often sufficient to generate a good quality model update. Implementing an early model update can save substantial processing time. The question is at what point is the partial results data sufficient to make a profitable split.

We have discovered that a useful indication as to whether or not an early split should be done can be obtained comparing the workers respective results for example their approximate histogram results while they are still processing their partition datasets. Referring now to in one embodiment the master process can request a partial result while the workers are processing their datasets. Each worker responds with partial results preferably in the form of an approximate histogram reflecting the data processed to that point using the current model. The master compares the partial results. If the partial results are similar the master updates the current model and immediately sends it to the workers. The meaning of similar is described shortly. The workers then process their partition datasets over the updated model and continue iteratively as described. If the master determines not to update the model early processing can continue as described above until a stop criterion is met.

Importantly the nature of the partial results can vary depending on how the model building is distributed. In one embodiment the job can be divided by dividing the training data set as described above. Each worker process is charged with processing only its partition dataset. That job of course can be distributed to children worker processes as further described below with regard to .

In another embodiment the job can be divided by node leaf . That is each worker is responsible for building the histograms and reporting results only for one or more assigned nodes. In another embodiment the model building job can be distributed by assigning to individual workers and their progeny the task of building an individual histogram and reporting those results for only one input variable. Whatever the arrangement at each level the corresponding local master process merges the results as appropriate and passes them up to the next level for the root master to assess updating the model under construction.

In one embodiment partial results may be assessed by a master process as follows. First a split score is generated for the current histogram before additional results are merged. A second split score is generated after merging the available or requested partial results into the histogram. If the merged result second split score is substantially the same as the prior result it is a useful indication that the subject histogram is stable and does not require further updates.

In an embodiment the method calls for using the merged histogram finding a set of points that partition the histogram into bins each having the same number of data instances and then applying that set of points to generate split scores for each histogram. We use these points to generate split scores for each histogram. And finally we calculate the earth mover distance between the score sets a known indicator of how different two histograms are. For the early model update procedure to be successful the data should not be ordered. To ensure a random ordering the original dataset preferably is shuffled before it is partitioned and sent to the workers. In a preferred embodiment along with a mean and a count each bin maintains a hash map of counts for each category or leaf as further illustrated below.

As mentioned we have expanded the use of histograms in new ways. In particular our extended approximate histograms provide enhanced efficiency in reporting results from workers to master processes and processing those results which may be partial results in connection with building a decision tree. Our goal is to capture and convey information that relates the input field to the objective field. The challenge is that some variables may be numeric while others are categorical. There are four types of field summaries to consider. The objective field can be either categorical which means a classification problem or numeric which means a regression problem . The input field also may either be categorical or numeric. We address all four possible combinations. We describe our extended approximate histograms in the following example. Assume the following dataset 

The numeric fields summaries for age and weight can use known histograms. Their bins contain only a mean and a count as we re not trying to capture correlations between fields. The categorical fields sex and first name can use a list of category counts as the summary. For the first example let s say we want to predict weight using age as an input field. This means a numeric input field and a numeric objective field.

The histogram shows for the input named age in each bin the mean value of weight number of counts and the sum of the weights for the people in the bin. This is sufficient information to evaluate potential splits for better predicting weight given age .

Next the object is to predict weight using sex as an input field. This is an example of a categorical input with a numeric objective.

The format is similar to the previous one except now each bin contains a category instead of a mean . Each bin still captures the count and the sum of the weights . This gives us enough information to evaluate a split on sex in order to predict weight .

Next assume the goal is to predict sex given weight . This is a numeric input field and a categorical objective field. This is the type of situation for which we devised extended histograms. In our extended histograms in some embodiments each bin may include a map of category counts in this case sex . These maps enable correlating how a numeric field weight in this illustration affects a categorical field sex . Here is the example 

Finally the case of predicting sex given a first name . This illustrates a categorical input field with a categorical objective field. In this case in some embodiments we maintain a map of maps for counting the occurrences of any of the possible combinations between those two categorical fields. Although we use a maps of maps a sparse encoding of the occurrences it could also be implemented with a matrix a dense encoding of the occurrences .

The previously defined tree growing method works well for a small number of workers but can be problematic when scaling to many workers. The bandwidth to receive the histogram results and send model updates could overwhelm the master. Also the master may not have the CPU resources required to merge the histogram results in an acceptable time frame.

To alleviate the pressure on the master in one embodiment a system may be used that comprises a tree structured network of computers to distribute the tasks. The master node preferably will only send models send update requests and receive histogram results from a small predefined number of children workers. For simplicity the drawings show two children . Those workers children will only communicate with their parents and their own children. The depth of the tree may be determined by the desired total number of computers.

In response to a request for partial results each child process sends its results for example in the form of a set of histograms to its immediate parent process. In this discussion we will sometimes refer to a histogram in the singular. It is understood that in practice many histograms will be processed as described and a histogram may in fact refer to a related set of histograms for example a set of histograms one per input variable at a given node. Referring now to child workers each send their histogram results respectively to their common parent namely worker . Worker combines its histogram with the children histograms to update the combined histogram . Worker then passes the updated histogram up to its parent namely master . The same process occurs on the other side of the drawing so that worker sends a combined histogram to the master . The histogram includes combined results from workers and . In general as illustrated in each parent process in the network combines its histogram results with those of its children and then transmits only the combined results to its parent. The master merges the histograms it receives as described above to form a merged histogram . This network structure allows the tree model building to be scaled to any number of machines without overwhelming the master.

The methods defined above can be used to create a variety of predictive tree models. One useful model is the gradient boosted regression tree or GBRT . GBRTs are collections of regression trees. To make a prediction with a GBRT each of its trees are evaluated and their outputs summed together prediction tree 1 output tree 2 output . . . tree n output .

GBRTs can also be used for classification problems where predictions are categories rather than numeric such as apple orange or banana . To do this a GBRT is built for each category. Their prediction outputs are a number from 0 to 1 representing their confidence that the example is a member of their class.

Grown on a large dataset a GBRT may include many regression trees. A GBRT with 500 trees would be large but not uncommon. GBRTs used for classification will have a GBRT for each class. This means a set of GBRTs for classification can lead to an explosion of individual trees. For example if we had a classification problem with 50 classes each might have a GBRT with 500 trees giving us 25 000 trees overall.

Traditionally when making predictions the trees for each class are summed to determine which class has the largest score and therefore the best answer . This requires evaluating each tree which as shown in our previous example may take a significant amount of computation time. Another aspect of the present invention includes a novel way to minimize the time necessary to make classifications.

Now that we have maximum and minimum values for each class s tree families our system in a preferred embodiment can use this data for fast predictions. is a simplified flow diagram illustrating a fast method for classifying a test instance using GBRTs in accordance another aspect of the present disclosure. The method may proceed generally as follows.

Our system in one embodiment uses a JSON format to represent our tree models and the results messages that are communicated between the worker nodes and the master. This JSON format allows our models to be much more compact than the equivalent model in the standard PMML format. is an example of a decision tree model expressed in a compact JSON format.

It will be obvious to those having skill in the art that many changes may be made to the details of the above described embodiments without departing from the underlying principles of the invention. The scope of the present invention should therefore be determined only by the following claims.

