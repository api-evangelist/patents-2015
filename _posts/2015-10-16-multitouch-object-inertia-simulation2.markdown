---

title: Multi-touch object inertia simulation
abstract: The inertia system provides a common platform and application-programming interface (API) for applications to extend the input received from various multi-touch hardware devices to simulate real-world behavior of application objects. To move naturally, application objects should exhibit physical characteristics such as elasticity and deceleration. When a user lifts all contacts from an object, the inertia system provides additional manipulation events to the application so that the application can handle the events as if the user was still moving the object with touch. The inertia system generates the events based on a simulation of the behavior of the objects. If the user moves an object into another object, the inertia system simulates the boundary characteristics of the objects. Thus, the inertia system provides more realistic movement for application objects manipulated using multi-touch hardware and the API provides a consistent feel to manipulations across applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09582140&OS=09582140&RS=09582140
owner: Microsoft Technology Licensing, LLC
number: 09582140
owner_city: Redwood
owner_country: US
publication_date: 20151016
---
This application is a continuation of application Ser. No. 13 919 957 filed Jun. 17 2013 titled MULTI TOUCH OBJECT INERTIA SIMULATION and now allowed. The aforementioned application is a continuation of application Ser. No. 12 258 439 filed Oct. 26 2008 titled MULTI TOUCH OBJECT INERTIA SIMULATION and now patented.

A tablet PC or pen computer is a notebook or slate shaped mobile computer equipped with a touch screen or graphics tablet screen hybrid technology that allows the user to operate the computer with a stylus digital pen or fingertip instead of a keyboard or mouse. Tablet PCs offer a more natural form of input as sketching and handwriting are a much more familiar form of input than a keyboard and mouse especially for people who are new to computers. Tablet PCs can also be more accessible because those who are physically unable to type can utilize the additional features of a tablet PC to be able to interact with the electronic world.

Multi touch or multitouch denotes a set of interaction techniques that allow computer users to control graphical applications using multiple fingers or input devices e.g. stylus . Multi touch implementations usually include touch hardware e.g. a screen table wall and so on and software that recognizes multiple simultaneous touch points. Multi touch stands in contrast to traditional touch screens e.g. computer touchpad ATM shopping kiosk that only recognize one touch point at a time. Multi touch hardware can sense touches using heat finger pressure high capture rate cameras infrared light optic capture tuned electromagnetic induction ultrasonic receivers transducer microphones laser rangefinders shadow capture and other mechanisms. Many applications for multi touch interfaces exist and application designers and users are proposing even more. Some uses are individualistic e.g. Microsoft Surface Apple iPhone HTC Diamond . As a new input method multi touch offers the potential for new user experience paradigms.

An application cannot use multi touch hardware without an interface for the application software to receive information from the multi touch hardware. Unfortunately each multi touch hardware device includes its own proprietary interface and application authors must have specific knowledge of a hardware device to write software that works with the device. For example a multi touch hardware provider may provide a kernel mode driver and a user mode application interface through which user mode software applications can communicate with the multi touch hardware to receive touch information. An application author writes software that communicates with the user mode application interface but the application author s software works only with that multi touch hardware. A computer user with a different multi touch hardware device cannot use the application author s software unless the application author produces a different version of the software that operates correctly with the computer user s device. This produces a very limited potential market for application authors reduces the incentive to write applications supporting multi touch interactions and keeps the cost of the most popular devices high for which the greatest number of applications is available.

Another problem is the difficulty for applications to determine a user s intentions based on touch input received from multi touch hardware. Touch input may be received as a list of coordinates where the hardware senses touch input at any given time. Each application has to include software to interpret the coordinates and determine the user s intention. In addition the user s intention may extend beyond the actual touch input received. The user may expect virtual objects to behave how they do in the physical world. For example a user may expect to be able to toss a file from one side of the desktop to another by flicking his her finger. This type of movement is not supported by current multi touch applications which would expect the user to drag his her finger from one side of the screen all the way to the other. Even if an application provides support for this type of movement other applications could not benefit from it and thus application authors would have to repeat the work of the first application author to offer the same functionality in their applications.

The inertia system provides a common platform and application programming interface API for applications to extend the input received from various multi touch hardware devices to simulate real world behavior of objects. The manipulations received by the application only describe the movement of an object based on the movement of contacts with the multi touch hardware. However to move naturally objects should also exhibit physical characteristics such as elasticity and deceleration. When a user lifts all contacts from an object the inertia system provides additional manipulation events to the application so that the application can handle the events as if the user was still moving the object with touch. However the inertia system actually generates the events based on a simulation of the behavior of the objects. If the user moves an object into another object the inertia system sends manipulation events based on the boundary characteristics of the objects. Thus the inertia system provides more realistic movement for application objects that a user manipulates using multi touch hardware and the API provides a consistent feel to manipulations across applications.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

The inertia system provides a common platform and API for applications to extend the input received from various multi touch hardware devices to simulate real world behavior of objects. For example real world objects do not typically stop moving when a user stops pushing them but rather exhibit some inertia and keep moving until friction slows them finally to a stop. In some embodiments the touch input first goes through a process to interpret the movement of one or more contacts as manipulations. Manipulations map more directly to user intentions than do individual touch inputs and add support for basic transformation of objects using multiple touch contacts. An application can use manipulations to support rotating resizing and translating multiple objects e.g. photos at the same time. The manipulations may be described as two dimensional 2D affine transforms that contain rotation scale e.g. zoom and translation e.g. pan information.

Each touch of the multi touch hardware is called a contact. For example when a user sets his her finger on the multi touch hardware moves his her finger around and lifts his her finger that series of events is a single contact. For example if the user moves two contacts closer together or further apart the system may determine that the user is scaling e.g. zooming into or out from an object. As another example if the user moves multiple contacts in a circular motion then the system may interpret the movement as a rotation of an object. Each application can define objects that are relevant differently so it is up to the application to attach an instance of the system called a manipulation processor to each object that a user can manipulate using touch input within the application. For example a photo browsing application may attach a manipulation processor to each displayed photo so that the user can move the photos around scale the photos rotate the photos and so forth.

The manipulations handled by the application only describe the movement of an object based on the movement of contacts. However to move naturally objects should also exhibit physical characteristics such as elasticity and deceleration. When a user lifts all contacts from an object the inertia system provides additional manipulation events to the application so that the application can handle the events as if the user was still moving the object with touch. However the inertia system actually generates the events based on a simulation of the behavior of the objects. For example if the user lifted the contacts while the object had a velocity in a particular direction then inertia system continues sending events that indicate that the object is moving in that direction slowing down over time as the object decelerates. If the user moves an object into another object such as the edge of the screen the inertia system sends manipulation events based on the boundary characteristics of the objects. For example if an application author defines two objects as being elastic then the two objects may bounce off each other when a user moves the objects into each other. Thus the inertia system provides more realistic movement for application objects that a user manipulates using multi touch hardware and the API provides a consistent feel to manipulations across applications.

The hardware interface communicates with the hardware to receive touch contacts and movements. The hardware interface may include several subcomponents that work together to provide touch input information. For example the operating system may provide a common driver model for multi touch hardware manufacturers to provide touch information for their particular hardware. The operating system may translate touch information received through this model into window messages e.g. WM TOUCH described herein and pass these messages to the application. Thus the hardware interface may involve the coordination of the hardware a hardware driver and an operating system layer. The result is a series of messages to the inertia system that identify a particular contact e.g. touch of a finger and the coordinates of the contact over time. For example the operating system may provide a message when a new contact is set down on the multi touch hardware a message each time the contact moves and a message when the contact is lifted away from the multi touch hardware.

One or more manipulation processors use the input transformation component to interpret movement of each contact associated with a particular application object. The manipulation processor may determine that a user is using multiple contacts to perform a single action. For example a user could touch a photo with all five fingers of one hand and twist his her hand to indicate an intention to rotate the photo. The manipulation processor receives five separate contacts one for each finger and the change in coordinates of each contact as the user rotates his her hand. The manipulation processor determines that each contact is grabbing the same object and performing the same rotation. The system will inform the application that the user rotated the object but the application can ignore whether the user used two five or any particular number of fingers or other contacts to perform the rotation. This greatly simplifies the authoring of the application because the application author can handle those types of manipulations that are relevant to the application and leave it to the inertia system to interpret the meaning of each low level touch input received from the multi touch hardware.

The manipulation processor uses the input transformation component to make determinations about the meaning of received movements of various contacts both alone and in concert. For example if a user is manipulating a photo with two fingers which creates two corresponding input contacts then the manipulation processor uses the input transformation component to determine the meaning of relative movements between the two contacts. If the two contacts move apart then the input transformation component may determine that the user is scaling the object to change the object s size. If the two contacts rotate then the input transformation component may determine that the user is rotating the object. If the two contacts both slide in a particular direction then the input transformation component may determine the user is panning the object to a new location. Although each type of movement is discussed separately note that a user can make all three types of movements at the same time and the input transformation processor can report the overall transformation to the application. For example a user can rotate scale and pan an object all in one motion.

The simulation component simulates the continued movement of an application object after the user stops touching the object based on initialization parameters and constraints defined for the object. An application may initialize the simulation component with the final state of the manipulation processor associated with the object. The application may also define various characteristics of the object such as how the object s boundaries should behave. The simulation component uses techniques based on physics to simulate the behavior of the object for a period after the user releases the object. For example the simulation component may continue to fire notifications to the application in the same form as the manipulation events received by the application while the user was moving the object. The application can then focus on reacting to the movement of the object rather than being concerned with what actions user or physical caused the object to move. Those of ordinary skill in the art will recognize numerous well known techniques for simulating the equivalent physical behavior of virtual objects in software.

The application interface communicates with the application to receive information and provide manipulation transforms to the application. The application interface receives initialization information from the application. The initialization information may specify which types of transforms the application object supports for a particular object and associated manipulation processor as well as initialization data for the simulation component when the user is no longer moving the object. For example some application objects may support scaling but not rotation. The initialization information may also specify a pivot point of the object. The inertia system provides manipulation transforms to the application through the application interface. For example when the inertia system receives low level touch input that the system interprets as a recognized transform e.g. a rotation the system fires an event to notify the application about the manipulation. The application processes the manipulation transform to modify the object based on the transform. For example if the user rotated the object then the application may store the new orientation of the object to use the next time the application displays the object. As another example if the object continued to rotate after the user released it based on calculations of the simulation component then the application may store the new orientation of the object.

The computing device on which the system is implemented may include a central processing unit memory input devices e.g. keyboard and pointing devices output devices e.g. display devices and storage devices e.g. disk drives . The memory and storage devices are computer readable media that may be encoded with computer executable instructions that implement the system which means a computer readable medium that contains the instructions. In addition the data structures and message structures may be stored or transmitted via a data transmission medium such as a signal on a communication link. Various communication links may be used such as the Internet a local area network a wide area network a point to point dial up connection a cell phone network and so on.

Embodiments of the system may be implemented in various operating environments that include personal computers server computers handheld or laptop devices multiprocessor systems microprocessor based systems programmable consumer electronics digital cameras network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and so on. The computer systems may be cell phones personal digital assistants smart phones personal computers programmable consumer electronics digital cameras and so on.

The system may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. Generally program modules include routines programs objects components data structures and so on that perform particular tasks or implement particular abstract data types. Typically the functionality of the program modules may be combined or distributed as desired in various embodiments.

When the user is done moving the object e.g. when the application receives a notification that each contact touching an object has been removed from the touch hardware the application sends initialization information to the inertia system . The inertia system determines a next position of the object and provides inertia events similar to the manipulation events that the manipulation system provided when the user was moving the object. The application also provides a driving timer to periodically call the inertia system to provide the next position of the object through inertia events . The application processes the inertia events in a way similar to manipulation events.

Although the diagram illustrates that the application first receives touch input and passes the touch input to the manipulation system and inertia in some embodiments these systems receive touch input directly from the hardware interface interpret the touch input and provides interpreted manipulation events to the application. Likewise the application may not know that a separate inertia system provides inertia events after a user stops moving an object with touch but rather may receive events from one interface during the time the user is moving the object and afterwards when the object is moving based on inertia. This represents an alternative architecture that provides similar resultant functionality but gives the application less control over the processing of the input. For example the application may not be able to define individual application objects to which the system attaches individual manipulation processors. The RTS plug in described herein is one example of this alternative architecture for the system.

Without inertia the document object would stop at the third manipulation location which is likely not what the user intends. The inertia system provides additional manipulation locations to the application as if the user was still touching and moving the document object based on the document object s velocity when the user releases the document object . The application receives the first inertia based manipulation location when the application initializes the inertia system and calls the inertia system s processing function for the first time. The application receives the second inertia based manipulation location as the application continues to call the inertia system s processing function. Because the final manipulation location of the document object is over the recycle bin the application processes the contact between the two objects e.g. by placing the document object in the recycle bin . In the example illustrated even though the inertia system decelerates the movement of the document object the document object is still able to move a fair distance across the display based on the high initial velocity of the user s movement of the document object at the start.

In block the application receives a manipulation event from the inertia system that describes one or more manipulations of the identified application object. For example the application may receive an event describing a 2D affine transform of the application object. Note that block is illustrated serially after block for simplicity of illustration. In practice the application may receive many touch input events before the inertia system notifies the application with a manipulation event. There is not necessarily a one to one mapping of touch input events to manipulation events. Because manipulation events represent a higher level interpretation of low level touch inputs multiple touch inputs may make up a single manipulation event. In block the application handles the received manipulation event. For example if the received manipulation event is a rotation then the application may rotate the application object on the screen and store the application objects new location for use when the application object is displayed again. The inertia system frees the application from performing steps specific to a particular multi touch hardware device or even from knowing which hardware device is providing the multi touch input. In addition the inertia system frees the application from processing individual contact movement and allows the application to focus on processing transforms at the application object level.

In block the application waits for the next touch input. For example the application may call an operating system provided message API such as GetMessage on Microsoft Windows that waits for the next message to be delivered to the application s message queue. In decision block if the application receives the next touch input then the application loops to block to process the input else the application loops to block to continue waiting for further input. When the application closes the application exits the input loop not shown .

In decision block if the received touch input indicates that the application received a new contact e.g. a touch down event then the system continues at block else the system continues at block . For example a user may make initial contact of a finger with an on screen object or set down another finger i.e. contact on a previously touched object. In block the system adds the new contact to the list of contacts associated with the manipulation processor and then continues at block . In decision block if the received touch input indicates that the application received notification that a touch contact was removed e.g. a touch up event then the system continues at block else the system continues at block . For example the user may lift one or more fingers from a previously touched object. In block the system removes the contact from the list of contacts associated with the manipulation processor and then continues at block . In block the system processes the touch input to determine any manipulations represented by the touch input. For example touch movement may indicate a rotation or translation manipulation while touch contact removal may indicate completion of a manipulation. In block the system fires a manipulation event to send transform information describing the manipulation to the application. For example the system may provide a degree of angular rotation of the object to the application. After block these steps conclude.

In block the application receives one or more inertia events that describe manipulations of the object e.g. rotation translation and or scaling based on simulated inertia. For example if the object was traveling in a particular direction the application may receive an inertia event that describes a translation manipulation in that direction. As another example if the object was expanding when the user released it the application may receive an inertia event that describes a scaling manipulation. Note that block is illustrated serially after block for simplicity of illustration. In practice the application may call the inertia processing function several times before the inertia system notifies the application with an inertia event. There is not necessarily a one to one mapping of calls to the processing function and inertia events. On the other hand the inertia system may notify the application of multiple inertia events after a single call to the processing function.

In block the application handles the received inertia event based on the meaning e.g. an effect of the manipulation in the context of the particular application. For example if the received inertia event is a rotation then the application may rotate the application object on the screen and store the application objects new location for use when the application displays the application object again. In decision block if the inertia events are complete then these steps conclude else the system continues at block . The inertia system may inform the application that a particular simulated manipulation is complete as a return value from the process function or through the notifications provided to the application e.g. through a Component Object Model COM event interface . In block the application waits for the next firing of the timer then loops to block to call the inertia system processing function.

In block the component simulates movement of the object based on the initial parameters any previous processing and the time passed since the last process call. The process call may also provide a timestamp that indicates the time that the application wants the simulation to use. This allows the application to simulate application behavior in other than real time e.g. for application testing or debugging . In decision block if the movement is complete then the component continues at block else the component continues at block . The component may determine that the movement is complete based on factors such as whether the object is still moving or whether the object movement has fallen below a certain threshold. In block the component sets a completion flag on the next inertia event. In block the component fires an inertia event to send transform information describing the current movement e.g. as a manipulation to the application. For example the system may provide a degree of angular rotation of the object to the application. After block these steps conclude.

In some embodiments the inertia system receives object constraints from the application. For example the application may define the elasticity of an object friction coefficient to determine how an object decelerates boundary characteristics of the object and so forth. For example an application author may define rigid objects that the user can move and a bouncy application window edge so that objects moved into the window edge bounce back from the window edge when the user releases them.

In some embodiments the inertia system receives initial object state information from a manipulation system that was tracking the movement of the object when the user was manipulating the object with touch input. For example the manipulation system may track the current position of each object the historical movement of the object the linear and angular velocity of the object and so forth. The application author can provide the output of the manipulation to the inertia system to initialize the inertia system so that the inertia system can smoothly continue the past movement of the object and slow it down based on appropriate physics and characteristics of the object.

In some embodiments the inertia system receives limits on the movement of objects from the application. For example the application author may define an upper bound on the distance that an object can move once a user releases the object. As another example the application may define an upper bound on how long the object can move once a user releases the object. These and other limits allow the application author to adjust the inertia system to suit the types of objects manipulated by the application and to enhance the user experience with the application.

In some embodiments the inertia system does not provide additional movement for objects with movement below a predefined threshold. The threshold may be configurable by the application. For example the inertia system may have a particular object linear or angular velocity below which the system will not continue movement of the object after the user releases the object. If the object is not moving very fast when the user releases it the user may expect that the object will stay put and not continue to move. The threshold allows the application or author of the inertia system to determine the level of movement after manipulation that provides a good user experience.

In some embodiments the inertia system receives instructions to simulate movement incrementally from the application. For example the inertia system may provide a Process or DoWork function that the application calls to instruct the inertia system to perform a portion of the overall simulation. The inertia system may expect the application to set a timer or otherwise periodically call the function to cause the inertia system to simulate movement over time according to a natural timeline. The application can affect the characteristics of the manipulation events provided by the inertia system by varying how often the application calls the function. In other embodiments the inertia system uses an internal timer to provide manipulation events on a regular schedule until each object has stopped moving e.g. due to deceleration or other simulated forces .

In some embodiments the inertia system is part of a message based operating system and the system receives messages related to touch input that the operating system receives from the hardware. For example using a paradigm similar to WM MOUSEMOVE for mouse messages future versions of Microsoft Windows may provide a WM TOUCH message that contains low level touch movement information received from multi touch hardware. The operating system may also provide finer grained messages such as WM TOUCHDOWN when a new contact is made with the multi touch hardware WM TOUCHMOVE when an existing contact moves and WM TOUCHUP when a contact is lifted from the multi touch hardware . An application that receives a WM TOUCH related message can invoke the inertia system and pass the message to the inertia system for interpretation and processing. The application then receives higher level events that represent the inertia system s interpretation of the manipulation intended by the user based on the received low level touch movement information.

In some embodiments the inertia system receives low level touch movement information from specialized hardware such as a real time stylus. For example the Microsoft Tablet PC Software Development Kit SDK provides a real time stylus RTS component that application authors can extend with hooks. RTS hooks receive input from the RTS hardware and can perform processing on the received input. The inertia system may provide a hook that an application can insert into the RTS component to automatically process RTS and other input to manipulate application objects as described herein. The RTS hook provides a different way for the inertia system to receive input but the inertia system interprets input and fires events to the application describing manipulations implied by the input as previously described. A user may use a combination of stylus and touch input. For example the user may draw an object with the stylus and then rotate the object using his her fingers.

In some embodiments the inertia system is part of a common control that an application can invoke to provide a common user interface. Microsoft Windows provides common controls for displaying lists trees buttons and so forth. Likewise the inertia system may provide a multi touch based control for manipulating application objects in the ways described herein. For example the system may provide a scatter control that allows the user to display one or more objects and manipulate the objects. The scatter control handles processing of low level touch input and associating the input with a particular application object and the application receives events from the control to handle the manipulations of the application objects. For example if the control indicates that the user resized an object then the application may store the objects new size.

In some embodiments the inertia system performs the processing described herein in three dimensions. Although two dimensional multi touch hardware is described herein those of ordinary skill in the art will recognize that the processing of the system described herein can be applied equally well to three dimensional 3D manipulations if hardware is available to provide coordinate movement in three dimensions. For example hardware that detects pressure or uses cameras to detect 3D movement of a user s fingers could provide the coordinates of movement in the third dimension to the inertia system and the inertia system could then produce 3D transforms that describe manipulations e.g. rotation scaling and translation of objects in multiple 3D directions.

The following table defines one API that the inertia system provides to applications for providing inertia based movement to application objects following user touch based movement of the objects.

In the table above the inertia system may provide the listed events on the same interface on which an application was previously receiving events based on user movement.

From the foregoing it will be appreciated that specific embodiments of the inertia system have been described herein for purposes of illustration but that various modifications may be made without deviating from the spirit and scope of the invention. For example although the system has been described in the context of multi touch manipulations the system provides simulation of inertia that could be used in other contexts such as games and other areas where simulation is commonly used. Accordingly the invention is not limited except as by the appended claims.

