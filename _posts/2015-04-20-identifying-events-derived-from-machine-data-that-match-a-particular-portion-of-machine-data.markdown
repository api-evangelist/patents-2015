---

title: Identifying events derived from machine data that match a particular portion of machine data
abstract: Methods and apparatus consistent with the invention provide the ability to organize and build understandings of machine data generated by a variety of information-processing environments. Machine data is a product of information-processing systems (e.g., activity logs, configuration files, messages, database records) and represents the evidence of particular events that have taken place and been recorded in raw data format. In one embodiment, machine data is turned into a machine data web by organizing machine data into events and then linking events together.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317582&OS=09317582&RS=09317582
owner: Splunk Inc.
number: 09317582
owner_city: San Francisco
owner_country: US
publication_date: 20150420
---
This application claims benefit as a CON of application Ser. No. 14 530 686 filed Oct. 31 2014 which claims benefit as a CON of application Ser. No. 14 266 831 filed May 1 2014 which claims benefit as a CON of application Ser. No. 14 170 228 filed Jan. 31 2014 which claims benefit as a CON of application Ser. No. 13 664 109 filed Oct. 30 2012 now U.S. Pat. No. 8 694 450 issued Apr. 8 2014 which claims benefit as a CON of application Ser. No. 13 099 268 filed May 2 2011 now U.S. Pat. No. 8 589 321 issued Nov. 19 2013 which claims benefit as a CON of application Ser. No. 11 459 632 filed Jul. 24 2006 now U.S. Pat. No. 7 937 344 issued May 3 2011 which claims benefit of PRO Ser. No. 60 702 496 filed Jul. 25 2005 the entire contents of the aforementioned is hereby incorporated by reference as if fully set forth herein under 35 U.S.C. 120. The applicant s hereby rescind any disclaimer of claim scope in the parent application s or the prosecution history thereof and advise the USPTO that the claims in this application may be broader than any claim in the parent application s .

This invention relates generally to information organization and understanding and more particularly to the organization and understanding of machine data.

Information systems invariably generate vast amounts and wide varieties of machine data e.g. activity logs configuration files messages database records whose value is widespread. Troubleshooting systems detecting operational trends catching security problems and measuring business performance for example typically require the organization and understanding of machine data. But the overwhelming volume different and changing formats and overall complexity of machine data create substantial difficulty for software developers system administrators and business people who want to make sense of it and gain insight into information system behavior. The problem is compounded by the fact that information systems and the machine data they generate continue to grow in complexity and size.

Consider for example an information system environment for web based applications consisting of web servers application servers databases and networks. Each information system component is constantly logging its own machine data documenting its activities. System administrators need to access and comprehend the machine data from one or more components to find and fix problems during operations. Security analysts want to understand patterns of machine data behavior from network devices to identify potential security threats. Business people are interested in tracing the machine data across components to follow the paths and activities customers perform when purchasing products or services.

Today people generally attempt to comprehend information system behavior by manually looking at and trying to piece together machine data using the knowledge from one or more individuals about one or more systems. Individuals typically have specific technology domain expertise like networking operating systems databases web servers or security. This expertise can also be in specific application domains like finance healthcare or communications. Manual approaches can be effective when considering small amounts of machine data in a single domain but humans are easily overwhelmed as the size variety and dynamic nature of the machine data grows.

Automated approaches like homegrown scripts data analysis programs and data warehousing software by contrast can work with large amounts of machine data. But organizing different types of frequently changing data and formats can be troublesome generally requiring specific methods for each type of data and necessitating modification of methods when the data formats change or new types of data are encountered. Automated approaches to building understanding from machine data are typically limited to finding simple predefined relationships between known data elements.

Generally machine data is organized today by relying on predefined data schemas and predetermined algorithms for parsing and categorizing data. In current approaches what data elements exist in a machine data set and how the data elements are classified generally must be known ahead of time. How the data is cleansed parsed and categorized is defined algorithmically in advance for different types of data formats resulting in systems that are brittle expensive to implement and have numerous functional shortcomings. For example unexpected types of data are typically ignored. As a result data categorization usefulness degrades quickly and unexpected data and behaviors are not observed or recorded. Given the inherent dynamic nature of information systems and the machine data they generate current organization methods have limited applicability.

Building understanding from machine data is inherently subjective and depends on the task scope of data and skill level of people using a solution. Deriving specific useful meanings from large quantities of machine data can require expertise in one or more domains and knowledge of how data from one domain relates to data from another domain. Current methods of deriving meaning from machine data are generally based on building simple pair wise relationships A B between predetermined data elements using data values. More advanced techniques may be able to find predetermined multi data element relationships A B C provided the data elements are described in advance requiring the availability of multiple domain experts to configure and continuously manage a solution.

Conventional methods whether human or automated of organizing and understanding machine data across multiple information systems and domains suffer from an inability to effectively keep up with changing machine data and are constrained by limited data relationships making these methods difficult time consuming expensive and often ineffective.

There exists therefore a need to develop other techniques for organizing and deriving understanding from machine data.

Methods and apparatus consistent with the invention address these and other needs by turning machine data MD into a machine data web MDW . A MDW is created by organizing MD into events representing discrete activities and dynamically linking events together representing larger more complex activities. Much like the World Wide Web is a hyperlinked information space of documents and web sites. A MDW is an interconnected information space of information system events and activities. The MDW can be searched browsed navigated and analyzed as a proxy for the information processing environment itself. Unlike the WWW s HTML documents and hyperlinks however the events organized from machine data and the links between these events do not generally exist and must be manufactured through the processing and analysis of MD.

In one implementation MD is organized into events using a collection of techniques including but not limited to aggregating a MD collection into discrete events extracting important entities from an event s data segmenting an event s data into tokens and classifying events into like categories. An important aspect is the ability to continuously learn and adapt keeping up with changes in the MD. In the example of a web based application information system environment data sources and data formats can be constantly changing. For example new web servers and network components can be added and old ones removed as the application requires more capacity or reconfiguration.

In another aspect knowledge or understanding is built from the organized MD as events are connected to one another by dynamically constructing links using a number of techniques including but not limited to the analysis of event data values timing patterns and statistics. One advantage of the MDW is that it can learn new types of links as they occur and build paths by chaining multiple links together. Another advantage is the ability to preserve integrity by reconstructing the original MD from the MDW events. Dynamic construction of links and paths through multiple machine data sources enables a system administrator working on a web based application information system to follow the sequence of activities from the web server to the application and eventually the database in order to locate the source of a problem.

In the example of the information processing environment includes hardware and software components such as computers routers databases operating systems and applications in a distributed configuration for processing information. Each component may be producing MD and there may be many MD sources and large quantities of MD across multiple technology and application domains. For example a computer may be logging operating system events a router may be auditing network traffic events a database may be cataloging database reads and writes or schema changes and an application may be sending the results of one application call to another across a message queue. In this embodiment individual IT personnel who may reside in different data centers companies or even geographies typically manage specific technology and application domains. Aspects of the invention will be described with respect to the information processing environments in but the invention can also be used with other information processing environments.

During the understanding process ED is analyzed to create dynamic links between events and build the MDW . As an example consider that a log from a web server may contain specific types of events with specific event data but a log from an application server or database may contain different events and event data specific to its own domain. A system administrator may for example locate the web server event by looking for a session ID found in a web server log locate the application server event by finding a process ID in the message queue and locate a database table update event by searching for a transaction ID in the database audit trail. All three sources may contain events that are part of a larger system activity yet there is no obvious or explicit common structure or data shared among the MD produced by each system. Common structure is manufactured across the three sources by analyzing the event data so that connections between events can be identified. In one implementation patterns of event behavior are recorded in real time and identified for example as frequently occurring or infrequently occurring. Frequent patterns identify typical system processes and well known links infrequent patterns identify deviations or anomalies and less well known links. Contrast this with the world of the web where hyperlinks are part of the formal common structure of HTML the language for building most web pages. Building links by hand for large volumes of ED is not an option for complex information processing environments.

In the collection step the MD may be collected directly from its original source or consolidated over a number of sources. Machine data can and often does arrive out of order. Collection of MD can be performed based on standard approaches to data access for example reading log files examining message bits traffic becoming a sync for logging systems like Syslog or connecting to database auditing systems. Parts of the collection module can be situated in different locations preferably with access to the MD .

Given the repetitive yet dynamic nature of MD an effective organization process such as shown in preferably will learn about data formats and structure automatically. In one implementation learning is separated into different domains based on the source of MD . Domains can be general system types such as log files message bus traffic and network management data or specific types such as cutout of a given application or technology Sendmail logging data Oracle database audit data and J2EE messaging. An MDW can include a mix of general domains and specific domains.

In this example organization process the domain for a given source of MD is identified so that domain specific organization methods can be applied. Domains are determined through a learning process. The learning process uses collections of MD from well known domains as input and creates a source signature for each domain. In one implementation source signatures are generated from representative samples of MD by creating a hash table mapping punctuation characters to their frequency. While the tokens and token values can change in a MB collection in this particular implementation the signature generated by the frequency of punctuation is quite stable and reliable within a specific domain. Other implementations could use functions of the punctuation and tokens such as the frequencies of the first punctuation character on a line or the first capitalized term on a line. Given that source signatures can be large and hard to read signatures can have a corresponding label in the form of a number or text that can be machine generated or human assigned. For example the source signature for an Apache web server log might be programmatically assigned the label 205 or a user can assign the label Apache Server Log .

In one embodiment clustering is used to classify collected MD into domains according to their source signatures . As collections of MD are encountered each collection s signature is matched to the set of known source signatures by performing a nearest neighbor search. If the distance of the closest matching signature is within a threshold the closest matching signature s domain is assumed to be the domain of the source. If no best match can be found a new source signature can be created from the sample signature and a new source domain created. Alternatively a default source domain can be used. In one implementation the distance between two signatures is calculated by iterating over the union of attributes of the two signatures with the total signature distance being the average of distances for each attribute. For each attribute A the value of A on Signature1 and Signature2 V1 and V2 are compared and a distance is calculated. The distance for attribute A is the square of V1 V2 IDF where IDF is the log N A where N is the number of signatures and A is the number of signatures with attribute A.

Some MD sources are non textual or binary and cannot be easily processed unless a known process is available to convert the binary MD into textual form. To classify a source as textual or binary a sample MD collection is analyzed. Textual MD can also have embedded binary MD such as a memory dump and the classification preferably identifies it as such. In one implementation the textual binary classification works as follows. The sample is a set of lines of data where a line is defined as the data between new lines i.e. n carriage returns i.e. r or their combination i.e. r n . For each line if the line s length is larger than some large threshold such as 2 k characters or if the line contains a character with an ASCII value of zero 0 a count of Binary looking lines is incremented. Otherwise if the line s length is shorter than a length that one would expect most text lines to be below such as 256 characters a count of Text looking lines is incremented. If the number of Text looking lines is twice as numerous as the Binary looking lines other ratios can be used depending on the context the source is classified as text. Otherwise the source is classified as binary.

When the source signature for a collection of MD has been identified. the corresponding aggregation rules are applied to the MD collection. Aggregation rules describe the manner in which MD from a particular domain is organized into event data by identifying the boundaries of events within a collection of MD for example bow to locate a discrete event by finding its beginning and ending. In one implementation the method of aggregation learns without prior knowledge by grouping together multiple lines from a sample of MD . Often MD contains events that are anywhere from one to hundreds of lines long that are somehow logically grouped together.

The MD collection may be known a priori or may be classified as single line type i.e. containing only single line events or multi line type i.e. possibly containing multi line events prior to performing aggregation. For those MD collections that are classified as single line type aggregation is simple single line type MD collections are broken on each line as a separate event. Multi line type MD collections are processed for aggregation. In one implementation a MD collection is classified as a multi line type if 1 there is a large percentage of lines that start with spaces or are blank e.g. if more than 5 of the lines start with spaces or are blank or 2 there are too many varieties of punctuation characters in the first N punctuation characters. For example if the set of the first three punctuation characters found on each line has more than five patterns e.g. .. the collection might be classified as multi line.

Another aspect of aggregation methods is the ability to learn and codify into rules what constitutes a break between lines and therefore the boundary between events by analyzing a sample of MD. For example in one implementation an aggregation method compares every two line pair looking for statistically similar structures e.g. use of white space indentation and time stamps to quickly learn which two belong together and which two are independent. In one implementation aggregation. works as follows. For each line first check if the Hue starts with a time stamp. If so then break. Typically lines starting with a time stamp are the start of a new event. For lines that do not start with a time stamp combine the current line with the prior line to see how often the pair of lines occurs one before the other as a percentage of total pairs in the MD sample. Line signatures are used in place of lines where a line signature is a more stable version of a line immune to simple numeric and textual changes. In this implementation signatures can be created by converting a line into a string that is the concatenation of leading white space any punctuation on the line and the first word on the line. The line 10 29 03 Host 191.168.0.1 rebooting normally is converted to .. Host. 

Now this current line signature can be concatenated with the previous line signature i.e. signature1 combined with signature2 and used as a combined key into a table of break rules. The break rule table maps the combined key to a break rule which determines whether there should be a break or not between the two lines i.e. whether they are part of different events or not . Break rules can have confidence levels and a more confident rule can override a less confident rule. Break rules can be created automatically by analyzing the co occurrence data of the two lines and what percent of the time their signatures occur adjacently. If the two line signatures highly co occur a new rule would recommend no break between them. Alternatively if they rarely co occur a new rule would recommend a break between them. For example if line signature A is followed by line signature B greater than 20 of the time A is seen then a break ride might be created to recommend no break between them. Rules can also be created based on the raw number of line signatures that follow proceed another line signature. For example if a line signature is followed by say ten different line signatures create a rule that recommends a break between them if there is no break rule in the break rule table the default behavior is to break and assume the two lines are from different events. Processing proceeds by processing each two line pair updating line signature and co occurrence statistics and applying and learning corresponding break rules. At regular intervals the break rule table is written out to the hard disk or permanent storage.

Following aggregation and before event segmentation various extraction methods can be applied to identify semantic entities within the data. In one implementation search trees or regular expressions can be applied to extract and validate for example IP addresses or email addresses. The goal of extraction is to assist the segmentation process and provide semantic value to the data.

Segmentation rules describe bow to divide event data into segments also known as tokens . It is important to note at this point that segments have little semantic value unless an extracted entity has been applied. In one implementation a segmentation rule examines possible separators or punctuation within the event for example commas spaces or semicolons. An important aspect of segmentation is the ability to not only identify individual segments but also to identify overlapping segments . For example the text of an email address bob.smith corp.com can be broken into individual and overlapping segments and can be identified as individual segments and can also be identified as an overlapping segment. In one implementation segmentation uses a two tier system of major and minor breaks. Major breaks are separators or punctuation that bound the outer most segment . Examples include spaces tabs and new lines. Minor breaks are separators or punctuation that break larger segments into sub segments for example periods commas and equal signs. In one implementation more complex separators and punctuation combinations are used to handle complex segmentation tasks for example handling Java exceptions in an application server log file.

In the embodiment of the final step of the organization process is the classification of events into event types. Examples of event types include a web server HTTP get an application server database connect or an email server send mail attempt. In one implementation an event signature is generated for each event type. One method for generating an event signature is to build a hierarchical scheme for identifying particular types of events based on the overall event structure segmentation segment values and extracted entities . The purpose of the event signature is to identify a type of event regardless of the situation. In this way a particular type of event can have the same signature in multiple MDWs. For example a mail server s send mail attempt generally has the same signature in every MDW regardless of the information processing environment.

In one implementation a hierarchical event signature v1 v2 v3 . . . vn is constructed from a list of successively more specific hash functions f1 f2 f3 . . . fn where each fn produces a value representing a level of the hierarchy. The event signature is most useful when each successive function is more specific. For example in one embodiment the following function list represents a 9 level event signature from most general to most specific 

f4 firstImportantKeywords returns a hash value of first word in the event that is an important keyword where there is a list of known important terms.

f5 firstKnownWord returns the first word in the event that is a known keyword where there is a list of known terms.

f6 importantKeywords returns the list of all hash values of important keywords that are found in the event.

In this implementation the event signature is a traversal through a hierarchy of possible values. Given that event signatures can be large and hard to read an event signature can have a corresponding label in the form of a number or text that can be machine generated or human assigned. For example an email server send mail attempt event might be programmatically assigned the label 500 but a user can assign the label send mail attempt .

By analyzing event data and possible link hints from external systems or human input links can be created . An important feature of the MDW approach is the ability to create link relationships dynamically and learn new possible link relationships on the fly. A number of methods can be used in the analysis of ED to create links including but not limited to value analysis statistical analysis timing analysis and the evaluation of link hints . These methods can be used individually or in combination with one another. From our previous example perhaps the link between the MIA and the spam filter events is a value association between the MTA message ID and the spam filter article ID or the link between the spam filter and the user email retrieval is an associative mail box name. All three events might be tied together for example by observing a timing pattern that occurs over and over again with statistically relevant frequency.

In one implementation link analysis takes place by creating a co occurrence table with an entry for pairs of event types or event data values that occur within a predetermined window of each other. In one aspect windows are bounded by a window threshold taking the form of time e.g. 10 minutes event types e.g. 50 unique event types or event instances e.g. 1000 events . The value of the co occurrence table entry is the distance between the pair time event types or event instances . Pairs that co occur often enough and meet a distance standard deviation threshold are deemed relevant and reliable links. For example assume that an event of type A occurred 50 times an event of type B occurred 40 times an event of type A was followed by an event of type B 20 of the time and the standard deviation of their distance was less than 5.0 a predetermined threshold then a link is created between events of type A and type B represented as A B . Standard deviation thresholds are based on a function of window thresholds and may change based on the time to complete analysis or the number of desired results. Window thresholds may change based on data density and time available to complete the analysis.

Paths are multi link collections representing a chain of linked events . Paths often represent a higher level of information system behavior possibly spanning multiple systems applications or data centers. Paths are useful for example for following more complex activities or transactions through one or more systems. In our email example a path could be the receiving or sending of an email including three or more events and two or more links . Similar to links paths are created. . by analyzing event data and possible path hints from external systems or human input. An important feature is the ability to create paths dynamically and learn new possible paths on the fly.

Paths are built by chaining together event links using a number of methods. In one implementation paths are discovered as chains of transitive links . For example given previously discovered links A B B C A C and C A transitively composition yields the following three event paths A B C B C A A C A C A B and C A C. These paths can also be combined to make larger and larger path chains. In one aspect certain restrictions are applied to reduce combinatorial explosion. One restriction might involve the elimination of cycles and repetitions. For example one rule might be that A C and C A cannot be combined to create A C A. In a second possible restriction. for A B and B C to be combined there must be an A C link with the average distance of A C being approximately equal to the sum of the average distances between A B and B C. In addition the standard deviation of the distance for A C must be approximately equal to the standard deviations of A B and B C. Finally paths that are rotations of other paths can be removed keeping the most reliable path. For example given paths A B C and C A B if the standard deviation of the distance between C A is greater than the standard deviation of the distance between B C then A B C would be kept and C A B removed.

Like the WWW and HTML hyperlinks event links and paths can be represented as a uniform resource locator URL . In one implementation a link from one event to another is represented by the following URL mdw event. A link can resolve to one of several destinations including but not limited to an event type an event instance or an event segment within an event instance.

In addition to links and paths another aspect of the MDW understanding process is the ability to generate historical information about itself for example statistics for event event type link or path occurrences. One aspect of historical data regarding the MDW is that it can reveal historical behavior of the information processing environment itself.

The MDW can be implemented in many different ways. In one approach each box in is implemented in software as a separate process. All of the processes can run on a single machine or they can be divided up to run on separate logical or physical machines. In alternate embodiments the invention is implemented in computer hardware firmware software and or combinations thereof. Apparatus of the invention can be implemented in a computer program product tangibly embodied in a machine readable storage device for execution by a programmable processor. Method steps of the invention can be performed by a programmable processor executing a program of instructions to perform functions of the invention by operating on input data and generating output. The invention can be implemented advantageously in one or more computer programs. Each computer program can be implemented in a high level procedural or object oriented programming language or in assembly or machine language if desired in any case the language can be a compiled or interpreted language. Any of the foregoing can be supplemented by or incorporated in ASICs application specific integrated circuits and other forms of hardware.

Although the detailed description contains many specifics these should not be construed as limiting the scope of the invention but merely as illustrating different examples and aspects of the invention. It should be appreciated that the scope of the invention includes other embodiments not discussed in detail above. For example not all of the steps shown are required in every implementation and they may be implemented in ways other than the examples given above. The order of the steps may also be changed in certain cases. Various other modifications changes and variations which will be apparent to those skilled in the art may be made in the arrangement operation and details of the method and apparatus of the present invention disclosed herein without departing from the spirit and scope of the invention.

