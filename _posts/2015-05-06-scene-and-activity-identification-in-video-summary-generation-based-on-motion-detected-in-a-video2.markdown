---

title: Scene and activity identification in video summary generation based on motion detected in a video
abstract: Video and corresponding metadata is accessed. Events of interest within the video are identified based on the corresponding metadata, and best scenes are identified based on the identified events of interest. In one example, best scenes are identified based on the motion values associated with frames or portions of a frame of a video. Motion values are determined for each frame and portions of the video including frames with the most motion are identified as best scenes. Best scenes may also be identified based on the motion profile of a video. The motion profile of a video is a measure of global or local motion within frames throughout the video. For example, best scenes are identified from portion of the video including steady global motion. A video summary can be generated including one or more of the identified best scenes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09646652&OS=09646652&RS=09646652
owner: GoPro, Inc.
number: 09646652
owner_city: San Mateo
owner_country: US
publication_date: 20150506
---
This application claims priority to and the benefit of U.S. Provisional Application No. 62 039 849 filed Aug. 20 2014 which is incorporated by reference herein in its entirety.

This disclosure relates to a camera system and more specifically to processing video data captured using a camera system.

Digital cameras are increasingly used to capture videos in a variety of settings for instance outdoors or in a sports environment. However as users capture increasingly more and longer videos video management becomes increasingly difficult. Manually searching through raw videos scrubbing to identify the best scenes is extremely time consuming. Automated video processing to identify the best scenes can be very resource intensive particularly with high resolution raw format video data. Accordingly an improved method of automatically identifying the best scenes in captured videos and generating video summaries including the identified best scenes can beneficially improve a user s video editing experience.

The figures and the following description relate to preferred embodiments by way of illustration only. It should be noted that from the following discussion alternative embodiments of the structures and methods disclosed herein will be readily recognized as viable alternatives that may be employed without departing from the principles of what is claimed.

Reference will now be made in detail to several embodiments examples of which are illustrated in the accompanying figures. It is noted that wherever practicable similar or like reference numbers may be used in the figures and may indicate similar or like functionality. The figures depict embodiments of the disclosed system or method for purposes of illustration only. One skilled in the art will readily recognize from the following description that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.

The camera can include a camera body having a camera lens structured on a front surface of the camera body various indicators on the front of the surface of the camera body such as LEDs displays and the like various input mechanisms such as buttons switches and touch screen mechanisms and electronics e.g. imaging electronics power electronics metadata sensors etc. internal to the camera body for capturing images via the camera lens and or performing other functions. As described in greater detail in conjunction with below the camera can include sensors to capture metadata associated with video data such as motion data speed data acceleration data altitude data GPS data and the like. A user uses the camera to record or capture videos in conjunction with associated metadata which the user can edit at a later time.

The video server receives and stores videos captured by the camera allowing a user to access the videos at a later time. In one embodiment the video server provides the user with an interface such as a web page or native application installed on the client device to interact with and or edit the videos captured by the user. In one embodiment the video server generates video summaries of various videos stored at the video server as described in greater detail in conjunction with and below. As used herein video summary refers to a generated video including portions of one or more other videos. A video summary often includes highlights or best scenes of a video captured by a user. In some embodiments best scenes include events of interest within the captured video scenes associated with certain metadata such as an above threshold altitude or speed scenes associated with certain camera or environment characteristics and the like. For example in a video captured during a snowboarding trip the best scenes in the video can include jumps performed by the user or crashes in which the user was involved. In addition to including one or more highlights of the video a video summary can also capture the experience theme or story associated with the video without requiring significant manual editing by the user. In one embodiment the video server identifies the best scenes in raw video based on the metadata associated with the video. The video server may then generate a video summary using the identified best scenes of the video. The metadata can either be captured by the camera during the capture of the video or can be retrieved from one or more metadata sources after the capture of the video.

Metadata includes information about the video itself the camera used to capture the video the environment or setting in which a video is captured or any other information associated with the capture of the video. For example metadata can include acceleration data representative of the acceleration of a camera attached to a user as the user captures a video while snowboarding down a mountain. Such acceleration metadata helps identify events representing a sudden change in acceleration during the capture of the video such as a crash the user may encounter or a jump the user performs. Thus metadata associated with captured video can be used to identify best scenes in a video recorded by a user without relying on image processing techniques or manual curation by a user.

Examples of metadata include telemetry data such as motion data velocity data and acceleration data captured by sensors on the camera location information captured by a GPS receiver of the camera compass heading information altitude information of the camera biometric data such as the heart rate of the user breathing of the user eye movement of the user body movement of the user and the like vehicle data such as the velocity or acceleration of the vehicle the brake pressure of the vehicle or the rotations per minute RPM of the vehicle engine or environment data such as the weather information associated with the capture of the video. The video server may receive metadata directly from the camera for instance in association with receiving video from the camera from a client device such as a mobile phone computer or vehicle system associated with the capture of video or from external metadata sources such as web pages blogs databases social networking sites or servers or devices storing information associated with the user e.g. a user may use a fitness device recording fitness data .

A user can interact with interfaces provided by the video server via the client device . The client device is any computing device capable of receiving user inputs as well as transmitting and or receiving data via the network . In one embodiment the client device is a conventional computer system such as a desktop or a laptop computer. Alternatively the client device may be a device having computer functionality such as a personal digital assistant PDA a mobile telephone a smartphone or another suitable device. The user can use the client device to view and interact with or edit videos stored on the video server . For example the user can view web pages including video summaries for a set of videos captured by the camera via a web browser on the client device .

One or more input devices associated with the client device receive input from the user. For example the client device can include a touch sensitive display a keyboard a trackpad a mouse a voice recognition system and the like. In some embodiments the client device can access video data and or metadata from the camera or one or more metadata sources and can transfer the accessed metadata to the video server . For example the client device may retrieve videos and metadata associated with the videos from the camera via a universal serial bus USB cable coupling the camera and the client device . The client device can then upload the retrieved videos and metadata to the video server .

In one embodiment the client device executes an application allowing a user of the client device to interact with the video server . For example a user can identify metadata properties using an application executing on the client device and the application can communicate the identified metadata properties selected by a user to the video server to generate and or customize a video summary. As another example the client device can execute a web browser configured to allow a user to select video summary properties which in turn can communicate the selected video summary properties to the video server for use in generating a video summary. In one embodiment the client device interacts with the video server through an application programming interface API running on a native operating system of the client device such as IOS or ANDROID . While shows a single client device in various embodiments any number of client devices may communicate with the video server .

The video server communicates with the client device the metadata sources and the camera via the network which may include any combination of local area and or wide area networks using both wired and or wireless communication systems. In one embodiment the network uses standard communications technologies and or protocols. In some embodiments all or some of the communication links of the network may be encrypted using any suitable technique or techniques. It should be noted that in some embodiments the video server is located within the camera itself.

A controller hub transmits and receives information from various I O components. In one embodiment the controller hub interfaces with LED lights a display buttons microphones such as microphones speakers and the like.

A sensor controller receives image or video input from an image sensor . The sensor controller receives audio inputs from one or more microphones such as microphone and microphone . Metadata sensors such as an accelerometer a gyroscope a magnetometer a global positioning system GPS sensor or an altimeter may be coupled to the sensor controller . The metadata sensors each collect data measuring the environment and aspect in which the video is captured. For example the accelerometer collects motion data comprising velocity and or acceleration vectors representative of motion of the camera the gyroscope provides orientation data describing the orientation of the camera the GPS sensor provides GPS coordinates identifying the location of the camera and the altimeter measures the altitude of the camera . The metadata sensors are rigidly coupled to the camera such that any motion orientation or change in location experienced by the camera is also experienced by the metadata sensors . The sensor controller synchronizes the various types of data received from the various sensors connected to the sensor controller . For example the sensor controller associates a time stamp representing when the data was captured by each sensor. Thus using the time stamp the measurements received from the metadata sensors are correlated with the corresponding video frames captured by the image sensor . In one embodiment the sensor controller begins collecting metadata from the metadata sources when the camera begins recording a video. In one embodiment the sensor controller or the microcontroller performs operations on the received metadata to generate additional metadata information. For example the microcontroller may integrate the received acceleration data to determine the velocity profile of the camera during the recording of a video.

Additional components connected to the microcontroller include an I O port interface and an expansion pack interface . The I O port interface may facilitate the receiving or transmitting video or audio information through an I O port. Examples of I O ports or interfaces include USB ports HDMI ports Ethernet ports audioports and the like. Furthermore embodiments of the I O port interface may include wireless ports that can accommodate wireless connections. Examples of wireless ports include Bluetooth Wireless USB Near Field Communication NFC and the like. The expansion pack interface is configured to interface with camera add ons and removable expansion packs such as a display module an extra battery module a wireless module and the like.

Each user of the video server creates a user account and user account information is stored in the user store . A user account includes information provided by the user such as biographic information geographic information and the like and may also include additional information inferred by the video server such as information associated with a user s previous use of a camera . Examples of user information include a username a first and last name contact information a user s hometown or geographic region other location information associated with the user and the like. The user store may include data describing interactions between a user and videos captured by the user. For example a user account can include a unique identifier associating videos uploaded by the user with the user s user account.

The video store stores videos captured and uploaded by users of the video server . The video server may access videos captured using the camera and store the videos in the video store . In one example the video server may provide the user with an interface executing on the client device that the user may use to upload videos to the video store . In one embodiment the video server indexes videos retrieved from the camera or the client device and stores information associated with the indexed videos in the video store. For example the video server provides the user with an interface to select one or more index filters used to index videos. Examples of index filters include but are not limited to the type of equipment used by the user e.g. ski equipment mountain bike equipment etc. the type of activity being performed by the user while the video was captured e.g. snowboarding mountain biking etc. the time and data at which the video was captured or the type of camera used by the user.

In some embodiments the video server generates a unique identifier for each video stored in the video store . In some embodiments the generated identifier for a particular video is unique to a particular user. For example each user can be associated with a first unique identifier such as a 10 digit alphanumeric string and each video captured by a user is associated with a second unique identifier made up of the first unique identifier associated with the user concatenated with a video identifier such as an 8 digit alphanumeric string unique to the user . Thus each video identifier is unique among all videos stored at the video store and can be used to identify the user that captured the video.

The metadata store stores metadata associated with videos stored by the video store . For instance the video server can retrieve metadata from the camera the client device or one or more metadata sources can associate the metadata with the corresponding video for instance by associating the metadata with the unique video identifier and can store the metadata in the metadata store . The metadata store can store any type of metadata including but not limited to the types of metadata described herein. It should be noted that in some embodiments metadata corresponding to a video is stored within a video file itself and not in a separate storage module.

The web server provides a communicative interface between the video server and other entities of the environment of . For example the web server can access videos and associated metadata from the camera or the client device to store in the video store and the metadata store respectively. The web server can also receive user input provided to the client device can request video summary templates or other information from a client device for use in generating a video summary and can provide a generated video summary to the client device or another external entity.

The video editing module analyzes metadata associated with a video to identify best scenes of the video based on identified events of interest or activities and generates a video summary including one or more of the identified best scenes of the video. The video editing module first accesses one or more videos from the video store and accesses metadata associated with the accessed videos from the metadata store . The video editing module then analyzes the metadata to identify events of interest in the metadata. Examples of events of interest can include abrupt changes or anomalies in the metadata such as a peak or valley in metadata maximum or minimum values within the metadata metadata exceeding or falling below particular thresholds metadata within a threshold of predetermine values for instance within 20 meters of a particular location or within and the like. The video editing module can identify events of interest in videos based on any other type of metadata such as a heart rate of a user orientation information and the like.

For example the video editing module can identify any of the following as an event of interest within the metadata a greater than threshold change in acceleration or velocity within a pre determined period of time a maximum or above threshold velocity or acceleration a maximum or local maximum altitude a maximum or above threshold heart rate or breathing rate of a user a maximum or above threshold audio magnitude a user location within a pre determined threshold distance from a pre determined location a threshold change in or pre determined orientation of the camera or user a proximity to another user or location a time within a threshold of a pre determined time a pre determined environmental condition such as a particular weather event a particular temperature a sporting event a human gathering or any other suitable event or any other event associated with particular metadata.

In some embodiments a user can manually indicate an event of interest during capture of the video. For example a user can press a button on the camera or a camera remote or otherwise interact with the camera during the capture of video to tag the video as including an event of interest. The manually tagged event of interest can be indicated within metadata associated with the captured video. For example if a user is capturing video while snowboarding and presses a camera button associated with manually tagging an event of interest the camera creates metadata associated with the captured video indicating that the video includes an event of interest and indicating a time or portion within the captured video at which the tagged event of interest occurs. In some embodiments the manual tagging of an event of interest by a user while capturing video is stored as a flag within a resulting video file. The location of the flag within the video file corresponds to a time within the video at which the user manually tags the event of interest.

In some embodiments a user can manually indicate an event of interest during capture of the video using a spoken command or audio signal. For instance a user can say Tag or Tag my moment during the capture of video to tag the video as including an event of interest. The audio tagged event of interest can be indicated within metadata associated with the captured video. The spoken command can be pre programmed for instance by a manufacturer programmer or seller of the camera system or can be customized by a user of the camera system. For instance a user can speak a command or other audio signal into a camera during a training period for instance in response to configuring the camera into a training mode or in response to the selection of a button or interface option associated with training a camera to receive a spoken command . The spoken command or audio signal can be repeated during the training mode a threshold number of times such as once twice or any number of times necessary for the purposes of identifying audio patterns as described herein and the camera system can identify an audio pattern associated with the spoken commands or audio signals received during the training period. The audio pattern is then stored at the camera and during a video capture configuration the camera can identify the audio pattern in a spoken command or audio signal received from a user of the camera and can manually tag an event of interest during the capture of video in response to detecting the stored audio pattern within the received spoken command or audio signal. In some embodiments the audio pattern is specific to spoken commands or audio signals received from a particular user and can be detected only in spoken commands or audio signals received from the particular user. In other embodiments the audio pattern can be identified within spoken commands or audio signals received from any user. It should be noted that manually identified events of interest can be associated with captured video by the camera itself and can be identified by a system to which the captured video is uploaded from the camera without significant additional post processing.

As noted above the video editing module can identify events of interest based on activities performed by users when the videos are captured. For example a jump while snowboarding or a crash while skateboarding can be identified as events of interest. Activities can be identified by the activity identifier module based on metadata associated with the video captured while performing the activities. Continuing with the previous example metadata associated with a particular altitude and a parabolic upward and then downward velocity can be identified as a snowboarding jump and a sudden slowdown in velocity and accompanying negative acceleration can be identified as a skateboarding crash .

The video editing module can identify events of interest based on audio captured in conjunction with the video. In some embodiments the video editing module identifies events of interest based on one or more spoken words or phrases in captured audio. For example if audio of a user saying Holy Smokes is captured the video editing module can determine that an event of interest just took place e.g. within the previous 5 seconds or other threshold of time and if audio of a user saying Oh no Watch out is captured the video editing module can determine that an event of interest is about to occur e.g. within the next 5 seconds or other threshold of time . In addition to identifying events of interest based on captured dialogue the video editing module can identify an event of identify based on captured sound effects captured audio exceeding a magnitude or pitch threshold or captured audio satisfying any other suitable criteria.

In some embodiments the video editing module can identify video that does not include events of interest. For instance the video editing module can identify video that is associated with metadata patterns determined to not be of interest to a user. Such patterns can include metadata associated with a below threshold movement a below threshold luminosity a lack of faces or other recognizable objects within the video audio data that does not include dialogue or other notable sound effects and the like. In some embodiments video determined to not include events of interest can be disqualified from consideration for inclusion in a generated video summary or can be hidden from a user viewing captured video in order to increase the chance that the remaining video presented to the user does include events of interest .

The activity identifier module can receive a manual identification of an activity within videos from one or more users. In some embodiments activities can be tagged during the capture of video. For instance if a user is about to capture video while performing a snowboarding jump the user can manually tag the video being captured or about to be captured as snowboarding jump . In some embodiments activities can be tagged after the video is captured for instance during playback of the video. For instance a user can tag an activity in a video as a skateboarding crash upon playback of the video.

Activity tags in videos can be stored within metadata associated with the videos. For videos stored in the video store the metadata including activity tags associated with the videos is stored in the metadata store . In some embodiments the activity identifier module identifies metadata patterns associated with particular activities and or activity tags. For instance metadata associated with several videos tagged with the activity skydiving can be analyzed to identify similarities within the metadata such as a steep increase in acceleration at a high altitude followed by a high velocity at decreasing altitudes. Metadata patterns associated with particular activities are stored in the activity store .

In some embodiments metadata patterns associated with particular activities can include audio data patterns. For instance particular sound effects words or phrases of dialogue or the like can be associated with particular activities. For example the spoken phrase nice wave can be associated with surfing and the sound of a revving car engine can be associated with driving or racing a vehicle. In some embodiments metadata patterns used to identify activities can include the use of particular camera mounts associated with the activities in capturing video. For example a camera can detect that it is coupled to a snowboard mount and video captured while coupled to the snowboard mount can be associated with the activity of snowboarding.

Once metadata patterns associated with particular activities are identified the activity identifier module can identify metadata patterns in metadata associated with other videos and can tag or associate other videos associated with metadata including the identified metadata patterns with the activities associated with the identified metadata patterns. The activity identifier module can identify and store a plurality of metadata patterns associated with a plurality of activities within the activity store . Metadata patterns stored in the activity store can be identified within videos captured by one user and can be used by the activity identifier module to identify activities within videos captured by the user. Alternatively metadata patterns can be identified within videos captured by a first plurality of users and can be used by the activity identifier module to identify activities within videos captured by a second plurality of users including at least one user not in the first plurality of users. In some embodiments the activity identifier module aggregates metadata for a plurality of videos associated with an activity and identifies metadata patterns based on the aggregated metadata. As used herein tagging a video with an activity refers to the association of the video with the activity. Activities tagged in videos can be used as a basis to identify best scenes in videos as described above and to select video clips for inclusion in video summary templates as described below .

Videos tagged with activities can be automatically uploaded to or shared with an external system. For instance if a user captures video the activity identifier module can identify a metadata pattern associated with an activity in metadata of the captured video in real time as the video is being captured or after the video is captured for instance after the video is uploaded to the video server . The video editing module can select a portion of the captured video based on the identified activity for instance a threshold amount of time or frames around a video clip or frame associated with the identified activity. The selected video portion can be uploaded or shared to an external system for instance via the web server . The uploading or sharing of video portions can be based on one or more user settings and or the activity identified. For instance a user can select one or more activities in advance of capturing video and captured video portions identified as including the selected activities can be uploaded automatically to an external system and can be automatically shared via one or more social media outlets.

The video editing module identifies best scenes associated with the identified events of interest for inclusion in a video summary. Each best scene is a video clip portion or scene video clips hereinafter and can be an entire video or a portion of a video. For instance the video editing module can identify video clips occurring within a threshold amount of time of an identified event of interest such as 3 seconds before and after the event of interest within a threshold number of frames of an identified event of interest such as 24 frames before and after the event of interest and the like. The amount of length of a best scene can be pre determined and or can be selected by a user.

The amount or length of video clip making up a best scene can vary based on an activity associated with captured video based on a type or value of metadata associated with captured video based on characteristics of the captured video based on a camera mode used to capture the video or any other suitable characteristic. For example if an identified event of interest is associated with an above threshold velocity the video editing module can identify all or part of the video corresponding to above threshold velocity metadata as the best scene. In another example the length of a video clip identified as a best scene can be greater for events of interest associated with maximum altitude values than for events of interest associated with proximity to a pre determined location.

For events of interest manually tagged by a user the length of a video clip identified as a best scene can be pre defined by the user can be manually selected by the user upon tagging the event of interest can be longer than automatically identified events of interest can be based on a user selected tagging or video capture mode and the like. The amount or length of video clips making up best scenes can vary based on the underlying activity represented in captured video. For instance best scenes associated with events of interest in videos captured while boating can be longer than best scenes associated with events of interest in videos captured while skydiving.

In some embodiments the video editing module identifies best scenes in a video based on various motion values associated with the video frames of a video. A motion value associated with a frame of a video is a measure of the motion of a portion of or the entire frame of the video. Motion values can include motion vectors which represent a movement of a frame macroblock from a first frame to a second frame. Similarly a motion value can represent a motion of an object depicted within the video from a location in a first frame to a location in a second frame and to subsequent locations in subsequent frames . Motion values can include a first value representative of a motion direction for a video frame portion and a second value representative of motion magnitude of the motion of the video frame portion. A motion value can also be representative of the motion within an entire frame for instance the sum of all motion vectors associated with portions of the frame. In some embodiments the video editing module analyzes video to identify motion values for each frame of the video for instance for each of a plurality of portions of each frame of the video .

The video editing module may identify portions of the video from which to identify best scenes based on the motion values associated with the frames of the video as is further described in conjunction with below. In addition to identifying best scenes based on the motion values associated with each frame the video editing module may identify a motion profile associated with a video and determine from the motion profile associated with the video portions of the video from which to identify best scenes. The motion profile associated with a video is a measure of global or local motion throughout the video. Portions of the video including steady global or local motion can be identified as best scenes as is further described in conjunction with below.

The identified video portions make up the best scenes as described herein. The video editing module generates a video summary by combining or concatenating some or all of the identified best scenes into a single video. The video summary thus includes video portions of events of interest beneficially resulting in a playable video including scenes likely to be of greatest interest to a user. The video editing module can receive one or more video summary configuration selections from a user each specifying one or more properties of the video summary such as a length of a video summary a number of best scenes for inclusion in the video summary and the like and can generate the video summary according to the one or more video summary configuration selections. In some embodiments the video summary is a renderable or playable video file configured for playback on a viewing device such as a monitor a computer a mobile device a television and the like . The video summary can be stored in the video store or can be provided by the video server to an external entity for subsequent playback. Alternatively the video editing module can serve the video summary from the video server by serving each best scene directly from a corresponding best scene video file stored in the video store without compiling a singular video summary file prior to serving the video summary. It should be noted that the video editing module can apply one or more edits effects filters and the like to one or more best scenes within the video summary or to the entire video summary during the generation of the video summary.

In some embodiments the video editing module ranks identified best scenes. For instance best scenes can be ranked based on activities with which they are associated based on metadata associated with the best scenes based on length of the best scenes based on a user selected preference for characteristics associated with the best scenes or based on any other suitable criteria. For example longer best scenes can be ranked higher than shorter best scenes. Likewise a user can specify that best scenes associated with above threshold velocities can be ranked higher than best scenes associated with above threshold heart rates. In another example best scenes associated with jumps or crashes can be ranked higher than best scenes associated with sitting down or walking. Generating a video summary can include identifying and including the highest ranked best scenes in the video summary.

In some embodiments the video editing module classifies scenes by generating a score associated with each of one or more video classes based on metadata patterns associated with the scenes. Classes can include but are not limited to content related classes snow videos surfing videos etc. video characteristic classes high motion videos low light videos etc. video quality classes mode of capture classes based on capture mode mount used etc. sensor data classes high velocity videos high acceleration videos etc. audio data classes human dialogue videos loud videos etc. number of cameras used single camera videos multi camera videos etc. activity identified within the video and the like. Scenes can be scored for one or more video classes the scores can be weighted based on a pre determined or user defined class importance scale and the scenes can be ranked based on the scores generated for the scenes.

In one example the video editing module analyzes metadata associated with accessed videos chronologically to identify an order of events of interest presented within the video. For example the video editing module can analyze acceleration data to identify an ordered set of video clips associated with acceleration data exceeding a particular threshold. In some embodiments the video editing module can identify an ordered set of events occurring within a pre determined period of time. Each event in the identified set of events can be associated with a best scene if the identified set of events is chronologically ordered the video editing module can generate a video summary by a combining video clips associated with each identified event in the order of the ordered set of events.

In some embodiments the video editing module can generate a video summary for a user using only videos associated with or captured by the user. To identify such videos the video editing module can query the video store to identify videos associated with the user. In some embodiments each video captured by all users of the video server includes a unique identifier identifying the user that captured the video and identifying the video as described above . In such embodiments the video editing module queries the video store with an identifier associated with a user to identify videos associated with the user. For example if all videos associated with User A include a unique identifier that starts with the sequence X1Y2Z3 an identifier unique to User A the video editing module can query the video store using the identifier X1Y2Z3 to identify all videos associated with User A. The video editing module can then identify best scenes within such videos associated with a user and can generate a video summary including such best scenes as described herein.

In addition to identifying best scenes the video editing module can identify one or more video frames that satisfy a set of pre determined criteria for inclusion in a video summary or for flagging to a user as candidates for saving as images photograph stills. The pre determined criteria can include metadata criteria including but not limited to frames with high motion or blur in a first portion of a frame and low motion or blur in another portion of a frame frames associated with particular audio data such as audio data above a particular magnitude threshold or audio data associated with voices or screaming frames associated with above threshold acceleration data or frames associated with metadata that satisfies any other metadata criteria as described herein. In some embodiments users can specify metadata criteria for use in flagging one or more video frames that satisfy pre determined criteria. Similarly in some embodiments the video editing module can identify metadata patterns or similarities in frames selected by a user to save as images photograph stills and can identify subsequent video frames that include the identified metadata patterns or similarities for flagging as candidates to save as images photograph stills.

In one embodiment the video editing module retrieves video summary templates from the template store to generate a video summary. The template store includes video summary templates each describing a sequence of video slots for including in a video summary. In one example each video summary template may be associated with a type of activity performed by the user while capturing video or the equipment used by the user while capturing video. For example a video summary template for generating video summaries of a ski tip can differ from the video summary template for generating video summaries of a mountain biking trip.

Each slot in a video summary template is a placeholder to be replaced by a video clip or scene when generating a video summary. Each slot in a video summary template can be associated with a pre defined length and the slots collectively can vary in length. The slots can be ordered within a template such that once the slots are replaced with video clips playback of the video summary results in the playback of the video clips in the order of the ordered slots replaced by the video clips. For example a video summary template may include an introductory slot an action slot and a low activity slot. When generating the video summary using such a template a video clip can be selected to replace the introductory slot a video clip of a high action event can replace the action slot and a video clip of a low action event can replace the low activity slot. It should be noted that different video summary templates can be used to generate video summaries of different lengths or different kinds.

In some embodiments video summary templates include a sequence of slots associated with a theme or story. For example a video summary template for a ski trip may include a sequence of slots selected to present the ski trip narratively or thematically. In some embodiments video summary templates include a sequence of slots selected based on an activity type. For example a video summary template associated with surfing can include a sequence of slots selected to highlight the activity of surfing.

Each slot in a video summary template can identify characteristics of a video clip to replace the slot within the video summary template and a video clip can be selected to replace the slot based on the identified characteristics. For example a slot can identify one or more of the following video clip characteristics motion data associated with the video clip altitude information associated with the video clip location information associated with the video clip weather information associated with the clip or any other suitable video characteristic or metadata value or values associated with a video clip. In these embodiments a video clip having one or more of the characteristics identified by a slot can be selected to replace the slot.

In some embodiments a video clip can be selected based on a length associated with a slot. For instance if a video slot specifies a four second length a four second give or take a pre determined time range such as 0.5 seconds video clip can be selected. In some embodiments a video clip shorter than the length associated with a slot can be selected and the selected video clip can replace the slot reducing the length of time taken by the slot to be equal to the length of the selected video clip. Similarly a video clip longer than the length associated with a slot can be selected and either 1 the selected video clip can replace the slot expanding the length of time associated with the slot to be equal to the length of the selected video clip or 2 a portion of the selected video clip equal to the length associated with the slot can be selected and used to replace the slot. In some embodiments the length of time of a video clip can be increased or decreased to match the length associated with a slot by adjusting the frame rate of the video clip to slow down or speed up the video clip respectively. For example to increase the amount of time taken by a video clip by 30 30 of the frames within the video clip can be duplicated. Likewise to decrease the amount of time taken by a video clip by 60 60 of the frames within the video clip can be removed.

To generate a video summary using a video summary template the video editing module accesses a video summary template from the template store . The accessed video summary template can be selected by a user can be automatically selected for instance based on an activity type or based on characteristics of metadata or video for use in generating the video summary or can be selected based on any other suitable criteria. The video editing module then selects a video clip for each slot in the video summary template and inserts the selected video clips into the video summary in the order of the slots within the video summary template.

To select a video clip for each slot the video editing module can identify a set of candidate video clips for each slot and can select from the set of candidate video clips for instance by selecting the determined best video from the set of candidate video clips according to the principles described above . In some embodiments selecting a video clip for a video summary template slot identifying a set of video characteristics includes selecting a video clip from a set of candidate video clips that include the identified video characteristics. For example if a slot identifies a video characteristic of velocity over 15 mph the video editing module can select a video clip associated with metadata indicating that the camera or a user of the camera was traveling at a speed of over 15 miles per hour when the video was captured and can replace the slot within the video summary template with the selected video clip.

In some embodiments video summary template slots are replaced by video clips identified as best scenes as described above . For instance if a set of candidate video clips are identified for each slot in a video summary template if one of the candidate video slips identified for a slot is determined to be a best scene the best scene is selected to replace the slot. In some embodiments multiple best scenes are identified for a particular slot in such embodiments one of the best scenes can be selected for inclusion into the video summary based on characteristics of the best scenes characteristics of the metadata associated with the best scenes a ranking of the best scenes and the like. It should be noted that in some embodiments if a best scene or other video clip cannot be identified as an above threshold match for clip requirements associated with a slot the slot can be removed from the template without replacing the slot with a video clip.

In some embodiments instead of replacing a video summary template slot with a video clip an image or frame can be selected and can replace the slot. In some embodiments an image or frame can be selected that satisfies one or more pre determined criteria for inclusion in a video summary as described above. In some embodiments an image or frame can be selected based on one or more criteria specified by the video summary template slot. For example if a slot specifies one or more characteristics an image or frame having one or more of the specified characteristics can be selected. In some embodiments the video summary template slot can specify that an image or frame is to be selected to replace the slot. When an image or frame is selected and used to replace a slot the image or frame can be displayed for the length of time associated with the slot. For instance if a slot is associated with a four second period of display time an image or frame selected and used to replace the slot can be displayed for the four second duration.

In some embodiments when generating a video summary using a video summary template the video editing module can present a user with a set of candidate video clips for inclusion into one or more video summary template slots for instance using a video summary generation interface. In such embodiments the user can presented with a pre determined number of candidate video clips for a particular slot and in response to a selection of a candidate scene by the user the video editing module can replace the slot with the selected candidate video clip. In some embodiments the candidate video clips presented to the user for each video summary template slot are the video clips identified as best scenes as described above . Once a user has selected a video clip for each slot in a video summary template the video editing module generates a video summary using the user selected video clips based on the order of slots within the video summary template.

In one embodiment the video editing module generates video summary templates automatically and stores the video summary templates in the template store . The video summary templates can be generated manually by experts in the field of video creation and video editing. The video editing module may provide a user with a user interface allowing the user to generate video summary templates. Video summary templates can be received from an external source such as an external template store. Video summary templates can be generated based on video summaries manually created by users or based on an analysis of popular videos or movies for instance by including a slot for each scene in a video .

Events of interest within the accessed video are identified based on the accessed metadata associated with the video. Events of interest can be identified based on changes in telemetry or location data within the metadata such as changes in acceleration or velocity data based on above threshold values within the metadata such as a velocity threshold or altitude threshold based on local maximum or minimum values within the data such as a maximum heart rate of a user based on the proximity between metadata values and other values or based on any other suitable criteria. Best scenes are identified based on the identified events of interest. For instance for each event of interest identified within a video a portion of the video corresponding to the event of interest such as a threshold amount of time or a threshold number of frames before and after the time in the video associated with the event of interest is identified as a best scene. A video summary is then generated based on the identified best scenes for instance by concatenating some or all of the best scenes into a single video.

A set of candidate video clips is identified for each slot for instance based on the criteria specified by each slot based on video clips identified as best scenes as described above or based on any other suitable criteria. For each slot a candidate video clip is selected from among the set of candidate video clips identified for the slot. In some embodiments the candidate video clips in each set of candidate video clips are ranked and the most highly ranked candidate video clip is selected. The selected candidate video clips are combined to generate a video summary. For instance the selected candidate video clips can be concatenated in the order of the slots of the video summary template with which the selected candidate video clips correspond.

A second video and associated metadata is accessed . The metadata pattern is identified within the metadata associated with the second video. Continuing with the previous example the metadata associated with the second video is analyzed and the defined change in acceleration metadata and altitude metadata is identified within the examined metadata. In response to identifying the metadata pattern within the metadata associated with the second video the second video is associated with the identified activity.

A motion value associated with each frame of the accessed video is identified . In one embodiment the motion value is determined based on the summation of the magnitude of all the motion vectors in each frame. In one example each frame is divided into a plurality of portions

 e.g. macro blocks and each portion is associated with a first motion vector along the horizontal axis and a second motion vector along the vertical axis. The motion vectors associated with the portions of a frame represent the motion of elements within the portions of the frame. In this example a motion value is identified for each frame by summing up the motion vectors associated with each portion of the frame. For instance a net motion vector for each portion is determined by summing the squares of the motion vectors along the horizontal and vertical axis of each portion and the motion value for the frame is determined by summing the net motion vectors of the portions of the frame. Thus in this example the motion value associated with a frame represents a measure of the total or collective motion of the objects in the frame.

A differential motion value is identified for each adjacent pair of frames of the accessed video. As the motion value of a frame represents the velocity of content of the frame the differential motion value between adjacent frames represents the acceleration of content between frames. In one embodiment the differential motion value for a pair of frames is identified by determining the difference between the motion value associated with each frame. In one example a differential motion value is associated with a frame by determining the difference in motion values between a frame and an immediately previous frame.

In some embodiments a threshold number of differential motion values associated with a sequence of frames of the accessed video are aggregated to identify a single differential motion value for the sequence of frames or an average differential motion value for each pair of frames in the sequence. This helps to reduce possible noise in motion vector values associated with the frames of the accessed video and increases the likelihood of identifying interesting areas of acceleration in the accessed video. For example motion vectors for macroblocks at corresponding locations for each frame in a group of pictures or GOP or for all frames in the GOP other than the I frame can be averaged producing an array of motion vector averages one for each macroblock location within a frame and reducing noise that may otherwise be present within the motion vectors.

Each frame of the accessed video is scored based on the motion value and the differential motion value associated with each frame. The motion value associated with a frame and the differential motion value associated with the frame may be combined in a plurality of different ways to generate a score for the frame. For example the motion value and the differential motion value may be summed to generate a score for a frame. The score associated with a frame indicates the amount of motion associated with the elements or objects within the frame. Thus frames with high amounts of motions are associated with higher scores as compared to frames with lower amounts of motion. In some examples after each frame is scored the plurality of frames are chronologically ordered and smoothed with a Gaussian filter for example to limit the possible undesirable effects of noisy video frames.

Best scenes are identified based on the scores associated with the frames of the accessed video. For instance a set of frames associated with greater than a threshold value of scores are identified and a portion of the accessed video corresponding to each frame in the set of frames such as a threshold amount of time or a threshold number of frames before and after the frame in the video is identified as a best scene. In another example a top number of scores of the plurality of scores e.g. the top 3 scores are identified. The corresponding frames associated with top number of scores are identified and portions of the accessed video corresponding to each of the identified frames such as a threshold amount of time or a threshold number of frames before and after the frame in the video are identified as best scenes. A video summary is then generated based on the identified best scenes for instance by concatenating some or all of the best scenes into a single video.

In some embodiments a set of frames associated scores that are lower than a threshold value or a threshold number of frames associated with the lowest scores are identified and a portion of the accessed video corresponding to each frame is identified as a best scene. In some embodiments sequence of successive frames associated with average scores that are greater than or lower than a pre determined threshold are identified and scene is identified based on each identified sequence of frames. In some embodiments frames are identified by one of motion values associated with the frames and differential motion values associated with the frames and best scenes are identified based on the identified frames. In some embodiments frames are identified based on motion values associated with parts of frames. For instance for frames representative of scenes including the sky the motion values associated with the portions of frame including the sky may be lower than the motion values associated with the remainder of the frame. In such embodiments frames can be identified that include a below threshold motion value associated with a first frame portion and an above threshold motion value associated with a second frame portion and scenes can be identified based on these identified frames.

Each video frame of the accessed video is associated with global motion and local motion. The global motion associated with a frame is a measure of the motion of the frame as a whole while the local motion associated with a frame is a measure of the motion of a portion of the frame with respect to the motion of the other portions of the frame. Thus a frame associated with a high local motion value includes a portion of the frame with a high motion value with respect to other portions of the frame. The global motion associated with a frame represents a measure of the total motion of all the portions of the frame.

Each video frame is divided into a number of tiles. The number of tiles each frame is divided into may be based on the resolution of the accessed video. For example higher resolution video may correspond to a greater number of tiles each frame of the accessed video is divided into. A motion value associated with each tile of each frame of the accessed video is identified . In one embodiment the motion value is determined based on the summation of the magnitude of all the motion vectors in each tile.

A motion profile of the accessed video is determined . The motion profile of the accessed video represents the percentage of tiles within each frame having a motion value that is greater than a first threshold value. In alternative embodiments the motion profile of the accessed video further represents the percentage of tiles within each frame having a motion value below a second threshold value less than the first threshold value. The motion profile of the accessed video may be examined to identify portions of steady global motion unsteady global motion and steady local motion in the accessed video. Steady global motion refers to motion associated with a frame or set of frames of the accessed video captured by a substantially motionless camera wherein the motion or activity within the frame or set of frames is substantially discernable to a human viewer or above a motion or activity threshold. Unsteady global motion refers to motion associated with a frame or set of frames of the accessed video captured by a camera in motion wherein the motion or activity within the frame or set of frames is substantially undiscernible to a human viewer or below a motion or activity threshold. Steady local motion refers to local motion within a frame or set of frames of the accessed video captured by a substantially motionless camera that is substantially discernable to a human viewer or above a motion or activity threshold. The motion profile of each accessed video can be determined in advance or in response to receiving the request to generate a video summary. The motion profile of the accessed video is a measure of global or local motion throughout the accessed video. Portions of the accessed video including steady global or local motion can be identified to determine best scenes for inclusion in a video summary for the accessed video.

In one embodiment the motion profile of the accessed video is generated by putting together motion profile values associated with each frame of the accessed video. A motion profile value for each frame is determined based on the number of tiles greater than the threshold value and dividing the number of tiles greater than the threshold value by the total number of tiles within each frame. For example a frame may include 10 tiles 5 of which have a motion value greater than the threshold value. The corresponding motion profile value associated with the frame is 0.5 or 50 as half the tiles are greater than the threshold value. The motion profile for the accessed video can be generated by compiling the motion profile values of each of the frames of the accessed video. Thus the motion profile for the accessed video can include a sequence of values wherein each value is associated with a frame and represents the percentage of tiles within the frame that are greater than a first threshold value. In some embodiments the motion profile for the accessed video can further include a second sequence of values each associated with a frame and representing the percentage of tiles within the frame that are less than a second threshold value less than the first threshold value.

The motion profile of the accessed video may vary for instance based on the selection of the first threshold value and if relevant second threshold value and thereby may be examined in a number of ways to identify best scenes in the accessed video. In one embodiment the first threshold value is set such that portions of the accessed video including unsteady global motion or fast global motion may be identified. For example a high threshold value results in portions of the video having motion profile values greater than a predetermined percentage e.g. 50 as representing portions of the video with undesirable amounts of fast global motion. Frames with motion profile values greater than the predetermined percentage have a relatively high global motion and can be classified as unsteady and undesirable possibly indicative of video captured by a shaky camera. Such frames can be disqualified from consideration as a best scene for inclusion in a video summary prior to the identification of best scenes as described above.

In one embodiment the second threshold value is set such that portions of the accessed video including slow moving motion such as steady global or local motion may be identified. For example a relatively low second threshold value results in portions of the video having motion profile values lower than a certain percentage e.g. 50 70 as representing portions of the video with slow moving motion or steady global and local motion. Frames with motion profile values lower than the predetermined percentage have a relatively low global and local motion and can be classified as steady and desirable possibly indicative of video captured by a steady camera. Best scenes can be identified based on such frames for inclusion in a video summary as described above.

In one embodiment the first and or second threshold values are set such that portions of the accessed video including bands of global motion in the motion profile of the accessed video may be identified. A band of global motion refers to portions of the motion profile of the accessed video including the same or similar motion profile values such as a first set of frames of the accessed video having a similar motion profile value as a second set of frames. For example a nominal threshold value results in the motion profile of the accessed video having a plurality of motion bands and a plurality of motion peaks. The motion peaks represent the rapid increases in motion profile values in short periods of time. Given the nominal threshold the motion profile is likely to have motion bands having values greater than a certain percentage as well as motion peaks that rise rapidly above the certain percentage. Motion peaks typically represent rapidly moving undesirable global motion as the percentage of tiles having large motion values rapidly increases between frames in a short period of time a characteristic that may be attributed to unsteady shots. In some embodiments motion peaks are scored and weighted based on a width of the motion peak determined for instance to be the full width of the peak at half the maximum amplitude of the peak . In such embodiments peaks with broader widths which can represent highlights with consistent motion velocities may be scored higher than peaks with narrow widths which can represent short jerk camera or object motion . Accordingly best scenes can be selected based on the scored peaks for instance scenes associated with above threshold peak scores can be selected .

Motion bands greater than a certain percentage may represent interesting portions of fast moving steady global motion as a similar percentage of tiles in a set of frames have motion values greater than the nominal threshold value. However in some examples motion bands that run for greater than a threshold period of time or include greater than a threshold number of frames with motion values greater than the nominal threshold may represent portions of unsteady global motion that may be undesirable to the user. In this example the portions of the video including motion bands that last or run for less than a threshold period of time or for less than a threshold number of frames may be identified as best scenes. For example the portions of the accessed video including motion bands may be scored based on one or more of the motion value associated with each frame the differential motion value associated with each frame the length of time of the motion band whether the motion band exceeds a threshold period of time corresponding to unsteady or undesirable video and or the number of frames within the motion band. Accordingly best scenes may be identified based on the scores as is further described above.

In one embodiment the first and or second threshold values are set such that portions of the accessed video including high amounts of local motion or fast local motion may be identified. For example a nominal threshold value can be selected such that the motion profile of the accessed video including motion peaks having maximum motion values lower than a certain percentage e.g. 30 or within a low range of percentages e.g. 10 30 can be determined. As motion peaks represent portions of the motion profile of the accessed video within which the motion values rise at a rapid pace or within a few number of frames motion peaks with relatively low motion profile values represent portions of the video including large amounts of local motion as the percentage of tiles with motion values higher than the threshold value within a frame is relatively low. However the remaining tiles within the frame and the frames near the frame in question have motion values less than the threshold value therefore implying that the motion peaks are associated with portions of the accessed video including fast local motion. In this example the portions of the video including fast local motion or motion peaks having maximum motion values lower than a percentage may be identified as best scenes. For example the portions of the accessed video including fast local motion may be scored based on one or more of the motion value associated with each frame the differential motion value associated with each frame the amount of fast local motion within each portion and the number of motion peaks having maximum values lower than a predetermined threshold. Best scenes may then be identified based on the scores as described above.

A video summary is then generated based on the identified best scenes for instance by concatenating some or all of the best scenes into a single video as described above.

Throughout this specification some embodiments have used the expression coupled along with its derivatives. The term coupled as used herein is not necessarily limited to two or more elements being in direct physical or electrical contact. Rather the term coupled may also encompass two or more elements are not in direct contact with each other but yet still co operate or interact with each other or are structured to provide a thermal conduction path between the elements.

Likewise as used herein the terms comprises comprising includes including has having or any other variation thereof are intended to cover a non exclusive inclusion. For example a process method article or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process method article or apparatus.

In addition use of the a or an are employed to describe elements and components of the embodiments herein. This is done merely for convenience and to give a general sense of the invention. This description should be read to include one or at least one and the singular also includes the plural unless it is obvious that it is meant otherwise.

Finally as used herein any reference to one embodiment or an embodiment means that a particular element feature structure or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase in one embodiment in various places in the specification are not necessarily all referring to the same embodiment.

Upon reading this disclosure those of skill in the art will appreciate still additional alternative structural and functional designs for a camera expansion module as disclosed from the principles herein. Thus while particular embodiments and applications have been illustrated and described it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications changes and variations which will be apparent to those skilled in the art may be made in the arrangement operation and details of the method and apparatus disclosed herein without departing from the spirit and scope defined in the appended claims.

