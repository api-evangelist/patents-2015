---

title: Systems and methods for a file-level cache
abstract: A multi-level cache comprises a plurality of cache levels, each configured to cache I/O request data pertaining to I/O requests of a different respective type and/or granularity. The multi-level cache may comprise a file-level cache that is configured to cache I/O request data at a file-level of granularity. A file-level cache policy may comprise file selection criteria to distinguish cacheable files from non-cacheable files. The file-level cache may monitor I/O requests within a storage stage, and may service I/O requests from a cache device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09405476&OS=09405476&RS=09405476
owner: SANDISK TECHNOLOGIES LLC
number: 09405476
owner_city: Plano
owner_country: US
publication_date: 20150331
---
This disclosure relates to caching input output request data on a non volatile cache which may comprise a plurality of distinct cache levels including a file level cache.

Various types of computing environments share resources across multiple hosts or other systems. For example virtualized systems and virtualized environments often support the sharing and load balancing of resources across multiple hosts or other systems. In this example a single host can support multiple virtual machines that share common hardware components storage systems and the like. These virtual machines may also be referred to as guest operating systems as each host is capable of supporting multiple instances of one or more operating systems.

When sharing a data storage system across multiple hosts or multiple virtual machines the computing environment must properly manage a high volume of data input output I O operations. The volume of I O operations is commonly measured in IOPS I O Operations Per Second . illustrates an example of an existing virtualized environment including multiple hosts and and a data storage system . In a particular implementation hosts are servers or other computing devices capable of performing a variety of processing and computing functions. Each host includes multiple virtual machines operating simultaneously.

During their normal operation virtual machines initiate data I O requests such as data read requests and data write requests associated with data storage system . Data storage system includes multiple data storage drives and or other data storage mechanisms. The storage resources associated with data storage system are shared among the multiple hosts and the virtual machines included in those hosts. Each host includes a virtualization kernel also referred to as a hypervisor that manages the virtual machines as well as shared resources such as data storage system .

As the number of virtual machines associated with a particular host increases there is a corresponding increase in demand for shared resources such as memory and I O resources. An increase in I O resource utilization includes an increased number of data I O operations that cause a corresponding increase in data communicated between a host and a data storage system. In existing virtualized systems the increased demand for shared resources such as I O bandwidth often degrades the performance or application throughput of latency sensitive workload operations within a virtualized system. In these situations one or more of the virtual machines experiences increased latency or decreased throughput which may decrease the performance of the virtual machines. Thus it is desirable to provide a computing environment that improves the handling of data I O operations associated with multiple hosts or other systems.

It is also desired to provide approaches that leverage existing virtual machine system applications that depend on offsite resources but still optimizes memory and I O resources. According to embodiments of the invention some of these approaches require a local system to cooperate with existing virtual system operating systems to appear to be operating as expected and without any different circumventing or disruptive operations. As will be seen the invention provides such approaches each in an elegant manner.

The systems and methods described herein relate to the management of data input output I O operations in a computing environment. Although particular examples discussed herein relate to virtualized environments the same systems and methods are applicable to any type of computing environment. In particular implementations the described systems and methods intercept I O operations in the virtualized environment to dynamically allocate resources such as cache resources across multiple virtual machines in the virtualized environment. This management of data I O operations improves the performance of the virtual machines and reduces the number of I O operations handled by the primary storage system. Additionally the management of I O operations is transparent to other components in the virtualized environment and can be implemented without modification to existing application software or existing data storage systems. Thus operating systems that currently exist will be oblivious to the operations of the embodiments described herein which will cooperate with the basic operation characteristics of virtual operating systems and not disrupt them while better optimizing the operations of virtual machines resident in hosts.

Specific systems and methods described herein utilize a cache memory constructed with various memory devices such as flash memory devices or RAM random access memory that may or may not be block oriented. The systems and methods described herein do not differentiate between Flash memory RAM or other types of memory and further envision new types of memory developed in the future that will utilize various embodiments described herein. The described systems and methods may utilize any type of memory device regardless of the specific type of memory device shown in any figures or described herein. Particular systems and methods described herein may generally be referred to as an I O hypervisor due to its management of I O operations in a virtualized environment.

Each virtual machine can implement a different operating system such as Windows Linux and so forth. In a particular embodiment host is a computing device capable of hosting the multiple virtual machines and supporting the applications executed by the virtual machines and the functions associated with those applications. Host includes for example one or more processors memory devices communication devices I O interfaces and related components. Although three virtual machines are shown within host a particular embodiment may include any number of virtual machines.

Virtualization kernel manages the operation of virtual machines as well as other components and services provided by host . For example virtualization kernel handles various I O operations associated with a primary storage system or other storage devices. Primary storage system is shared among the multiple virtual machines and may be shared by multiple hosts. In a particular embodiment primary storage system includes multiple disk drives or other storage devices such as storage arrays.

Host also includes a cache provisioner and a cache containing one or more memory devices such as flash memory devices or RAM. A flash memory device is a non volatile memory that can be repeatedly erased and reprogrammed. A cache memory constructed using flash memory may also be referred to as a solid state drive SSD . Cache is managed by cache provisioner to dynamically provision capacity and IOPS to virtual machines . Cache provisioner allows multiple virtual machines to share the same cache without risk of having two virtual machines access the same cache page. Additional details regarding the operation of cache provisioner and cache are discussed herein.

Each virtual machine includes an I O driver and a cache management system also referred to as a CFS Cache File System . I O driver intercepts I O operations generated by the associated virtual machine and directs the I O operation to cache provisioner for processing. I O driver is particularly effective at intercepting I O operations due to its location within the virtual machine and its close proximity to the source of the data associated with the I O operation. I O driver may also be referred to as a device driver . In a particular embodiment the I O drivers are included with an operating system. For example each device may comprise a respective device driver. These device drivers have a generic component that is a part of the operating system and there is a device specific component that is typically supplied by the device vendor. In a particular embodiment the I O drivers discussed herein are implemented on top of both these drivers. These I O drivers are in the path of the device driver and intercept well known I O APIs that are published by the operating system. This architecture is often referred to as a filter driver. In a particular implementation this is referred to as a filter driver that sits above standard device drivers for I O operations.

Cache management system contained in each virtual machine interacts with cache provisioner and other components to manage access to cache . For example cache management system includes multiple cache tags that are used in associating an address in a virtual machine with a physical address in cache . Cache provisioner manages the storage capacity of cache by for example allocating cache space among the multiple virtual machines as discussed herein. The allocation information associated with a particular virtual machine is communicated to the cache management system in that virtual machine. Additional details regarding the operation of I O driver and cache provisioner as well as the use of cache tags are provided below.

In a particular embodiment each virtual machine represents a virtual desktop such as a desktop environment associated with a particular user. In this embodiment the user accesses the desktop environment via a terminal or other system. This desktop environment is commonly referred to as VDI Virtual Desktop Infrastructure . Thus a single host can replace hundreds or more individual desktop computing systems. In another embodiment each virtual machine represents a server application. In this embodiment a single host can replace any number of individual software or application servers running multiple server applications.

Virtualized environment also includes a primary storage system that is shared among the multiple host systems and the multiple virtual machines in those host systems. In a particular embodiment primary storage system includes multiple disk drives or other storage devices.

Cache management system also includes a steal candidate module that identifies stored cache data that are candidates for removal from the cache. A cache page management module manages various cache page data and related operations. A valid unit map module identifies valid data stored in a cache and or a primary storage system. A page size management module performs various page size analysis and adjustment operations to enhance cache performance. Finally an interface module allows cache management system to interact with other components devices and systems.

Procedure continues as the cache management system determines whether the requested data is available in the cache block such as cache or . If the data is determined to be in the cache block the procedure branches to block where the requested data is retrieved from the cache. If the data is not available in the cache the procedure branches to block where the requested data is retrieved from the primary storage system such as primary storage system or discussed above. After retrieving the requested data from the primary storage system the procedure determines whether to write the retrieved data to the cache block to improve the storage I O performance of the virtual machine. This determination is based on various cache policies and other factors.

The cache management system discussed herein also referred to as a Cache File System or CFS treats the flash memory devices as a cache but uses a file system model. The cache management system develops and maintains a working set for the cache. In general the working set is the set of data that should be contained in the cache to support optimal performance of the host and its supported virtual machines.

As mentioned above the cache is created using flash memory devices. These devices typically provide fast read operations but slow write operations. These slow write operations can result in a significant delay when initially developing the working set for the cache. Additionally flash devices can generally accept a limited number of write operations. After reaching the write lifetime of the flash device portions of the flash device become unusable and the integrity of the device begins to deteriorate. These characteristics of flash devices are taken into consideration by the cache management system when managing the cache.

Cache chunks are assigned to virtual machines based on the cache needs of each virtual machine. The number of chunks assigned to a particular virtual machine can change over time as the cache needs of the virtual machine changes. The number of chunks assigned to a specific virtual machine defines the cache capacity of that virtual machine. For example if two 256 MB chunks are assigned to a specific virtual machine that virtual machine s cache capacity is 512 MB. The assignment of chunks to particular virtual machines is handled by the cache provisioner such as the cache provisioner described above.

Cache tags are used in mapping storage I O addresses in a virtual machine to actual cache pages e.g. physical addresses in the cache . The cache tags can cache data associated with any storage device assigned to a virtual machine. These cache tags perform translations between the address of blocks on the storage device e.g. the primary storage system and a cache address. In some embodiments cache tags may be organized linearly in RAM or other memory. This allows the address of the cache tag to be used to locate a physical cache page because of the algorithmic assumption that each cache tag has a linear 1 1 correspondence with a physical cache page. Alternatively or in addition cache tags may be organized into another data structure such as a hashtable tree or the like.

As shown in cache tags associated with a particular virtual machine are stored within that virtual machine. The cache tags contain metadata that associates storage I O addresses to specific cache pages in the cache. In a particular embodiment each cache tag is associated with a particular page in the cache.

In a particular embodiment a thin provisioning approach is used when allocating cache chunks to the virtual machines. In this embodiment each virtual machine is allocated a particular number of cache chunks as discussed above. However the entire cache capacity is published to each of the virtual machines. For example if the total cache size is 1 TB each virtual machine reports that has access to the entire 1 TB of storage space. However the actual allocation of cache chunks may be considerably smaller e.g. 256 MB or 512 MB based on the current needs of the virtual machine. The allocated cache chunks represent a specific range of cache addresses available within the cache. The cache provisioner dynamically changes these cache chunk allocations as each virtual machine s working set requirements change. Regardless of the number of cache chunks actually allocated to a particular virtual machine that virtual machine reports that it has access to the entire 1 TB cache. The guest OS of the virtual machine operates with a virtual disk of size 1 TB. By using a thin provisioning approach the actual storage space allocated to the virtual machine can be changed dynamically without the guest operating system indicating an error condition.

If the decision is to write the retrieved data to the cache the cache management system uses the memory address of the cache tag to determine a physical cache address associated with the data to be written. The data is then written to the cache using the physical cache address associated with the data.

If the requested data is in the cache block the cache management system uses the memory address of the cache tag to determine a physical cache address associated with the requested data block . The requested data is then retrieved from the cache using the physical cache address associated with the requested data block .

Storing the cache tag information within the associated virtual machine allows the virtual machine to easily determine where the data is stored physically in the cache without having to access a different system or process. Instead the systems and methods described herein allow each virtual machine to quickly access cache tags which increases the speed and efficiency of the I O operations. Additionally the virtual machine typically understands the data it is processing better than other systems. For example the virtual machine understands the nature and context of the data it is processing. This understanding of the data enhances the development and management of an effective working set. Other systems that are external to the virtual machine may simply see the data as raw data without any context or other understanding. Thus having the cache tag information stored locally in the virtual machine enhances the operation of the virtual machine and the I O operations.

Next the virtual machine writes the data associated with the data write operation to the cache using the physical cache address block . The virtual machine also simultaneously writes the data associated with the data write operation to the primary storage system block . The original data write operation is completed when the primary storage system acknowledges a completed write operation block .

In a particular implementation the cache discussed herein is a write through cache. This type of cache writes data to both the primary storage system and the cache. A write completion is acknowledged after the write operation to the primary storage system is completed regardless of whether a corresponding write operation to the cache has completed. In specific embodiments cache write operations can be queued and completed as the cache speed allows. Thus a cache with a slow write speed or a queue of pending write operations does not degrade performance of the overall system. Cache tags associated with incomplete or queued write operations are identified as pending. After the write operation completes the associated cache tag is identified as valid . When the cache tag is identified as pending any attempted read of the data associated with the cache tag results in a cache miss causing retrieval of the requested data from the pending memory buffer associated with the I O or from the primary storage system.

As mentioned above each cache tag stored in a virtual machine is associated with a particular cache page. Additionally the systems and methods described herein are capable of dynamically allocating cache resources e.g. cache chunks to the virtual machines in a virtualized environment. Using the features of the present invention the number of cache tags associated with a particular virtual machine can be increased beyond the number of cache pages actually associated with the virtual machine. In certain embodiments a user changes the configuration of the cache management system and cache allocations by increasing the number of cache tags allocated such that a determination can be made whether a given number of cache tags will provide for efficient use of the cache by a particular virtual machine. This increase in cache tags allows the cache management system to determine whether increasing the number of cache pages assigned to the particular virtual machine will likely improve the cache hit rate for that virtual machine. In other words the systems and procedures described herein assist in determining cache misses caused by limited cache storage capacity. Specifically this allows us to determine cache capacity misses. In other embodiments a module of the cache management system may dynamically change the configuration of the cache management system and cache allocations by increasing or decreasing the number of cache tags allocated such that a determination can be made whether a given number of cache tags will provide for efficient use of the cache by a particular virtual machine.

The procedure increases the number of cache tags associated with the virtual machine without increasing the cache size block . For example the procedure may increase the number of cache tags by an amount that corresponds to assigning an additional cache chunk to the virtual machine. However the additional cache chunk is not actually assigned to the virtual machine at this point in the evaluation procedure. Next procedure monitors the cache hit rate using the increased number of cache tags block . After monitoring the cache hit rate with the increased number of cache tags for a period of time the procedure determines whether the cache hit rate has improved block . If the cache hit rate has improved as a result of the additional cache tags the procedure returns to block to further increase the number of cache tags associated with the virtual machine.

The process of increasing the number of cache tags and monitoring the results continues until the increase in cache tags does not improve the cache hit rate. At this point procedure determines the minimum number of cache tags that provide improved cache performance block . In an alternate embodiment the procedure determines an optimal number of cache tags that provide optimal cache performance. The procedure then adjusts the cache size allocated to the virtual machine based on the number of cache tags that provide improved cache hit rate performance block . Dynamic addition of cache chunks or capacity to a virtual machine is based on both the hit rate and other policy that handles cache resource provisioning to other virtual machines. The hit rate IOPS improvements and cache capacity are also adjusted using policy that can be controlled by the user or implemented algorithmically based on rules specified by the user.

In a particular embodiment the number of cache tags added at block is substantially the same as the number of the cache pages in a particular cache chunk. Thus allocating additional cache resources to the virtual machine is performed by allocating a number of cache chunks that corresponds to the minimum number of cache tags that provide improved cache performance.

Each clock hand has a different time interval. In the example of one clock hand has a time interval of ten minutes and the other clock hand has an interval of one hour. The time interval associated with each clock hand indicates the frequency with which the clock hand sweeps the clock hand data bits. For example a clock hand with a time interval of ten minutes clears one of the two clock hand data bits every ten minutes. Each time a cache page is accessed a cache hit all clock hand bits associated with the cache page are reset to a value of 1 .

As shown in all clock hand bits are initially set to 1 e.g. at time 00 00 . After the first ten minute clock sweep Bit of clock hand is cleared to 0 . The clock hand bits associated with the one hour clock hand are unchanged because the one hour clock sweep has not yet occurred. In this example the ten minute clock sweep occurs at time 00 08 which is less than ten minutes. This occurs because the initial time 00 00 is not necessarily aligned with a clock sweep time.

After a second ten minute clock sweep without any access of the cache page the Bit of clock hand is cleared leaving a clock hand value of 00 . At this time the cache page associated with this example is identified as a steal candidate i.e. the cache page is a candidate for removal from the cache due to a lack of access of the cache page data. A separate table or other listing is maintained for cache pages in which both clock hands have been cleared. Cache pages with both clock hands cleared are top candidates for steal prior to cache pages with only one clock hand cleared.

As shown in if a cache page data access occurs at time 00 22 all clock hand bits are set to 1 . At time 00 31 the one hour clock hand sweeps causing the clearing of Bit of clock hand . That bit is set along with setting all other clock hand bits at time 01 04 due to a cache page data access. Although the particular example of uses two clock hands with ten minute and one hour intervals alternate embodiments may use any number of clock hands each having any time interval.

Finally the cache tag data structure includes a valid unit map field is a dynamic field that identifies which units in a page are cached. An example of a unit within a cache page is a sector. For example a particular page may have one or more sectors that are missing or no longer valid. The valid unit map identifies the status of all units associated with a particular cache page to prevent accessing data in units that is not valid.

As discussed above cache chunks and corresponding cache tags are allocated dynamically among multiple virtual machines. The allocation is dynamic due to changes in data storage requirements changes in applications executing on the virtual machines and the like.

In a particular implementation of the systems and methods described herein the cache supports multiple page sizes. Different applications executing in the virtual environment may require different page sizes to function properly. For example some applications always perform 32K data I O operations. For these applications it is desirable to use a large cache page size such as 16K or 32K to minimize the number of data I O operations necessary to handle the 32K of data. For example if the cache page size is 4K and the application performs a 32K data I O operation eight cache pages must be accessed to read or write the 32K of data. Performing eight separate I O operations to accommodate the 32K of data is a burden on system resources and dramatically increases the number of I O operations that must be processed by the system. In contrast if the cache page size is 16K only two I O operations are required to process the 32K of data. Thus the larger cache page size reduces I O operations and the corresponding burden on system resources.

Using larger cache page sizes also reduces the number of cache tags thereby reducing the memory space required to store the cache tags. For example in a one terabyte cache having 4K cache pages 256 M cache tags are necessary to provide a single cache tag for each cache page. In the same system using 16K cache pages 64 M cache tags are needed. Thus the larger cache page size reduces the number of cache tags and the memory resources needed to store the cache tags.

Although larger cache page sizes can reduce I O operations and reduce the number of cache tags in certain situations a larger cache page size can result in underutilized cache resources. For example if a system is using a 32K cache page size and an application performs a 4K I O operation only a small fraction of the 32K page is used 28K of the page is not needed . This situation results in significant unused cache resources. Therefore the systems and methods described herein support multiple cache page sizes to improve utilization of system resources such as I O resources and cache storage resources.

Different applications have different data storage characteristics. Applications can be characterized as having sparse address spaces or dense address spaces . Sparse address spaces tend to have scattered data with significant gaps between different groupings of data. In contrast dense address spaces tend to have data that is more compact with fewer or smaller gaps between different groupings of data. When selecting cache page sizes for a particular virtual environment it is important to consider the data storage characteristics e.g. sparse or dense address spaces associated with applications executing in the virtual environment. There can be exceptions where a sparse address space may comprise groups of contiguous data where the groups are sparsely located. In such cases one can use large pages even though the address space is sparse.

In a particular embodiment data associated with existing applications can be analyzed prior to implementing a system or method of the type described herein. This prior analysis allows the system to be tuned based on typical application data. After the systems and methods are implemented the dynamic nature of the system allows for adjustments to cache page sizes cache allocations system resources and other parameters based on changes in the operation of the application.

In a particular implementation a cache is divided into multiple sections such that each section supports different cache page sizes. Because application I O workloads can vary a particular cache page size for one application may be more efficient than for another application. One objective in using different cache page sizes is to minimize the number of I O requests that cross over a cache page boundary in order to make the I O operations as efficient as possible. For example a cache may be divided into four sections two of which support 4K cache pages one that supports 16K cache pages and one that supports 32K cache pages. The cache pages in these different sections are allocated to different applications based for example on the data storage characteristics of the applications.

In one embodiment a different hash table is used for each different cache page sizes. Each hash table has its own associated hash function that identifies a particular hash slot in the table based on an address provided to the hash function. When using multiple hash tables such as a 4K hash table and a 16K hash table the systems and methods perform a lookup operation for each hash table. Performing a lookup in both hash tables is necessary because a 4K address could be contained within a 16K entry in the 16K hash table. To enhance the lookup process the systems and methods described herein apply one or more algorithms based on a percentage of cache hits associated with different cache page sizes a success rate associated with different hash tables and other factors to weight the lookup between the different hash tables and thereby improve the lookup efficiency.

In a particular implementation an algorithm uses both the percentage of cache hits associated with cache page sizes and the success rate associated with different hash tables to search for data in a cache.

In other embodiments the systems and methods use a single hash table associated with the smallest cache page size such as 4K and still presents the feature of a virtual machine using multiple different page sizes. Although the cache supports multiple cache page sizes the hash table uses a 4K page size exclusively. This approach eliminates the need to perform a lookup in multiple hash tables associated with different cache page sizes. In this scheme a 16K page I O would require four hash table lookups in the single has table and groups of cache tags are managed as one.

In certain situations it is desirable to prevent one or more cache pages from being stolen or usurped by another virtual machine. This is accomplished in the systems and methods discussed herein by pinning the cache tags associated with the cache pages that are to be protected from being stolen. Cache tags are pinned by setting the state bit to pinned state in the cache tag.

Pinning cache tags is used in a variety of situations. For example a system may freeze a group of cache tags associated with a virtual machine and move the cache tags to a persistent storage device to preserve the virtual machine s working set. Later when the virtual machine warms up the cache tags are retrieved from the persistent storage device actual data is read back from the primary or shared storage thereby recreating the working set. This allows the virtual machine to resume operation immediately with a fully functioning working set rather than taking a significant period of time recreating the working set.

Pinning cache tags are also useful to lock a range of addresses in the cache. For example a user can pin specific data within the cache to prevent the data from being replaced or modified. The user may know that the specified data is critical to the operation of the virtual machine and wants to ensure that the data is always available in the cache.

In certain situations a portion of data associated with a read operation is available in the cache but a portion is not available or not valid in the cache. This condition is referred to as a partial cache hit or a partial cache miss. In these situations the system must decide whether to retrieve all of the data from the primary storage system or retrieve a portion from the cache and the remainder from the primary storage system. The decisions involving what s available in the cache can result in more than one I O to primary or shared storage which may be more efficient when doing sequential I Os .

In certain embodiments the cache management system is configured to manage a partial cache miss as efficiently as possible to minimize the number of I O requests forwarded on to the primary storage. In addition to managing partial cache miss I O requests the cache management system mitigates the amount of fragmentation of I Os to primary storage based on I O characteristics of the I O requests. Fragmentation of I Os also known as I O splitting refers to an I O request that crosses a cache page boundary or is divided between data that resides in the cache and data that resides on the primary storage. The I O characteristics may include whether the I O is contiguous the size of the I O request the relationship of the I O request size to the cache page size and the like. In affectively managing partial cache hits and fragmentation of I O requests the cache management system may coalesce I O requests for non contiguous address ranges and or generate additional I O requests to either the cache or the primary storage.

In a particular embodiment a checksum is calculated for each cache page. When calculating the checksum the system only performs the calculation on the valid data based on a valid unit map e.g. the valid data sectors . When a write operation is performed that increases the number of valid data sectors the checksum is recalculated to include the new valid data sectors.

Computing device includes one or more processor s one or more memory device s one or more interface s one or more mass storage device s one or more Input Output I O device s and a display device all of which are coupled to a bus . Processor s include one or more processors or controllers that execute instructions stored in memory device s and or mass storage device s . Processor s may also include various types of computer readable media such as cache memory.

Memory device s include various computer readable media such as volatile memory e.g. random access memory RAM and or nonvolatile memory e.g. read only memory ROM . Memory device s may also include rewritable ROM such as Flash memory.

Mass storage device s include various computer readable media such as magnetic tapes magnetic disks optical disks solid state memory e.g. Flash memory and so forth. As shown in a particular mass storage device is a hard disk drive . Various drives may also be included in mass storage device s to enable reading from and or writing to the various computer readable media. Mass storage device s include removable media and or non removable media.

I O device s include various devices that allow data and or other information to be input to or retrieved from computing device . Example I O device s include cursor control devices keyboards keypads microphones monitors or other display devices speakers printers network interface cards modems lenses CCDs or other image capture devices and the like.

Display device includes any type of device capable of displaying information to one or more users of computing device . Examples of display device include a monitor display terminal video projection device and the like. Interface s include various interfaces that allow computing device to interact with other systems devices or computing environments. Example interface s include any number of different network interfaces such as interfaces to local area networks LANs wide area networks WANs wireless networks and the Internet. Other interfaces include a user interface and a peripheral device interface .

Bus allows processor s memory device s interface s mass storage device s and I O device s to communicate with one another as well as other devices or components coupled to bus . Bus represents one or more of several types of bus structures such as a system bus PCI bus IEEE 1394 bus USB bus and so forth.

For purposes of illustration programs and other executable program components are shown herein as discrete blocks although it is understood that such programs and components may reside at various times in different storage components of computing device and are executed by processor s . Alternatively the systems and procedures described herein can be implemented in hardware or a combination of hardware software and or firmware. For example one or more application specific integrated circuits ASICs can be programmed to carry out one or more of the systems and procedures described herein.

In another embodiment an issue of compatibility that occurs within virtual systems is addressed. In certain virtual systems some of the processes make certain assumptions about the environment in order to properly operate.

In a single host there typically will be multiple virtual machines operating in the host. Each virtual machine will have its own separate I O drivers and also separate cache management module to manage local storage operations from the perspective of each particular virtual machine. Each virtual machine needs to share the local storage cache and each virtual machine will have its own unique demand for space on the local storage cache during its operation. Multiple virtual disks may be created on the local cache storage and these can be exposed to the local virtual machines. During operation of the various virtual machines the demand can vary among the different virtual machines. As a result capacity in the local cache may not be efficiently utilized by the virtual machines and cache capacity may be wasted.

In one example a thin provisioned storage is provided such as a thin provisioned cache for dynamic allocation of storage space among multiple virtual machines within a host. Since virtual machines are dynamic in nature their demand for storage space may vary. If they share actual storage space with other virtual machines the use of the storage space by a group of virtual machines may conflict. For example if one or more virtual machines experience a higher than normal I O traffic rate their operations may become bogged down causing lags in output. Other machines may experience a lower than normal I O traffic rate at the same time leaving their allocated storage space unutilized. Thus in some cases the higher I O virtual machines use of actual storage space may be unnecessarily restricted by rigid or inefficient allocation schemes. Virtual machines may be transferred from one host to another may become inactive or offline for some period of time may power down or rest on a host that needs to power down or its demand for storage space may change up or down during operation. Thus it would be useful if the storage space allocated to the group of virtual machines could be dynamically allocated and balanced where actual storage space allocated to any one machine can be apportioned more intelligently. As such dynamic allocation of storage space could serve to reduce lag time for virtual machines that demand more space and I O transfers by provisioning more space when other virtual machines associated with the same storage demand less space. The embodiment provides such solutions in an elegant manner.

In typical virtual machine environments shared storage is utilized among multiple hosts that have equal access to the common storage space. The shared storage may be a clustered file system a virtual machine file system VMFS where the system provides correctness and consistency among the various virtual machine hosts using file based locking and other methods.

One common feature in virtual machine systems is the ability to move a virtual machine from one host to another host. VMWare has a product called VMotion that enables virtual machines to move from one host to another where the main storage of the moving virtual machine is maintained on storage that is shared among two or more hosts. The virtual machine may be a live operating virtual machine located on one host and the desire is to be able to move the virtual machine from one host to another without interruption in the virtual machine during relocation. This is possible because the multiple hosts see and share the common data storage system. Thus the virtual machine may move from one host to another without shutting down or rebooting the virtual machine the move is transparent to the moving virtual machine.

When a virtual machine boots up and begins to run it communicates with its available resources such as storage devices network devices etc. similar to a physical machine. It may send out Small Computer System Interface SCSI inquiries out to connected storage devices to determine what resources are available and it discovers what storage is available to the virtual machine. The storage available to the virtual machine is virtual storage that is encapsulated in a file. The encapsulated file is the main storage space for the virtual machine. Thus the storage for the virtual machine is now instantiated in a file and becomes a virtual hard drive. In prior art devices this file is stored in the common data storage system shared among multiple hosts.

According to one embodiment it is desired to store the virtual disk of the virtual machines hosted on a single host in local storage such as the cache storage. In such a system if a virtual machine existed that stores its main drive storage on the local cache storage located on the host a virtual machine would not be able to move from one host to another host. Again in prior art systems the virtual disk of the virtual machines is located on storage that is shared among the host that are physically separate but commonly connected to the shared storage system.

A virtual disk s block number zero translates to offset zero in the file encapsulating the virtual disk. In response to the virtual machine sending out inquires to define its storage the system replies that the virtual machine has a virtual storage. As that layer begins to receive reads and writes as SCSI Small Computer System Interface traffic it will convert this into file I O and read and write to the shared file. Thus a seed of a virtual disk is created on the shared storage that may be visible by the separate hosts. As a result once the virtual machine moves from one host to another the virtual machine may continue to operate because it can communicate with the original virtual disk associated with the virtual machine that was moved to the second host just as it did from the prior host. Therefore in order to move a virtual machine from one host to another there must be shared storage.

Once provisioned each virtual machine expects to have access to predetermined and contiguous storage space for which it has the cache tags discussed above . In one embodiment a dynamic provisioning approach is provided to divide the cache storage into chunks that can be dynamically provisioned to the separate virtual machines according to their demand for space. According to one embodiment a cache provisioner is encapsulated in a virtual logical unit number VLUN driver provided to manage the chunks of storage data that is allocated to each virtual machine. A LUN is a misnomer of an acronym known in the art as a place where a machine can read and write a block of data for example an array of storage disks or other storage devices. In a system storage devices or arrays publish storage space as addressed that do not necessarily identify a particular storage device or individual storage disks. According to one embodiment a VLUN disk is a virtual storage space allocated to a virtual machine. Since multiple virtual machines will typically be operating on a single host the chunks of storage space that come available will likely be located in different physical areas of the cache storage. A VLUN device driver creates a VLUN disk that is assigned to the virtual machine.

In virtual systems the virtual operating systems run processes and manage operations within the system with fundamental assumptions that allow different processes within the virtual system to properly operate and not conflict with other processes. In one example virtual operating systems operate with the assumption that each separate virtual machine operates with a fixed amount of storage space that typically does not change. Thus an operating system may react adversely or may not operate properly if there is sudden atypical change in storage space size for a virtual machine operating within a host or other device. Thus it may be important for a virtual machine to appear to have a fixed allotment of storage space such as cache storage space. According to one embodiment this is achieved by allocating a limited amount of physical storage space to any one virtual machine as needed by the particular machine s needs. And to avoid any potential conflict with a virtual operating system that expects to detect a fixed storage space allocated to a particular virtual machine a virtual amount of space is allocated to each virtual machine that is equal to a set amount of space that a virtual machine s operating system expect to detect. Thus in operation the virtual operating system will detect the set amount of virtual storage space that is allocated and it will appear to the operating system that that amount of space is constant and consistent. However in actual operation the space allocated to a particular virtual machine may vary according to the machine s demand for storage space. And the overall space will be traded among the virtual machines accessing storage cache to ensure that each virtual machine has no more cache storage than it actually needs while the operating system is essentially fooled to thinking that each virtual machine has a fixed amount of cache storage space allocated to it.

Thus the VLUN manager is configured to manage the dynamic allocation of the available chunks to the virtual machines that need them. The storage is thus physically managed in chunks by the VLUN driver that provides each virtual machine with the notion of contiguous chunks of storage space. The VLUN driver thus in a sense translates the allocation from the virtual space into the underlying physical chunks allocated to each virtual machine by the VLUN driver. As a result the embodiment allows the system within the host to divide up the cache storage into chunks that it can allocate on the fly to the various virtual machines using virtualization of the storage space allocated to the individual virtual machines. In operation the VLUN driver maintains mapping of the virtual space of each virtual machine to actual physical storage space located in the cache storage. This allows the VLUN to dynamically increase and decrease the size of the allocated storage space of each virtual machine.

Referring to a block diagram is shown illustrating example components of a host operating in a virtualized environment with further details regarding VLUN operations. Similar to systems discussed above host includes a user space and a virtualization kernel . User space includes multiple virtual machines and . Each virtual machine communicates with a VLUN driver that communicates with cache storage . Each virtual machine has individual I O drivers and cache management modules for managing the communications with the VLUN driver and cache. The cache memory utilized in one embodiment is Flash storage but may be other types of storage devices. Flash memory is an expensive resource that should be used efficiently and sparingly to keep costs of devices down. In one embodiment the amount of physical storage that is allocated to any one virtual machine may be varied. This would allow a relatively small cache storage space to dynamically allocate storage space to a number of virtual machines without having to allocate a fixed storage space for each virtual machine. If done properly virtual machines may be serviced with varying amounts of storage space as they each need them according to their actual demand for cache storage space and space may be allocated to increase allocations to some virtual machines and decrease allocations to others to keep the net amount of space required for a group of virtual machines to a minimum. Fixed space allocations for each virtual machine would require allocating the maximum capacity needed by each virtual machine. Thus allowing for variable allocations of space allows for device designs to have a smaller overall flash storage required compared to a device that sets a fixed amount for each virtual machine.

Map module may be configured within the VLUN to map the virtual space allotted to each virtual machine to physical space existing in the cache storage. Since the actual physical space allocated to a particular virtual machine may not be the same as the virtual cache storage space of the host the two need to be reconciled so that the virtual machine can properly store and retrieve data read and write data stored in the physical cache storage.

For example cache space allocated for Virtual Machine is illustrated diagrammatically as space in cache . The virtual space allocated to this virtual machine in this example is two terabytes 2 TB and the physical cache storage space that is actually allocated to this virtual machine in this example is four gigabytes 4 GB . As discussed above in one embodiment a virtual machine is configured to have an allocated physical storage space appear to its operating system as a fixed amount 2 TB in this example but to have an actual physical storage allocation that is necessary for the particular virtual machine s operation 4 GB in this example . The actual physical storage space for any particular virtual machine may be more or less than that which appears to the operating system. Thus the virtual barrier between a particular host s actual allocated physical space and virtual physical space may be different and may vary dynamically as groups of virtual machines that share common cache storage operate.

Furthermore the chunks of storage space allocated to a particular virtual machine may be disbursed within the physical cache space in an incongruous manner where the physical storage locations of data for one virtual machine may be interleaved with the storage locations of another virtual machine. This is a result of a configuration where chunks of physical cache storage space are allocated dynamically. The VLUN driver may usurp space from other machines that are not utilizing all of their allocated space and allocate the space to other virtual machines that need more allocated space in cache storage. Referring to a diagram illustrating the mapping function of a VLUN driver is shown. The allocated space of a virtual cache for a virtual machine is shown as spaces VM VM VM VM are shown as consecutive. The actual cache space shows the corresponding locations in actual cache space where the locations of actual cache storage space is interleaved and in different order among another virtual machine s space VM VM VM VM VM . In practice with multiple virtual machines sharing a common cache the interleaving of allocated space used by the various machines can become quite complex particularly as space gets allocated dynamically according to the need of the various virtual machines sharing the actual physical space of the common cache storage. Also though the illustration in shows some of the different locations in some physical order in practice the spaces allocated may be located in other orders including random order where space is allocated as available. Thus the VLUN driver is configured to manage the allocation of the different chunks of physical storage space within cache storage.

Further Referring to a diagrammatic system is shown to illustrate an example of a unique and novel process for changing cache capacity at run time. This is done by dynamically provisioning the amount of space a given virtual machine is allocated according to its current needs. As discussed above from the view of applications outside a virtual machine there is a fixed amount of cache storage space allocated to a virtual machine and it may or may not be the same for each virtual machine. Since multiple virtual machines may exist in a particular host machine the different machines may have varying demands for storage space and managing the allocations of space to the various virtual machines is greatly desired in order to optimize the use of the cache space. The VLUN driver therefore represents to the operating system that a large fixed amount of space is allocated to the virtual machine even though a lesser amount is actually allocated to any one virtual machine. Thus it appears fixed for the virtual machine to appear as having fixed space similar to hosts within conventional virtual systems. According to one embodiment though this appearance of a fixed amount of allocated cache space is reported the actual cache space allocated to a particular virtual machine may be dynamically provisioned by a VLUN driver according to of each virtual machine.

The host system illustrated in includes one or more virtual machines and each includes its own SCSI filter that is incorporated into the virtual machine OS SCSI stack. Each windows driver includes a cache file system CFS that is configured to operate a cache storage device in the manner of a file system. The CFS may have components that are distributed between the virtual machine and other components of the system but the individual CFS serves to manage data transfers between the virtual machine and various storage devices. An input output I O filter cooperates with the CFS to service I O requests directed toward primary storage either directly from the primary storage or from cache storage located within the host device . The primary storage may comprise a physical storage device located within the host device or a virtual disk defined on shared storage . The virtual disk may be available only to a single virtual machine while the shared storage may be accessible by a number of virtual machines. A lower level filter the small computer system interface SCSI filter is configured to manage transfers between the CFS and I O filter and the various storage devices. Traditionally SCSI has been used for transferring data between computers and peripheral devices but the SCSI filter in this embodiment is configured to manage the transfer of data among physical and virtual entities within the system . Within the virtual machine the SCSI filter is configured to determine which disk is a VLUN disk and to manage capacity changes that occur in a virtual disk that is allocated to the particular virtual machine. A VLUN disk is a virtual storage space which provides raw storage capacity for the CFS . In some embodiments the guest operating system recognizes the existence of the VLUN disk . As mentioned above the size of the VLUN disk is reported as larger than the actual raw storage capacity being made available by the VLUN driver so that the actual storage capacity can change dynamically without causing an error in the guest operating system. The SCSI filter is configured to manage the actual raw capacity of the VLUN disk and other applications in the guest operating system are unaware of the existence of the VLUN disk . In one embodiment the VLUN disk is presented to the guest operating system as a read only storage device. Consequently the guest operating system prevents other applications of the guest operating system from writing data to the VLUN disk .

As discussed above in operation though the actual storage space that is allocated to a particular virtual machine is one value another value is represented to the operating system so that the system as a whole operates in a stable manner. Thus a virtual machine may have 4 GB of actual cache storage space allocated to it but it may appear to the operating system by the virtual machine s representations that it has 2 TB of storage space allocated to it. Within the host there is a user space where the virtual machines reside and there is a virtualization kernel where a VLUN SCSI driver resides and is configured to allocate the actual space that is allocated to each virtual machine in cache storage . In order for the SCSI filter and CFS to properly operate and manage I O operations they both need to be informed of the actual storage space that is allocated to the virtual machine within the cache and they need to not be fooled that there is more space allocated to the virtual machine than has actually been provisioned to the virtual machine. There is a communication link that communicates separately from the I O data traffic between the VLUN driver and SCSI filter that informs CFS and I O filter via the SCSI filter of the actual cache storage space allocated to the virtual machine . Thus asynchronous out of band messages may be sent between the VLUN driver and the SCSI filter to inform the Windows driver of actual space allocated to the virtual machine in the system. The information reaches CFS so that CFS manages the cache tags used to manage the data stored in the allocated cache storage space within cache storage . Thus the cache is a thin provisioned cache where the operating system perceives the appearance of a large amount of space such as 2 TB for example but each virtual machine actually gets allocated the amount of storage space it actually needs 4 GB for example. The communication path allows the ability to inform the Windows driver particularly CFS of cache storage capacity changes when actual cache storage space that is allocated to the virtual machine changes.

Thus in underlying operation each virtual machine is actually allocated an amount of actual cache storage space that may vary over time as each virtual machine s storage needs change or in the event of power on off events and also events where virtual machines move from one host to another while the operating system perceives another set value that appears to not change from the perspective of the operating system. The VLUN driver manages this deception to the operating system together with the SCSI filter . For example assume for a moment that virtual machine had 4 GB of actual cache storage space located in cache storage allocated to it.

During operation a virtual machine s needs for cache storage may increase and it thus needs more cache storage space allocated to it. According to one embodiment the virtual machine may have its allocated cache storage space changed as its needs for cache storage changes. Referring to a process flow chart of a change in allocation of cache storage space is shown and will act as a process guide in conjunction with the system diagram of to illustrate how cache storage space is allocated when the cache storage needs of a virtual machine changes. As with other processes described herein certain of these specific process steps may be combined with other steps or other steps may be added in particular applications but this would not depart from the spirit and scope of the invention as defined in claims as the processes described herein are intended as merely illustrative. Assume that virtual machine needs an increase to 8 GB of cache storage as an example. The VLUN driver a SCSI type device that monitors and manages use and allocations of cache storage space for each virtual machine causes the SCSI filter to resize allocated cache storage space from 4 GB to 8 GB in this example. In step the VLUN driver instructs the SCSi filter to stop sending I O data traffic relating to the caching of data during the dynamic provisioning or re provisioning of cache storage space. The SCSI filter instructs CFS that a resize is about to take place so stop sending I O data traffic to the cache storage device . Alternatively the instruction from the VLUN driver may communicate to CFS through the SCSI filter via path to stall I O operations to the cache storage . In one embodiment while CFS stalls it does not mean that applications communicating with the operating system stop working or stop performing I O operations. Rather the I O data traffic continues to communicate between the CFS and shared storage via path through SCSI filter and virtualization kernel but not through VLUN Disk so that application I O operations in the virtual machine continue uninterrupted but will not benefit from use of the cache storage device during this brief period of re provisioning of actual storage capacity in the cache storage device . Thus applications such as Iometer Microsoft Office SQL Server and other applications can continue to operate and the I O traffic destined to the shared storage continues. CFS may also invalidate pertinent cache tags when application write operations occur during this process. CFS waits for any outstanding I O data traffic to and from the cache storage to complete in step . In step CFS notifies the VLUN driver that I Os are complete. Once the outstanding I O transfers complete a CFS stall is engaged. Thus the VLUN driver initiates the resizing from 4 GB to 8 GB in step and instructs the SCSI filter that the new allocation of cache storage space is 8 GB in step . In step the SCSI filter then instructs CFS to resize the allocation of storage space to 8 GB. In one embodiment when this is done CFS maintains the previously allocated 4 GB of cache storage space and simply adds the newly allocated space to its operations. Thus CFS can maintain the cache tags and metadata associated with the previously allocated 4 GB of cache storage space and allocates the additional 4 GB of cache storage space and assigns new cache tags as needed. In step control is returned to SCSI filter and in step the SCSI filter instructs VLUN driver that the provision change of cache storage space for virtual machine is completed. In step the VLUN driver instructs SCSI filter to resume operations. In step the SCSI filter instructs CFS to resume operations. In step the cache storage device is enabled and I O data traffic can resume to the cache storage device and the virtual machine can continue to send I O data traffic to either the cache storage device or the shared storage .

Thus a thin provisioned cache device is provided where the limitation of a fixed disk capacity requirement in conventional virtual systems has been addressed. Thus the operating system can essentially be deceived into thinking that a fixed amount of cache storage has been allocated so that applications in the operating system have no impact. And the actual cache storage space allocated to any virtual machine may be resized on the fly without impacting other system operations. The result is an intelligent and optimized utilization of cache storage where the available cache storage space is more efficiently utilized. Multiple virtual machines are dynamic in nature and their data flow and cache storage needs change dynamically. A virtual machine substantially reduces its demand for cache storage in different modes or circumstances. For example it may power off or go into sleep mode it may stall while moving from one host to another and its needs will necessarily change when these operational changes occur. A virtual machine may alternatively increase its demand for cache storage in other modes or circumstances such as when it wakes up from a sleep mode arrives at a new host after moving or simply experiences an upsurge in usage operations. This embodiment gives the host system the flexibility to dynamically change and optimizes the use of cache storage at the same time. Accordingly the amount of cache storage designed in a host system can be minimized substantially saving costs in a host system or device. The cache device which is commonly implemented in expensive flash memory is itself virtualized in this embodiment and its operations are intelligently managed in a way that optimizes the use of its storage space allocating cache storage to the various virtual machines according to their needs.

One fundamental precept of virtual systems is that shared storage must be secured among the different virtual machines. This is important because the different virtual machines may store confidential information in the various storage chunks in cache storage that possibly could be accessed by other virtual machines in the dynamic provisioning process. For example a person s confidential financial and identity information may be stored by one virtual machine in one chunk of allocated cache data storage and that machine s allocated cache storage may be resized as a result of low demand. The virtual machine may then give up allocated cache storage space to another machine in the dynamic allocation process also giving the second virtual machine that acquires the data chunk having the person s confidential information stored in that chunk. This is thus a security risk and the dynamic allocation process that has been designed to optimize the use of the cache storage may cause a problem when resizing cache space of particular machines and allocating cache storage chunks from one virtual machine to another. One embodiment of the invention addresses this security risk in an elegant manner without substantial impact to the work flow within the system while dynamically provisioning cache storage chunks.

Referring to a virtual system is illustrated showing a host having virtual machines and corresponding VLUN disks all located within guest area . The virtualization kernel has a cache storage that is divided up into physically identifiable chunks where the chunks shown are showing chunks allocated to virtual machine as VM VM and VM and chunks allocated to Virtual Machine as VM VM and VM . In operation the individual chunks must be properly handled when being allocated from one virtual machine to another in the dynamic provisioning process where the subsequent virtual machine acquiring a new chunk of cache storage space is ensured not to ever gain access to any leftover information from any prior virtual machine remaining in the chunk.

One way to address this issue is to erase or overwrite all prior information from any chunk that is allocated from one virtual machine s cache space to another ensuring that the data is deleted or otherwise rendered inaccessible to any subsequent virtual machine that gains access to the chunk. Though this is an attractive option that provides definiteness to securing information in reallocated chunks of cache storage it has drawbacks. One primary drawback is that this option requires I O data transfers to zero out or otherwise delete the stored information causing a burden on the system. In a dynamic allocation process this would require that all chunks of data storage that are to be transferred for use by a different dynamic machine to be deleted prior to the transfer by writing zeros into the chunks space which adds no value to the virtual system. One characteristic of cache storage devices that are made up of flash memory is that writing takes a long time to perform compared to reading. Thus such overhead may cause unacceptable increases in latency during dynamic provisioning such that such an option is undesirable in certain embodiments.

Another approach is to use a new primitive I O operation that flash memory storage device vendors refer to as TRIM. However not all vendors of flash memory support the TRIM command and the contents of a data block that have been trimmed are undefined and could potentially be available to another virtual machine. Thus there may be no guarantee that the old data that is in the reallocated chunk has been deleted and or is inaccessible .

Yet another approach is to monitor the chunk provisioned to a virtual machine to ensure that the virtual machine acquiring the chunk does not gain access to the old data written by another virtual machine. In certain embodiments chunks allocated to a virtual machine may come from a pool of previously erased media in certain embodiments the pool may include virgin storage media . If the chunk comprises virgin media or has been erased by the cache storage device and has received no I O write operations the chunk can be provisioned to the virtual machine without any further monitoring. As used herein such a chunk is referred to as an unused chunk. A used chunk refers to a chunk that has been previously allocated to a virtual machine and thus has the potential to contain and may in fact contain data that needs to be protected from a virtual machine that is subsequently allocated the chunk.

While the state of a chunk being monitored may be persisted for use after a power cycle event in certain embodiments in one embodiment of the present invention the VLUN driver monitors whether a particular chunk is written to or otherwise modified referred to herein as a used chunk or dirty chunk after being allocated to a previous virtual machines during a current power cycle. In one such embodiment a bit mask is used to prevent reading of data in a used chunk by a virtual machine allocated the used chunk. Preferably reading of portions of the used chunk is prevented until the portions are written to by the virtual machine now allocated the used chunk. In one embodiment each 4 kb sub portion of the used chunk is monitored to determine whether there has been a read or a write in each 4 kb sub portion. This is determined at the time the subsequent virtual machine accesses the used chunk and is performed only when necessary to prevent the reading of old data by the acquiring virtual machine. After the provisioning of the used chunk to a virtual machine each sub portion of the chunk is tested prior to any read operation on the used chunk by the acquiring virtual machine.

Referring to a process flow chart illustrates an example of such a testing operation that may be used in conjunction with the read operations of a virtual machine. The process for securing old data is called a read before write protection. This means that read operations for the old data are prevented until the address for old data has received a write operation an overwrite from the virtual machine allocated the newly provisioned chunk . Thus if any sub portion is read by a virtual machine that has acquired a chunk from one or more virtual machines it is presumed that the chunk has old data and that it must not be accessed by the later acquiring virtual machine. Here a chunk is provisioned from a first virtual machine to a second virtual machine for example chunk VM having subdivided spaces mthrough m. In step a read is initiated by the second virtual machine. Next it is determined whether there was a prior full write to entire space mby the second virtual machine.

In step a read is initiated. The process then proceeds to step where it is determined whether all pages were written to in the chunk at least once by the second virtual machine and in particular whether this has occurred since the chunk was provisioned to the second virtual machine. If yes then the need for this security test is obviated and the reads to this chunk by the second virtual machine may resume in step without further testing. If all pages of the chunk in question have not been fully written over by the second virtual machine then the reads resume in step with testing and continues on an as needed basis until each page is fully written over at least once so long as the second virtual machine continues to be allocated the chunk in question and continues to initiate reads into the chunk s pages. In step it is determined whether there was a full write of the page that covers any possible old data. This determination may be done in various ways. In one embodiment an indication of whether a particular page is partially written to or completely written over may be indicated by a bit that is on or off logic 1 or logic 0 for example. This bit may be recorded in a table such as table . This may be in the form of a type of bit mask that can be stored in cache or other storage location. The indication of whether a page is entirely written over may be indicated by a single bit that is toggled to a binary 1 when a full write over occurs. When a partial write occurs to the page of a chunk the VLUN driver converts the partial write to a full write by filling in zeros for the data space that is not covered by the partial write from CFS . Any other write to a page while the very first write is in progress will be returned with error. If a read occurs before a write then a test would show that a full write has not occurred and the binary bit should be 0 . As the chart shows the table may start out as all logical 0 indicating that the individual pages have not been fully written over since being allocated to the second virtual machine. As full write over occurs in each page the full write indicator bits eventually become more populated across the array eventually ending up over time with all logical 1 bits indicating that each and every page has been written over by the second virtual machine at least once.

If there was a prior full page write then the read is allowed in step otherwise the read is failed and not allowed in step . This process is an intelligent and efficient process for preventing read before write security breaches and substantially reduces the amount of I O traffic and latencies. Using the bit indicator approach the trade off is the use of some memory space for the indicator bits for each sub portion and the use of processor resources to perform the testing but the valuable security and the minimization of unnecessary I O traffic in the process makes this read before write testing process valuable and useful.

Referring to a virtual system configured for cache allocation management is shown where a host includes virtual machines and corresponding VLUN disks located within a guest area and that communicate with common VLUN driver located within the virtualization kernel . Cache storage is configured to store data from the virtual machines as allocated by VLUN driver . The system further includes a VLUN manager that communicates between host and management module via a TCP IP protocol connection.

The VLUN Manager is a user space daemon that configures the provisioning of the portions of the cache storage among the different virtual machines. This is an application that runs on the host to parcel out the cache storage device fairly and efficiently among the virtual machines residing on the host. When a management module is establishing allocations for virtual machines. This sets the relative proportions allocated among the various virtual machines. Shares of storage space are then defined for each virtual machine for example 

These shares are used as an abstract definition of proportions of cache storage that is allocated to particular machines without regard to actual size and space that is allocated to a particular virtual machine. This way the definition and system works with any size cache storage space or device whether it is flash memory or other type of memory and the shares can be divided up and allocated among multiple virtual machines as they are added or subtracted from the system as further cache memory is added and as the system changes over time. The shares allow for a relative dynamic percentage for each virtual machine as the system usage demands change. Thus for each virtual machine VM the amount of cache storage it receives can be calculated as follows Capacity of VM 1 shares VM 1 shares total active VM shares Cache Capacity where the total active VM shares are the total number of shares allocated to total number of powered on virtual machine. Thus for virtual machines that are not up and running their shares are not accounted for in the capacity equation. Thus for the example in and given the allocated shares set forth above since only Virtual Machine and Virtual Machine are active and given the example of a 100 GB cache storage capacity the following capacities may be calculated VM1 Capacity 1000 4000 100 GB 25.0 GB VM2 Capacity 3000 4000 100 GB 75.0 GB The different virtual machines may be powering on and off vMotionting migrating away and back to the host so the capacity allocations can change over time. At the time Virtual Machine for example virtual machine having VLUN disk in comes on line the capacity of each host would be calculated as follows VM1 Capacity 1000 6000 100 GB 16.7 GB VM2 Capacity 3000 6000 100 GB 50.0 GB VM3 Capacity 2000 6000 100 GB 33.3 GB Thus the current percentage may be calculated based on current allocations. In performing this transition of VM online and being allocated its percentage or shares of cache storage VM must be allocated its percentage shares and virtual machines VM and VM must relinquish storage space. This is accomplished by the methods discussed above in connection with . Each machine must stall operations change capacity and then resume operations. Thus for each machine VM must shrink from 75 to 50 VM must shrink from 25 to 17 and VM can then be given its 33.3 which is taken from the relinquished storage space from VM and VM. Thus the embodiment provides a dynamic provisioning of cache using a virtual disk approach.

Additionally to the extent virtual machines can be provisioned storage space according to the shares concept IOPS capacity can also be allocated among the virtual machines. Thus for each machine VM1 Capacity 1000 6000 100 k IOPS VM2 Capacity 3000 6000 100 k IOPS VM3 Capacity 2000 6000 100 k IOPS In one embodiment the VLUN driver manages the cache device such that each VM receives its allocated shares of IOPS capacity. Typically a cache device operates at a single IOPS rate for each request that it services. Consequently the VLUN driver in one embodiment manages IOPS shares amongst VM VM and VM by giving each VM an opportunity to use the cache device in a given time period. In other words each VM gets a time slice within a given time period to use the cache device . In this manner the IOPS capacity between VMs can be managed. In some embodiments the IOPS rate of a VM may be throttled to allow other VMs to access the cache device in accordance with IOPS share allocation therebetween.

One feature that is desired in virtual systems is the ability to move virtual machines from one host to another without powering down or taking the virtual machine offline in the process.

In conventional systems since hosts are usually connected to shared storage this process is well defined and seamless. However in systems configured according to the various embodiments described above that utilize local cache storage rather than shared storage for certain virtual system operations such as a thin provisioned cache there are conflicts that result from exercising certain features common in virtual systems such as moving virtual machines from one host to another.

In conventional virtual systems a virtual machine may be moved from one host to another by utilizing shared storage. However moving virtual machines from one host to another while utilizing the various embodiments described herein problems would occur with the transfer and critical data and virtual systems operations may be compromised. In some virtual systems the move simply would not be allowed such as by VMWare virtual system products for example.

According to one embodiment the issues related to the transfer of a virtual machine from one host to another without the aid of shared storage are addressed in an elegant manner and transfers of virtual machines from one host to another is made seamless while also utilizing the various embodiments described herein. According to one embodiment the virtual system may be configured to deceive the system into thinking that the local cache storage located in or communicating with the host device is essentially a shared device.

In one embodiment in establishing a virtual system and adding virtual machines on separate hosts without shared storage copies of the VLUN disks may exist on two or more different host in anticipation that the virtual machines may be moved from one host to another. Referring to one example of such a configuration is illustrated in a system that includes two hosts Host and Host where Host includes instances of two virtual machines VM and VM that have VLUN disks and respectively. Host includes virtual machine VM having VLUN disk . Host further includes virtualization kernel and VLUN driver instantiated therein as well as cache storage for storing cache data from the virtual machines . Each cache storage may be divided up into chunks as discussed above where the chunks are identified as holding either VM or VM data in Host and VM data in Host . Host includes its own virtualization kernel VLUN driver and cache storage . In one embodiment the system is configured to allow a transfer of one or any virtual machine such as VM for example from Host to Host and to do so substantially seamlessly without the need to completely shut down.

In conventional virtual systems shared storage may store the instances of the primary virtual disks of the virtual machines located among different hosts. These primary virtual disks are accessible to virtual machines operating on hosts that have shared access to the shared storage . In order to enable the transfer of virtual machines from one host to another the virtualization kernel requires that the source host e.g. Host and the destination host e.g. Host both have shared access to each storage device of the transferring virtual machine.

Embodiments of the present invention allow transfer of virtual machines between hosts even though each host does not have access to all physical storage devices of the transferring virtual machine. For example Host and Host both have access to shared physical storage but Host does not have access to the physical storage device serving as the cache device . Similarly Host does not have access to the physical storage device serving as the cache device .

According to one embodiment virtual machine transfers from one host to another is accomplished by instantiating the VLUN disk A in an active state on Host and also instantiating a corresponding VLUN disk B in a dormant state on Host . In certain embodiments these instantiations are performed before the virtual machines power on. In some embodiments during a configuration phase the VLUN driver instantiates a dormant VLUN disk C N on each host a user may desire to use for transferring of virtual machines for example each host in a cluster of hosts.

The VLUN disk A N having the same serial number either active or dormant on each host satisfies the requirements of the virtualization kernel requires that the source host e.g. Host and the destination host e.g. Host both have shared access to each storage device of the transferring virtual machine.

For example an instantiation of VLUN disk A having a serial number of naa.200.cd123. An identical instantiation may be made in on Host including the same serial number but it is dormant where VM does not actively use the copy B but rather uses it as a type of holding place for VM when and if VM transfers from Host to Host . In response to the transfer the naa.200.cd123 disk on Host becomes dormant and the corresponding disk on Host becomes active.

In a system of multiple host computers that each have multiple virtual machines multiple VLUN disks may be instantiated on the different host computers to aid in transferring virtual machines from one host to another with the VLUN disk of the source host transitioning to a dormant state and the VLUN disk of the destination host transitioning to an active state.

It has been observed that in typical computing systems with peripheral and other system devices such as virtual computing systems for example SCSI operations serve as interfaces for devices within a system and can be utilized to fool the virtualization kernel into believing that the cache storage devices located in individual host devices are actually accessible by each host in the cluster. When an operating system communicates to components within the system and discovers devices within the purview of operating system such as storage disks VLUN disks and other devices it initiates queries when a device is found to learn the device s identity and relevant operating information. It questions who the manufacturer is what the model number is what the capacity is and importantly for this embodiment what the serial number is. The serial number is configured to be globally unique within the system. Thus in a virtual system the operating system queries discovered devices such as disks to identify them and to derive a serial number that will be used by the operating system to identify the storage device. For virtual machines the operating system in conventional virtual systems identifies shared storage devices and derives a unique serial number to identify it within the virtual system. Once the virtual machines are created the conventional virtual systems identify each virtual machine as a shared storage device by using this unique serial number assigned to the shared storage.

According to the embodiments discussed herein however cache storage devices are not shared among different hosts but are local to the hosts and shared among virtual machines within the hosts. In operation conventional virtual systems require that the virtual machines are assigned to shared storage in order to enable a transfer of a virtual machine from one host to another. According to one embodiment fictitious shared storage is created and exported to the host as a Fibre channel or SAS device. Thus the Fibre channel or SAS device is artificially recognized as a shared storage device with a unique serial number and is instantiated when a VLUN disk is created. VLUN disk devices are fictitious shared storage spaces that are associated with actual storage space in the local cache storage devices. Once created these VLUN disks are treated as actual devices from the perspective of the operating system. The unique serial numbers for VLUN disks instantiated within the local cache devices such as naa.200.cd123 are derived by the virtualization kernel from the serial number of the shared local storage and each are unique and associated with a particular VLUN disk. Thus when the VLUN disk is created it is created with the unique serial number and these are recognize by the operating system as legitimate entities but are fictitious shared storage. This derived serial number is also used to create another VLUN disk in Host such as VLUN disk B so that a virtual machine such as VM will have a corresponding VLUN disk in the other host to communicate to and continue its I O data traffic after being transferred to Host .

While running in Host prior to moving VM through CFS believes it has some amount of data stored in the cache storage having chunks designated VM in this illustration and these chunks of data storage are not resident in Host after the move. Prior to the move and referring together with a flow chart showing general steps of a transfer process in step CFS is operating it is actively caching data and issuing I O data transfers to and from the cache storage in normal operation. CFS is doing read operations to the designated chunks of data storage prior to the move. Once the move is initiated in step and then in step the hypervisor first completes VM s initiated I O transfers to the cache and any shared storage and then stops these I O transfers for a small period of time prior to the transfer of the virtual machine. In step the VM then stops operating and essentially disappears from Host and then reappears on Host and begins operations.

Once a virtual machine moves from one host to another the data is left resident on the prior host Host for example and when the virtual machine arrives at the destination host Host in this example the data is left behind. Again this breaks the design assumption of conventional virtual systems that requires and assumes the existence of having the I O data transfers associated with the virtual machine to be available to the virtual machine when it transfers which is typically located on remote shared storage that is shared among different hosts. Having copies of the VLUN disks of the different virtual machines is an approach used to essentially deceive existing virtual systems into believing that each virtual machine is storing I O data transfers in remote shared storage.

In conventional systems the hypervisor stalls I O transfers to the remote storage device prior to transferring from one host to another. The virtual machine is then transferred to another host instantiated on that host and operations resume. In this embodiment however there is not only remote shared storage for operations but also local storage. After the transfer in step VM is associated with VLUN driver and Cache which does not have the former cached data and VM has essentially zero capacity in the local cache . Referring to the post move system is illustrated with VM appearing in Host VLUN disk associating with VLUN driver and VLUN disk now designated as B and being in the active state with identical serial number naa.200.cd123. After the move CFS of VM still registers that it has 4 GB of data and that it has data chunks stored in the cache but is now located in Host without access to that cache storage with that capacity and also without access to that stored data that is still resident in cache storage . Thus VM is essentially not aware that the move has occurred. Referring back to prior to the move Host has only VM that has the entire capacity of Cache Note In a typical system multiple virtual machines VM VM . . . VM n exists in a host and there is a complex mapping of shared cache storage. For simplicity of discussion and to avoid obscuring the description of the embodiments only these three virtual machines are illustrated . When VM arrives in Host VM has substantially the entire capacity of Cache and VM needs to get acclimated and acquire capacity in resident cache storage . shows an example acclamation process .

After arriving in Host in step CFS will continue to send I O requests to the VLUN driver in step the new VLUN driver. The VLUN driver will fail the I O requests in step with errors traveling up the storage stack to the CFS that cache addresses assigned to VM are out of VM s range it has no capacity. The error code is interpreted and recognized by the SCSI filter within VM . The SCSI filter will fail the I O requests to the CFS and request that CFS invalidate the cache tags associated with the I O requests. Thus there is a small period of time after the transfer from Host to Host when there is no cache storage capacity of Host being used by VM . The small number of I O requests that are issued are failed and the cache tags are invalidated.

The CFS will then reissue the I O requests that failed to primary virtual disk storage typically stored on shared storage in step . Later VLUN Manager recognizes the arrival of VM in Host and the VLUN driver provisions cache storage capacity for VM in step according to an allocation of shares as discussed above. Subsequent IO requests will benefit from local cache storage once CFS acquires capacity in local cache storage . The VLUN driver stalls CFS as discussed above to perform capacity allocation.

In step the VLUN driver instructs the CFS to purge its data related to the I O data transfers because CFS essentially thinks that it has stored data in the cache storage and 4 GB in space but that is all left behind in Host s local cache storage . This is different than the capacity allocation as discussed above because the resize that occurs after a transfer from one host to another leaves the data behind and the stored data in the allocated space is not the same data that CFS registers as the stored data because it is left behind and does not exist in Host s cache storage . Thus this is a unique resize and allocation of cache space. In step CFS will invalidate all cache tags resize to new allocation of space and resume operation. Also the allocation will utilize the bit mask processes described above to protect against VM reading any old data that may have been written by VM or any other virtual machine. Thus VM would need to write to the chunks of allocated cache storage space before it can read or the read requests will get a fail error. Thus this embodiment allows the VLUN driver to essentially fool the virtual system to believe that shared storage exists among the hosts involved in the virtual machine transfer then allow a virtual machine to move from one host to another then because VM has a SCSI filter the SCSI filter can talk to the VLUN driver to account for the transfer between hosts initially failing the I O s to the VLUN device with an error invalidate all past cache tags allocating space to VM and resuming operation of VM . Also the VLUN manager of B will allocate the space relinquished by VM to virtual machines that are local to Host . Thus virtual machines may be moved around to different hosts for non destructive upgrades balancing among hosts failure recovery and other operations that aid the virtual system.

The operating system may be a host operating system operating on a bare metal computing device and or may be a guest operating system operating within a virtual machine on another host as shown in . The storage stack the I O request monitors A N and or cache device manager and or other modules described in this disclosure may be configured to operate on a processor of a computing device and or may be embodied as one or more computer readable instructions on a non transitory computer readable storage medium such as the primary storage .

The storage stack may be configured to service I O operations of the operating system and or storage clients . The storage stack may comprise a storage system of the operating system such as an I O manager the I O driver of and or the Windows Driver described above in conjunction with . The disclosure is not limited in this regard however and could be adapted to operate in any suitable storage and or I O management system of any suitable operating system including but not limited to Microsoft Windows NT Microsoft Windows Server 2008 UNIX LINUX Solaris or the like.

The storage stack may comprise one or more storage stack layers A N which may include but are not limited to a file system layer e.g. a file system driver a volume layer a disk layer and so on. For example a file system driver of a file stack layer A A may be configured to maintain file system data on one or more primary storage devices . The primary storage device may comprise one or more non volatile storage devices e.g. hard disks that are accessible via a bus network or other suitable communication mechanism.

The system may comprise a multi level cache that is configured to cache I O request data on the cache storage device . In certain embodiments the cache storage device may comprise a volatile cache e.g. a cache storage device implemented using volatile memory a non volatile cache e.g. a cache storage device implemented using non volatile storage media such as for example a hard drive battery backed volatile memory solid state storage media or the like. The cache storage device may be separate and or independent of an existing cache of the storage stack .

The multi level cache may cache I O request data within one or more different cache levels. As used herein a cache level refers to a cache directed to I O requests of a particular type and or granularity e.g. cache data identified and services at a particular stack layer A N . For example a file level cache may be configured to cache I O request data at a file level of granularity a volume level cache may be configured to cache I O request data at a volume level of granularity a disk level cache may be configured to cache I O request data at a disk level of granularity a SCSi level cache may be configured to cache data at a SCSi level of granularity and so on. Each cache level may be configured to identify cacheable I O request data using respective selection criteria of the cache level as discussed below. Although particular caching levels are described herein the disclosure is not limited in this regard and could be adapted to cache data at any suitable level of granularity and or pertaining to any type of I O request or operation. In some embodiments the multi level cache may comprise cache levels that do not correspond to a particular storage stack layer but instead correspond to another level of granularity where caching is desirable. For example the multi level cache may comprise an object level cache that is configured to cache object data of a storage client . The object level cache may comprise an I O request monitor e.g. an object monitor that is configured to monitor I O requests pertaining to object I O requests within the storage client operating system or other storage system or subsystem. Similarly the multi level cache may comprise an application level cache or storage client level cache that is configured to monitor and or cache I O requests of a particular application or storage client .

The multi level cache may comprise a plurality of I O request monitors A N each of which is configured to monitor or filter I O requests at a particular layer A N of the storage stack e.g. I O requests of a particular type or granularity . As used herein an I O request refers to any type or granularity of I O request at any layer A N of the storage stack including but not limited to a read I O request a write I O request a modify I O request a truncate I O request etc. a file related I O request such as file open close write truncate delete modify etc. a volume related I O request such as mount unmount etc. a disk related I O request SCSi related I O requests e.g. I O requests at a SCSi protocol layer and so on. The monitored I O requests may be examined by one or more cache levels of the multi level cache to identify cacheable I O requests. As used herein a cacheable I O request refers to an I O request that may be serviced by the multi level cache . Data of a cacheable I O request may be stored on the cache storage device subject to cache policy cache directives availability of cache resources cache level specific policies cache admissions policies cache level specific admissions policies and or other considerations. A non cacheable I O request refers to an I O request that may be ignored by the multi level cache e.g. not cached in the cache storage device . Each I O request monitor A N described below may be configured to monitor and or identify cacheable I O requests. The multi level cache may be configured to service the cacheable I O requests using the cache storage device . As used herein servicing an I O request using the cache storage device refers to performing any suitable cache related I O operation which may include but is not limited to reading data from the cache storage device e.g. servicing a read I O request writing data to the cache storage device e.g. servicing a write I O request modifying data on the cache storage device truncating data on the cache storage device deleting and or trimming data on the cache storage device e.g. issuing a delete or TRIM command performing one or more I O operations on the primary store in relation to a cache related I O operation or the like.

Each cache level of the multi level cache may be configured to apply cache level specific policy such as for example selection criteria to distinguish cacheable I O requests from non cacheable I O requests. As used herein selection criteria refers to any criteria for distinguishing I O request data that may be cached from I O request data that should not be cached. As discussed below selection criteria may be cache level specific e.g. a file level cache may comprise file selection criteria a volume level cache may comprise volume selection criteria and so on . The selection criteria may be determined by a user through a configuration interface such as the interface of and B may be determined by storage clients may be determined by the multi level cache e.g. predefined selection criteria determined or adapted automatically according to performance constraints or the like.

The multi level cache may be further configured to apply cache size constraints to I O requests. The multi level cache may be configured to prevent caching data lager than a maximum size e.g. 1 MB and or smaller than a minimum size e.g. less than 4 k . The cache size limits may be used to prevent cache pollution and or fragmentation of I O request data within the cache. Alternatively or in addition the multi level cache may be configured to implement a cache admission policy to prevent cache poisoning. In some embodiments the cache admission policy comprises detecting sequential I O requests applying application specific criteria or the like.

In some embodiments the storage stack may comprise an existing cache manager to improve the performance of I O operations. For example the storage stack may comprise buffered or cached I O operations using the existing cache manager . The existing cache manager may comprise and or be communicatively coupled to a volatile memory cache not shown . As described below the systems and methods described herein may be used to extend the existing cache manager using the cache storage device . The system may leverage the cache storage device without modification to the storage stack and or existing cache manager .

The multi level cache may operate within a non virtual bare metal system or may operate within a virtual machine as described above.

The multi level cache may comprise one or more cache levels A N. As described above each cache level may be configured to cache I O request data of a different respective type and or granularity. For example a first cache level A may be a file level cache that is configured to cache file I O request data at a file level of granularity e.g. cache file I O requests a second cache level B may be volume level cache that is configured to cache volume I O request data at a volume level of granularity another cache level C may be a disk level cache that is configured to cache I O request data at a disk level of granularity other cache levels may include a SCSi level cache that is configured to cache I O request data at a SCSi level of granularity a network cache configured to cache network data and so on. Although particular caching levels A N are described herein the disclosure is not limited in this regard and could be adapted to cache I O request data at any suitable level of granularity and or pertaining to any type of I O request or operation.

As described above the storage stack may comprise a plurality of layers A N. In the and B example the storage stack comprises a file system layer A a volume layer B and a disk layer N. The disclosure is not limited in this regard however and could be adapted to use any storage stack comprising any number and or types of different storage layers A N.

Each cache level A N may comprise a respective I O request monitor A N which may be configured to monitor storage requests of a particular type and or granularity as described above. In some embodiments and as depicted in the storage stack may provide an interface e.g. API through which the I O request monitors are notified of I O events or requests at a particular layer A N. The I O request monitors A N may comprise filter drivers such as file filter drivers volume filter drivers disk filter drivers SCSi filter drivers or the like. The I O request monitors A N may therefore comprise kernel mode drivers operating within the operating system . Alternatively one or more of the I O request monitors A N may be implemented in a user or application space of the operating system .

Each cache level A N may further comprise a respective cache management system CMS A N. Each CMS A N may act as a separate and independent cache maintaining its own respective cache metadata A N e.g. cache tags as described above . A cache device manager may allocate cache storage space to each of the cache management systems A N e.g. each CMS A N may be allocated one or more chunks or portions within the cache storage device . The cache device manager may allocate storage locations directly e.g. using an addresses of the cache storage device and or through a virtual storage device e.g. VLUN disk or virtual disk a cache device driver not shown such as a virtual storage layer VSL or the like.

The I O request monitors A N may be configured to monitor or filter I O requests within the storage stack . In some embodiments the I O request monitors A N register with the storage stack . Each I O request monitor A N may register at a different respective layer A N of the storage stack . Accordingly each I O request monitor A N may be configured to monitor I O requests of a different respective granularity and or at a different hierarchical layer within the architecture defined by the storage stack .

The I O request monitors A N may be configured to monitor I O requests of a particular storage stack architecture or implementation. For example in a Microsoft Windows operating system the I O request monitors may may be configured to monitor e.g. intercept I O request packets IRP passing through the storage stack. The disclosure is not limited in this regard however and could be adapted to monitor I O requests of any suitable type in any suitable storage stack and or storage architecture.

The I O request monitors A N may be configured to monitor I O requests of a particular type or granularity. For example the I O request monitor A of a file level cache A may be configured to monitor I O requests pertaining to file storage operations. Accordingly the I O request monitor A may comprise a file filter driver or the like. Other I O request monitors B N of other cache levels may be configured to monitor I O requests of different types or granularities. For example a second I O request monitor B may be configured to monitor volume level I O requests another I O request monitor N may be configured to monitor disk level I O requests other I O request monitors not shown may be configured to monitor SCSi level I O requests such a SCSi protocol traffic e.g. using a SCSI filter of and so on. Although particular examples of different I O request types and or granularities are described herein the disclosure is not limited in this regard. The I O request monitors A N could be configured to monitor any type or granularity of I O request using any suitable monitoring and or filtering mechanism.

As described above each cache level A N may comprise a separate independent cache comprising a respective CMS A N. Each CMS A N may maintain respective cache metadata A N which may comprise a set of cache tags in accordance with the portion of the cache storage device allocated to the CMS A N by the cache device manager .

Each I O request may comprise a respective source identifier in accordance with the type and or granularity thereof. As used herein a source identifier refers to an identifier pertaining to the source of an I O request. For example the source identifier of a file I O request may comprise a file identifier such as a file name e.g. a.txt file name and path a unique file identifier or the like the source identifier of a volume I O request may comprise a volume identifier such as a volume name e.g. c a unique volume identifier or the like a disk I O request may comprise a disk identifier such as a disk name unique disk identifier disk address e.g. Disk block address length or the like and so on. Alternatively or in addition a source identifier may correspond to a storage location of a file or other storage entity on the primary storage . For example a source identifier may comprise a block address disk block address logical block address or the like.

In some embodiments the cache metadata A N maintained by each CMS A N may associate the source identifier of an I O request with a respective cache storage location e.g. cache address or cache page address comprising data of the I O request. Accordingly the cache tag data structure of may comprise a source identifier field and or cache address field. depicts one example of a cache tag data structure comprising a source identifier field . In some embodiments the source identifier field comprises a storage location of data on the primary storage device e.g. a block address disk block address logical block address disk address or the like . The data structure may further comprise a state field clock hands field checksum and valid unit map as described above.

Referring back to in some embodiments the cache management systems A N may index the cache tags by source identifier disk address e.g. disk block address on the primary storage cache storage location a combination of these or the like to enable fast cache tag lookups. In some embodiments cache tags may be arranged in a hashtable data structure a tree data structure or the like. Alternatively or in addition the cache tags may be arranged in contiguous memory storage locations as described above.

The multi level cache may be configured to monitor I O requests using the I O request monitors A N identify cacheable I O requests based upon a cache policy of a respective CMS A N and to service cacheable I O requests using the respective CMS A N as described above. In some embodiments each cache management system A N comprises a respective cache policy which is used to identify cacheable I O requests. The cache policy of a cache level A N may comprise cache level specific selection criteria which as discussed above may be used to identify cacheable I O requests. The selection criteria may be specific to a particular cache level. For example the cache selection criteria of a file level cache may comprise file selection criteria configured to identify I O requests pertaining to cacheable files an object level cache may comprise object selection criteria to identify cacheable object I O requests a volume level cache may comprise volume selection criteria configured to identify cacheable volume I O requests a disk level cache may comprise disk selection criteria to identify cacheable disk I O requests a SCSi level cache may comprise SCSi selection criteria to identify cacheable SCSi I O requests and so on.

The selection criteria of the cache levels A N may be defined by one or more of a user a storage client the operating system the multi level cache a particular cache level A N or the like. In some embodiments the multi level cache comprises a configuration interface through which the multi level cache may be configured. A user or other entity such as a software tool may define selection criteria for one or more of the cache levels A N through the configuration interface . The configuration interface may comprise an API through which storage clients or other entities may programmatically configure the multi level cache e.g. define selection criteria for one or more of the cache levels A N .

As described above the storage stack may comprise an existing cache manager that is used to cache or buffer I O request data. In some embodiments the storage stack may issue non paging I O requests pertaining to cached or buffered I O which may be serviced using the existing cache manager . The storage stack may issue paging I O requests pertaining to operations that are directed to the primary storage such as misses in the existing cache manager flushes of the existing cache manager or the like. In one embodiment the multi level cache may be configured to ignore non paging I O requests and to monitor and or service paging I O requests exclusively. Accordingly the multi level cache may extend caching benefits of the existing cache manager transparently e.g. without modifying the storage stack and or cache manager .

In some embodiments the multi level cache is further configured to ignore particular types of I O requests. For example the storage stack may comprise direct I O requests which are configured to bypass the existing cache manager to access the primary storage directly e.g. provide unbuffered I O . The multi level cache e.g. I O request monitors A N may be configured to ignore these direct I O requests in certain embodiments.

In some embodiments the cache storage device may have a larger storage capacity than the existing cache manager and or may be configured with a different cache policy or configuration. Accordingly the multi level cache may act as a type of victim cache for the existing cache manager . In the event of a miss in the existing cache manager one or more of the cache levels A N may be used to service the I O storage request rather than the primary storage which may significantly improve performance.

In another example and in response to an I O request that is a miss in each of the cache levels A N data may be accessed from the primary storage . The I O request data may be cached in the cache storage device in conjunction with the appropriate cache level A N . Due to space constraints of the existing cache manager certain previously requested data may be evicted. However due to increased storage space in the cache storage device the I O request data may be retained for a longer period of time. A subsequent I O request for the data may result in a miss in the existing cache manger but a hit in the cache storage device . The I O request may be serviced using the cache storage device rather than slower primary storage . Servicing the I O request may comprise repopulating the existing cache manager with the previously evicted data.

In another instance the multi level cache may be configured to cache I O request data in response to a write I O request. The existing cache manager may not cache the data due to capacity limitations or policy constraints . Alternatively the existing cache manager may cache the data but the data may be evicted before being requested in a subsequent I O request. In response to the subsequent I O request the data may be accessed from the cache storage device rather than primary storage . Accordingly the cache storage device may transparently extend the existing cache manger e.g. act as a type of victim cache or cache extension for the cache manager and increase overall I O performance.

In some embodiments the I O request monitors A N may monitor and cache overlapping I O requests. For example an I O operation pertaining to a file e.g. a.txt may result in file layer I O requests volume layer I O requests disk layer I O requests SCSi layer I O requests and so on each of which may be monitored and or cached by a different cache level A N. As result a particular file a.txt could be redundantly cached in more than one cache level A N. Accordingly in some embodiments each cache level A N may implement cache policy comprising one or more exclusion rules. As used herein an exclusion rule refers to cache policy information that is configured to prevent redundant monitoring and or caching of I O request data. Exclusion rules may be maintained by each cache management system e.g. in cache metadata A N and may be enforced by the I O request monitors A N and or cache management systems A N. For example an exclusion rule may configure a file level cache A to ignore file I O requests that will be monitored and or cached by another cache level B N such as a volume cache disk cache or the like. Exclusion rules may be enforced when the cache management systems A N are configured. For example at configuration time an exclusion rule may prevent a file level cache A from being configured to cache files that will be cached according to the configuration of another cache level B N. Alternatively or in addition exclusion rules may be implemented during run time e.g. during operation of each of the cache management systems A N . The exclusion rules may allow configuration time redundancy and may enforce the exclusion rules dynamically at run time. For example file level cache A may be configured to cache data of a.txt even though another cache level B N is also configured to cache data of a.txt. During runtime exclusion rules of the cache levels A N may be enforced such that only one of the cache levels A N actually caches the data of a.txt. For example the file level cache A may cache data of a.txt when the other cache level B N does not cache the data e.g. due to cache policy availability or other constraints or vice versa.

In some embodiments the exclusion rules of the multi level cache may be defined through the configuration interface . Exclusion rules may be defined by a user storage client the multi level cache one or more cache levels A N or the like.

The cache device manager may be configured to allocate cache storage space between one or more of the cache levels A N e.g. cache management systems A N . In some embodiments the cache management systems A N may be allocated cache storage e.g. one or more chunks of the cache storage device as described above . The allocation may refer to cache addresses of the cache storage device and or may reference a virtual address space of a virtual storage device such as the VLUN disk of or a virtual storage layer .

The cache device manager may allocate cache storage using shares as described above. Alternatively or in addition the cache device manager may allocate cache resources according to a ratio between cache levels A N in which each cache level A N may be allocated a proportional share of the cache storage device . Referring to in some embodiments the multi level cache operates within a guest operating system of a virtual machine VM . The cache storage device may be shared among a plurality of virtual machines VM N. A VLUN driver may allocate portions e.g. chunks of the cache storage device to one or more of the virtual machines and the multi level cache may access the cache storage device through a VLUN disk as described above in relation to . In some embodiments the multi level cache may comprise a SCSi I O filter or monitor A N which may be configured to identify the VLUN disk receive capacity changes from the VLUN driver and or manage commands between the multi level cache and the virtualization kernel as described above.

Although depict a multi level cache comprising separate I O request monitors A N the disclosure is not limited in this regard. In some embodiments the multi level cache may not comprise a one to one association between I O request monitors A N and cache management systems A N. For example in some embodiments the multi level cache may comprise a single monitoring module configured to monitor I O requests for each of the cache levels A N e.g. monitor I O requests at a plurality of different layers within the storage stack . Moreover the multi level cache is not limited to monitoring I O requests using a storage stack and could be adapted to monitor I O requests using any suitable mechanism.

In some embodiments the cache device manager allocates cache capacity to the cache levels A N of the multi level cache according to an allocation pattern. The allocation pattern may correspond to the share or ratio allocated to each cache level A N. The cache device manager may be configured to allocate cache space in accordance with user preferences the requirements of different cache levels and or the like. The cache device manager may not allocate cache space to cache levels A N that are unused and or inactive. The cache device manager may be configured to dynamically adjust the allocation ratio in response to cache levels A N being activated and or deactivated in response to changing demand conditions updated configuration and so on. In some embodiments the cache device manager holds cache storage space in reserve. The reserved cache storage space may be held in reserve and unused until required by a cache level A N e.g. in response to a cache level A N being activated etc. .

The allocation pattern may be configured to maintain the share or ratio allocation between cache levels A N despite changes to the overall cache capacity allocated to the virtual machine of the multi level cache e.g. VM . As discussed above the cache storage device may be shared between a plurality of VMs N each of which may comprise a respective cache e.g. multi level cache . The cache space that is allocated to a particular virtual machine may change depending upon the requirements of other virtual machines on the host . As the allocated cache space changes the allocation pattern implemented by the cache device manager may maintain the allocation share or ratio between the cache levels A N.

The cache space allocated to the virtual machine may decrease from sixteen 16 to twelve 12 chunks. The allocation may be implemented linearly within the cache address space from the front or rear of the address space . depicts the cache address space after this deallocation. As shown in the allocation pattern maintains the one 1 one 1 two 2 allocation ratio between cache levels A B and C. Moreover the remaining chunks may remain allocated to their respective cache levels A B and C e.g. there is no need to reassign the remaining chunks . depicts the cache address space after increasing the cache space allocated to the virtual machine to twenty 20 chunks. The increase in allocation may be linear as described above. The newly allocated chunks may be allocated to the cache levels A B and C in accordance with the interleaved allocation pattern described above. The allocation increase may be made linearly within the address space without reassigning the chunks or that were already allocated to the virtual machine. Although particular cache allocation patterns and allocation ratios are described herein the disclosure is not limited in this regard. The systems and methods described herein could be adapted to implement any suitable allocation pattern for any suitable allocation ratio and within any suitable cache address space.

Although describe an allocation pattern configured to maintain a cache level allocation ratio despite linear modifications to available cache space the disclosure is not limited in this regard. In some embodiments the cache device manager may be configured to allocate cache space to the cache levels A N in a different type of allocation pattern e.g. in contiguous chunks non linearly or the like . In response to a change in the available cache space the cache device manager and or VLUN driver may reclaim or add cache space non linearly in the cache address space according to the cache level allocation ratio. For example cache space may be reclaimed from arbitrary chunks of the cache space allocated to each cache level A N in accordance with the allocation ratio between the cache levels A N.

Referring back to in some embodiments the cache device manager may be configured to allocate IOPS between cache levels. As described above a VLUN driver e.g. VLUN driver may be configured to balance IOPS requirements between virtual machines. The cache device manager may operate similarly. In some embodiments the cache device manager balances IOPS between the cache layers and the cache storage device in accordance with a pre determined cache level IOPS ratio. The balancing function of the cache device manager may prevent a particular cache layer A N from using an inordinate amount of a limited I O bandwidth between the multi level cache and the cache storage device .

In some embodiments one or more of the cache levels A N may be configured to compress I O request data for storage on the cache storage device . The I O request data may be compressed according to a pre determined compression ratio e.g. 2 4 8 etc. . The I O request data may be compressed using an application programming interface of the storage stack e.g. file compression provided by a file system driver or the like . For example a file level cache A may compress I O request data using an LZNT1 compression algorithm provided by the operating system and or storage stack .

Compressing I O request data may increase the effective storage capacity of a cache level A N. Each cache level A N may implement a different respective compression ratio and or compression algorithm. In some embodiments when a cache level A N is configured to compress I O request data the cache device manager and or VLUN disk may present a corresponding increase in logical cache space to the cache level A N. For example the cache space presented to a cache level A that is configured for 2 compression may appear to double the cache space available to a cache level B configured for 4 compression may appear to quadruple and so on. The physical cache capacity allocated to the cache level A N is unchanged however there is a potential that the cache layer can store n times more data where n 2 4 8 16. Accordingly the cache layer A N provides more cache tags and accepts more data for caching before the cache layer is full and eviction of data is needed. The cache management systems A N may update cache metadata A N identifying the compression level if any of data in the cache storage device .

The cache management system A N may track the apparent increase in cache storage space using cache metadata A B e.g. by increasing the number of cache tags . For example each cache tag and or corresponding cache page may be split in accordance with the compression ratio e.g. each cache tag and or cache page may be capable of storing multiple pages of compressed data . Therefore a particular cache page may be represented by two 2 or more cache tags depending upon the compression ratio implemented by the cache level A N. The cache metadata A N may comprise additional cache tags or a cache tag offset to track the compressed I O request data. The additional cache tags and or corresponding cache pages may be represented in cache metadata A N.

Some types of I O request data may be incompressible and or may not be compressible to the compression ratio of a particular cache level A N e.g. the data may be capable of 2 compression but not 4 compression . In response the cache management system A may represent the uncompressed data or under compressed data as larger compressed data for storage on the cache storage device . For example in a cache management system A comprising 4 k cache pages and configured for 2 compression 4 k of incompressible data may be stored on the cache storage device in association with two 2 cache tags each cache tag corresponding to 2 k of raw storage space. In a 4 compression example a 4 k segment of incompressible data may be stored on the cache storage device in association with four 4 cache tags and so on. Alternatively in such an embodiment the incompressible data may be associated with a single tag identifying a first 4 k segment and the remaining 1 3 tags depending on the compression ratio for the remaining 4 k segments of the incompressible data may be reserved and simply identify the single tag identifying the first 4 k segment. In some embodiments partial compression may result storing a data segment that is smaller than a page size e.g. smaller than 4 k . In this example the compressed data may be stored with padding data to fill out the remainder of the page e.g. with zeros or other suitable padding .

In some embodiments uncompressed data may be stored across a storage boundary e.g. a page boundary . For instance in a 4 compression example a cache page may comprise three 3 pages of 4 compressed data. Another 4 k data segment to be stored within the remaining 1 k of the compressed page may be uncompressible. As such the uncompressed data may be represented as 4 cache tags e.g. 4 pages of compressed data. A 1 k portion of the incompressible data may be stored on the current cache page with the three 3 compressed pages for a previous IO operation and the remaining 3 k may be stored on another page of the cache storage device . Accordingly reading and or writing the incompressible data may comprise reading and or writing two or more pages on the cache storage device .

As described above each cache level A N may comprise a respective cache management system A N. Each cache management system A N may implement a respective cache policy. The cache policy may be used to determine cache admissions cache evictions and so on. In some embodiments the cache policy comprises a clock sweep module to identify steal candidates e.g. eviction candidates as described above. The cache policy may further comprise cache size limits such as a maximum cache size and or minimum cache size and or sequential I O detection as described above.

In some embodiments the cache levels A N may be configured to communicate information pertaining to a particular I O request type and or granularity to other cache levels A N. For example a volume level cache B may be configured inform a file level cache A that a particular volume has been unmounted. In response the file level cache may remove any I O request data pertaining to files on the unmounted volume from the cache storage device . In another example a disk level cache may inform a volume level cache that a disk comprising one or more cacheable volumes has been removed. In response the volume level cache may remove cached data pertaining to the affected volumes. In some embodiments removing data from the cache storage device may comprise invalidating cache tags of the data at one of the cache levels A N e.g. a message may be sent to a particular cache management system A N that was caching the data affected . Alternatively or in addition removing the data may comprise issuing a TRIM command or other message to inform the cache storage device that the data need not be preserved.

In embodiments the cache levels A N may be configured by bypass one or more levels of the storage stack . For example a file storage operation may result in separate I O requests passed between layers of the storage stack including but not limited to file layer A I O requests volume layer B I O requests disk layer N I O requests and so on. A cache layer A N may be configured to bypass one or more of these I O request layers under certain pre determined conditions. For example referring to a file level cache A may monitor an I O request pertaining to file data that is cached in the cache storage device . The file level cache A may service the I O request directly through the cache storage device and or through a SCSi level cache N and or the VLUN disk bypassing intervening volume layer I O B request s disk layer C I O request s and so on. In some embodiments the file level cache A may bypass subsequent I O requests by consuming the I O request within the storage stack such that the subsequent lower level I O requests are not passed on through the storage stack . The disclosure is not limited in this regard however and could be adapted to use any suitable mechanism for bypassing storage layers A N.

Step comprises monitoring I O requests for each of a plurality of different cache levels each cache level configured to monitor I O requests of a particular type and or granularity e.g. different layer A N of a storage stack . The cache levels of step may comprise but are not limited to a file level cache a volume level cache a disk level cache a SCSi level cache or the like. Each cache level may be configured to monitor I O requests of a different respective type and or granularity as described above. The monitoring of step may comprise one or more I O request monitors monitoring I O requests in a storage stack such as the I O request monitors A N described above.

In some embodiments step comprises ignoring pre determined types of I O requests such as non paging I O requests e.g. buffered I O requests direct I O requests or the like.

In some embodiments step may comprise configuring the multi level cache e.g. through a configuration interface . Step may comprise defining cache policy such as selection criteria of one or more cache levels defining exclusion rules and so on.

At step each of the plurality of cache levels may determine if the monitored I O request is a cacheable request and if so service the I O request using a respective cache management system e.g. CMS A C . Although step depicts an iteration of cache level steps in series each cache level could implement steps and or independently and or in parallel.

Step comprises a respective cache level determining if a monitored I O request is cacheable. Step may comprise applying a cache level policy to the monitored I O request such as file selection criteria volume selection criteria disk selection criteria SCSi selection criteria or the like. The selection criteria may be set via a configuration interface of the multi level cache e.g. user defined selection criteria . Step may further comprise applying a cache admission policy such as cache size constraints a maximum and or minimum size etc. In some embodiments step further comprises applying a cache admission policy to prevent cache poisoning as described above.

If the I O request is cacheable by a cache level the flow may continue to step otherwise the flow may end at step where the I O request is serviced by primary storage. In some embodiments step may comprise evaluating one or more exclusion rules to prevent redundant caching. As described above other cache levels may be prevented from servicing an I O request that has already been serviced by a cache level.

Step may comprise servicing the I O request using one of a plurality of cache management systems e.g. CMS A N . Step may comprise the cache management system maintaining cache metadata e.g. cache tags to associate source identifiers of I O request data with storage locations of the cache storage device e.g. cache addresses of the cache storage device . In some embodiments the cache addresses may reference virtual cache addresses of a VLUN disk e.g. VLUN disk .

In response to a read I O request step may comprise determining whether data of the I O request is available on the cache storage device. Step may comprise using a source identifier and or primary address of the I O request to access a cache address in the cache metadata e.g. access a cache tag as described above . If data of the I O request is available step may further comprise accessing data on the cache storage device at a specified cache storage location. In some embodiments the cache storage device may be accessed directly. Alternatively the cache storage device may be accessed through a virtual storage interface such as a VLUN disk .

In response to a write I O request step may comprise storing data on the non volatile storage device e.g. cache storage device . Step may further comprise acknowledging completion of the I O request when the data has been written to primary storage as described above. The non volatile storage device may be accessed directly through a VLUN disk or the like.

Step may comprise servicing other types of I O requests including but not limited to update modify truncate delete TRIM or the like. Step may comprise servicing such requests by accessing the cache storage device directly and or via a virtual machine interface as described above.

In some embodiments step comprises bypassing one or more layers of a storage stack. For example when an I O request can be serviced at a first cache level e.g. a file level cache step may comprise bypassing other lower level I O requests e.g. volume layer I O requests disk layer I O requests etc. . In some embodiments the other layers are bypassed by consuming the I O request. However the disclosure is not limited in this regard and could be adapted to bypass storage layers using any suitable mechanism. After servicing the I O request in a cache layer the flow ends at .

Step may comprise determining a cache level allocation ratio between a plurality of cache levels of a multi level cache. The cache levels may correspond to different respective I O request types and or granularities as described above. In some embodiments the cache level ratio may be determined by a cache device manager such as the cache device manager described above. The cache level allocation ratio may be determined based upon user preferences set via a user interface may be determined dynamically according to cache level requirements or the like.

Step may comprise determining a cache level allocation pattern that is configured to preserve the cache level allocation ratio of step despite modifications to the cache space allocated to the cache levels e.g. a linear modification or other modification . Step may therefore comprise determining an interleaved allocation pattern as described above. Alternatively the allocation pattern may comprise a non linear interleaved allocation pattern.

Step comprises allocating cache storage according to the cache level allocation pattern of step . The allocation pattern may comprise a repeated interleaved allocation pattern as described above. In some embodiments step may comprise the cache device manager allocating cache tags to each of the cache levels in accordance with the cache storage space allocated to the cache level.

Step comprises modifying the amount of cache storage space available to the multi level cache while preserving the allocation ratio of step . The modification of step may be made by the cache device manager and or VLUN driver as described above. Step may comprise adding or removing cache storage space. In some embodiments cache storage space may be modified linearly. The allocation pattern of steps and may preserve the cache level allocation ratio of step despite the linear addition or removal of cache space. Alternatively the allocation pattern may maintain the allocation ratios under other non linear allocation changes. When cache storage is added step may comprise allocating the additional cache storage space to the cache levels in accordance with the interleaved cache allocation pattern determined at step . When cache storage is removed step may comprise removing storage space linearly within the cache address space that was allocated to the cache levels in accordance with the interleaved cache level allocation pattern of steps and . The method ends at step until the cache space is reallocated and or modified.

The file level cache may comprise and or be communicatively coupled to a cache device manager which may manage the allocation of cache space to the file level cache . As depicted in the operating system may comprise a bare metal operating system operating directly on a computing device not shown . Accordingly the cache device manager and or file level cache may be configured to access the cache storage device directly as described above. Referring to in some embodiments the operating system may comprise a guest operating system of one of a plurality of virtual machines VM to N operating on a host . Accordingly the cache device manager and or file level cache may be configured to access the cache storage device through a VLUN disk and or SCSi filter e.g. the SCSi filter of as described above.

The file level cache may comprise an I O request monitor A that is configured to monitor I O requests in a storage stack . As described above the storage stack may comprise a plurality of layers A N including a file system layer A. The I O request monitor A may be configured to monitor I O requests pertaining to file operations such as file open close read write modify and the like. The I O request monitor A may monitor other types of I O requests such as volume mount and or unmount disk mount and or unmount paging I O and so on. The I O request monitor A may monitor I O requests using an interface provided by the operating system such as the storage stack as described above. Accordingly the I O request monitor A may comprise a file filter driver and or other suitable monitoring and or filtering modules.

The file level cache may comprise a cache management system that is configured to maintain cache metadata such as cache tags as described above. The cache management system may comprise a cache policy which includes file selection criteria . The file selection criteria may comprise a plurality of rules and or filters which may be used to identify cacheable files e.g. distinguish cacheable files from non cacheable files . As used herein a cacheable file refers to a file that may be serviced by the cache management system e.g. satisfies the file selection criteria . Data of a cacheable file may be stored on the cache storage device so long as one or more other criteria are satisfied including cache policies cache directives availability of cache resources and or other considerations. A non cacheable file may refer to a file that does not satisfy the file selection criteria and as such may not be serviced using the cache management system . However even if a particular file does not satisfy the file selection criteria another cache level not shown such as a volume level cache disk level cache SCSi level cache or the like may be configured to cache data of the file.

The file level cache may be configured to transparently extend an existing cache manager of the operating system as described above. Accordingly the file level cache e.g. I O request monitor A may be configured to distinguish different I O request types including but not limited to non paging I O requests paging I O requests direct I O requests and the like. The file level cache may be configured to ignore I O requests such as direct I O requests that are expected to access primary storage directly and or non paging I O requests pertaining to the existing cache manager .

The file level cache may identify cacheable I O requests by applying the file selection criteria to monitored I O requests. I O requests pertaining to cacheable files may be identified as cacheable I O requests and I O requests pertaining to non cacheable files may be identified as non cacheable I O requests. In some embodiments the I O request monitor A may track file I O operations using metadata such as an I O request context or the like. The I O request monitor A may generate this context metadata in response to detecting an I O request pertaining to a file operation such as a file open read copy delete create truncate modify write or the like. The I O request monitor A may use the I O request metadata e.g. context to associate the I O request and subsequent I O requests with a source identifier such as file identifier file name volume identifier disk identifier or the like. In some embodiments the I O request monitor A may include an indication of whether a particular file and or corresponding I O request is cacheable. The file level cache may access this indication to determine whether subsequent I O requests are cacheable without re applying the file selection criteria. Although a particular example of I O request metadata is described herein the disclosure is not limited in this regard the systems and methods disclosed herein could be adapted to maintain I O request metadata using any suitable mechanism.

In one example a storage client may open a plurality of files resulting in corresponding file open I O requests. The I O request monitor A may associate the I O requests with corresponding I O request metadata which may include a source identifier of the file an indication of whether the file is cacheable and so on. The storage client may perform I O operations on the files which may be serviced using the existing cache manager . The I O request monitor A may ignore corresponding non paging I O requests as described above. Later in accordance with cache policy for the existing cache manager the existing cache manager may be flushed resulting in a one or more paging I O requests. The I O request monitor A may access the I O request metadata e.g. context of the paging I O requests to determine the source identifier e.g. file name associated with the requests determine whether the I O requests pertain to cacheable files and so on. I O requests that pertain to cacheable files may be serviced using the cache management system as described above. The I O request metadata may comprise context data of an I O request and or may be maintained in a separate datastructure within the cache metadata A. The I O request metadata may be stored in any suitable datastructure e.g. table hashtable map tree etc. and may be indexed by source identifier file name or the like.

As discussed above the file level cache may identify cacheable files using selection criteria e.g. file selection criteria which may comprise any suitable criteria for selecting cacheable files. The file selection criteria may allow a user or other entity to specify files to be cached using the cache storage device . Accordingly the file selection criteria may allow a user to control the operation of the file level cache at a file level of granularity e.g. specify particular files to be cached .

In some embodiments the file selection criteria may comprise matching rules such as a file name match e.g. cache files named name .dat file extension match e.g. cache all .dat files regular expression match file path match e.g. cache all files in the c dat directory file application association e.g. cache all files associated with application X and so on. The file selection criteria may be set by a user or other entity via the configuration interface . Alternatively or in addition file selection criteria may be defined another entity such as a storage client operating system or the like. For example a storage client may configure file selection criteria to cache certain pre determined files that are critical to the performance of the storage client . In some embodiments the file level cache may be configured to automatically identify files for caching e.g. by profiling file I O requests etc. . Files may be identified based upon access frequency time or the like. The file level cache may automatically configure the file selection criteria to cache the identified files.

In some embodiments the configuration interface may comprise one or more exclusion rules . As described above the exclusion rules may be configured to prevent the file level cache from caching I O request data that other cache levels not shown are configured to service. A user or other entity may define exclusion rules that allow redundant caching between cache levels and or allow run time evaluation of exclusion rules as described above.

In some embodiments file selection criteria may be used to tune the behavior of the file level cache at a file level of granularity. For example file selection criteria may indicate a relative priority of a file. The cache management system may be configured to admit and or retain higher priority files into the cache storage device in lieu of lower priority files. For example some operating systems may utilize a page file in support of a virtual memory system e.g. pagefile.sys in Microsoft Windows systems . The page file may be assigned a high priority in the file selection criteria to ensure that it is retained in the cache. The tuning behavior of the file selection criteria may be used in conjunction with other factors of the cache policy such as access frequency access time e.g. LRU clock sweep and the like.

In some embodiments file selection criteria may be used to define cache directives at a file level of granularity. For example the file selection criteria may be used to pin data of a particular file within the cache storage device e.g. pin pagefile.sys in the cache storage device . Other file selection criteria may be used to prefetch file data into the cache storage device . In some embodiments file selection criteria may be used to prefetch and or pin files of the operating system and or storage client . For example files required to boot the operating system may be pinned in the cache storage device to reduce system startup time. In another example application files of a storage client may be prefetched and or pinned within the cache storage device to improve the performance of the storage client .

In some embodiments certain resources such as boot files for the operating system may be shared between virtual machines and or virtual machines and a host. Referring to the cache storage device may be shared between a plurality of virtual machines VM to N operating on a host . The cache storage device may be configured to pin files of an operating system shared by two or more of the virtual machines VM N and or host .

The cache policy may further comprise application criteria which may be used to determine whether a particular I O request should be cached based upon application specific rules pertaining to the I O request. As used herein application criteria refers to application specific cache directives and or rules. Application criteria may reflect application level knowledge such as storage requirements of an application data access patterns of an application and so on. As discussed above I O requests are typically performed on behalf of a storage client such as an application file system server or the like. The application criteria may be used to tune the file level cache in accordance with application level knowledge pertaining to the storage client .

In some embodiments application criteria may be used to prevent caching for certain storage clients . For example a backup application storage client may generate a large number of I O requests in the storage stack . Application criteria pertaining to the backup storage client may indicate that these accesses are one time use copying files to a backup storage location and that the I O request data should not be cached. In fact caching this data would poison the cache storage device with data that is unlikely to be requested again. Accordingly a user or other entity may define application criteria that indicates that I O requests associated with the backup application storage client should be ignored even if the particular file is a cacheable file per the file selection criteria . Other cache levels may comprise similar application criteria. Application criteria may be defined to prevent other types of storage clients from poisoning the cache storage device such as sequential streaming storage clients e.g. video music or other media players virus scanner storage clients and the like.

In some embodiments the I O request monitor A is configured to identify an application e.g. storage client associated with an I O request. The I O request monitor A may include an application identifier in I O request metadata e.g. in a context of the I O request as described above. The I O request monitor A may provide an indicator of the application associated with an I O request to the cache management system along with other I O request metadata such as source identifier and the like.

The file selection criteria and or the application criteria may be used in conjunction with the I O cache limits of the cache management system discussed above. For example I O requests that exceed a maximum cache size or are smaller than a minimum cache threshold may be ignored even if the I O request pertains to a cacheable file per the file selection criteria and or application criteria .

Other application criteria may be used to tune the file level cache for certain storage clients . For example a particular storage client may access data according to a pre determined access pattern e.g. access data in N kb sized portions . When servicing a cache miss for the particular storage client the application criteria may be used to prefetch data into the cache storage device in accordance with the access pattern e.g. prefetch N kb into the cache storage device .

In some embodiments application criteria may be used to modify the behavior of the storage stack . Application criteria may indicate that some write operations of a particular storage client should not be persisted to the primary storage . The I O request monitor A may consume such requests to bypass other lower levels of the storage stack e.g. prevent the write to primary storage as discussed above.

Step comprises monitoring I O requests on a storage stack. The monitoring of step may comprise filtering I O requests using a file filter or other suitable monitoring module such as the I O request monitor and or A described above. The monitoring of step may be configured to ignore pre determined storage requests such as direct I O requests non paging I O requests and the like. In some embodiments step comprises configuring file selection criteria as described above.

Step may comprise applying a cache policy to the monitored I O requests to identify cacheable I O requests. Step may comprise applying file selection criteria to identify I O requests pertaining to cacheable files as described above. In some embodiments the file selection criteria may be user defined. Accordingly in some embodiments a cacheable I O request may be an I O request that pertains to a cacheable file e.g. a file that satisfies the file selection criteria . As described above the file selection criteria may comprise any suitable criteria including but not limited to file name file extension regular expression file path application association and the like.

In some embodiments step may comprise accessing and or updating I O request metadata of the monitored I O requests as described above. For example in response to an I O request pertaining to a file e.g. a file open read write etc. step may comprise updating metadata of the request e.g. updating a context of the I O request with a source identifier an indicator of whether the file is cacheable and so on.

Alternatively or in addition step may comprise accessing existing metadata pertaining to a monitored I O request. As discussed above step may comprise updating I O request metadata in response to certain I O requests e.g. file open read write etc. . Subsequent non paging I O requests which are serviced using an existing cache manager may be ignored. However these I O requests may result in one or more paging I O requests that may be serviced using the cache storage device. Step may comprise accessing existing I O request metadata which was updated in response to one or more initial I O requests e.g. file open to determine the source identifier of the I O request determine whether the I O request pertains to a cacheable file and so on.

Step may comprise servicing the I O request using a file level cache e.g. using a cache management system A N . When the cacheable I O request is a read step may comprise determining whether a cache storage device e.g. cache storage device comprises data pertaining to the request. Step may therefore comprise accessing cache metadata to determine a cache storage location comprising the requested data e.g. using a cache tag associated with a source identifier of the I O request . When the cache storage device comprises the requested data step may comprise reading the data from the cache storage device. When the cache storage device does not comprise the requested data step may comprise servicing a cache miss which may comprise accessing the requested data from primary storage and storing the data in the cache storage device. Step may further comprise pre caching a portion of data that is not requested by the I O request in accordance with a cache policy.

When the cacheable I O request is a write and or modify step may comprise determining whether the cache storage device comprises the data as described above. When the cache storage device comprises data pertaining to the write operation step may comprise updating the cache storage device with data of the I O request. Step may further comprise acknowledging the I O request when the data is stored on a primary store. Alternatively the I O request may be acknowledged when the data is stored on the cache storage device. In some embodiments step may comprise allocating cache storage for the I O request data. Step may comprise evaluating cache policy to identify one or more eviction candidates e.g. steal candidates based upon file priority data cache policy data e.g. pin directives or the like as described above. A source identifier of the I O request may be associated with a cache storage location using the allocated cache tags.

As discussed herein the invention may involve a number of functions to be performed by a computer processor such as a microprocessor. The microprocessor may be a specialized or dedicated microprocessor that is configured to perform particular tasks according to the invention by executing machine readable software code that defines the particular tasks embodied by the invention. The microprocessor may also be configured to operate and communicate with other devices such as direct memory access modules memory storage devices Internet related hardware and other devices that relate to the transmission of data in accordance with the invention. The software code may be configured using software formats such as Java C XML Extensible Mark up Language and other languages that may be used to define functions that relate to operations of devices required to carry out the functional operations related to the invention. The code may be written in different forms and styles many of which are known to those skilled in the art. Different code formats code configurations styles and forms of software programs and other means of configuring code to define the operations of a microprocessor in accordance with the invention will not depart from the spirit and scope of the invention.

Within the different types of devices such as laptop or desktop computers hand held devices with processors or processing logic and also possibly computer servers or other devices that utilize the invention there exist different types of memory devices for storing and retrieving information while performing functions according to the invention. Cache memory devices are often included in such computers for use by the central processing unit as a convenient storage location for information that is frequently stored and retrieved. Similarly a persistent memory is also frequently used with such computers for maintaining information that is frequently retrieved by the central processing unit but that is not often altered within the persistent memory unlike the cache memory. Main memory is also usually included for storing and retrieving larger amounts of information such as data and software applications configured to perform functions according to the invention when executed by the central processing unit. These memory devices may be configured as random access memory RAM static random access memory SRAM dynamic random access memory DRAM flash memory and other memory storage devices that may be accessed by a central processing unit to store and retrieve information. During data storage and retrieval operations these memory devices are transformed to have different states such as different electrical charges different magnetic polarity and the like. Thus systems and methods configured according to the invention as described herein enable the physical transformation of these memory devices. Accordingly the invention as described herein is directed to novel and useful systems and methods that in one or more embodiments are able to transform the memory device into a different state. The invention is not limited to any particular type of memory device or any commonly used protocol for storing and retrieving information to and from these memory devices respectively.

Embodiments of the systems and methods described herein facilitate the management of data input output operations. Additionally some embodiments may be used in conjunction with one or more conventional data management systems and methods or conventional virtualized systems. For example one embodiment may be used as an improvement of existing data management systems.

Although the components and modules illustrated herein are shown and described in a particular arrangement the arrangement of components and modules may be altered to process data in a different manner. In other embodiments one or more additional components or modules may be added to the described systems and one or more components or modules may be removed from the described systems. Alternate embodiments may combine two or more of the described components or modules into a single component or module.

Finally although specific embodiments of the invention have been described and illustrated the invention is not to be limited to the specific forms or arrangements of parts so described and illustrated. The scope of the invention is to be defined by the claims appended hereto and their equivalents.

