---

title: Method and system for efficient execution of ordered and unordered tasks in multi-threaded and networked computing
abstract: The present disclosure provides methods for concurrently executing ordered and unordered tasks using a plurality of processing units. Certain embodiments of the present disclosure may store the ordered and unordered tasks in the same processing queue. Further, processing tasks in the processing queue may comprise concurrently preprocessing ordered tasks, thereby reducing the amount of processing unit idle time and improving load balancing across processing units. Embodiments of the present disclosure may also dynamically manage the number of processing units based on a rate of unordered tasks being received in the processing queue, a processing rate of unordered tasks, a rate of ordered tasks being received in the processing queue, a processing rate of ordered tasks, and/or the number of sets of related ordered tasks in the processing queue. Also provided are related systems and non-transitory computer-readable media.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09600335&OS=09600335&RS=09600335
owner: WIPRO LIMITED
number: 09600335
owner_city: Bangalore
owner_country: IN
publication_date: 20151120
---
This application claims priority under 35 U.S.C. 119 to Indian Application 4584 CHE 2013 filed Oct. 9 2013. The aforementioned application is incorporated herein by reference in its entirety.

Over the past sixty years the use of computing devices to perform large scale calculations has become an integral force in almost all technology sectors. Over much of that time the paradigm of computing has largely adhered to a serial execution model in which calculations are performed sequentially until a desired solution has been achieved. Archetypal problems falling into this paradigm include iterative solutions to non linear differential equations such the determination of solutions for many body problems in Newtonian and non Newtonian systems. Serial execution is also the manner in which humans have naturally approached most problems including for example the familiar algorithms for long division or for multiplication taught to young elementary school students. A hallmark of serial execution has been the reliance of subsequent calculations on solutions determined by earlier calculations. This reliance implies the need for serial execution because without the solution obtained in an initial phase a later phase is unable to begin.

But as problems in computing have become ever larger and more massive to the point where the amount of computing time needed by a state of the art hardware processor to arrive at a solution often exceeds the average human lifespan there has been a shift to parallel or concurrent execution models also known as parallel processing. For example graphics processing is one domain in which concurrent execution has led to huge advances. Electronic displays e.g. LED computer screens have resolutions on the order of millions of pixels. The calculation of each pixel s hue color etc. at frequencies often in excess of 30 Hz is a massive problem that has now become routine thanks to Graphics Processing Units GPUs capable of concurrently executing the needed calculations for each pixel using specialized processors. In a serial execution model the calculation of each pixel would be sequentially calculated one after the other requiring millions of steps. But in a concurrent execution model the calculation of each pixel occurs at the same time essentially in a single step. The predicate condition enabling this enormous speed up is that the calculations needed for each pixel do not rely on the solutions obtained for other pixels. Thus concurrent execution takes advantage of the observation that large problems can often be broken into many smaller problems each of which can be solved independently of the others.

Within the field of parallel computing two main paradigms of concurrent execution have emerged. In a shared memory model different processors are capable of reading and writing to the same location in computer memory. In this model parallel processing can be achieved by placing multiple hardware processors in communication with the same memory space or by using execution threads a feature common in many modern operating systems which can take advantage of multithreaded or multicore hardware processors. Alternatively processors may be allocated exclusive areas of memory either on the same physical hardware memory multiple physically exclusive hardware memory modules or a combination thereof. In this second model some form of communication is necessary between processors usually achieved using processor interconnects or networking protocols e.g. Ethernet or HTTP. Thus one term for this model of parallel processing is message passing. Hybrid implementations using elements of both shared memory and message passing have also been used to achieve concurrent execution.

Algorithms in the parallel processing space have sought to address a recurrent set of technical issues. One common theme among these issues is the problem of data synchronization. Though the parallel calculations performed by each processor in a parallel processing algorithm may be performed independently the division of those calculations from a larger problem often depends on the nature of the larger problem as a whole more fundamentally many large problems of interest cannot be neatly divided into wholly independent pieces and thus require a great deal of data management. The individual solutions obtained must also often be recombined in some fashion in order to achieve a comprehensible solution to the larger problem. Another theme concerns the problem of resource allocation or load balancing. One aspect of this problem relates simply to the number of processors available to tackle a set of parallel calculations. Another aspect relates to variations in complexity of individual parallel calculations as well as the throughput of the individual processors. Stated differently each calculation in a set of parallel calculations may require longer or shorter periods of computing time. Because the overall speed of parallel calculation is often limited to the speed of the slowest processor or the processor requiring the most computing time effective parallel processing seeks to minimize processor idle time. This is generally accomplished by distributing the set parallel calculations evenly among the available processors.

Data synchronization and load balancing issues are both present in a situation where the set of parallel processing tasks comprises unordered tasks and ordered tasks. Unordered tasks may be processed by any available processor without regard to the order they are processed. To process a set of ordered tasks however requires each ordered task to be processed in a given sequence relative to other ordered tasks in the set. Because of the serial nature of ordered task processing independence of the tasks cannot be assumed. Accordingly algorithms for processing ordered and unordered tasks must ensure that processors do not process tasks out of order data synchronization while at the same time minimizing the time that processors wait for each other to finish processing earlier ordered tasks load balancing . One solution to this problem has been the use of multiple data structures. For example unordered tasks may be assigned to one data structure while ordered tasks may be assigned to one or more additional data structures e.g. one set of related ordered tasks per data structure . In this scenario the unordered task data structure may be simultaneously processed by multiple processors but ordered task data structure s may be assigned only one processor at a time thereby precluding out of order processing. The use of memory locks presents another solution by preventing processors from writing to same memory location s currently being accessed by other processors. Yet another solution entails the use of delay in which processors are kept deliberately idle. The use of delays thereby ensures sufficient time for processors to complete their tasks before receiving additional tasks.

These solutions however do not adequately address issues of data synchronization and load balancing. Furthermore these solutions are not sensitive to the dynamic mix of unordered and ordered tasks e.g. the proportion of unordered tasks relative to the total number of tasks the ratio of unordered tasks to ordered tasks etc. which may change over time. Ignorance of this factor leads to wasted computer resources disruption of information technology services time and money. Thus additional solutions are desired.

Embodiments of the present disclosure provide methods for concurrently executing ordered and unordered tasks using a plurality of processing units. In some embodiments such a method may comprise receiving an unordered task or an ordered task into a processing queue and processing the task in the processing queue using at least one processing unit. If the task is an unordered task or an ordered task with no other processing unit processing a related ordered task the processing unit may processes the task. If the task is an ordered task and another processing unit is processing a related ordered task the processing unit may preprocess the ordered task if the ordered task needs preprocessing. The method may also comprise dynamically managing the number of processing units. In certain embodiments the ordered and unordered tasks may be stored in the same processing queue. The processing queue may comprise a linear data structure. The processing unit may comprise a hardware processor and or a software processing unit such as a thread. In certain embodiments dynamically managing the number of processing units may comprise determining a required number of processing units based on at least one of a rate of unordered tasks being received in the processing queue a processing rate of unordered tasks a rate of ordered tasks being received in the processing queue a processing rate of ordered tasks and the number of sets of related ordered tasks in the processing queue. The method may also include adjusting the number of processing units based on the required number of processing units. The required number of processing units rT may be equal to the lesser of MTor uOT OT wherein MTis a maximum number of processing units uOT f x uOT f is the rate of unordered tasks being received in the processing queue x is the processing rate of unordered tasks and uOTis a threshold number of processing units per unordered task and OTis determined based on values of pT epT and uK wherein pT pM OT wherein pMa number of tasks to be preprocessed concurrently and OTis a threshold number of processing units per ordered task wherein epT z y OT wherein z is the rate of ordered tasks being received in the processing queue y is the processing rate of ordered tasks and uKcorresponds to the number of sets of related ordered tasks in the processing queue and OTis equal to the greater of pTand epTif pT 0 or OTis equal to lesser of epTand uKif pT 0.

Embodiments of the present disclosure also provide systems for executing ordered and unordered tasks. According to some embodiments such a system may comprise one or more hardware processors configured to execute instructions stored in a computer readable medium. The one or more hardware processors may be configured to receive an unordered task or an ordered task into a processing queue and process the task using at least one processing unit. If the task is an unordered task or an ordered task with no other processing unit processing a related ordered task the processing unit may process the task. If the task is an ordered task and another processing unit is processing a related ordered task the processing unit may preprocess the ordered task if the ordered task needs preprocessing. The one or more hardware processors may be further configured to dynamically manage the number of processing units. In certain embodiments the ordered and unordered tasks may be stored in the same processing queue. The processing queue may comprise a linear data structure. The processing unit may comprise a hardware processor and or a software processing unit such as a thread. In certain embodiments dynamically managing the number of processing units may comprise determining a required number of processing units based on at least one of a rate of unordered tasks being received in the processing queue a processing rate of unordered tasks a rate of ordered tasks being received in the processing queue a processing rate of ordered tasks and the number of sets of related ordered tasks in the processing queue. Managing the number of processing units may also comprise adjusting the number of processing units based on the required number of processing units. The required number of processing units rT may be equal to the lesser of MTor uOT OT wherein MTis a maximum number of processing units uOT f x uT wherein f is the rate of unordered tasks being received in the processing queue x is the processing rate of unordered tasks and uOTis a threshold number of processing units per unordered task and OTis determined based on values of pT epT and uK wherein pT pM OT wherein pMis a number of tasks to be preprocessed concurrently and OTis a threshold number of processing units per ordered task wherein epT z y OT z is the rate of ordered tasks being received in the processing queue y is the processing rate of ordered tasks and uKcorresponds to the number of sets of related ordered tasks in the processing queue and OTis equal to the greater of pTand epTif pT 0 or OTis equal to lesser of epTand uKif pT 0.

Embodiments of the present disclosure also relate to non transitory computer readable mediums storing instructions for executing ordered and unordered tasks. Upon execution of the instructions by one or more hardware processors the one or more hardware processors may be configured by the instructions to receive an unordered task or an ordered task into a processing queue. The one or more hardware processors may be further configured to process the task in the processing queue using at least one processing unit. If the task is an unordered task or an ordered task with no other processing unit processing a related ordered task the processing unit may processes the task. If the task is an ordered task and another processing unit is processing a related ordered task the processing unit may preprocess the ordered task if the ordered task needs preprocessing. The one or more hardware processors may be further configured to dynamically manage the number of processing units. In certain embodiments the ordered and unordered tasks may be stored in the same processing queue. The processing queue may comprise a linear data structure. In certain embodiments dynamically managing the number of processing units may comprise determining a required number of processing units based on at least one of a rate of unordered tasks being received in the processing queue a processing rate of unordered tasks a rate of ordered tasks being received in the processing queue a processing rate of ordered tasks and the number of sets of related ordered tasks in the processing queue. Managing the number of processing units may also include adjusting the number of processing units based on the required number of processing units. The required number of processing units rT may be equal to the lesser of MTor uOT OT wherein MTis a maximum number of processing units uOT f x uOT wherein f is the rate of unordered tasks being received in the processing queue x is the processing rate of unordered tasks and uOTis a threshold number of processing units per unordered task and OTis determined based on values of pT epT and uK wherein pT pM OT wherein pMis a number of tasks to be preprocessed concurrently and OTis a threshold number of processing units per ordered task wherein epT z y OT z is the rate of ordered tasks being received in the processing queue y is the processing rate of ordered tasks and uKcorresponds to the number of sets of related ordered tasks in the processing queue and OTis equal to the greater of pTand epTif pT 0 or OTis equal to lesser of epTand uKif pT 0.

The present disclosure also relates to methods for dynamically managing processing units concurrently processing ordered tasks and unordered tasks stored in processing queue. In some embodiments such a method may comprise determining a required number of processing units based on at least a rate of unordered tasks being received in the processing queue a processing rate of unordered tasks a rate of ordered tasks being received in the processing queue a processing rate of ordered tasks and the number of sets of related ordered tasks in the processing queue. The method may also include adjusting the number of processing units based on the required number of processing units. The required number of processing units rT may be equal to the lesser of MTor uOT OT wherein MTis a maximum number of processing units uOT f x uOT f is the rate of unordered tasks being received in the processing queue x is the processing rate of unordered tasks and uOTis a threshold number of processing units per unordered task and OTis determined based on values of pT epT and uK wherein pT pM OT wherein pMis a number of tasks to be preprocessed concurrently and OTis a threshold number of processing units per ordered task wherein epT z y OT wherein z is the rate of ordered tasks being received in the processing queue y is the processing rate of ordered tasks and uKcorresponds to the number of sets of related ordered tasks in the processing queue and OTis equal to the greater of pTand epTif pT 0 or OTis equal to lesser of epTand uKif pT 0.

Additional objects and advantages of the present disclosure will be set forth in part in the following detailed description and in part will be obvious from the description or may be learned by practice of the present disclosure. The objects and advantages of the present disclosure will be realized and attained by means of the elements and combinations particularly pointed out in the appended claims.

As used herein reference to an element by the indefinite article a or an does not exclude the possibility that more than one of the element is present unless the context clearly requires that there is one and only one of the elements. The indefinite article a or an thus usually means at least one. The disclosure of numerical ranges should be understood as referring to each discrete point within the range inclusive of endpoints unless otherwise noted.

Embodiments in accordance with the present disclosure provide methods systems and non transitory computer readable media capable of receiving an unordered task or an ordered task into a processing queue and processing a task in the processing queue using at least one processing unit. If the task is an unordered task or an ordered task with no other processing unit processing a related ordered task the processing unit processes the task. But if the task is an ordered task and another processing unit is processing a related ordered task the processing unit preprocesses the ordered task if the ordered task needs preprocessing. The number of processing units is also dynamically managed. Thus embodiments in accordance with the present disclosure do not require the use of multiple processing queues to store the unordered and ordered tasks and in certain embodiments do not require the use of lock and or delay mechanisms. In a scenario where an ordered task may comprise a preprocessing phase that may be executed out of order and or without regard to the execution of other related ordered tasks embodiments in accordance with the present disclosure enable concurrent preprocessing of ordered tasks thus reducing the amount of processing unit idle time and improving load balancing across available processing units. By dynamically managing the number of processing units embodiments of the present disclosure may also conserve computational resources. The following detailed description along with the accompanying drawings serve to illustrate one implementation of an embodiment in accordance with the present disclosure. Variations of following detailed description should considered within the scope of the present disclosure.

The tasks are then processed in step for example in task processor using a plurality of processing units. In certain embodiments or scenarios only one processing unit may be used for example when processing queue has very few tasks controller may reduce the number of processing units available to task processor . Even in this case however embodiments consistent with the present disclosure retain the capability of using more than one processing unit. Processing units may include hardware processors e.g. central processing units CPUs found in most computer hardware or specialized chips such a GPUs. Processing units may also include software processing units e.g. execution threads including operating system threads which may take advantage of multithread multicore hardware processing units. Combinations of hardware processors and software process units may also be used e.g. a combination of multicore hardware processors each providing multiple execution threads.

Monitoring of task processing including task inflow outflow and adjusting the number of processing units occurs in step which may be carried out by controller as discussed above. As discussed above controller may monitor parameters related to task processing such as inflow of unordered ordered tasks into processing queue outflow of processed unordered ordered tasks time elapsed number of processing units available to the task processor processor idle time etc. Such information may be reported by task handler and or task processor using notifications or may be obtained by controller via querying appropriate locations in shared memory or message passing.

Importantly though shown as sequential in for representational convenience steps and may occur simultaneously in a concurrent execution environment through for example the concerted action of the components shown in . Thus application and or task handler may continuously receive ordered unordered tasks perform initial processing and add tasks to processing queue . At the same time task processor may continuously process tasks in processing queue using one or more processing units. Processing queue may accordingly have tasks continuously being added and removed as new tasks are received and processed.

In certain embodiments along with generating or assigning an ordered key in step task handler may also determine if the received ordered task requires preprocessing and provide an indication regarding the same by for example adding meta data or marking the ordered task as requiring preprocessing. It also contemplated that certain embodiments in accordance with the present disclosure may not require unordered keys to be assigned in step e.g. the absence of a key may be used to determine that the task is an unordered task in the case where no unordered keys are assigned task handler may proceed directly from step to step if the received task in step is an unordered task.

In certain embodiments the steps in may be performed in a serial execution model in which all tasks received may be initially processed and added to processing queue in the order they are received. The method illustrated in however may also be used in an embodiment amenable to parallel execution where multiple processing units may perform initial processing and queue multiple tasks simultaneously. Parallel execution of the method of by multiple processing units may require the use of lock and or delay mechanisms to prevent race conditions in processing queue duplication of received tasks etc. In this embodiment ordered keys generated in step may comprise information regarding processing order because even in the case where related ordered tasks are to be processed in the order they are received they may not be added to processing queue e.g. a FIFO linked list in that order.

Beginning in each processing unit available to task processor may begin by reading the next task in processing queue step . Reading the next task may vary according the data structure of the processing queue. Under the assumption that processing queue comprises a FIFO linked list reading the next task in processing queue may comprise for example reading a task at one of the list ends depending on the particular implementation of the linked list. Next in step the processing unit may determine whether the task is an ordered task or an unordered task. Such determination may comprise determining whether the task was assigned an unordered key or an ordered key in the initial processing by task handler as discussed above. Alternatively the processing unit may determine whether the task is ordered or unordered using the same or similar techniques used in the initial processing by task handler e.g. by querying meta data associated with the task by the nature of the task itself i.e. object type a notification from source originating the task etc. If the task is an unordered task step NO the processing unit may remove the task from the processing queue and process it in steps and respectively. As described in the processing unit may then return to reading the next task in processing queue step . After processing the unordered task from processing queue in step the processing unit may output the unordered task to inter alia application .

If the processing unit determines that the task is an ordered task step YES it may determine whether to process the ordered task preprocess the ordered task or read the next task in the processing queue based on i whether a related ordered task is being processed and or ii whether the ordered task has been preprocessed. Thus in the implementation illustrated in the processing unit may determine whether related ordered task is being processed step . For example the processing unit may check a processing table indicating the status of the ordered key assigned to the ordered task by task handler . If the processing table indicates that the ordered key is not being processed step NO the processing unit may update the processing table in step to indicate the ordered key as being processed thereby preventing other processing units from processing related ordered tasks out of order. The processing unit may then remove the ordered task from processing queue in step and processes the ordered task by determining if the ordered task has been preprocessed step preprocessing the ordered task if the ordered task has not be preprocessed step and post processing the task step . The processing unit may determine if the ordered task has been preprocessed in step by for example querying the appropriate memory location storing meta data identifying whether the task has been preprocessed e.g. whether the task has been marked as preprocessed. Having completed processing of the ordered task the processing unit may update the processing table to indicate the ordered task as not being processed. As described in the processing unit may then return to reading the next task in processing queue step . After processing the ordered task from processing queue in step the processing unit may also output the ordered task to inter alia application .

If the processing unit determines that a related ordered task is being processed step YES the processing unit may attempt to preprocess the ordered task. In certain embodiments the processing unit may determine that a related ordered task is being processed by checking the processing table indicating the status of the ordered key assigned to the ordered task by task handler in step . Thus as shown in the processing unit may determine whether the ordered task has been preprocessed in step . The processing unit may determine if the ordered task has been preprocessed in step by for example querying the appropriate memory location storing meta data identifying whether the ordered task has been preprocessed e.g. whether the task has been marked as preprocessed. If the ordered task has been preprocessed step YES the processing unit then proceeds to read the next item in processing queue in step as shown in . If the ordered task has not been preprocessed step NO the processing unit preprocesses the ordered task and marks the ordered task as preprocessed e.g. by modifying the appropriate memory location storing meta data identifying whether the ordered task has been preprocessed in steps and respectively. Then if the processing unit determines that other tasks remain in processing queue step YES it proceeds to read the next task in step of . If the processing unit determines that no other tasks remain in the processing queue step NO it proceeds to determine if it can process the ordered task starting at step in . The processing units used by task processor thus concurrently process tasks in processing queue wherein if the task being processed by a processing unit is an unordered task or an ordered task with no other processing unit processing a related ordered task the processing unit processes the task and if the task is an ordered task and another processing unit is processing a related ordered task the processing unit preprocesses the ordered task if the ordered task needs preprocessing.

In parallel with the steps performed by task handler and task processor discussed above controller may dynamically manage the number of processing units. Controller may dynamically manage the number of processing units by for example determining a required number of processing units based on a rate of unordered tasks being received in processing queue a processing rate of unordered tasks a rate of ordered tasks being received in processing queue a processing rate of ordered tasks and or the number of sets of related ordered tasks in processing queue . Controller may also adjust the number of processing units based on the required number of processing units. Typically however application operates in a runtime environment of finite resources and may only use a certain maximum number of processing units. For example each application running in a typical modern operating system may be allocated on a certain number of execution threads. Thus there may be a maximum number of processing units available to process the unordered and ordered tasks in processing queue .

In certain embodiments the required number of processing units rT is equal to the lesser of MTor uOT OT wherein MTis a maximum number of processing units e.g. the maximum number of process units allocated to application . Moreover uOT f x uOT wherein f is the rate of unordered tasks being received in processing queue x is the processing rate of unordered tasks and uOTis a threshold number of processing units per unordered task. OTis determined based on values of pT epT and uK wherein pT pM OT wherein pMis a number of tasks to be preprocessed concurrently and OTis a threshold number of processing units per ordered task. epT z y OT wherein z is the rate of ordered tasks being received in processing queue y is the processing rate of ordered tasks and uKcorresponds to the number of sets of related ordered tasks in processing queue . Finally OTis equal to the greater of pTand epTif pT 0 or OTis equal to lesser of epTand uKif pT 0.

Parameters f x z and y correspond to rates which may be obtained by dividing the relevant number of tasks e.g. ordered unordered tasks entering processing queue over a certain time period by the time period. For example f may be obtained by determining the number of unordered tasks received in processing queue over the course of a second. Because processed tasks may be removed from processing queue x and y may also correspond to a rate of unordered tasks and ordered tasks respectively removed from processing queue . rTmay be limited to integer values by appropriately rounding up or down the final value of rTto the nearest integer or by appropriately rounding up and or down intermediate parameters used to calculate rTto the nearest integer.

As a proof of concept the table below shows values of uOTand OTbased on parameters y x z uK f pM OT uOT OT and uOT.

Adjusting the number of processing units based on the required number of processing units may comprise setting the number of processing units to the of required number processing units at regular or irregular intervals based on e.g. time elapsed number of tasks processed number of tasks in processing queue etc. Thus for example adjusting the number of processing units may comprise setting the number of processing units to the of required number processing units at regular time intervals or every time a certain number of tasks e.g. 100 1000 10000 etc. is received into processing queue .

