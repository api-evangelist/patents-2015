---

title: Dynamic synchronization object pool management
abstract: A number of synchronization objects simultaneously usable during runtime by a group of threads within a multi-threaded execution environment is predicted by a processor that manages synchronization object allocations within the multi-threaded execution environment. A synchronization object pool is allocated with the predicted number of synchronization objects, each initialized with a deployment state of undeployed and an acquisition state of unlocked. Over time, the deployment state is changed between deployed and undeployed in response to requests by threads to deploy and undeploy the synchronization objects. The acquisition state is independently controlled as the synchronization objects are acquired and released by the threads. The allocated number of synchronization objects within the synchronization object pool is adjusted during the runtime in response to determined deployment rates of the allocated number of synchronization objects.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09569281&OS=09569281&RS=09569281
owner: INTERNATIONAL BUSINESS MACHINES CORPORATION
number: 09569281
owner_city: Armonk
owner_country: US
publication_date: 20150813
---
The present invention relates to synchronization of executing threads. More particularly the present invention relates to dynamic synchronization object pool management.

Computing applications may be written as uniprocessor applications or may be written as multi threaded applications. Each thread of a multi threaded application may be executed by a different processor or may be allocated different intervals of time for sequential execution on a single processor.

A computer implemented method includes predicting by a processor that manages synchronization object allocations within a multi threaded execution environment a number of synchronization objects simultaneously usable during runtime by a group of threads within the multi threaded execution environment allocating a synchronization object pool of the predicted number of synchronization objects each initialized with a deployment state of undeployed and an acquisition state of unlocked changing over time the deployment state between deployed and undeployed in response to requests by threads to deploy and undeploy the synchronization objects where the acquisition state is independently controlled as the synchronization objects are acquired and released by the threads and adjusting during the runtime the allocated number of synchronization objects within the synchronization object pool in response to determined deployment rates of the allocated number of synchronization objects.

A system that performs the method and a computer program product that causes a computer to perform the method are also described.

The examples set forth below represent the necessary information to enable those skilled in the art to practice the invention and illustrate the best mode of practicing the invention. Upon reading the following description in light of the accompanying drawing figures those skilled in the art will understand the concepts of the invention and will recognize applications of these concepts not particularly addressed herein. It should be understood that these concepts and applications fall within the scope of the disclosure and the accompanying claims.

The subject matter described herein provides dynamic synchronization object pool management. The present technology solves a recognized problem with prior technologies related to synchronization objects i.e. locks that resulted in either synchronization induced bottlenecks or unsafe parallelism involving resource management within multi threaded computing environments. The present technology operates by providing a new approach to synchronization that removes synchronization induced bottlenecks and that improves safety and granularity of parallel processing within multi threaded computing environments by use of predictive analysis and demand driven synchronization object quantity adjustments. The improved granularity may be achieved with the present technology because the technology described herein provides for the management of synchronization objects in sufficient quantities to protect many individual resources such as individual objects of a class without incurring wasted overhead of significant numbers of unnecessary synchronization objects. In a conventional execution environment that lacks the present technology where a synchronization object protects a less fine grained data structure for example all objects of a class simultaneously any thread s access to one object of the class may delay other threads in their attempts to access other objects of the same class. These delays in conventional technologies impose inefficiency in thread performance and the present technology solves this recognized problem. The technology described herein adjusts synchronization object allocations over time to provide more effective utilization of available system resources e.g. memory processor occupancy etc. in view of varying resource demands over time than may otherwise be possible. As such the technology described herein improves computing performance within complex computing environments by solving several issues that were recognized to exist in prior technologies.

The present technology operates by programmatically predicting by a processor that manages synchronization object allocations within a multi threaded execution environment a number of synchronization objects simultaneously usable during runtime by a group of threads within the multi threaded execution environment. A synchronization object pool is allocated with the predicted number of synchronization objects with each synchronization object initialized with a deployment state of undeployed and an acquisition state of unlocked. Over time the deployment state is changed between deployed and undeployed in response to requests by threads to deploy and undeploy the synchronization objects. The acquisition state is independently controlled as the synchronization objects are acquired and released by the threads. The allocated number of synchronization objects within the synchronization object pool is adjusted during the runtime in response to determined deployment rates of the allocated number of synchronization objects.

For purposes of the description herein a multi threaded execution environment is understood to include a processing platform that supports execution of multiple concurrent application level threads such as one or more of a cloud or grid computing system or any other system subsystem or system component that provides execution support for multiple threads that may be executed within one or more of an operating system an executing process a computer program a software component a service a device driver a firmware component an application or any other form of executable programmatic component that may be executed by one or more hardware processors. A multi threaded execution environment may further include a virtual machine environment provided by one or more hardware processors where software runs on a virtual machine that supports multi threaded execution. Further a multi threaded execution environment may include a physical hardware environment where software runs on the physical hardware that supports multi threaded execution. A number of synchronization objects simultaneously usable during runtime by threads within a multi threaded execution environment may include a set of synchronization objects usable to ensure thread safety for the threads that may execute within such a multi threaded execution environment as defined herein.

Deployment rates may include time varying measures related to runtime utilization or real time demand for synchronization objects such as demand above or below a threshold percentage of locks in a pool or demand relative to one or more baseline values or other metric values. Demand above a percentage baseline or other metric may provide an indication to increase the pool size. Demand below a percentage baseline or other metric may provide an indication to decrease the pool size. Any change in synchronization object demand over time with respect to a defined rate percentage baseline or other metric may provide an indication to increase or decrease the pool size according to the particular demand. Determination of synchronization object deployment rates may include determination of synchronization object deployment or utilization at one or more instants during a run for comparison with one or more percentages baselines or other metrics and may utilize information other than a determination based on a literal rate calculation.

The technology described herein involves two primary aspects predictive deployment and management of a synchronization object pool and dynamic demand based size adjustment of the synchronization object pool. First the present technology predictively manages a pool of synchronization objects e.g. critical sections semaphores mutexes spin locks etc. such that each may be dynamically deployed for use in protecting one or more objects and such that each may be subsequently undeployed by being returned to the pool. Secondly the present technology tracks a deployment state of each synchronization object e.g. undeployed and deployed and in certain implementations tracks an associated acquisition state of each synchronization object e.g. unlocked and locked . The present technology utilizes this information to dynamically adjust the size of e.g. number of synchronization objects in the synchronization object pool. Once an initial predictively sized synchronization object pool has been established the number of allocated synchronization objects in the pool may be dynamically increased or decreased over time as system demands change over time. The adjustments may occur at runtime as applications or services start and stop sharing resources as software components load and unload and or as inbound workload changes.

It should be noted that the description below uses the term lock in certain locations as a synonym for synchronization object for ease of description. However it should be understood that use of the term lock as a synonym for synchronization object is not limiting the description to a particular style of implementation of a synchronization object and that as a result the term lock refers to any form of synchronization object within the scope of the present description such as but not limited to those described by example above.

At the beginning of a system run or during a system run locks may be allocated initialized and designated to be undeployed members of a synchronization object pool alternatively pool hereinafter . The locks may be provided by the operating system OS provided or may be implemented by developers of applications services device drivers and other components. Alternatively the locks may be a combination of OS provided locks and component developer implemented locks as appropriate for a given implementation.

Regarding predictive initial determination and subsequent adjustment of the number of locks to be allocated within a pool to optimize memory requirements the number of locks in the pool may be set to be equal to a number of objects of classes that may require protection via locks in the pool. Alternatively to improve memory utilization a more optimum number of locks in the pool may be determined by performing predictive analytics based upon the characteristics of the applications threads that are intended to utilize or are actually utilizing the pool.

For example the following Equation 1 may be applied to determine a maximum number of locks a thread may acquire at one time e.g. Maximum Locks Held Simultaneously by a Thread Maximum Locks Held Simultaneously by a Thread Maximum Locks Acquirable Simultaneously Per Routine Maximum Call Chain Depth Equation 1 

Within Equation 1 the term Maximum Locks Acquirable Simultaneously Per Routine represents a maximum number of locks that may be simultaneously acquired per routine within a given thread. Further the term Maximum Call Chain Depth represents a maximum depth of a call chain that may include such a routine. These values may be determined by programmatic analysis of code produced by a code build process or otherwise as appropriate for a given implementation.

Using the result from Equation 1 e.g. Maximum Locks Held Simultaneously by a Thread Equation 2 below may be applied to determine a maximum number of objects that may require simultaneous protection e.g. Maximum Simultaneous Objects Maximum Simultaneous Objects Number of Threads Maximum Locks Held Simultaneously by a Thread Equation 2 

Within Equation 2 the term Number of Threads represents a number of threads that may be active within an executing process i.e. an instance of a computer program loaded into an address space and associated with an execution context . Further the term Maximum Locks Held Simultaneously by a Thread represents a maximum number of locks that a thread may acquire at once. As with the above parameters these values may be determined by programmatic analysis of code within a code build or otherwise as appropriate for a given implementation.

Using the result from Equation 2 e.g. Maximum Simultaneous Objects the number of locks to create e.g. Num Locks either initially or as adjusted over time may be identified programmatically to equate to the determined maximum number of objects that may require simultaneous lock protection.

It should be noted that the Equation 1 above may result in an overestimation and static analysis or other code analysis techniques may be employed to refine improve that estimate of the maximum number of locks a thread may acquire at one time. It should additionally be noted that the above values may change over the course of the system run so that the pool may be updated to contain more or fewer locks as appropriate to improve runtime efficiency performance and resource utilization as described above and in more detail below. Changeable values may include each of the number of threads e.g. Number of Threads value and the maximum number of locks a thread may acquire at one time e.g. Maximum Locks Held Simultaneously by a Thread value. These values may change in response to the changing resource requirements that result as software components are loaded and unloaded within a process. As such the present technology may perform iterative determinations of pool size and utilization over time and may perform appropriate dynamic e.g. run time adjustments of the pool size to accommodate real time demand for pooled synchronization objects.

Regarding tracking of utilization of synchronization objects within a given pool a data structure associated with the pool such as one or more pool management bit vectors or structures of other forms may be created. In certain implementations a deployment state bit vector may be used to track the deployment states of locks within a given pool. As an alternative a second data structure e.g. a bit array or other structure may be used either separately or as an additional field associated with each deployment state bit to indicate the acquisition state of any given lock within a given pool. As an alternative an additional acquisition state bit vector or integer vector may be utilized to track acquisition states of locks. As an alternative to tracking the acquisition state of the locks at the pool level the locks themselves may be managed as locked or unlocked within the memory space allocated to the respective locks.

From the pool management perspective the deployment states of locks within the pool may be one of deployed and undeployed. The acquisition state of the locks may be one of locked potentially also with a recursion count and unlocked.

Regarding implementation of either of the state data structures described above e.g. deployment and acquisition as one possible example if a pool is associated with an object class then a state data structure e.g. bit may be associated with the object. The state data structure may be a member of the object or a member of a set of state data structures e.g. a bit vector associated with the class. A pool identifier may identify the respective pool associated with the respective bit or bit vector. The set of state data structures associated with the class may be of adjustable size such that the number of locks in the pool may be increased or decreased over time based upon demand as objects of the class are created and destroyed respectively. To implement the dynamic size adjustment the data structure may be implemented as a reallocatable array or other structure of bits.

To improve readability of the remaining description a bit vector or bit array will be utilized for purposes of example with the understanding that the description below applies at least equally to other types of in memory data structures as described above. As such use of bit arrays vectors for purposes of the additional examples and description below does not limit the description herein to any particular form of data structure.

For purposes of example the reallocatable array of bits may be implemented by allocating a new bit array of the target adjusted size e.g. larger or smaller such as by use of a memory allocation or reallocation routine e.g. malloc realloc or a similar routine and the active contents of the current bit array may be migrated to the new bit array. As such where a bit array is to be increased in size the new bit array may be allocated to be larger than the current bit array and the current locks represented within the bit array may be likewise represented within the new bit array e.g. without any changes in bit positions etc. . Alternatively where the bit array is to be decreased in size the new bit array may be allocated to be smaller than the current bit array the current locks represented within the bit array may be represented in condensed form to remove unused lock bit positions and the condensed bit array may be moved to the new bit array. The migration of the active contents of the current bit array to the new bit array may be performed by mapping the deployed state bits of the current bit array to new bits within the new bit array and setting the deployed state within the respective bit positions e.g. as deployed or as undeployed .

For a bit array size reduction unused bit positions may be omitted from the mapping. As such where it is determined that the currently allocated number of synchronization objects is in excess of what is being used the number of synchronization objects and the associated pool management state bits may be reduced in number. A mapping between the bits in the array and the deployed locks protecting objects associated with those bits may be maintained for example by maintaining bits indicating a deployed state in ordinal positions in a bit array that are identical to ordinal positions of protected objects in an array of objects. Other mappings may serve for objects arranged in other non array data structures.

For a bit array size increase all bits of the current bit array may be directly mapped into identical bit positions within the new bit array to expedite processing. The respective bit states may also be directly copied into identical bit positions within the new bit array to further reduce real time processing.

Regarding logic preference for bit values it should be noted that either active high or active low logic values may be used as appropriate for a given implementation. Using active high logic the default value may be cleared e.g. zero 0 . Alternatively using active low logic the default value may be set e.g. one 1 depending upon the logic preference within the particular system to indicate an undeployed state of a lock. When a lock is deployed for use in protecting an individual object of a class then a specific bit within a vector specifically corresponding to that class may be set to indicate that the lock is deployed e.g. set to one 1 for active high logic or alternatively cleared to zero 0 using active low logic again respectively and with respect to the logic preference within the particular system to indicate the deployed state for the lock .

In implementations that associate an acquisition state bit with each deployment state bit when an object update is detected the corresponding bit for that object within the acquisition state bit vector may first be checked via an atomic operation that may also then set the bit if the check reveals it to be clear for acquisition of the lock. As such the atomic check set represents an indivisible uninterruptible operation. The lock bit in the acquisition state bit vector or in the memory allocated for the lock itself may be subsequently cleared again when the update of the object is completed. As such within such an implementation the pool management data structure may also serve as at least a portion of the underlying lock mechanism itself.

With further reference to the predictive determination and adjustment of the number of locks to be allocated within a pool to optimize memory requirements where locks are created based on predictive programmatic computations and or static analysis as described above one or more of the following processing variations may be utilized as appropriate for a given implementation. For example the creation and use of a synchronization object pool lock pool for the protection of objects of a class may eliminate performance bottlenecks caused by conventional lock contention that may occur when a single shared lock is used to protect all of the objects of the class. The lock pool may be originally sized and or resized to fit with the number of objects of the class for which simultaneous protection is provided. Routines that implement the present technology may thus be designed to reduce contention for locks and data overhead required by the implementation.

All threads that attempt to access an object of a class associated with a synchronization object pool may collectively safely do so using routines designed for use with the pool. These routines may include one or more routines associated with each of lock deployment acquisition release and undeployment. A lock deployment routine may accept as input a handle or pointer to a specified object of the class and may set a state bit associated with that object in a bit vector associated with the pool to indicate lock deployment. A lock acquisition routine may ensure that a lock is deployed for protection of a particular object such as a handle or pointer that may arrive as input to the routine within some implementations before attempting to acquire the lock. As the thread that has invoked the lock acquisition routine takes ownership of the lock the routine may arrange for an acquisition state bit and the related state data to be set accordingly. A lock release routine may accept as input an indication as to whether further protection of the object is expected by any thread for example if the lock is being held at certain times only during processing performed within the context of a component that is unloading or about to be unloaded. If no further use of the lock is expected then the lock release routine may undeploy the lock by clearing the bit associated with that object. A lock undeployment routine may determine whether the lock is held and if not may undeploy the lock by clearing that bit.

A routine invoked to acquire a lock for protection of an individual object may first determine whether a lock has already been deployed to protect the object. If no lock has been deployed to protect the object the routine may deploy a lock from a specified pool on demand and may acquire lock that deployed lock so that lock protected processing may continue without additional delays that may otherwise be imposed if the locks were conventionally assigned to protect all objects of a class as opposed to protecting individual objects within a class as described herein. The deployment and acquisition of the lock may be performed together again via an atomic operation or under the protection of some other pool management lock specific to the pool and in some implementations the class . As such on systems that support multiple memory updates in an atomic operation two memory locations may be updated by the atomic operation to deploy and acquire the lock the deployment state indicator designating the lock as deployed and the separate acquisition state indicator designating the lock as locked. On systems that support only single address atomic operations the bits representing the deployment and acquisition state may be arranged to share a common address or other synchronization techniques may be applied to ensure thread safe access to those bits as appropriate for a given implementation.

A routine invoked to acquire a lock for protection of an individual object may determine that the lock has already been deployed to protect the respective object and if so the routine may further determine whether another thread is holding the lock. If another thread is holding the lock the routine may wait for the lock to be released. Alternatively if the lock is deployed and available e.g. not locked the routine may acquire lock the lock again so that processing may continue under the lock s protection. Another routine may subsequently be invoked to release unlock the lock and to determine whether the lock may also be undeployed e.g. if no other usage of that lock by the current thread is foreseen in the short term such as by use of a flag passed to a lock releasing routine.

It should be noted that the respective routines may be implemented as procedure calls e.g. methods by application programming interface API functions or otherwise as appropriate for a given implementation. The routine s and or API functions may be implemented at an operating system OS level. The operating system may provide API functionality to create locks as pool members and or to assign locks to pools to deploy the locks in the pool to un deploy those locks and to perform other management functions e.g. acquire lock after a prospective wait try acquire with no wait release increment decrement recursion count change owning thread etc. .

Any number of pools may be created again within memory space limits. In some implementations a single pool may serve to provide thread safe resource access among all of the threads that execute in the context of a single executing process. The resources protected by a single pool may include resources utilized by an entire application a component of an application or one or more specific object classes or subclasses. As such synchronization object pools may be implemented granularly as appropriate for the given implementation. A pool may be identified by a handle an ordinal value or any other suitable identifier.

In view of the description above many possibilities exist for tracking of locks and for dynamic size adjustment of the data structures e.g. bit vectors etc. used to manage synchronization objects within a pool. All such possibilities are considered to be within the scope of the present subject matter.

It should be noted that conception of the present subject matter resulted from recognition of certain limitations associated with thread safety. For example it was observed that within complex highly parallelized systems synchronization objects alternatively referred to as locks tend to be relatively few in number e.g. a few thousand operating system provided synchronization objects in relation to the number of data structures the synchronization objects protect e.g. hundreds of thousands millions etc. . It was further observed that as a result of the limited number due to resource limitations these synchronization objects are typically applied to all of the data structures of a given class. However it was determined that if a thread e.g. thread A needs to update one member of the class as a consequence of the class level granularity of synchronization objects a separate thread e.g. thread B cannot update another member object of the class type at the same time even in cases where there is no actual overlap of the data owned by those individual class members objects. For example if the thread A adds event specific data structures representing observations of entities to lists and queues and the thread B cleans up the lists and those queues as the events expire it was observed that prior technologies that locked all elements of all queues during an update imposed unnecessary wait states involving the two threads. These unnecessary wait states decrease application performance and may even affect overall system performance. It was further determined that for these reasons synchronization by use of prior technologies has become increasingly limited both as systems grow and as the systems involve increased parallelism with respect to utilization of available system resources. From these several observations and determinations it was further determined that a new form of synchronization was desirable that provides for the runtime on demand application of synchronization objects to individual members objects of a class rather than to a class as a whole and that predictively and on demand dynamically adjusts allocations of synchronization objects over time to enhance utilization of available system resources relative to synchronization object use. As such the present subject matter improves complex parallel processing within complex systems by reducing synchronization induced bottlenecks and unsafe parallelism thereby increasing granularity of object synchronization and safe parallelism each with dynamic resource allocations in accordance with demand driven behavior that may improve both system performance and resource exploitation as described above and in more detail below. As such improved parallel processing may be obtained through use of the present technology.

The dynamic synchronization object pool management described herein may be performed in real time to allow prompt determinations of changes to the allocated number of synchronization objects within a pool and to perform pool management over time using predictive analytics and demand driven synchronization object allocation and deployment. For purposes of the present description the term real time shall include any time frame of sufficiently short duration as to provide reasonable response time for information processing acceptable to a user of the subject matter described. Additionally the term real time shall include what is commonly termed near real time generally meaning any time frame of sufficiently short duration as to provide reasonable response time for on demand information processing acceptable to a user of the subject matter described e.g. within a portion of a second or within a few seconds . These terms while difficult to precisely define are well understood by those skilled in the art.

As will be described in more detail below in association with through the computing device  through the computing device N may each provide automated dynamic synchronization object pool management. The automated dynamic synchronization object pool management is based upon predictive analysis of synchronization object utilization with dynamic adjustments of synchronization object pool sizes to improve execution efficiency and to reduce memory processor utilization. The present technology may be implemented at a user computing device or server device or in hardware firmware application software service software or operating system software on one or more of these devices in any combination thereof as appropriate for a given implementation. A variety of possibilities exist for implementation of the present subject matter and all such possibilities are considered within the scope of the present subject matter.

The network may include any form of interconnection suitable for the intended purpose including a private or public network such as an intranet or the Internet respectively direct inter module interconnection dial up wireless or any other interconnection mechanism capable of interconnecting the respective devices.

The server  through the server M may include any device capable of providing data for consumption by a device such as the computing device  through the computing device N via a network such as the network . As such the server  through the server M may each include a web server application server or other data server device.

Further the core processing module may provide different and complementary processing of synchronization objects and synchronization object pools in association with each implementation. As such for any of the examples below it is understood that any aspect of described functionality with respect to any one device in conjunction with another device e.g. sends sending etc. is to be understood to concurrently describe the functionality of the other respective device e.g. receives receiving etc. .

A central processing unit CPU processor provides hardware that performs computer instruction execution computation and other capabilities within the core processing module . A display provides visual information to a user of the core processing module and an input device provides input capabilities for the user.

The display may include any display device such as a cathode ray tube CRT liquid crystal display LCD light emitting diode LED electronic ink displays projection touchscreen or other display element or panel. The input device may include a computer keyboard a keypad a mouse a pen a joystick touchscreen voice command processing unit or any other type of input device by which the user may interact with and respond to information on the display .

It should be noted that the display and the input device may be optional components for the core processing module for certain implementations devices or may be located remotely from the respective devices and hosted by another computing device that is in communication with the respective devices. Accordingly the core processing module may operate as a completely automated embedded device without direct user configurability or feedback. However the core processing module may also provide user feedback and configurability via the display and the input device respectively as appropriate for a given implementation.

A communication module provides hardware protocol stack processing and interconnection capabilities that allow the core processing module to communicate with other modules within the system . The communication module may include any electrical protocol and protocol conversion capabilities useable to provide interconnection capabilities as appropriate for a given implementation. As such the communication module represents a communication device capable of carrying out communications with other devices.

A memory includes a synchronization object pool storage area that stores one or more dynamically managed synchronization object pools in association with the core processing module . The memory also includes an application area that provides storage and execution space for one or more multi threaded applications and storage and execution space for one or more objects instantiated by the respective multi threaded application s . The objects instantiated by the respective multi threaded application s within the application area may be protected using locks provided and managed within the synchronization object pool storage area . One or more dynamically managed synchronization object pools may be implemented as part of one or more multi threaded applications themselves or they may be implemented via separate services operating system provided features or loadable components or modules.

It is understood that the memory may include any combination of volatile and non volatile memory suitable for the intended purpose distributed or localized as appropriate and may include other memory segments not illustrated within the present example for ease of illustration purposes. For example the memory may include a code storage area an operating system storage area a code execution area and a data area without departure from the scope of the present subject matter.

A dynamic synchronization object pool management module is also illustrated. The dynamic synchronization object pool management module provides analytical processing for predictive creation of appropriately sized synchronization object pools according to predicted utilization of locks and also provides ongoing demand driven pooled synchronization object adjustments for the core processing module as described above and in more detail below. The dynamic synchronization object pool management module may include a prediction engine component that may execute independently of and in some cases prior to a process that will use a synchronization object pool of a predicted size along with a dynamic synchronization object pool management runtime component that may execute as part of such a process. The dynamic synchronization object pool management module implements the automated dynamic synchronization object pool management of the core processing module .

It should also be noted that the dynamic synchronization object pool management module may form a portion of other circuitry described without departure from the scope of the present subject matter. Further the dynamic synchronization object pool management module may alternatively be implemented as an application service or operating system provided feature stored within the memory . In such an implementation the dynamic synchronization object pool management module may include instructions executed by the CPU for performing the functionality described herein. The CPU may execute these instructions to provide the processing capabilities described above and in more detail below for the core processing module . The dynamic synchronization object pool management module may form a portion of an interrupt service routine ISR a portion of an operating system a portion of a web server or browser application or a portion of any application or system software without departure from the scope of the present subject matter.

A timer clock module is illustrated and used to determine timing and date information such as wait times for releases of locks as described above and in more detail below. As such the dynamic synchronization object pool management module may utilize information derived from the timer clock module for information processing activities associated with the dynamic synchronization object pool management described herein.

The CPU the display the input device the communication module the memory the dynamic synchronization object pool management module and the timer clock module are interconnected via an interconnection . The interconnection may include a system bus a network or any other interconnection capable of providing the respective components with suitable interconnection for the respective purpose.

Though the different modules illustrated within are illustrated as component level modules for ease of illustration and description purposes it should be noted that these modules may include any hardware programmed processor s and memory used to carry out the functions of the respective modules as described above and in more detail below. For example the modules may include additional controller circuitry in the form of application specific integrated circuits ASICs processors antennas and or discrete integrated circuits and components for performing communication and electrical control activities associated with the respective modules. Additionally the modules may include system level interrupt service routine level ISR level and application level modules as appropriate. Furthermore the modules may include any memory components used for storage execution and data processing for performing processing activities associated with the respective modules. The modules may also form a portion of other circuitry described or may be combined without departure from the scope of the present subject matter.

Additionally while the core processing module is illustrated with and has certain components described other modules and components may be associated with the core processing module without departure from the scope of the present subject matter. Additionally it should be noted that while the core processing module is described as a single device for ease of illustration purposes the components within the core processing module may be co located or distributed and interconnected via a network without departure from the scope of the present subject matter. Many other possible arrangements for components of the core processing module are possible and all are considered within the scope of the present subject matter. Accordingly the core processing module may take many forms and may be associated with many platforms.

At block the process allocates the determined number of locks within a synchronization object pool. As described above the acquisition state indicators may be allocated within a separate data structure or as a paired indicator associated with each deployment state indicator used to manage deployment of each synchronization object or alternatively the acquisition state indicators may be implemented within the locks themselves. At block the process allocates one or more pool management data structure s e.g. a deployment state bit vector and allocates the additional acquisition state data structure and acquisition state bit vector if appropriate for the given implementation. As such the process maintains a pool management data structure comprising for each allocated synchronization object within the synchronization object pool a deployment state indicator that identifies the deployment state of the respective allocated synchronization object as one of deployed and undeployed. The process may further maintain within the pool management data structure for each allocated synchronization object within the synchronization object pool an acquisition state indicator that identifies the acquisition state of the respective allocated synchronization object as one of locked and unlocked as appropriate for the given implementation. It should be noted that as described above the acquisition state data structure indicators may alternatively be provided at an operating system OS level or otherwise as appropriate for a given implementation. As described in more detail below the allocated number of synchronization objects and the allocated number of data structure indicators may be adjusted over time in accordance with changes in demand for the pooled synchronization objects.

In response to determining at decision point to change the allocated synchronization object pool size the process determines a pool allocation size adjustment at block which as described above may include a determination of an increased number or decreased number of synchronization objects. At block the process adjusts during runtime the number of synchronization objects and deployment state indicators in the synchronization object pool. The process may dynamically adjust the number of allocated synchronization objects and may dynamically adjust the number of deployment state indicators within a pool management data structure during runtime responsive to adjusting during the runtime the allocated number of synchronization objects within the dynamic synchronization object pool. The adjusted number of deployment state indicators within the pool management data structure correlates with the adjusted number of synchronization objects within the dynamic synchronization object pool. The process returns to decision point and iterates responsive to determinations to adjust the allocated pool size as described above.

As such the process performs an analysis which may include use of static analysis control flow analysis synchronization object deployment rate analysis or probabilistic analysis to predict a number of synchronization objects simultaneously usable during runtime by threads within a multi threaded execution environment and allocates the predicted number of synchronization objects within a synchronization object pool along with pool level object management bits. The process performs runtime adjustment of the allocated number of synchronization objects within the dynamic synchronization object pool according to the runtime demand for the synchronization objects within the pool.

As described above in association with through the example systems and processes provide dynamic synchronization object pool management. Many other variations and additional activities associated with dynamic synchronization object pool management are possible and all are considered within the scope of the present subject matter.

Those skilled in the art will recognize upon consideration of the above teachings that certain of the above examples are based upon use of a programmed processor such as the CPU . However the invention is not limited to such example embodiments since other embodiments could be implemented using hardware component equivalents such as special purpose hardware and or dedicated processors. Similarly general purpose computers microprocessor based computers microcontrollers optical computers analog computers dedicated processors application specific circuits and or dedicated hard wired logic may be used to construct alternative equivalent embodiments.

The present invention may be a system a method and or a computer program product. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.

The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present invention may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present invention.

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art based upon the teachings herein without departing from the scope and spirit of the invention. The subject matter was described to explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

