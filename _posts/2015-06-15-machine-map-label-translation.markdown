---

title: Machine map label translation
abstract: Map label translation implementations described herein transliterate and translate map labels in a first language, even those written in a non-Latin script, into a Latin script and into a second language. In some implementations a translation candidate generator process generates possible translation candidate sequences for each n-gram in the map label, for example, an entity's name in Japanese and possibly in native Japanese (non-Latin, non-Roman) script. A candidate sequence selector selects a number of top possible translation candidate sequences. A ranking feature extraction process is used to rank the selected top number of top possible translation candidate sequences by using a trained probabilistic classifier and geospatial and linguistic context information as ranking features. A post ranker then re-ranks the selected ranked translation candidates depending on neighboring proximity information around the location of the entity and outputs the best map label translation in the second language.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09519643&OS=09519643&RS=09519643
owner: Microsoft Technology Licensing, LLC
number: 09519643
owner_city: Redmond
owner_country: US
publication_date: 20150615
---
In general maps of a city state or country are usually labeled in the language of the location they represent. Often the map labels are also in a native non Latin script. Map labels can be manually translated from a first language in a non Latin script to a second language. In order to do so it is often necessary to transliterate the map label from a native non Latin script into a phonetic representation in a Latin script. This can be labor intensive and as a result map providers do not translate labels on the maps they produce even those designed for use by tourists. For example Japanese map providers provide map labels in Japanese often in a native Japanese script. Some do provide versions of maps that have Romanized names or Latin names but the coverage of entities in these maps is only 30 of all map labels depicted on a Japanese map of the same geographic area. Without a map with labels translated into a Roman script and into a global language visitors have difficulty finding their way around.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

In general the map label translation implementations described herein translate map labels in a first language even those written in a non Latin script into a second language in Latin script. To this end in some implementations a translation candidate generator generates possible translation candidate sequences for each n gram in a map label in a first language that is to be translated into a second language. For example the map label can be the entity s name in Japanese and possibly in native Japanese script non Latin non Roman such as for example Kanji . A candidate sequence selector then selects a number of these translation candidate sequences for further processing. A ranking feature extraction process is used to extract features based on geospatial and linguistic context from the map label and the selected translation candidate sequences. The features are used to rank the selected translation candidate sequences using a probabilistic classifier trained at least in part with the same features. A post ranker then re ranks the ranked translation candidates output by the probabilistic classifier based on the labeling of neighboring entities in proximity to the location of the entity represented by the map label. The best translation for the input map label in the second language is then output.

The map label translation implementations described herein are advantageous in that they are capable of providing transliterations and translations of map labels using geographical context give appropriate abbreviations suffixes or prefixes to translated map labels and make a reliable translation prediction when there is no truth data as to what the translation for the map label should be. Furthermore many map label translations can be performed in a short period of time especially when compared to manual translation. This is especially true when the translation requires transliteration of a native script into a Roman and Latin script.

In the following description of map label translation implementations reference is made to the accompanying drawings which form a part thereof and which show by way of illustration examples by which implementations described herein may be practiced. It is to be understood that other embodiments may be utilized and structural changes may be made without departing from the scope of the claimed subject matter.

The following sections provide an introduction and overview of the map label translation implementations described herein as well as exemplary processes and a system for practicing these implementations. Details of various embodiments and exemplary computations are also provided.

As a preliminary matter some of the figures that follow describe concepts in the context of one or more structural components variously referred to as functionality modules features elements etc. The various components shown in the figures can be implemented in any manner. In one case the illustrated separation of various components in the figures into distinct units may reflect the use of corresponding distinct components in an actual implementation. Alternatively or in addition any single component illustrated in the figures may be implemented by plural actual components. Alternatively or in addition the depiction of any two or more separate components in the figures may reflect different functions performed by a single actual component.

Other figures describe the concepts in flowchart form. In this form certain operations are described as constituting distinct blocks performed in a certain order. Such implementations are illustrative and non limiting. Certain blocks described herein can be grouped together and performed in a single operation certain blocks can be broken apart into plural component blocks and certain blocks can be performed in an order that differs from that which is illustrated herein including a parallel manner of performing the blocks . The blocks shown in the flowcharts can be implemented in any manner.

The map label translation implementations described herein are advantageous in that they are capable of providing transliterations and translations of map labels using geographical context with a probabilistic classifier to give appropriate abbreviations suffixes or prefixes to translated map labels and make a reliable translation prediction when there is no truth data as to what the translation for the map label should be. Furthermore many map label translations can be performed in a short period of time especially when compared to manual translation because the probabilistic classifier can process large amounts of data in a relatively short time. This is especially true when the translation requires transliteration of a native script into a Roman and Latin script.

Japanese is a complex language consisting of three different types of native script character sets Kanji Hiragana and Katakana . A fourth character set which is a Latin script known as Romaji in Japanese or the Romanization of Japanese names is also used. Moreover how a Romanized name is spelled has many variations and introduces an additional complexity.

One aspect of translating Japanese local entities is the phonetic ambiguity of Kanji. The phonetic of a Kanji word can change depending on the location or the entity the word represents. For example the Japanese word can be read as Shinjyuku which is a ward in Tokyo prefecture but it can also be read as Arajyuku the name of a town in Ibaragi Prefecture or even Niijyuku the name of a neighborhood . There could even be other possibilities.

Map label translation is also non trivial when it comes to abbreviations. Since a label occludes the map ideally one needs abbreviations for languages that tend to occupy more space than others. For example Arakawa is the Japanese term for the Arakawa River. One can notice immediately that Arakawa River is longer than Since map readability is highly dependent on the reduction of label cluttering one can abbreviate River to be Riv. making it Arakawa Riv. Additionally Arakawa by itself contains the nuance of river kawa river so the map label can be reasonably abbreviated to Ara Riv. Japan s Ministry of Land Infrastructure Transport and Tourism however has unanimously agreed that Ara. Riv. is a special case that must be denoted in its fuller form as Arakawa Riv. This does not apply to the abbreviations for other rivers though such as Kandagawa which can be abbreviated as Kanda Riv. 

Additionally suffixes and prefixes can be trans positioned depending on the entity type. For example Riv. is a suffix but Mt. Mountain is a prefix. In Japanese both Riv. and Mt. are suffixes.

When dealing with the names of local entities in Japanese such as the name of a restaurant or a building not all local entities have a translation at all so it is difficult to find the true translation in the first place.

Referring again to map label translation implementations described herein use the following exemplary process to translate a map label in a first language into a second language. As shown in block an entity s map label in a first language that is to be translated into a second language is received. If the map label has no pair data e.g. the label has no Latin equivalent in the second language translation candidates are generated for it. To this end as shown in block translation candidates in the second language are generated for each n gram in the map label in the first language. Each n gram is a contiguous sequence of items from a given sequence of text or speech e.g. phoenemes syllables letters words etc. . These translation candidates are used to generate translation candidate sequences e.g. by using combinations of translated terms for each n gram in the map label in the first language .

As shown in block a number of the generated translation candidate sequences are then selected by performing an interim ranking. For example the number of the selected translation candidate sequences can be a prescribed number for example the top two or three. In some map label translation implementations this interim ranking is done by using a conventional Viterbi algorithm that finds the most likely translation candidate sequence s given the input map label. For a given translation candidate sequence the Viterbi algorithm uses the transition probabilities between terms in the given translation candidate sequence. The transition probabilities between terms are pre calculated. Details of how transition probabilities can be pre computed are provided in Section 2.3 and 2.3.1.

As shown in block features are extracted from the map label and the number of selected translation candidate sequences by using geospatial and linguistic context information. This context information can include for example location character type geographical coordinates proximity information linguistic features and so forth.

Using the extracted features as shown in block the selected translation candidate sequences are then ranked using a probabilistic classifier trained at least in part with the features of the same type as the extracted features.

As shown in block the ranked translation candidate sequences received from the probabilistic classifier are then re ranked or post ranked using information about entities neighboring the location corresponding to the map label. In some implementations the translation candidate sequences are re ranked to align with the labeling or numbering of neighboring buildings or entities. The highest re ranked translation candidate sequence is output as the translated map label e.g. in the second language and Roman script .

The n grams in Japanese script e.g. Kanji Romaji etc. are then transliterated into Latin script if the entity s map label is not already in Latin script as shown in block . Transliteration is the conversion of text from one script or alphabet to another. A transliteration engine such as for example a Yomi Predictor model or a Romani mapper can be used to transliterate the n grams into Latin script. A white list a list that has approved translations for a given transliterated Japanese term or n gram can be used to translate the transliterated n grams into Latin script and or the second language. Various sources can be used to generate the white list of paired data e.g. Japanese English for given terms. Other methods of obtaining translations include obtaining them from common sources of translation that include for example pre globalized map data from a data provider that provides both Japanese and Latin equivalents for the same entity entities from Wikipedia that contain the name of the entity in both Japanese and a Latinized form or existing machine translation results. The translated n grams are then combined to create translation candidate sequences as shown in block .

The translation candidate generator then generates possible translation candidate sequences by combining translations for each n gram in the map label.

A candidate sequence selector selects a number translation candidate sequences generated by the translation candidate generator by using the transition probability e.g. ranking score of each translation candidate sequence and selecting a given number of these based on their score translation candidate sequences with higher transition probabilities being ranked higher . The translation candidate selector can employ a Viterbi model to rank the translation candidate sequences. The translation candidate sequences are each defined by transition probabilities output by the Viterbi model. The transition probabilities can be based on pre computed probabilities for the probability of transition between the transliterated terms of a given translation candidate sequence. Exemplary computations are provided in Section 2.3. A number of translation candidate sequences are selected by the translation candidate sequence selector for further processing based on their transition probability the higher the transition probability the higher they are ranked for selection .

A ranking feature extractor is used to extract features from the map label and the selected number of translation candidate sequences by using geospatial and linguistic context information as ranking features which can consist of location character type etc. . The features are then input into a probabilistic classifier that is trained at least in part with training data comprising similar features . The probabilistic classifier then ranks the selected translation candidate sequences using the features .

A post processor then re ranks the ranked translation candidate sequences received from the probabilistic classifier depending on neighboring proximity information around the location of the entity corresponding to the input map label . Different rules based on the proximity of entities surrounding the entity corresponding to the map label can be used to re rank the translation candidate sequences. The highest ranked translation candidate sequence is then output as the best translation for the input map label .

The following paragraphs provide details and exemplary computations for practicing various map label translation implementations.

Map label translation implementations described herein make an international friendly map by translating map labels in a first language into a second language. These implementations can translate the map labels even if the first and second languages use different scripts. Map label translation implementations as described herein meticulously deal with high demand and high visibility areas of a map and provide a user friendly Latin name to each map entity.

For purposes of the following description a first language of Japanese and a second language of English are presumed. However those with ordinary skill in the art will know that the map label translation implementations described herein can be applied to various input and output languages and scripts. For example the first language could be languages such as Korean Chinese Arabic Hindi or other language written in their native non Roman scripts and the second language could be English French German Portuguese or any other language written in a Latin script.

Some map label translation implementations described herein use a loose form of Hepburn spelling to transliterate native Japanese script into Latin Roman script. For example transliterates to erimochou or erimocho in Hepburn spelling. Furthermore map label translation implementations described herein can use various available translation techniques such as Wikipedia translation where possible. Implementations aim for delimiting input map labels by spaces dashes for example is broken into n grams comprising Fujimi machi 3 cyoume. Implementations also determine suffix or prefix placement depending on context e.g. Arakawa Riv. but Mt. Fuji . Nuances that are repeated as a result of translation are typically ignored e.g. as Nijyuubashi Bridge is considered a good translation but not Nijyuu Bridge . Additionally names originally in the second language are not converted.

In some implementations an entity s map label in a first language that is to be translated into a second language is received. For an entity whose map label has no pair data e.g. the label on the map has no Latin equivalent in the second language translation candidate sequences are generated for this map label by using combinations of n grams derived from the entity s map label. A number of the translation candidate sequences are then selected by performing an interim ranking of them. For example the number of selected translation candidate sequences can be a prescribed number for example the top three. In some implementations this interim ranking is done by using a conventional Viterbi algorithm. For a given translation candidate sequence the Viterbi algorithm uses the transition probabilities between terms e.g. n grams in the given translation candidate sequence. The transition probabilities between terms are pre calculated for example by looking at the source from which the terms of the translation candidate sequences are derived. Features are extracted from the entity s map label and the number of generated top translation candidate sequences by using geospatial and linguistic context information. This context information can include for example location character type and so forth. Using the extracted features the selected possible translation candidate sequences are then ranked by inputting them into a probabilistic classifier trained at least in part with the same features. The ranked translation candidate sequences are then post ranked using neighboring proximity information to output the best e.g. highest ranked translation for the input map label. The following paragraphs provide details for each of these steps used in various map label translation implementations described herein.

In some map label translation implementations the map label in the first language e.g. Japanese script must be tokenized in order to transliterate the words written in Japanese script into Latin script. Although English is already naturally tokenized using spaces Japanese and other east Asian languages are not because they do not use spaces between words. To this end for map labels which have no Latin equivalent translation candidate sequences are generated for them. For example a conventional word breaker that breaks Japanese words into separate tokens n grams is used to break each Japanese map label into its constituent parts and its part of speech category of words . For example given the Japanese script below which represents Hilton Shinjyuku Hotel the word breaker breaks the script into n grams and their associated parts of speech 

In some map label translation implementations a conventional Yomi Predictor that uses a Yomi Predictor model is used to transliterate the n grams. These implementations first convert a map label in Kanji to its most probable readable phonetic form in Kana. The Yomi Predictor model is a machine trained model of past inputs of Kanji Kana transliterations to get the highest probability in a Kanji to Kana transliteration problem using Bayes rule. Then a conventional Romaji Converter is used that transliterates the phonetic Kana form for each constituent part into Latin script using a Romaji mapper example given below . For each constituent part the Romaji Converter may use a transliteration white list to prevent proper nouns from becoming inappropriate transliterations an example of a bad transliteration would be as Hiruton instead of Hilton the universally recognized spelling of Hilton . For example a Romaji mapper maps to la to xa to a and to li. There are also many known cases of having Latin mixed with Japanese. In these cases the Latin part becomes a constituent part. The transliterated n grams are then translated. For example they can be translated using a white list by using a machine translator by using a dictionary of by using other conventional means of translation. Existing sources may be chosen to get a white list of possible candidates utilizing the English Japanese example . There may also be an additional option of digital curation e.g. web crawling of data sources. Data sources can include pre globalized map data from a data provider which contains both Japanese and Latin equivalents for the same entity entities from Wikipedia that have the page that contains the name of the entities in both the Japanese Latinized form and existing machine translation results.

For example for the input map label of discussed in the example above which represents Hilton Shinjyuku Hotel the results of the candidate generation can be something like the following where each column represents a possible translation of an n gram of the input map label 

Translation candidate sequence selection in some implementations involves a conventional Viterbi algorithm or model to compute the most likely state sequence in a HMM given a set of observed outputs. In this case the hidden states are the various translation candidate sequences that have been generated. Probabilities for components of the translation candidate sequences are pre calculated. In one implementation for a first language of Japanese and a second language of English two probabilities a first language e.g. Japanese Romanization translation probability e.g. P jaToken i enToken i and the translated n gram sequential probability e.g. P enToken I 1 enToken i are obtained. These two probabilities are then multiplied to compute the resulting transition probability for a given translation candidate sequence e.g. P Transition i 1 i P jaToken i enToken i P enToken I 1 enToken i . Each of these probabilities can be calculated in a manner of ways. Below is one exemplary way to calculate them.

In one implementation P jaToken i enToken i is calculated statistically based on frequency that an n gram or token TokenA in Language 1 e.g. Japanese is translated into other n grams or tokens TokenB0 TokenB1 TokenB2 . . . in Language 2 e.g. English in a set of data such as for example a training corpus.

For example in one implementation if a token is found in a white list that contains a known translation for a given token or n gram in the first language e.g. Japanese the probability is set to a fixed value of x where x can be value that is configurable but is generally given a high probability such as x 0.9. Else if more than one translation is found to be in the white list for a given n gram the probability can be further adjusted. For example a term or n gram that occurs at a higher frequency is given the highest probability and the next highest occurring term is given the next highest probability. In some implementations the exact gap between each probability is normalized according to the number of possible translations found in the white list. For example if there are two terms one can set x 0.9 and x 0.89 gap of 0.01 . However if there are 80 candidates to a translation found in the white list one can make the transitions even smaller. A candidate that is not from the white list but generated by other methods such as some more unreliable translation method e.g. by using a machine translation is set to a probability of some prescribed value. Any translated pairs can be added in to the white list in case the corpus does not have pairs or for any other purpose.

In some map label translation implementations to compute the translated n gram sequential probability P enToken I 1 enToken i of a translation candidate sequence location related administrative hierarchy context information with pre set values of probabilities can be used. Generally the idea is that more famous locations that match a translation term are assigned higher probabilities.

In order to obtain the translated n gram sequential probability P enToken I 1 enToken i two probabilities P Hierarchical Level and P Prominence are multiplied e.g. P enToken I 1 enToken i P Hierarchical Level P Prominence .

P Hierarchical Level can be preset based on an administrative model that breaks each geographic location into a hierarchy of smaller divisions. In one implementation these divisions are prefecture municipality and neighborhood.

P Prominence can be a normalized stack rank of all known municipal entities by population count. e.g. 

Prominence can be inherited. For example a neighborhood that does not have a prominence assigned but belongs to a municipality that has a prominence can be assigned the prominence of the municipality. If there is no prominence information available a similar but lower prominence is assigned based on the average of adjacent municipalities that exist in the same prefecture e.g. district and of the same class.

In some map label translation implementations the source of the candidate can be used to determine its probability. For example the word Hilton from the candidate generation was found in the white list therefore it has a high probability of 0.9. The word Hiruton was generated using a machine translator such as for example a Romaji Converter so its probability is lower. Shinjyuku is the name of a ward in Tokyo which has a higher prominence than Arajyuku the name of a neighborhood in Kyusyu region therefore a probability of 0.8 is assigned for Shinjyuku while a probability of 0.6 is assigned for Arajyuku.

As discussed above in determining the transition probabilities of a translation candidate sequence both P jaToken i enToken i and P enToken i 1 enToken i are taken into account. For example as shown in for the first words Hilton and Hiruton there is no transition probability of Hilton for this P jaToken i enToken i is used. For transitions Hilton Shinjyuku and Hilton Arajyuku the probability of 0.5 could be assigned. Therefore the final transition probability is 0.5 0.8 0.4. The top x candidates are chosen as shown in as an example. For example continuing with the example above for x 2 Hilton Shinjyuku Hotel 0.9 0.4 0.9 0.324 Hilton Arajyuku Hotel 0.9 0.3 0.9 0.243 The top translation candidate sequence in this example is shown in dashed lines.

One implementation for finding the n gram sequential probability which in this example is represented by P secondLangNgram i secondLangNgram j is as follows 

2. Split the second language label to unigrams by using a word breaker or by simply space delimiting.

5. Calculate translated n gram sequential probabilities secondLangNgram i to secondLangNgram j as the next n gram from those results. For example in the case where secondLangNgram i has secondLangNgram j0 as the following n gram 5 times secondLangNgram j1 3 times secondLangNgram j2 2 times P secondLangNgram i secondLangNgram j0 5 5 3 2 0.5.

These two probabilities are then multiplied to compute the resulting transition probability for a given translation candidate sequence e.g. P Transition i 1 i P jaToken i enToken i P enToken I 1 enToken i .

Map label translation implementations use features based on geospatial and linguistic context to train a probabilistic classifier. Features are extracted from the map label to be translated and the selected translation candidate sequences and input into the trained probabilistic classifier in order to find the best translation candidate sequences. The following paragraphs discuss the classifier training the classifier ranking and the features used.

To use the probabilistic classifier to rank the selected translation candidate sequences it must first be trained using the set of features that will be used in the ranking. Some features that are used for this purpose in some implementations will be explained in greater detail below. Training the classifier can be done in a number of conventional ways. In one implementation however to train the probabilistic classifier a training sample is used that represents the inputs and outputs of a truth set which each represents the expected results of a translation for a given entity in the first language e.g. Japanese . The training sample can be for example 2000 inputs and outputs. The training samples can be the results obtained by asking a human or several humans to label the expected result given an input and the output. In one implementation the input is represented as a list of input variables x1 x2 x3 . . . xN and each variable is a numerical value that is 0 or 1 for a IS or IS NOT a type of feature or that is an arbitrary numerical value such as for example the number of Romaji map labels being translated. These numerical values are extracted using information from the map label in the first language and the translation candidates. A process is run that contains rules for each of the features to determine the numerical values. For example to determine if a map label corresponds to a prominent landmark this can be determined simply from an existing source such as for example Wikipedia. It is assumed that the existence of the entity on Wikipedia is a proof of its prominence . For example in the case of computing prominence e.g. that the entity exists in an on line dictionary if a name in that matches the map label in the first language exists in the on line dictionary then the IS value is set to 1 otherwise it is set to 0. In order for humans to label the training data instead of just the input parameters the possible translation candidates for the input map label corresponding to an entity are also provided. The output represents the corresponding label that indicates if a transliteration translation is a likely one. For example a 5 point label can be chosen such as Perfect Excellent Satisfactory Poor Bad for an input map label corresponding to an entity and its corresponding translation.

Once the probabilistic classifier is trained an entity s map label in the first language and its selected translation candidate sequences are input and the classifier outputs the ranked translation candidate sequences. Each of these ranked translation candidate sequences is assigned a score. For example scores can be perfect 1.0 excellent 0.8 satisfactory 0.5 poor 0.5 and bad 1.0.

The paragraphs below describe some exemplary features that can be used by the probabilistic classifier but they are not exhaustive. The features enable the processing of a large combination of factors that may not be solvable by a rule based approach.

The context of the administrative region in which the entity corresponding to the map label is found can be used as a feature. For example since the place and coordinates of the entity for which the map label translation is sought are known given a map data source if there are two exact strings hypothetically that were found in Japan Hilton Arajyuku Hotel and Hilton Shinjyuku Hotel if the name of one entity exists in Shijyuku ward some map label translation implementations described herein further boost the score of the translation candidate sequence Hilton Shinjyuku Hotel. The other translation candidate sequence s Hilton Arajyuku Hotel rank found in the Arajyuku neighborhood in Kyuusyuu will similarly be boosted for the same reason. This will result in the appropriate translation based on the context of the administrative or unofficial region in which the map label for the entity to be translated lies. To find this context a reverse geo coding service can be used to pin point the address of the latitude longitude and to determine if the context hierarchy contains Shinjyuku . The context hierarchy would be Tokyo Shinjyuku ku and Tokyo Katsushika ku Niijyuku in English. In this case the Japanese equivalent is used.

The class of an entity for example if it is a landmark entity or a miscellaneous structure can have an impact on its translations and can be used as a feature. For example a landmark that is more likely to be a tourist attraction or a globally well known structure is more likely to have a well known translation. For example Tokyo Tower is a well known landmark. No one would want a translation such as Tokyo Towa so the Tokyo Tower probability or score output by the probabilistic classifier will be boosted.

Some map label translation implementations determine whether the terms in a translation candidate sequence indicate a well known entity category and use this as a feature. If it is determined that a translation candidate sequence contains a well known entity category its probability or score is boosted. For example Musashino Park is better translated as Musashino Park rather than Musashino Koen in Japanese since park is such a well known category of entity. Other examples of well known category entities are various levels of schools office halls police stations fire stations courts libraries museums aquariums zoos amusement parks gymnasiums fields pools bowling alleys tennis courts marinas ports airports factories hot springs shrines temples and cemeteries.

Whether an entity contains a type of nature n gram can also be used as a feature to determine its translation and can be used as a feature. Natural entities tend to have common prefixes and suffixes such as Riv. River or Mt. Mountain . These can be reflected in the associated probability or score output by the probabilistic classifier for a translation candidate sequence containing a nature n gram.

Entrance type entities tend to have common patterns for translation and these can be used as a feature. They boost the score or probability for a translation candidate sequence if a given pattern is found. For example entrance central entrance entrance and east west south north .

Some map label translation implementations determine whether the terms in a translation candidate sequence indicate a company n gram e.g. Corp. Org. and use this as a feature. If it is determined that a translation candidate sequence contains a well known entity category its probability or score output by the probabilistic classifier is boosted.

Some map label translation implementations determine whether the terms in a translation candidate sequence indicate numeric n grams and use this as a feature. If it is determined that a translation candidate sequence contains a numeric n gram assumptions are made as to how it should be translated. For example should most likely be translated to its Romanized form 1 2 3 . . . 10 so the probability or score for the translation candidate sequences that have this form will be boosted.

For a Japanese map label the number of Katakana n grams and Katakana n gram proportions can be used as features. From observation it is known that names in Katakana in Japanese are likely to have a foreign origin. Therefore a name in Katakana is typically a direct translation to its originating English or other European languages and therefore its score or probability is boosted. For example comes from French Le Rivage Gran therefore if such a sequence of transitions if found it should have a higher score than say Ribajyu Gran from other sources. Similarly the number of Katakana n grams will suggest how foreign the originating word is.

For a Japanese map label the number of Kanji n grams and Kanji n gram proportions can be used as features. A word in Kanji or an label having a large proportion of Kanji makes it a high probability that it is of Japanese origin and needs a transliteration closer to Japanese as there is unlikely to be a translated equivalent or the translation is simply too inappropriate.

For a Japanese map label the number of Hirangana n grams and Hiragana n gram proportions can be used as features. A word in Japanese consisting of Hiragana may be a simplified version of the word in Kanji since the Kanji can be too obscure to read. This may suggest that transitions that lead to a Hiragana n gram or having a high Hiragana n gram proportion may need a Latin transliteration sounding closer to Japanese. The probabilistic classifier will have the tendency to boost those translation candidates that sound more like transliteration if the originating entity is likely to have a Japanese originating name that has no known English translation.

For an input Japanese map label the number of transliterated Romaji candidates can also be used as a feature. For example if the resultant transliterated results have too many tokens words in the map label the result may be hard to read therefore the feature of the number of Romaji candidates is used. For example G can be transliterated translated as Hakone Yunohana Onsen Golf Course but it turns out that in truth Hakone Yunohana Golf Course is a better result. The probabilistic classifier will have the tendency to suppress candidate translations that have a high number of Romaji candidates.

An entity s proximity to a surrounding building pattern can also be used as a feature. Given a known example of an exit name called Truth Konan Exit and an untranslated building name called Assuming these entities are in the same neighborhood there would be transition probabilities P1 P Minato Minami Building and P2 P Konan Building . Since the building is close to the Konan Exit which is known to be the truth of the translation the score or probability of P2 would be boosted by the probabilistic classifier.

For performance reasons it might not be desirable to re train a probabilistic classifier with new data in case such new data is received. Therefore some map label translation implementations instead rely on re ranking a ranked set of translation candidate sequences output by the probabilistic classifier on the fly. This can be done by evaluating various aspects of the ranked translation candidate sequences. The following paragraphs describe various methods of re ranking the top translation candidate sequences output by the probabilistic classifier by applying a set of rules based on the proximity of other entities to the entity corresponding to the input map label to boost or diminish the score as output by the probabilistic classifier associated with the translation candidate sequences.

The ranked translation candidate sequences can be re ranked by using the proximity to surrounding buildings of a similar numbering sequence. For example in Japanese given a structure called The true translation could be No. 1 Miyaniwa Mansion but another candidate could be Daiichi Miyaniwa Mansion. There might be other mansions of similar names like 2 in close proximity to this structure. In this case it is assumed that x is a No. x suffix instead of a unique name Daiichi. can be read to be the phonetic literal Daiichi . In such a case the surrounding area with a threshold of Euclidean distance x m can be searched for the pattern of x. If found the candidate which has the No. x translation is given a higher ranking score.

Take another example given a business name known as also known as Daiichi Seimei a life insurance company . A direct translation which could be a candidate would end up as Number One Life. Number one . The true translation is Daiichi Seimei. In this case there are no other x buildings around this business. Also Daiichi Seimei is a famous entity so if it fits within the names database such a unique candidate sequence the ranking score for Daiichi Seimei would be boosted above every other candidate.

The proximity to administrative buildings can also be used to re rank the ranked list output by the probabilistic classifier. Many public facilities administrative and government buildings such as city halls courts incineration plants are found in proximity to one another. If the translation n gram sets can be identified for example by labeling those as having any suffixes or prefixes commonly seen in these types of buildings these results can be boosted.

For a particular term that was not translated to its expected form some map label translation implementations overwrite a translated entity by using a pre defined list. For example for a golden set of entities those which one absolutely cannot fail in translation e.g. related to geopolitical aspects like the Imperial Palace foreign embassies or religious structures some implementations overwrite the translation with an entry from the pre defined list.

What has been described above includes example implementations. It is of course not possible to describe every conceivable combination of components or methodologies for purposes of describing the claimed subject matter but one of ordinary skill in the art may recognize that many further combinations and permutations are possible. Accordingly the claimed subject matter is intended to embrace all such alterations modifications and variations that fall within the spirit and scope of detailed description of the map label translation implementation described above.

In regard to the various functions performed by the above described components devices circuits systems and the like the terms including a reference to a means used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. a functional equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary aspects of the claimed subject matter. In this regard it will also be recognized that the foregoing implementations include a system as well as a computer readable storage media having computer executable instructions for performing the acts and or events of the various methods of the claimed subject matter.

There are multiple ways of realizing the foregoing implementations such as an appropriate application programming interface API tool kit driver code operating system control standalone or downloadable software object or the like which enable applications and services to use the implementations described herein. The claimed subject matter contemplates this use from the standpoint of an API or other software object as well as from the standpoint of a software or hardware object that operates according to the implementations set forth herein. Thus various implementations described herein may have aspects that are wholly in hardware or partly in hardware and partly in software or wholly in software.

The aforementioned systems have been described with respect to interaction between several components. It will be appreciated that such systems and components can include those components or specified sub components some of the specified components or sub components and or additional components and according to various permutations and combinations of the foregoing. Sub components can also be implemented as components communicatively coupled to other components rather than included within parent components e.g. hierarchical components .

Additionally it is noted that one or more components may be combined into a single component providing aggregate functionality or divided into several separate sub components and any one or more middle layers such as a management layer may be provided to communicatively couple to such sub components in order to provide integrated functionality. Any components described herein may also interact with one or more other components not specifically described herein but generally known by those of skill in the art.

The following paragraphs summarize various examples of implementations which may be claimed in the present document. However it should be understood that the implementations summarized below are not intended to limit the subject matter which may be claimed in view of the foregoing descriptions. Further any or all of the implementations summarized below may be claimed in any desired combination with some or all of the implementations described throughout the foregoing description and any implementations illustrated in one or more of the figures and any other implementations described below. In addition it should be noted that the following implementations are intended to be understood in view of the foregoing description and figures described throughout this document.

Various map label translation implementations are by means systems processes or techniques for translating map labels from a first language to a second language even if the map labels in the two languages are in different script. As such some map label translation implementations described herein have been observed to improve the speed efficiency and accuracy of the translation of map labels in a first language and possibly first script to a second language and possibly second script . Additionally in some map label translation implementations by using machine learning and a probabilistic classifier the computing efficiency to determine the translated map labels is increased and therefore the power consumption is reduced.

As a first example in various implementations a process for translating map labels is provided via means processes or techniques for receiving an entity s map label in a first language that is to be translated into a second language and generating translation candidates for each n gram in the entity s map label. These translation candidates are used to generate translation candidate sequences for the received map label. In various implementations a prescribed number of translation candidate sequences are selected. Features are selected from the translation candidate sequences and the entity s map label using geospatial and linguistic context information. A probabilistic classifier trained at least in part with the extracted features is used to rank the selected translation candidate sequences. The ranked translation candidate sequences are then re ranked using neighboring proximity information of the entity s location to disclose the highest re ranked translation candidate sequence as the translated map label.

As a second example in various implementations the first example is further modified via means processes or techniques such that each translation candidate sequence comprises a translated term for each n gram of the map label and a probability of transition.

As a third example in various implementations any of the first example and the second example are further modified via means processes or techniques so that the transition probability for each translated term is pre calculated.

As a fourth example in various implementations the third example is further modified via means processes or techniques such that a transition probability is pre calculated by for each n gram multiplying a first language to Roman script translation probability by a transliterated n gram sequential probability in a second language.

As a fifth example in various implementations any of the first example the second example the third example and the fourth example are further modified via means processes or techniques for using the transition probabilities to rank each of the translation candidate sequences by using the transition probabilities to rank each translation candidate sequence and select the prescribed number of the top ranked translation candidates.

As a sixth example in various implementations any of the first example the second example the third example the fourth example and the fifth example are further modified via means processes or techniques by using a Viterbi process to select the prescribed number of translation candidate sequences.

As a seventh example in various implementations any of the first example the second example the third example the fourth example the fifth example and the sixth example are further modified via means processes or techniques in that the receipt map label in the first language is in a non Latin script.

As an eighth example in various implementations any of the first example the second example the third example the fourth example the fifth example the sixth example and the seventh example are further modified via means processes or techniques such that the n grams of the map label are transliterated into Latin script before translation.

As a ninth example in various implementations any of the first example the second example the third example the fourth example the fifth example the sixth example the seventh example and the eighth example are further modified via means processes or techniques such that the trained probabilistic classifier receives the map label in the first language and outputs the translation candidate sequence in the second language.

As a tenth example in various implementations any of the first example the second example the third example the fourth example the fifth example the sixth example the seventh example the eighth example and the ninth example are further modified via means processes or techniques for generating the possible translation candidate sequences for each n gram in the map label by breaking the map label in the first language into separate n grams transliterating each n gram that is not in Latin script into a phonetic representation in Latin script translating each n gram in Latin script into a term in the second language and creating the possible translation candidate sequences by combining different translated terms for each n gram in the map label.

As an eleventh example in various implementations any of the first example the second example the third example the fourth example the fifth example the sixth example the seventh example the eighth example the ninth example and the tenth example are further modified via means processes or techniques such that one or more n grams in Latin script are translated by using a white list that has approved translations in the second language for each n gram in the first language.

As a twelfth example in various implementations any of the first example the second example the third example the fourth example the fifth example the sixth example the seventh example the eighth example the ninth example the tenth example and the eleventh example are further modified via means processes or techniques such that the first language is Japanese and the second language is English.

As a thirteenth example in various implementations any of the first example the second example the third example the fourth example the fifth example the sixth example the seventh example the eighth example the ninth example the tenth example the eleventh example and the twelfth example are further modified via means processes or techniques such that re ranking the selected translation candidate sequences use neighboring proximity information further comprises for each translation candidate sequence applying a set of rules based on the proximity of other entities in relation to the entity of the input map label to boost or diminish the ranking score associated with the translation candidate sequence.

As a fourteenth example in various implementations a process for translating map labels is provided via means processes or techniques for translating a map label from a first label to a second language. This can be done by receiving an entity s map label in a first language that is to be translated into a second language and generating translated terms in the second language for constituent parts of the map label in the first language. Translation candidate sequences are generated by using combinations of translated terms for each constituent part of the received map label. A number of the highest interim ranking generated translation candidate sequences are selected by performing an interim ranking of generated translation candidate sequences by using transition probabilities between terms of each translation candidate sequence to determine a resulting transition probability for that translation candidate sequence. Features are extracted from the entity s map label in the first language and the number of generated highest interim ranking translation candidate sequences by using geospatial and linguistic context information. The extracted features and a trained classifier trained at least in part with the extracted features are used for re ranking the interim ranking of the translation candidate sequences. A post ranking of the re ranked top translation candidate sequences is then performed using neighboring proximity information to disclose the highest ranking sequence as the translated map label.

As a fifteenth example in various implementations the fourteenth example is further modified via means processes or techniques so that the extracted features comprise at least one of the geographic context of the region in which the entity is located or the prominence of the entity or whether or not the entity falls into a well known category or whether or not the entity contains a type of nature constituent part or whether or not the entity contains a type of entrance constituent part or whether or not the entity contains a company or organizational constituent part or whether or not the entity contains a numeric constituent part or the proportion of the constituent parts that are in a non Latin script or the proximity to surrounding building patterns.

As a sixteenth example in various implementations a system is provided via means processes or techniques for translating a map label from a first label to a second language. This can be done by a processor and a memory that includes a word breaker a translation candidate generator a translation candidate sequence selector a feature extractor a candidate sequence re ranker and a post ranker. The word breaker receives an entity s map label in a first language that is to be translated into a second language and breaks the map label into constituent parts. The translation candidate generator generates translation candidates for each constituent part in the entity s map label and uses these translation candidates to generate translation candidate sequences for the map label. The candidate sequence selector selects a number of highest interim ranking translation candidate sequences by using transition probabilities between terms in each of the translation candidate sequences. The feature extractor extracts features from the highest interim ranking translation candidate sequences and the entity s map label by using geospatial and linguistic context information. The candidate sequence re ranker uses the extracted features to re rank the highest interim ranking translation candidate sequences using a probabilistic classifier trained at least in part using the extracted features wherein each translation candidate sequence is assigned a ranking score. The post ranker re ranks the re ranked translation candidate sequences by using rules based on the map labels of entities in the neighboring proximity of the received entity s map label location.

As a seventeenth example in various implementations the sixteenth example is further modified via means processes or techniques to use the extracted features to adjust the ranking score in the highest interim ranking translation candidates by the post ranker receiving the interim ranking of the translation candidate sequences and for each translation candidate sequence applying a set of rules based on the proximity of other entities in relation to the entity of the input map label to boost or diminish the ranking score associated with the translation candidate sequence.

As an eighteenth example the seventeenth example is modified. so that the rules are based on the proximity of the entity associated with the input label to buildings with a similar numbering sequence and or the proximity of the entity associated with the input label to administrative buildings.

As a nineteenth example in various implementations the seventeenth example is further modified via means processes or techniques so that the rules specify overwriting a translated map label with a label from a pre defined list.

As a twentieth example in various implementations any of the sixteenth example seventeenth example eighteenth example and the nineteenth example are further modified via means processes or techniques such that the map label in the first language is in non Latin script and the translated map label in the second language is in Latin script.

The map label translation implementations described herein are operational within numerous types of general purpose or special purpose computing system environments or configurations. illustrates a simplified example of a general purpose computer system on which various implementations and elements of the privacy preserving energy efficient parametric speaker as described herein may be implemented. It is noted that any boxes that are represented by broken or dashed lines in the simplified computing device shown in represent alternate implementations of the simplified computing device. As described below any or all of these alternate implementations may be used in combination with other alternate implementations that are described throughout this document.

The simplified computing device is typically found in devices having at least some minimum computational capability such as personal computers PCs server computers handheld computing devices laptop or mobile computers communications devices such as cell phones and personal digital assistants PDAs multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers and audio or video media players.

To allow a device to realize the map label translation implementations described herein the device should have a sufficient computational capability and system memory to enable basic computational operations. In particular the computational capability of the simplified computing device shown in is generally illustrated by one or more processing unit s and may also include one or more graphics processing units GPUs either or both in communication with system memory . Note that that the processing unit s of the simplified computing device may be specialized microprocessors such as a digital signal processor DSP a very long instruction word VLIW processor a field programmable gate array FPGA or other micro controller or can be conventional central processing units CPUs having one or more processing cores and that may also include one or more GPU based cores or other specific purpose cores in a multi core processor.

In addition the simplified computing device may also include other components such as for example a communications interface . The simplified computing device may also include one or more conventional computer input devices e.g. touchscreens touch sensitive surfaces pointing devices keyboards audio input devices voice or speech based input and control devices video input devices haptic input devices devices for receiving wired or wireless data transmissions and the like or any combination of such devices.

Similarly various interactions with the simplified computing device and with any other component or feature of the map label translation implementation including input output control feedback and response to one or more users or other devices or systems associated with the map label translation implementation are enabled by a variety of Natural User Interface NUI scenarios. The NUI techniques and scenarios enabled by the map label translation implementation include but are not limited to interface technologies that allow one or more users user to interact with the map label translation implementation in a natural manner free from artificial constraints imposed by input devices such as mice keyboards remote controls and the like.

Such NUI implementations are enabled by the use of various techniques including but not limited to using NUI information derived from user speech or vocalizations captured via microphones or other input devices or system sensors . Such NUI implementations are also enabled by the use of various techniques including but not limited to information derived from system sensors or other input devices from a user s facial expressions and from the positions motions or orientations of a user s hands fingers wrists arms legs body head eyes and the like where such information may be captured using various types of 2D or depth imaging devices such as stereoscopic or time of flight camera systems infrared camera systems RGB red green and blue camera systems and the like or any combination of such devices. Further examples of such NUI implementations include but are not limited to NUI information derived from touch and stylus recognition gesture recognition both onscreen and adjacent to the screen or display surface air or contact based gestures user touch on various surfaces objects or other users hover based inputs or actions and the like. Such NUI implementations may also include but are not limited to the use of various predictive machine intelligence processes that evaluate current or past user behaviors inputs actions etc. either alone or in combination with other NUI information to predict information such as user intentions desires and or goals. Regardless of the type or source of the NUI based information such information may then be used to initiate terminate or otherwise control or interact with one or more inputs outputs actions or functional features of the map label translation implementation.

However it should be understood that the aforementioned exemplary NUI scenarios may be further augmented by combining the use of artificial constraints or additional signals with any combination of NUI inputs. Such artificial constraints or additional signals may be imposed or generated by input devices such as mice keyboards and remote controls or by a variety of remote or user worn devices such as accelerometers electromyography EMG sensors for receiving myoelectric signals representative of electrical signals generated by user s muscles heart rate monitors galvanic skin conduction sensors for measuring user perspiration wearable or remote biosensors for measuring or otherwise sensing user brain activity or electric fields wearable or remote biosensors for measuring user body temperature changes or differentials and the like. Any such information derived from these types of artificial constraints or additional signals may be combined with any one or more NUI inputs to initiate terminate or otherwise control or interact with one or more inputs outputs actions or functional features of the map label translation implementation.

The simplified computing device may also include other optional components such as one or more conventional computer output devices e.g. display device s audio output devices video output devices devices for transmitting wired or wireless data transmissions and the like . Note that typical communications interfaces input devices output devices and storage devices for general purpose computers are well known to those skilled in the art and will not be described in detail herein.

The simplified computing device shown in may also include a variety of computer readable media. Computer readable media can be any available media that can be accessed by the computing device via storage devices and include both volatile and nonvolatile media that is either removable and or non removable for storage of information such as computer readable or computer executable instructions data structures program modules or other data.

Computer readable media includes computer storage media and communication media. Computer storage media refers to tangible computer readable or machine readable media or storage devices such as digital versatile disks DVDs blu ray discs BD compact discs CDs floppy disks tape drives hard drives optical drives solid state memory devices random access memory RAM read only memory ROM electrically erasable programmable read only memory EEPROM CD ROM or other optical disk storage smart cards flash memory e.g. card stick and key drive magnetic cassettes magnetic tapes magnetic disk storage magnetic strips or other magnetic storage devices. Further a propagated signal is not included within the scope of computer readable storage media.

Retention of information such as computer readable or computer executable instructions data structures program modules and the like can also be accomplished by using any of a variety of the aforementioned communication media as opposed to computer storage media to encode one or more modulated data signals or carrier waves or other transport mechanisms or communications protocols and can include any wired or wireless information delivery mechanism. Note that the terms modulated data signal or carrier wave generally refer to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. For example communication media can include wired media such as a wired network or direct wired connection carrying one or more modulated data signals and wireless media such as acoustic radio frequency RF infrared laser and other wireless media for transmitting and or receiving one or more modulated data signals or carrier waves.

Furthermore software programs and or computer program products embodying some or all of the various map label translation implementation implementations described herein or portions thereof may be stored received transmitted or read from any desired combination of computer readable or machine readable media or storage devices and communication media in the form of computer executable instructions or other data structures. Additionally the claimed subject matter may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device or media.

The map label translation implementations described herein may be further described in the general context of computer executable instructions such as program modules being executed by a computing device. Generally program modules include routines programs objects components data structures and the like that perform particular tasks or implement particular abstract data types. The map label translation implementations may also be practiced in distributed computing environments where tasks are performed by one or more remote processing devices or within a cloud of one or more devices that are linked through one or more communications networks. In a distributed computing environment program modules may be located in both local and remote computer storage media including media storage devices. Additionally the aforementioned instructions may be implemented in part or in whole as hardware logic circuits which may or may not include a processor.

Alternatively or in addition the functionality described herein can be performed at least in part by one or more hardware logic components. For example and without limitation illustrative types of hardware logic components that can be used include field programmable gate arrays FPGAs application specific integrated circuits ASICs application specific standard products ASSPs system on a chip systems SOCs complex programmable logic devices CPLDs and so on.

The foregoing description of the map label translation implementation has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the claimed subject matter to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. Further it should be noted that any or all of the aforementioned alternate implementations may be used in any combination desired to form additional hybrid implementations of the map label translation implementation. It is intended that the scope of the invention be limited not by this detailed description but rather by the claims appended hereto. Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims and other equivalent features and acts are intended to be within the scope of the claims.

