---

title: Interactive content generation
abstract: Generation of interactive content. In an embodiment, a representation of candidate object(s) in content of a digital media asset are received. For each of the candidate object(s), feature(s) of the candidate object are compared to corresponding feature(s) of a plurality of reference objects to identify reference object(s) that match the candidate object. For each of the matched candidate object(s), a hotspot package is generated. The hotspot package may comprise a visual overlay which comprises information associated with the reference object(s) matched to the respective candidate object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09336459&OS=09336459&RS=09336459
owner: OIM SQUARED INC.
number: 09336459
owner_city: Skaneateles
owner_country: US
publication_date: 20151020
---
The present application is a continuation of U.S. patent application Ser. No. 14 790 602 filed on Jul. 2 2015 which claims priority to U.S. Provisional Patent App. No. 62 020 873 filed on Jul. 3 2014 the entireties of both of which are hereby incorporated herein by reference.

The present application generally relates to image processing and object recognition and more specifically relates to locating and identifying objects from a plurality of images in a database.

In the context of digital content e.g. images video etc. the consumer may view and interact with content through a dedicated network device e.g. desktop computer laptop computer smart phone tablet computer personal digital assistant and or any other type of processing device . Conventionally there is no suitable way to identify product information about objects present in such content in an automated rapid or scalable manner. Rather product information is generally conveyed via a point and click approach tailored for a single object represented in a single image. For example an image representing the product may be displayed with a hyperlink to the product information.

It would be advantageous if a network device running an interactive application e.g. a consumer application or other content could be provided e.g. by a media analysis server with product information about objects present in that content.

It would also be advantageous if unknown content could be discovered e.g. by a media analysis server when browsing a large database of content. This way a list of object s that are visually similar to a known object but previously unknown due to the large size of the content database could be returned to a network device running an interactive application via a content server.

It would also be advantageous if objects could be located i.e. detected from content and corresponding visually similar objects could be identified e.g. by a media analysis server solely based on visual characteristics without the need to add any information in text form such as keywords or labels or other types of metadata stored in a database.

It would also be advantageous if objects could be located i.e. detected from content and corresponding visually similar objects could be identified e.g. by a media analysis server and information about these objects could be returned e.g. by the media analysis server to a network device running an interactive application to allow visual interaction without modification or distortion of the objects without the need for special markings on the objects without requirement for special lighting conditions and all without human intervention in the process i.e. automatic .

It would also be advantageous if information about objects located i.e. detected and identified from content e.g. by a media analysis server could be returned e.g. by the media analysis server to a network device running an interactive application and presented without obscuring the desired digital content or perimeter frames without annoying the user consumer with pop up windows as commonly practiced by conventional solutions for capitalizing on advertising revenue from digital content.

It would also be advantageous if objects could be located i.e. detected from content and corresponding visually similar objects could be identified e.g. by a media analysis server within a reasonable amount of time suitable for interactive applications.

Accordingly the present application describes embodiments of methods and systems for overcoming the issues with conventional technology discussed above and or achieving one or more of the advantages discussed above. This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key factors or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

In an embodiment a computer implemented method is disclosed. The method comprises using at least one hardware processor to receive a representation of one or more candidate objects in content of a digital media asset for each of the one or more candidate objects compare one or more features of the candidate object to one or more corresponding features of a plurality of reference objects to identify one or more reference objects that match the candidate object and for each of the one or more candidate objects that is matched to one or more reference objects generate a hotspot package wherein the hotspot package comprises a visual overlay and wherein the visual overlay comprises information associated with the one or more reference objects matched to the candidate object.

In an embodiment a system is disclosed. The system comprises at least one hardware processor and one or more software modules that are configured to when executed by the at least on hardware processor receive a representation of one or more candidate objects in content of a digital media asset for each of the one or more candidate objects compare one or more features of the candidate object to one or more corresponding features of a plurality of reference objects to identify one or more reference objects that match the candidate object and for each of the one or more candidate objects that is matched to one or more reference objects generate a hotspot package wherein the hotspot package comprises a visual overlay and wherein the visual overlay comprises information associated with the one or more reference objects matched to the candidate object.

In an embodiment a non transitory computer readable medium is disclosed. The medium has instructions stored thereon wherein the instructions when executed by a processor cause the processor to receive a representation of one or more candidate objects in content of a digital media asset for each of the one or more candidate objects compare one or more features of the candidate object to one or more corresponding features of a plurality of reference objects to identify one or more reference objects that match the candidate object and for each of the one or more candidate objects that is matched to one or more reference objects generate a hotspot package wherein the hotspot package comprises a visual overlay and wherein the visual overlay comprises information associated with the one or more reference objects matched to the candidate object.

In an embodiment a computer implemented method for visual searching is disclosed. The method comprises using at least one hardware processor to enroll a plurality of reference objects into a database by for each of the plurality of reference objects receiving a reference image of the reference object deriving a plurality of reference features of the reference object based on visual attributes of the reference object in the reference image generating a reference object model comprising the derived plurality of reference features and storing the reference object model in the database and retrieve one or more of the plurality of reference objects from the database in response to a visual query by receiving the visual query wherein the visual query comprises a query image of a query object deriving a plurality of query features of the query object based on visual attributes of the query object in the query image for each reference object model in at least a subset of the plurality of reference object models enrolled in the database for each of the derived plurality of query features calculating a feature score representing a similarity between the query feature and a corresponding one of the plurality of reference features in the reference object model and calculating a matching score based on each feature score wherein the matching score represents an overall similarity between the query object and the reference object represented by the reference object model generating a query response comprising data associated with each reference object model for which the matching score satisfies a predetermined criteria and returning the query response in response to the visual query.

In an embodiment a system for visual searching is disclosed. The system comprises at least one hardware processor and one or more software modules that are configured to when executed by the at least one hardware processor enroll a plurality of reference objects into a database by for each of the plurality of reference objects receiving a reference image of the reference object deriving a plurality of reference features of the reference object based on visual attributes of the reference object in the reference image generating a reference object model comprising the derived plurality of reference features and storing the reference object model in the database and retrieve one or more of the plurality of reference objects from the database in response to a visual query by receiving the visual query wherein the visual query comprises a query image of a query object deriving a plurality of query features of the query object based on visual attributes of the query object in the query image for each reference object model in at least a subset of the plurality of reference object models enrolled in the database for each of the derived plurality of query features calculating a feature score representing a similarity between the query feature and a corresponding one of the plurality of reference features in the reference object model and calculating a matching score based on each feature score wherein the matching score represents an overall similarity between the query object and the reference object represented by the reference object model generating a query response comprising data associated with each reference object model for which the matching score satisfies a predetermined criteria and returning the query response in response to the visual query.

In an embodiment a non transitory computer readable medium is disclosed. The medium has instructions stored thereon wherein the instructions when executed by a processor cause the processor to enroll a plurality of reference objects into a database by for each of the plurality of reference objects receiving a reference image of the reference object deriving a plurality of reference features of the reference object based on visual attributes of the reference object in the reference image generating a reference object model comprising the derived plurality of reference features and storing the reference object model in the database and retrieve one or more of the plurality of reference objects from the database in response to a visual query by receiving the visual query wherein the visual query comprises a query image of a query object deriving a plurality of query features of the query object based on visual attributes of the query object in the query image for each reference object model in at least a subset of the plurality of reference object models enrolled in the database for each of the derived plurality of query features calculating a feature score representing a similarity between the query feature and a corresponding one of the plurality of reference features in the reference object model and calculating a matching score based on each feature score wherein the matching score represents an overall similarity between the query object and the reference object represented by the reference object model generating a query response comprising data associated with each reference object model for which the matching score satisfies a predetermined criteria and returning the query response in response to the visual query.

In an embodiment a computer implemented method for tagging an object in a digital media asset comprising one or more images is disclosed. The method comprises using at least one hardware processor to receive a first query image wherein the query image comprises at least a portion of a first image from the one or more images of the digital media asset wherein the first query image comprises a visual representation of an object normalize the first query image by removing image noise from the first query image and converting the first query image to a photometric invariant color space determine a boundary that surrounds the visual representation of the object within the normalized first query image by segmenting the normalized first query image into regions merging similar neighboring regions until no similar neighboring regions remain and determining a boundary around one or more remaining regions as the boundary that surrounds the visual representation of the object derive one or more features from visual attributes within the boundary that surrounds the visual representation of the object within the normalized first query image for each of a plurality of reference object models representing reference objects compare the derived one or more features to one or more corresponding features of the reference object model to determine a similarity score for the reference object represented by the reference object model identify one or more reference objects based on their respective similarity scores retrieve information associated with the identified one or more reference objects and link the visual representation of the object in the digital media asset with the retrieved information associated with the identified one or more reference objects.

In an embodiment a system for tagging an object in a digital media asset comprising one or more images is disclosed. The system comprises at least one hardware processor and one or more software modules that are configured to when executed by the at least one hardware processor receive a first query image wherein the query image comprises at least a portion of a first image from the one or more images of the digital media asset wherein the first query image comprises a visual representation of an object normalize the first query image by removing image noise from the first query image and converting the first query image to a photometric invariant color space determine a boundary that surrounds the visual representation of the object within the normalized first query image by segmenting the normalized first query image into regions merging similar neighboring regions until no similar neighboring regions remain and determining a boundary around one or more remaining regions as the boundary that surrounds the visual representation of the object derive one or more features from visual attributes within the boundary that surrounds the visual representation of the object within the normalized first query image for each of a plurality of reference object models representing reference objects compare the derived one or more features to one or more corresponding features of the reference object model to determine a similarity score for the reference object represented by the reference object model identify one or more reference objects based on their respective similarity scores retrieve information associated with the identified one or more reference objects and link the visual representation of the object in the digital media asset with the retrieved information associated with the identified one or more reference objects.

In an embodiment a non transitory computer readable medium is disclosed. The medium has instructions stored thereon wherein the instructions when executed by a processor cause the processor to receive a first query image wherein the query image comprises at least a portion of a first image from the one or more images of the digital media asset wherein the first query image comprises a visual representation of an object normalize the first query image by removing image noise from the first query image and converting the first query image to a photometric invariant color space determine a boundary that surrounds the visual representation of the object within the normalized first query image by segmenting the normalized first query image into regions merging similar neighboring regions until no similar neighboring regions remain and determining a boundary around one or more remaining regions as the boundary that surrounds the visual representation of the object derive one or more features from visual attributes within the boundary that surrounds the visual representation of the object within the normalized first query image for each of a plurality of reference object models representing reference objects compare the derived one or more features to one or more corresponding features of the reference object model to determine a similarity score for the reference object represented by the reference object model identify one or more reference objects based on their respective similarity scores retrieve information associated with the identified one or more reference objects and link the visual representation of the object in the digital media asset with the retrieved information associated with the identified one or more reference objects.

In an embodiment a system for extensible media analysis is disclosed. The system comprises at least one hardware processor at least one object localization software module that when executed by the at least one hardware processor locates objects in a digital media asset at least one feature extraction software module that when executed by the at least one hardware processor derives one or more features from visual attributes of objects located in a digital media asset by the at least one object localization software module at least one feature matching software module that when executed by the at least one hardware processor compares one or more features derived by the at least one feature extraction software module to one or more reference features of reference objects at least one data fusion software module that when executed by the at least one hardware processor determines a similarity score between a reference object and an object located in a digital media asset by the at least one object localization software module based on a comparison by the at least one feature matching software module an object localization interface for replacing or extending the at least one object localization software module a feature extraction interface for replacing or extending the at least one feature extraction software module and a data fusion interface for replacing or extending the at least one data fusion software module.

In an embodiment a non transitory computer readable medium is disclosed. The medium has stored thereon at least one object localization software module that when executed by the at least one hardware processor locates objects in a digital media asset at least one feature extraction software module that when executed by the at least one hardware processor derives one or more features from visual attributes of objects located in a digital media asset by the at least one object localization software module at least one feature matching software module that when executed by the at least one hardware processor compares one or more features derived by the at least one feature extraction software module to one or more reference features of reference objects at least one data fusion software module that when executed by the at least one hardware processor determines a similarity score between a reference object and an object located in a digital media asset by the at least one object localization software module based on a comparison by the at least one feature matching software module an object localization interface for replacing or extending the at least one object localization software module a feature extraction interface for replacing or extending the at least one feature extraction software module and a data fusion interface for replacing or extending the at least one data fusion software module.

In an embodiment a computer implemented method for generating interactive content is disclosed. The method comprises using at least one hardware processor to detect one or more objects visually represented in displayable content of a digital media asset and for each of the one or more objects visually represented in the displayable content of the digital media asset identify one or more reference objects that match the object retrieve data associated with the identified one or more reference objects generate a hotspot package wherein the hotspot package comprises a visual overlay and wherein the visual overlay comprises information based on the retrieved data determine a display position for the hotspot package based on a position of the object visually represented in the displayable content of the digital media asset and embed the hotspot package with the displayable content of the digital media asset to be rendered at the determined display position.

In an embodiment a system for generating interactive content is disclosed. The system comprises at least one hardware processor and one or more software modules that are configured to when executed by the at least one hardware processor detect one or more objects visually represented in displayable content of a digital media asset and for each of the one or more objects visually represented in the displayable content of the digital media asset identify one or more reference objects that match the object retrieve data associated with the identified one or more reference objects generate a hotspot package wherein the hotspot package comprises a visual overlay and wherein the visual overlay comprises information based on the retrieved data determine a display position for the hotspot package based on a position of the object visually represented in the displayable content of the digital media asset and embed the hotspot package with the displayable content of the digital media asset to be rendered at the determined display position.

In an embodiment a non transitory computer readable medium is disclosed. The medium has instructions stored thereon wherein the instructions when executed by a processor cause the processor to detect one or more objects visually represented in displayable content of a digital media asset and for each of the one or more objects visually represented in the displayable content of the digital media asset identify one or more reference objects that match the object retrieve data associated with the identified one or more reference objects generate a hotspot package wherein the hotspot package comprises a visual overlay and wherein the visual overlay comprises information based on the retrieved data determine a display position for the hotspot package based on a position of the object visually represented in the displayable content of the digital media asset and embed the hotspot package with the displayable content of the digital media asset to be rendered at the determined display position.

After reading this description it will become apparent to one skilled in the art how to implement the described techniques for intelligent tagging of digital media assets in various alternative embodiments and alternative applications. However although various embodiments will be described herein it is understood that these embodiments are presented by way of example and illustration only and not limitation. As such this detailed description of various embodiments should not be construed to limit the scope or breadth of the present application as set forth in the appended claims.

In an embodiment objects located i.e. detected and identified in content of a digital media asset e.g. in an image or the frame of a video are made available to a user for direct interaction e.g. through an interactive client application . Advantageously embodiments described herein may not require special lighting and may work under normal lighting conditions. In addition such embodiments may be robust to noise perspective distortion rotation translation scaling shading shadows highlights reflections and or other illumination variations and or partial occlusion of objects.

In an embodiment a media analysis server is provided. As used herein the term media analysis server should be understood to include any software and or hardware capable of performing one or more of the functions processes or methods described herein. For instance the media analysis server may be embodied entirely in software may comprise one or more hardware processors executing software stored in volatile and or non volatile memory or may be embodied entirely in hardware. The media analysis server may comprise means e.g. software hardware or a combination of software and hardware to locate and identify objects in content such as images and video. The media analysis server may be connected via one or more networks e.g. including the Internet a wireless communications network etc. to a network device that is configured to execute software including a client application such as an interactive video application.

In an embodiment a digital media asset may be received by or stored on the network device. The digital media asset may have been captured at the network device e.g. by a camera integrated in or communicatively coupled to the network device or transferred to the network device and or stored in primary or secondary memory on the network device.

In an embodiment a user of the network device may interact with objects e.g. portions of an image or video within the content of a digital media asset being viewed on a client application by for example selecting a portion of a video frame or a portion of an image both of which may be collectively referred to herein as an image as an object of interest within the client application executing on the network device. It should be understood that this selection may be performed using any of a variety of well known user operations such as a touch operation on a touch enabled network device e.g. via a touch panel an input operation using a pointing device e.g. mouse etc. In response to the user selection the client application may generate a visual query e.g. via a visual query generator module to be used for locating objects in an input image and identifying objects corresponding or visually similar to the located objects in a database of enrolled object models.

In an embodiment the media analysis server receives the visual query generated by the client application executing on the network device from the network device via one or more networks. This visual query for object localization and identification may be provided to an object localization server which may be internal to or separate from the media analysis server.

In an embodiment the visual query undergoes a noise reduction process and is converted into a photometric invariant color space through an image normalization process. This allows the system to be robust to shadows shading highlights reflections and other factors caused by changes in illumination.

In an embodiment the media analysis server determines the type of digital media asset being processed e.g. by checking if the digital media asset is a single image or a video and selects one of at least two different processing methods based on the determined type of digital media asset.

In an embodiment when the media analysis server determines that the input digital media asset is a single image the normalized query image is further processed using an image segmentation. During image segmentation the input query image is processed to extract perceptually homogeneous regions or segments that together compose the input query image. The goal is to represent each object in the query image by a single segment. To improve the quality of the segmentation result small segments are removed using mathematical morphology techniques and only segments large enough to correspond to objects in the input query image remain. Iteratively each remaining segment in the segmented query image is then processed to determine the contours of that segment. Contours may be found by transforming each segment into a single channel image e.g. black and white image . Then edges in the single channel image for each segment are located using an edge detection operation e.g. Canny edge detector . Contours of each segment can then be retrieved from the binary edge image resulting from the edge detection operation and the points of the contours may be approximated by polygons. The contours of each segment correspond to the contours of the object in the query image corresponding to that segment. Further structural characteristics of the contours are derived and only those contours whose structural characteristics are within a predetermined range are retained as object candidates. The maximum convexity defect of each contour is obtained from the contour convex hulls. The moments areas and the mass center are derived along with geometrical properties of the contour e.g. relative area compactness solidity aspect ratio etc. and a predetermined threshold is set for each contour s structural characteristics. In addition the orientation of the detected object candidate for a given segment may be determined by first calculating the ellipse that best fits the points on the contour of the object using an ellipse fitting algorithm. The orientation of the object may then be determined by the orientation of the major axis of the fitting ellipse. The bounding rectangle of each segment is derived by calculating the minimal up right bounding rectangle of the points on the contour of the object in the considered segment. The bounding rectangles of those segments that meet certain requirements related to the desired objects e.g. maximum convexity defect moments areas mass center relative area compactness solidity aspect ratio etc. are retained and the portions of the image delimited by these bounding rectangles in the original query image are the object candidates.

When the media analysis server determines that the input digital media asset is a video processing is accomplished on a frame by frame basis. Processing may be performed on each frame in the same or similar manner as discussed above with respect to an input image except that now scene change detection may be performed and a comparison of the content of a pair of frames i.e. a current frame and a preceding frame may be performed. When a new scene is encountered in the current frame of the video the image segmentation process decomposes the normalized current frame of the video into perceptually homogeneous segments. For each of these segments the object candidate generator module locates potential objects in the current frame of the video by analyzing the regions in the image segment for geometry convexity defects etc. as described for the previous case of a single image. For subsequent frames when the content of the current frame is determined to be similar to the content of the preceding frame these potential objects or object candidates are followed from one frame to subsequent frame s by an object tracking module until another scene change is encountered. The object candidates remain the same through these tracked frames but their positions within these frames are updated by the object tracking module. At the current frame in which a scene change is detected image segmentation is reiterated generating a new set of object candidates. An illustration of this scheme is the detection of moving objects in a video e.g. person wearing a white shirt and blue pants and walking along a solid gray wall . In this case despite motion the content of a plurality of consecutive frames is the same e.g. person wearing a white shirt and blue pants and walking along a solid gray wall . Therefore until a scene change is detected e.g. same person wearing same clothes now walking along the background of a busy street only the first frame will be subjected to the image segmentation and object candidate generation processes whereas the subsequent frames will not. Rather in the subsequent frames i.e. until a scene change is detected only the new positions of the objects will be updated thereby improving efficiency.

In an embodiment regardless of the type of the digital media asset the output of the object localization server is a list of one or more object candidates.

In an embodiment the object localization server provides the list of object candidate s to an object identification server. The object identification server may also be internal to or separate from the media analysis server.

In an embodiment the object identification server comprises a geometric normalization module. The geometric normalization module receives as input a bounding rectangle enclosing each of the object candidates in the list of object candidates output from the object localization server described above and normalizes the bounding rectangle to have the same width and height and or aspect ratio as a pre defined standard image size and or aspect ratio e.g. the same pre defined standard image size and or aspect ratio used when generating object models enrolled in a reference database . This normalization of size and or aspect ratio ensures consistency of results when comparing local feature descriptors from an input image with local feature descriptors from reference images modeled in a database and avoids issues that may arise from variations in scale.

In an embodiment the object identification server comprises a feature extraction module that derives features from each of the geometrically normalized object candidates in the list of object candidates and a feature matching module that uses the derived features to attempt to match each object candidate to one or more of a plurality of enrolled object models in a database. For example the feature extraction module may derive features based on different attributes such as color texture shape keypoints etc. The feature matching module may then compare one or more including in some embodiments each of these derived features with the corresponding features of one or more including in some embodiments all of the enrolled object model in the database and calculate a similarity score s representing a similarity of the object candidate with each of one or more enrolled object models in the database relative to other similarity scores.

In an embodiment the calculation of a similarity score for an object candidate and an enrolled object model in the database may comprise calculating similarity scores for each feature e.g. color texture shape keypoints etc. . The feature specific similarity scores may then be combined into a single matching score that represents the total degree of similarity of the input object candidate image to the enrolled object model found in the database.

It is expected that given a certain level of confidence e.g. a threshold of similarity scores several images from the database may be a match to the input image. In such instances it is practical to rank the matching object models or the objects represented by the matching object models by their degree of similarity e.g. according to their similarity scores . In an embodiment the results may be reduced to the N matching object models with the highest scores e.g. ten matching object models with the highest scores . This result may be presented as a list of identified items and for each identified item a list of visually similar items and or fed into a content interactivity synthesizer which embeds hotspot packages relevant to located and identified objects inside the original content making the content interactive and ready for consumption by the client application e.g. interactive video application running on the network device via the content server.

In an embodiment the list of object candidates is input to a content interactivity synthesizer. The content interactivity synthesizer embeds hotspot packages relevant to the listed object candidates inside the original content of the digital media asset. In this manner the content of the digital media asset is made interactive and ready for consumption for example by an interactive application running on a network device e.g. via a content server . Alternatively or additionally the interactive content may be published on a brand s portal on a social network and or the like e.g. via the content server .

In an embodiment the enrolled object models in the database comprise features derived from objects in an image catalogue using the same attributes e.g. color texture shape keypoints etc. as are used to derive features from object candidates. In such an embodiment the feature extraction module can be used to extract features from the objects to be enrolled from the image catalogue as well as to extract features from object candidates. The extracted features for each image in the catalog of images may be used as or to generate an enrolled object model for that image. The enrolled object model for each image in the catalog may then be stored in the database for comparison to the extracted features of input image by the matching module.

In embodiments in which the located and identified objects i.e. represented by enrolled object models matching a given object candidate image are purchasable products or other items the media analysis server and or client application may facilitate the purchase of the identified objects. For instance a client application e.g. interactive application executing on a network device may generate a visual query e.g. via visual query generator from the content of the digital media asset e.g. from at least a portion of an image or video frame . The visual query which may comprise at least a portion of the digital media asset e.g. a portion of an image or video frame containing an object of interest may be transmitted by the client application from the network device to a media analysis server. The media analysis server may comprise or be interfaced with the object localization server which performs the two stage process of object localization e.g. detection and object identification using the feature extraction module the feature matching module and the database of enrolled object models as described above. The media analysis server may return a list of one or more identified objects representing enrolled object models from the database that were matched e.g. to a given degree of confidence to a given object e.g. identified as the object in the digital media asset to the client application executing on the network device. The list of identified objects may also comprise transaction information associated with each identified object in the list. One or more modules at the client application and or the media analysis server may then facilitate a selection of at least one of the one or more identified objects for purchase and or initiate a purchase of the selected object. For example the client application may receive the list of identified objects including the transaction information and render the interactive content of the digital media asset with hotspot package s that comprise a user interface for purchasing or initiating a purchase of identified object s in the list.

In this manner the user may indicate whether and how many of the items represented in the list of identified objects to purchase. The client application may then communicate with an e commerce platform to complete the transaction. In an embodiment the user registers with a commerce platform supporting the client application to provide transaction information such as name credit card number shipping address etc. This information whether provided in advance or during the transaction is used by the commerce platform to complete the transaction. Different options may be available during or as the transaction such as allowing the user to request more information about the item s to be sent to the user s email address having hardcopy information about the item s mailed to a mailing address allowing the user to specify that the item s be delivered to another party selecting the item s as a gift obtaining information on related item s etc.

It should be understood that the input image from which object are located and identified may comprise only a portion of an image displayed at the network device e.g. within the client application and or only a portion of an object within the image displayed at the network device. Image segmentation scene change detection object candidate generation object tracking feature extraction matching and data fusion may be performed in the same manner as discussed above resulting in a ranked list of identified objects and for each identified object a list of visually similar items that is returned to the network device running the client application via the content server thereby enhancing user interaction and usability. For example the object localization server and the object identification server may also be used to identify a list of one or more objects represented in the database of enrolled object models that are similar to an identified object from a digital media asset in the same or similar manner as described above. In addition this list of visually similar objects may be ranked according to a similarity score in the same or similar manner as discussed above.

In an embodiment in which the objects correspond to products one or more modules may be provided which enable a consumer to make an online purchase from the ranked list of visually similar objects. For example a representation of a visually similar product may be selected e.g. clicked if the list is implemented as a list of hyperlinks from the list of visually similar objects by the consumer to initiate a purchase of the product.

In an embodiment a platform is provided for purchasing an item that corresponds to an object identified in a digital media asset or visually similar to an object identified in a digital media asset. The platform comprise a client application e.g. interactive video application executing on a network device and which generates a visual query e.g. comprising an image or a portion of an image containing at least a portion of an object of interest . The platform also comprises a media analysis server which performs object localization and feature extraction. Alternatively object localization and feature extraction for each of the located objects may be performed by the client application and the visual query may comprise a representation of the extracted features rather than an input image. The platform further comprises a database comprising a plurality of enrolled object models. The media analysis server may comprise an object localization server to detect potential object s in the visual query a feature matching module that matches the object candidates from object localization to enrolled object models in the database. In addition the media analysis server may comprise a transaction module that initiates a transaction and which may send transaction information to the client application executing on the network device.

The processes described herein for feature extraction e.g. by the feature extraction module allow the system e.g. media analysis server client application or combination of media analysis server and client application to be robust to variations in scale to the presence of shadows to shading to reflections to partial occlusions and to geometrical viewpoint. In an embodiment no special lighting conditions are required for capturing images and video. This provides means for a practical object identification method and object identification system that satisfies requirements for deployment in real world situations.

As a non limiting example of an application of an object identification service a consumer may use a mobile network device e.g. PDA cell phone tablet etc. to capture or otherwise generate an image or video of an object or group of objects and use a client application to select one or several images or a video. A visual query is generated by the client application in response to the user s selection s and sent by the client application to the media analysis server via one or more networks. The media analysis server receives the visual query from the network device via the network s . The selected image s represented by the visual query are analyzed by the object localization server to detect potential objects that are returned as candidate objects. These candidate objects are identified by matching features extracted from each candidate object image to the features represented by enrolled object models in a database. Product information relevant to the matched enrolled object model s is embedded in the original image or video in association with the user selected image s so as to render the user selected image s as interactive and an interactive image or video is provided to the consumer via a content server. By selecting any of the interactive object s in the processed image or video product information embedded for that object may be provided to the consumer. In addition selecting interactive object s may also generate a query to the media analysis server for visually similar objects in a database along with corresponding product information for the visually similar objects.

Continuing the example the media analysis server returns a list of identified objects and for each object a list of items corresponding to the visually similar objects. A user may select any of the listed items and in response a dedicated user interface may be overlaid on the corresponding video frame or image with object information e.g. product information if the object is a product relevant to this item including an image of the selected item.

Again continuing the example the media analysis server may receive a query for object identification during interaction with objects embedded in an interactive image or video and in response perform object localization and identification and return a link to a list of identified objects. In addition the media analysis server may return for each identified object a list of items corresponding to visually similar objects. The list of identified objects and or visually similar objects may be provided as another overlay on the video frame or image. In an embodiment all items in the list of identified objects and or visually similar objects may be scrolled and or selected for additional information.

As another non limiting example of an application of an object identification service a consumer may use a mobile network device e.g. PDA cell phone tablet etc. to capture or otherwise generate an image or video of a person wearing a certain outfit. The consumer uses the client application to select the outfit in the image or the video. A visual query is generated by the client application in response to the user s selection s and sent by the client application to the media analysis server via one or more networks. The visual query may comprise an image containing the outfit e.g. a portion of the image or video of the person wearing the outfit . The media analysis server receives the visual query from the network device via the network s . The image containing the outfit represented by the visual query is analyzed by the object localization server of the media analysis server to detect the outfit that that is output to the object identification server of the media analysis server as a candidate object. The candidate object representing the outfit is identified by the object identification server by matching features extracted from the image representing the outfit to the features represented by enrolled object models in a database. Product information relevant to the matched enrolled object model s e.g. a description of the outfit a price of the outfit an image of the outfit etc. is embedded in the original image or video in association with the portion of the original image or video containing the outfit so as to render the outfit as interactive within the original image or video and an interactive image or video is provided to the consumer e.g. via a content server . Throughout the process of interacting with the interactive image or video one or more visual queries may be generated and sent to the media analysis server to obtain information related to the previously identified outfit which has been rendered as interactive and or a list of items that are visually similar e.g. similar outfits to object s identified in the interactive video or image. It should be understood that the visual queries for visually similar object s may be in addition to or part of the visual query generated in response to the user s selection s of the outfit discussed above. This information related to the outfit worn by the person and the object s that are visually similar to the outfit worn by the person in the video or image may then be presented to the consumer e.g. via a content server . In this manner the consumer may discover new previously unknown items based on the information related to the visually similar object s potentially resulting in the purchase of item s corresponding to the visually similar object s for example through an e commerce platform.

As another non limiting example of an application of an object identification service a consumer may use a mobile network device e.g. PDA cell phone tablet etc. to capture or otherwise generate an image or video of an object. The consumer uses the client application to select a portion of the image or video. A visual query is generated by the client application in response to the user s selection s and sent by the client application to the media analysis service via one or more networks. The media analysis server receives that visual query from the network device via the network s . The image represented by the visual query is analyzed by the object localization server to detect potential objects that are returned as object candidates. These object candidates are identified by matching extracted features from each object candidate image to the features represented by enrolled object models in a database. Alternatively or additionally objects that are visually similar to the object candidates are identified. Information relevant to the identified objects and to these visually similar objects are embedded in the original image or video in association with the user selected object s so as to render the user selected object s as interactive and the interactive image or video is provided to the consumer e.g. via a content server . In this manner while a consumer interacts with the image or video e.g. by selecting an interactive object of interest the results of a visual query for matching or visually similar objects in the database may be obtained and provided to a consumer. In a realistic example the interaction by the consumer may comprise the consumer selecting a portion of the image or video representing a product such as an item of clothing or clothing accessory e.g. shirt t shirt pants dress overcoat jacket sunglasses watch jewelry etc. . In response matching and or similar products e.g. shirts t shirts pants dresses overcoats jackets sunglasses watches jewelry etc. will be identified e.g. by the media analysis server using the search process described herein and information related to these matching and or similar products may be provided to the user in a ranked list.

In another realistic example the interaction by the consumer may comprise the consumer selecting an entire image or video representing for example people wearing products such as an item of clothing or clothing accessory e.g. shirt t shirt pants dress overcoat jacket sunglasses watch jewelry etc. . In response objects within the content will be located by the localization server e.g. shirt t shirt pants dress overcoat jacket sunglasses watch jewelry etc. and identified e.g. by the media analysis server using the identification process described herein followed by identification of similar products e.g. shirts t shirts pants dresses overcoats jackets sunglasses watches jewelry etc. . Information related to identified objects and to similar products may then be provided to the user in a ranked list.

Accordingly the present application discloses technology that enables a user to identify items in content such as a video or image in the context of interactive applications. In embodiments described herein object identification capabilities or results are embedded in the original content e.g. original video or image .

In an embodiment one or more modules e.g. software modules hardware modules and or a combination of software and hardware modules for example of a media analysis server a network device or distributed across a media analysis server and network device process digital media asset s e.g. image s or video s to locate and identify objects within each digital media asset. Each digital media asset may have a single frame e.g. a digital image or a plurality of frames e.g. a digital video . The module s may further associate each of the identified objects with a set of commerce related information e.g. related information about the identified object . For example in instances in which the identified objects represent products the related information may include product information e.g. descriptions prices images sizes etc. links to websites where the product can be purchased and the like.

In an embodiment the module s generate a hotspot package that includes commerce related information as well as a small graphical overlay that is superimposed on the original digital media asset e.g. on a frame of a video when the frame is presented on a display . One or more hotspot packages can be associated with each of one or more commercial objects e.g. consumer products appearing in a single image or video frame. For instance a small graphical overlay may be provided on or in close proximity to each commercial object in the single image or video frame. Thus it should be understood that a plurality of graphical overlays representing one or more hotspot packages may be provided on a single image or video frame for each of a plurality of commercial objects in the single image or video frame.

In the same manner for a digital media asset comprising multiple frames e.g. a video or animation one or more hotspot packages can be associated with one or more commercial objects appearing in each frame e.g. using the small graphical overlay . During playback of the multi frame digital media asset the small graphical overlay of the hotspot package may appear to persist in association with the commercial object over time e.g. over a plurality of video frames . In an embodiment selection of a small graphical overlay from a hotspot package by a user results in the client application presenting on the display of the network device the commerce related information from the hotspot package in place of or in addition to the small graphical overlay.

In an embodiment the interactive multi frame digital media asset e.g. interactive video that is returned to the interactive client application comprises all hotspots which prompt the graphical overlays of the corresponding hotspot packages in real time when the interactive multi frame digital media asset is played back. The interactive multi frame digital media asset can comprise a singular hotspot or a plurality of hotspots depending on the digital content being utilized and or the number of detected objects. According to an embodiment the interactive multi frame digital media asset can be edited to add additional hotspots for additional identified objects remove hotspots and or change the placement of hotspots for example using a dedicated editing tool. The location of hotspots and or hotspot packages across frames may be recorded using object tracking techniques after appropriate selection of the targeted hotspots and or hotspot packages. An object in the database that is associated with the hotspot and or hotspot package may be changed e.g. using the editing tool along with a corresponding product description. The editing tool may enable particular overlays for a hotspot and or hotspot package to be turned on or off in a given frame depending on the presence or absence of a desired object within the frame. In addition the editing tool may enable modifications on hotspots and or hotspot packages to be undertaken concurrently in the same digital media asset by multiple users.

In an embodiment visual query generator generates visual queries using image region selection module and or image selection module . Image region selection module provides for selection of a portion or region of an image e.g. a portion of a video frame in a digital media asset whereas image selection module provides for selection of an entire image e.g. an entire video frame .

In an embodiment the processing of the digital media asset to generate an interactive digital media asset with embedded hotspots is performed prior to playback of the digital media asset. The processed digital media asset can be stored in its processed form i.e. including the various hotspots packages that are associated for example with the various frames of the digital media asset . One advantageous application of this embodiment is for regular television content and pre recorded video streaming e.g. YouTube and Netflix .

In an alternative embodiment the processing of the digital media asset to generate an interactive digital media asset with embedded hotspots is performed in real time during presentation of the digital media asset on a display device e.g. network device . In such an embodiment individual frames of the digital media asset or simply the image if the digital media asset is an image are analyzed e.g. by media analysis server during spooling or buffering of the content to identify objects e.g. commercial objects such as products .

In an embodiment hotspot packages for objects are pre generated and stored and available for use e.g. for embedding within digital media assets . In such an embodiment a digital media asset e.g. the frames of a video is analyzed e.g. by media analysis server to identify an object in the digital media asset. The identified objects are associated with related information e.g. a product identifier and a location and that information may be combined with a pre stored hotspot package for rendering as an overlay on the digital media asset e.g. on the frame of a video during playback of that portion e.g. frame of the digital media asset on the display. One advantageous application of this embodiment is for interactive television content and live video streaming.

In an embodiment users may interact directly with one or more inputs of the visual overlay of a rendered hotspot package for a social e commerce video experience. For instance the user may receive social media updates and upload e commerce object brands. The user may also share e commerce objects to social media assets like e commerce object to social media sites e.g. Facebook Google Twitter Yelp etc. learn more by linking to an e commerce object website etc. In an embodiment the hotspot package is a live rendering of all information presented to a user to ensure consistency with all current information available on the presented e commerce object.

In the illustrated embodiment network device runs interactive application e.g. interactive video player and or interactive image browser which plays back a digital media asset and provides a user interface during playback for directly interacting with objects identified in the digital media asset. In an embodiment a visual query may be generated by query generator by at least two different methods.

According to a first method a portion of an image of interest is selected e.g. by a user via a visual query input through image region selection module . For example a user may freely select a region or a portion of interest of an image via image region selection module . In response visual query generator module may refer to image region selection module and generate a visual query based on the selected region of interest.

According to a second method an image of interest is selected e.g. by a user via image selection module . For example a user selection of an entire image of interest through interactive application may result in a call to visual query generator module to refer to image selection module and generate a visual query based on the entire image of interest.

In embodiments in which object localization server and object identification server are physically separated from network device e.g. in embodiments in which object localization server and object identification server are executed by another device such as a device comprising media analysis server the query image e.g. comprising a region or entire image of interest is transmitted in a visual query from the client application e.g. comprising interactive module or application to object localization server using conventional digital wired network and or wireless network means.

Object localization server receives the visual query generated by query generator module and uses the visual query to locate i.e. detect objects in the query image to be used by object identification server to identify objects which match or are visually similar to object s detected in the query image.

Query image normalization module conditions query images prior to image segmentation. This process is illustrated in according to an embodiment. For example query image normalization module may comprise color image pre processing module and photometric invariant color space module . Color image pre processing module pre processes the query image to remove image noise. In addition to facilitate the subsequent stages of the object recognition process color image pre processing module may perform additional pre processing steps for example to enhance brightness and or contrast. During this pre processing the query image may also be normalized to a predefined size and or aspect ratio.

In an embodiment photometric invariant color space module converts the query image e.g. the pre processed query image output from color image pre processing module to a photometric invariant color space. This allows the object recognition process to be robust to shadows shading highlights reflections and other factors caused by differences in illumination. Transformation of images into photometric invariant color spaces will be examine in greater details elsewhere herein e.g. when discussing pre processing to enroll images of objects into a database .

In an embodiment the normalized query images output by image normalization module are further processed by four additional modules image segmentation module object tracking module scene change detection module and object candidate generation module . Image segmentation module decomposes the visual query image into perceptually homogeneous segment s . Each of the segment s corresponds to one or more potential objects in the query image.

If the input digital media asset is an image object tracking module and scene change detection module are bypassed such that the segment s output by image segmentation module may be input to object candidate generator module without use of object tracking module or scene change detection module .

In an embodiment the scene change detection module analyzes each frame and detects whether or not a scene change occurs between consecutive frames such that the content of the frames is different. The purpose of change scene detection module is to limit processing by image segmentation module to just those video frames whose content is different than the content in the preceding frame. For example there is a scene change when a video frame depicting a car driving in a forest is followed by a video frame depicting the same car driving in city traffic. On the other hand there would be no scene change when a video frame depicting a car driving in a forest is followed by a video frame depicting the same car driving in the same forest. Thus motion of an object within the same background may not may not be detected as a scene change whereas motion of an object from one background to a different background may be detected as a scene change.

When there is no scene change detected by scene change detection module the same objects identified in the previous frame are reconsidered in the current frame. However the position of these objects may be different due to motion. Object tracking module tracks the identified objects from one frame to the next until a scene change is detected by scene change detection module .

This combination of scene change detection by scene change detection module and the tracking of moving objects by object tracking module results in efficient processing of video content for object localization.

Object candidate generator module detects potential objects in the visual query image by processing the segmented regions output by image segmentation module . At this stage contours in each segment are found by transforming the segment e.g. represented as an image region into a single channel image e.g. black and white image and by detecting the underlying edges using an edge detector e.g. Canny edge detector . The contours of each segment are derived from the binary edge image resulting from applying the edge detector on each segment and the points of the contours are approximated by polygons. The structural characteristics of contours are derived and used to retain only those contours whose structural characteristics that are within a predetermined range e.g. maximum convexity defect moments areas mass center relative area compactness solidity aspect ratio etc. . A predetermined threshold or range may be set for each of these contour structural characteristics. The idea is to only retain object candidates with shapes cleared from unusual irregularities e.g. severely disproportionate aspect ratio unusually high convexity defect etc. . The orientation of the detected object candidate for a given segment may be determined by first calculating the ellipse that best fits the points on the contour of the object using an ellipse fitting algorithm. The orientation of the object may then be determined by the orientation of the major axis of the fitting ellipse. A bounding rectangle for each segment is derived by calculating the minimal up right bounding rectangle of the points on the contour of the object in the considered segment. The bounding rectangles of those segments that meet certain requirements related to the desired objects e.g. relative area solidity compactness aspect ratio and maximum convexity defect are retained and the portions of the query image delimited by these bounding rectangles are the object candidates.

In an embodiment representations of object candidates are output from object localization server and input to object identification server . The representations of object candidates may comprise images e.g. areas of the query image delimited by the corresponding bounding rectangles produced by object candidate generation module or may comprise a mathematical representation instead of images e.g. a vector of features extracted from the cropped images produced by object candidate generation module .

Object identification server receives the representations of the object candidates generated by the object candidate generator module and uses the representations of the object candidates to identify objects detected in the query image and or objects which are visually similar to objects detected in the query image by object localization server .

In an embodiment image normalization module performs a geometric normalization of each representation of an object candidate. Image normalization module may receive as input the cropped segment s of the query image produced by object candidate generation module and each representing at least one object candidate e.g. each representing a single object candidate or a group of multiple object candidates . Image normalization module may normalize each cropped image segment to have the same width and height or aspect ratio as a pre defined standard image size and aspect ratio used when creating object models enrolled in reference database . This normalization of image size and aspect ratio ensures consistency of results when comparing local feature descriptors from the query image with local feature descriptors represented in database and avoids issues that may arise from variations in scale between different images. Image normalization module may output the normalized image segments to feature extraction module .

In an embodiment feature extraction module receives the normalized image segments from image normalization module as input images and for each input image extracts visual characteristics or attributes inherent to the input image as features. Attributes that may be used for this purpose include but are not limited to color texture shape and or small regions around keypoints. Feature extraction module may comprise separate feature extractors each tailored to a specific attribute e.g. a feature extractor for color a feature extractor for texture a feature extractor for shape features and a feature extractor for keypoint features . Feature extraction module may output the extracted features to feature matching module .

In an embodiment feature matching module comprises sub systems for matching features generated by each feature extractor within feature extraction module e.g. color texture shape features and or keypoints . Feature matching module compares each extracted feature from feature extraction module with reference features stored in database via database management module and generates a matching score for each comparison. A normalized matching score may be generated for each individual extracted feature e.g. color feature matching score texture feature matching score shape feature matching score and or keypoint feature matching score . Feature matching module may output the normalized matching scores to data fusion module .

In an embodiment data fusion module combines the individual matching scores for all of the extracted features of a given input image into a single matching score for the input image with respect to one or more enrolled objects in database . This combination of feature matching scores minimizes the likelihood of a false match. The combination of feature matching scores may be performed using a variety of strategies or algorithms. As one example the combined matching score may be generated by a linear combination of a weighted sum of individual feature matching scores. As an alternative example the combined matching score may be generated by a straight or weighted average of individual feature matching scores.

In an embodiment data fusion module outputs a list of matching objects enrolled in database . Each matching object in the list may be associated with a degree of confidence e.g. the combined matching score . It should be understood that the list may consist of only those enrolled objects from database that were matched to a certain degree of confidence e.g. having a combined matching score greater than a threshold may consist of only a predetermined number of enrolled objects having the highest combined matching scores and or may be limited or restricted in some other suitable manner. It should also be understood that the list of matching objects may represent the matching objects in any manner e.g. as data structures unique object identifiers etc. suitable for manipulation of the matching objects and or access to object specific information by one or more subsequent modules. Data fusion module may output the list of matching objects to result ranking and management module .

In an embodiment result ranking and management module receives the list of matching objects from data fusion module and sorts the list of matching objects by level of similarity or confidence level e.g. in order of their respective combined matching scores from highest to lowest or vice versa . Accordingly result ranking and management module may output an ordered list of visually similar objects. Result ranking and management module may also insert add or otherwise associate information related to each matching object e.g. metadata including images descriptions links etc. with its respective matching object in list . For example result ranking and management module may retrieve such information using a unique object identifier associated with each matching object in the list. It should be understood that each object enrolled in database may comprise a unique object identifier and that related information may be linked to each enrolled object via its unique object identifier.

Result ranking and management module may send list of matching objects and related information to content interactivity synthesizer . Content interactivity synthesizer receives list and produces interactive content that is sent to interactive application for rendering on network device e.g. directly to network device or indirectly via content server . Interactive content may result in enhanced user interaction and experience over conventional content and or the initiation and fulfillment of a commercial transaction.

In the illustrated embodiment of there is no network device as in the embodiment depicted in . Instead of being triggered by a client application on network device the processing of digital media assets according to the embodiment of is triggered when a digital media asset e.g. image or video is uploaded to content server . This embodiment may be beneficial when digital media assets are continuously uploaded onto content server in a very large quantity. Those digital media assets are stored in a content repository via database management module . In an embodiment content server schedules the detection and identification of objects in the digital media assets stored in content repository by object localization server and object identification server . The underlying process of object location and identification may be the same as or similar to the process described above with respect to . The only difference may be that the interactive content generated by content interactivity synthesizer is output to content server i.e. instead of network device which stores the interactive content from content interactivity synthesizer in content repository via database management system module . The interactive content may be subsequently published to a digital network by content server e.g. for downloading or other access by devices such as network device .

As previously stated when the input digital media asset is determined to be an image in step a process for tagging images is performed in step . depicts a flowchart for this process of tagging images in step according to an embodiment. The process is initiated in step . In step the normalized query image resulting from steps and in the process depicted in is loaded into the system. In step this normalized query image is segmented e.g. by image segmentation module into perceptually homogeneous segments by a graph based segmentation algorithm e.g. graph cut . The goal of this segmentation is to group regions similar in appearance where similarity is based on color difference while taking into account color variation within a region.

The segmentation algorithm performed in step is a two stage algorithm and may be summarized as follows according to an embodiment. In the first stage a graph is constructed over the entire normalized query image by representing each pixel in the query image as a node with the nodes connected by edges based on 8 neighborhood. Initial edge weights are calculated based on a normalized color difference from each pixel. The internal color variation of each region defined by the edges in the query image is calculated as the maximum edge weight of its Minimum Spanning Tree. Meanwhile the difference between two regions is defined as the minimum weight edge connecting the two regions. Regions are merged by traversing the edges in a sorted order by increasing weight and verifying if the difference between two regions is small e.g. as determined based on a threshold relative to the internal difference within at least one of the regions under consideration. In this case the regions are merged into a composite region and the internal color variation of that composite region is updated. The degree of this difference is controlled by a threshold function. In practice this threshold is set to be a function of desired segmentation granularity a large threshold leads to larger segments. Image segmentation is set to provide an over segmentation of the query image to ensure features from all relevant objects are represented in the segmentation result. To improve the performance of this segmentation procedure at run time the graph derived from the input image is constructed in an efficient way. Every pixel is projected into a feature space e.g. x y c1 c2 c3 . The weight between pixels may then be determined using L1 or L2 Euclidean distance in the constructed feature space. Instead of choosing edges of each node from the entire image edges are only chosen for the top K e.g. top 10 nearest neighbors in the feature space. The nearest neighbors for each pixels may be found using the approximate nearest neighbor algorithm. In the second stage for each of the regions obtained in the first stage of graph based segmentation module a robust feature descriptor in the form of LOcal Feature descripTor LOFT described in greater elsewhere herein is used to represent each region. In this second stage another graph is formed using each of the previously segmented regions as a node and each region is connected to its adjacent regions by an edge with a weight based on the difference between their LOFT descriptors. The distance between two LOFT descriptors could be used as the edge weight. Examples of distances between descriptors could be the Chi square the correlation or the Bhattacharrya distances.

In step a region merging algorithm based on the similarity of regions of the segmented query image is used e.g. by image segmentation module or object candidate generation module to aggregate small regions into larger regions corresponding to objects in the input query image. The similarity of any two given regions may be judged by a measure of similarity of the LOFT descriptors for those regions.

In step potential objects are detected e.g. by object candidate generation module in the query image by analyzing the regions within each segment e.g. produced by image segmentation module to calculate contour characteristics e.g. maximum convexity defect moments areas mass center relative area compactness solidity aspect ratio etc. and use these characteristics to determine if the current segmented region meets the requirements of a potential object of interest. The ideal object is defined in term of contour characteristics and only segmented regions whose contour characteristics e.g. maximum convexity defect moments areas mass center relative area compactness solidity aspect ratio etc. are within a predetermined range are selected for further processing. The pixels that make each segmented region are connected to each other to form the region. That connection could be defined in terms of a so called 4 neighborhood each pixel connected to its top bottom left and right neighboring pixels or in terms of a so called 8 neighborhood each pixel connected to its top bottom left right north east north west south east and south west neighboring pixels . The region of a segmented region is thus made of a plurality of such pixels with their neighboring pixels. These basic components of a region are called connected components. The orientation of the detected object candidate for a given segment may be determined by first calculating the ellipse that best fits the sequence of individual points on the contour of the object using an ellipse fitting algorithm. The orientation of the object may then be determined by the orientation of the major axis of the fitting ellipse. A bounding rectangle or bounding box of another shape is generated around the connected components of each segment providing localization information for the detected object candidate. The bounding rectangles around sets of segments are used to create cropped images from the query image. Each cropped image represents at least one detected object candidate. Graph based image segmentation step region merging step and object candidate generation step may together form at least a portion of an object localization step .

Step begins an iteration of steps represented collectively as object identification step which is performed for each object candidate generated in step in order to identify an object corresponding to each object candidate. As discussed above each object candidate may be represented by a cropped image segment from the query image.

The details of each of these iterations are as follows according to an embodiment. In step a geometric normalization process is performed e.g. by image normalization module to normalize the object candidate e.g. represented as a cropped image segment to have the same width and height or aspect ratio as a pre defined standard image size and or aspect ratio that is used when enrolling object models in the reference database.

In step features are extracted e.g. by feature extraction module from the normalized image produced in step . The features are derived from visual characteristics or attributes inherent to the object candidate under consideration such as color texture shape and or small regions around keypoints.

In step weight values corresponding to each feature extracted from the object candidate in step are calculated e.g. by feature matching module . The weight value for a given feature represents the relative contribution of the factor to the calculation of similarity e.g. the combined matching score calculated in step between an object candidate and a reference object. In addition in step the features extracted from the object candidate in step are matched e.g. by feature matching module against reference features stored in a database e.g. stored as enrolled object models in database . The result of this matching process in step is a list of reference objects e.g. associated with corresponding to representing comprising or consisting of enrolled object models from database . Each object in the list of reference objects is associated with confidence score s representing a relative degree of visual similarity to the object candidate and the weight value s calculated in step . For each object in the list of reference objects confidence scores and weight values may be provided for each feature type extracted in step . For instance if color texture shape and keypoints features are extracted in step for each reference object in the list of reference objects that were matched to a given object candidate step may produce a color confidence score representing a relative visual similarity in color between the object candidate and the reference object a texture confidence score representing a relative visual similarity in texture between the object candidate and the reference object a shape confidence score representing a relative visual similarity in shape between the object candidate and the reference object and a keypoints confidence score representing a visual similarity in keypoints between the object candidate and the reference object and step may provide a color weight value a texture weight value a shape weight value and a keypoints weight value.

In step for each reference object in the list of reference objects produced by step a combined matching score is calculated e.g. by data fusion module based on the confidence scores and weight values associated with that reference object. In addition the reference objects in the list of reference objects produced by step are ranked in order of their respective combined matching scores. In an embodiment a higher combined matching score represents a higher degree of similarity and therefore the list of reference objects are ranked in increasing order of their respective combined matching scores. However it should be understood that in an alternative embodiment the combined matching scores may be calculated such that a lower combined matching score represents a higher degree of similarity in which case the list of reference objects could be ranked in decreasing order of combined matching score. In step the ordered list of reference objects is stored e.g. in a memory accessible to object identification server .

It should be understood that the ordered list of reference objects may be stored in association with the object candidate to which they were matched. This list of reference objects may comprise objects that are identified as the object candidate e.g. based on a combined matching score above a first threshold and or are identified as visually similar to the object candidate e.g. based on a combined matching score above a second threshold which may be lower than the first threshold . In addition it should be understood that there may be object candidates which cannot be matched to any reference objects e.g. derived from the enrolled object models in database to an acceptable level of confidence. Thus while some object candidates may be identified as a reference object and or matched to visually similar reference objects other object candidates may remain unidentified and or unmatched to any visually similar reference objects. Accordingly the output at the end of the tagging process may be a set of representations of object candidates with each representation of an object candidate associated with representations of the reference objects which were identified as the object candidate and or were identified as visually similar to the object candidate. Object candidates which were not identified as a reference object and which were not determined to be visually similar to any reference objects e.g. for which the combined matching score for all reference objects fell below a similarity threshold may be discarded or simply represented in the set without any associations to reference objects.

In step the next object candidate in a list of object candidates generated by step if any is selected and processed by steps in the same manner as discussed above. In other words the loop formed by steps is performed iteratively on each of the object candidates generated in step . The illustrated embodiment of image tagging ends in ends in step . Geometric normalization step feature extraction step weights configuration step feature matching step and data fusion step may together form at least a portion of an object identification step .

In step the normalized query image e.g. normalized according to the process discussed with respect to loaded in step is received. A multi channel two dimensional Gabor filter bank is constructed. In this filter bank two dimensional Gabor functions are convolved with the normalized photometric invariant color space version of the query image by rotating and scaling the basis Gabor function resulting in a multi resolution decomposition of the input query image in the spatial and spatial frequency domains. The highest magnitude of these Gabor filter outputs over each channel may be used to represent a filter response. A feature vector is constructed based on the Gabor filter output. However in order to reduce the variability of Gabor filter responses within regions of the query image with homogeneous visual characteristics the Gabor filter bank may be followed by filtering with a Gaussian function. To capture all important visual characteristics of the input image a large number of scales and orientations may be required resulting in a high dimension feature space.

Filtering images using this multi channel Gabor filter bank may be implemented using parallel processing techniques on multi core Central Processing Units CPUs Graphics Processing Units GPUs or other dedicated hardware platforms.

In step a principal component analysis is performed on the Gabor filter output of step to reduce the dimensionality of the feature space constructed from the responses of the multi channel Gabor filter.

In step a k means clustering is performed on the reduced feature space output by step . A known problem with the application of the k means algorithm is the appropriate selection of the number of classes K to allow the system to find all objects in most practical situations. As applied here the number of classes K is set to a large value. This k means clustering procedure results in a segmentation of the query image.

In step statistically similar regions in the segmented query image may be further merged using a merging algorithm. To evaluate two regions for merging the mean vectors and the covariance matrices of both regions are calculated from their feature vectors. Adjacent regions are merged in step if their similarity measure e.g. Mahalanobis distance is smaller than a predetermined threshold. Finally to improve the quality of image segmentation small segmented regions are removed using mathematical morphology techniques.

In step potential objects are detected e.g. by object candidate generation module by analyzing the regions within each segment in the segmented query image for contour structural characteristics e.g. convexity defects moments relative area compactness solidity aspect ratio etc. . Only those potential objects whose contours have structural characteristics within a predetermined range are retained. Thus shapes that have for example a severely disproportionate aspect ratio or a high convexity defect are discarded and only those shapes that have an appearance commonly expected of commercial objects or merchandise are retained. The orientation of the detected object candidate for a given segment may be determined by first calculating the ellipse that best fits the points on the contour of the object using an ellipse fitting algorithm. The orientation of the object may then be determined by the orientation of the major axis of the fitting ellipse. A bounding rectangle is generated around the region of each segment providing localization information for the detected object candidate. The areas of the input image delimited by the bounding rectangles are returned by the object candidate generation module as object candidates. Multi channel Gabor wavelets based segmentation step principal component analysis step k means clustering step region merging step and object candidate generation step may together form at least a portion of an object localization step .

In step scene change detection e.g. by scene change detection module is performed on the current frame to determine whether the scene in the current frame represents a change over a scene in a previous frame if any. In an embodiment scene change detection module works as follows. During the initialization stage of the scene change detector in step a normalized histogram of the initial frame is calculated e.g. two dimensional hue saturation histogram and stored in memory. For the next frame considered the current frame a normalized histogram of that frame is computed in a similar manner as during the initialization phase of step e.g. two dimensional hue saturation histogram . Next the histogram of this current frame is compared to that of the previous frame stored in memory by calculating the distance between the two histograms. Methods for this comparison include without limitation Chi square correlation and Bhattacharrya distances. The computed distance between the two histograms is normalized resulting in a normalized score. It is determined e.g. by scene change detection module that a scene change has occurred in the current frame if the normalized score is above a predetermined threshold. Conversely it is determined e.g. by scene change detection module that there is no scene change if the normalized score is below the predetermined threshold. In other words peaks in the calculated histogram that are above a predetermined threshold correspond to scene changes. It should be understood that if the normalized score is equal to the predetermined threshold it may be determined that a scene change has or has not occurred depending on the particular implementation.

In step it is determined e.g. by scene change detection module whether or not a scene change has been detected in step . It should be understood that if the current frame is the first frame of the video i.e. there is no prior frame it may be determined in step that a scene change has been detected.

If it is determined in step that no scene change has been detected the previously described processes for tagging an image e.g. the tagging processes described with respect to may be carried out in step followed by an initialization of an object tracker e.g. object tracking module in step . The object tracker e.g. object tracking module tracks each detected object candidate e.g. generated in step of or step of for example by object candidate generation module across subsequent frames i.e. subsequent iterations of steps until a scene change is detected in step .

If it is determined in step that a scene change has been detected each object identified or matched in step of a prior iteration i.e. for a prior frame at the start of a scene change may be analyzed iteratively using the loop defined by steps and .

In step tracking is performed e.g. by object tracking module to follow the change in position i.e. motion from the preceding frame to the current frame of the current object being processed by loop . For tracking the current object algorithms such as Mean shift may be used. Mean shift is an algorithm for finding local extrema in the density distribution of data. The underlying process is as follows 1 After a search window has been selected the weighted center of mass of this window is calculated 2 The window is centered at the computed center of mass and 3 Steps 1 and 2 are repeated until the window stops moving. The algorithm converges to a local peak of the distribution under the window. In the context of object tracking it is this peak finding procedure that is at the core of Mean shift tracking.

In an embodiment the Mean shift algorithm is adapted for tracking multiple objects. In such an embodiment the feature distribution used to represent each moving object is selected during the initialization phase in step for a frame in which a scene change was detected. A Mean Shift rectangular window is then started over the feature distribution generated by each object in that frame. To calculate the feature distribution of the moving object in subsequent frame s in step the Mean Shift algorithm will determine the mode of this feature distribution which is expected to be centered over the moving object. Other embodiments of object tracking in step may adopt other tracking methods such as the Kanade Lucas Tomasi KLT tracker the CONDENSATION algorithm or the sequential Monte Carlo SMC algorithms which have the ability to model non linear non Gaussian cases. An advantage of stochastic tracking approaches such as the SMC algorithms is that they achieve robustness by being able to escape local minima since their search directions are mostly random.

In step the position of the current object being tracked is updated using object location information e.g. generated or determined by object tracking module . In step the next object being tracked is selected and processed through the loop defined by steps . Thus this loop processes each object identified or matched in step of a prior iteration for a prior frame. Once all such objects have been processed the loop is closed in step .

In step the set of identified or matched objects e.g. with associated representations of identified or visually similar reference objects is output or stored. In step the loop for processing frames of the video is closed and the video tagging process is terminated in step . The steps may together form at least a portion an object localization and tracking step .

In an embodiment portions of the processes and algorithms described for tagging a digital media asset e.g. an image or a video are subjected to parallel processing using multi core Central Processing Units CPUs multiple Graphical Processing Units GPUs and or distributed machines on a computer cluster with an implementation of Map Reduce. For example the loop for processing each object candidate in the object identification step in for tagging an image may be subjected to such parallel processing. In addition for tagging video the loop processing for each video frame in step in and the loop processing for each identified object defined by steps in may also be subjected to such parallel processing.

To enable object matching and identification by feature matching module reference objects are enrolled in i.e. inserted into database . In an embodiment database stores digital media assets related to objects e.g. images of the object video of the object etc. along with associated metadata. Database may be created using database management system module which interfaces directly with database . In addition representations of the objects e.g. object models representing the visual attributes of objects such as color texture shape and or keypoints are also stored in database for comparison with features extracted by feature extraction module from query images e.g. after normalization by image normalization module . It should be understood that database may comprise multiple databases and that the representations of objects the digital media assets related to objects and or the associated metadata may all be stored separately from each other e.g. across two or more tables in rows linked by a unique object identifier .

The process of generating the stored representations of objects is referred to herein as enrollment. Embodiments of normalization and feature extraction will now be described in detail. While these embodiments are described primarily with respect to enrollment it should be understood that these embodiments of normalization and feature extraction may also be performed in an identical or similar manner during the object localization and identification processes described elsewhere herein for example with respect to media analysis server . For example the disclosed embodiments of normalization may be performed by portions of object localization server e.g. by query image normalization module and or object identification server e.g. by image normalization module . In addition the disclosed embodiments of feature extraction may be performed by portions of object identification server e.g. by feature extraction module .

Images that meet the qualification requirements undergo embodiment normalization process implemented by image normalization module . It should be understood that this normalization process may be identical or similar to the normalization process that is performed by normalization module on query images and illustrated in . The normalization process may comprise a noise reduction stage followed by a linear mapping of pixels through changes of gain and offset yielding brightness and contrast enhancement of the input image using color image pre processing module . This linear transformation widens the dynamic range of all the color channels of the input image. Another aspect of the normalization process is to normalize each input image to have the same width and height and or aspect ratio using a pre defined standard image size and or aspect ratio. This normalization of size and or aspect ratio ensures consistency of results when using local feature descriptors in images and avoid issues that may arise from variations in scale. This normalization of size and aspect ratio may also be performed by color image pre processing module .

It is typical for object recognition systems to represent color measurements using for example the CIE L a b system. This color system is very appealing considering it is device independent and corresponds with human perception an important property when retrieving image that are perceptually similar. However it is well known that the CIE L a b and the RGB color systems are dependent on the imaging conditions such as variations due to camera pose and orientation changes in illumination due to light sources having different power spectral distributions and variations in object pose causing changes in shadows shading and highlights. Moreover scene changes may also be caused by changes in camera viewpoint and by the presence of clutter in the background and by occlusions. In most cases to recognize an object a few details may be enough if and only if recognition is invariant to color changes and to shape changes. In this regard it is desirable that any successful object recognition system exhibits a great degree of color and shape invariance. Accordingly the normalization process may also comprise conversion of the input image to a photometric invariant color space by module in .

In an embodiment photometric invariant color space module may implement color invariance using the normalized red green blue RGB color space the simplest color space that provides some invariance to shadows shading and illumination intensity. The RGB color space is derived from the RGB color components as shown below. However normalized colors may become unstable and meaningless when the intensity is small.

In an alternative embodiment photometric invariant color space module may utilize a color invariant photometric space such as c1c2c3. The c1c2c3 color space is derived from the RGB color components as shown below. The c1c2c3 color space also provides a degree of photometric invariance to shadows shading and illumination intensity.

In an alternative embodiment photometric invariant color space module may utilize a color invariant photometric space such as L1L2L3. The L1L2L3 color space is derived from the RGB color space as shown below. The L1L2L3 color space provides photometric invariance to shadows shading highlights and illumination intensity.

In an embodiment object enrollment module comprises two modules image categorization module and feature extraction module . Image categorization module clusters the database to speed up search and matching. For instance image categorization module may cluster enrolled object models in database according to certain criteria such that during object identification e.g. by object identification server for an object meeting a set of criteria only those cluster s in database corresponding to that set of criteria need to be searched. Information related to these database clusters may be stored in database through database management system module .

In an embodiment the feature extraction process for enrolling objects is the same or similar to the feature extraction process performed on object candidates by object identification server as discussed above with respect to .

Color feature extraction module extracts color cues based on global color local color and or other color features that may be added to extend system capabilities and outputs color features. Texture feature extraction module extracts texture features based on contrast coarseness directionality Gabor wavelets and or others attributes that may be added to extend system capabilities and outputs texture features. Shapes feature extraction module performs shape processing based on contours polygons Hu moments and or additional shape attributes that may be added to extend system capabilities and outputs shape features. In addition keypoint feature extraction module outputs keypoint features. In the event that feature extraction is being performed on an input query image the color features outputted by color feature extraction module the texture features outputted by texture feature extraction module the shape features outputted by shape feature extraction module and the keypoint features outputted by keypoint features extraction module are compared by feature matching module to features of object models enrolled in database . On the other hand in the event that feature extraction is being performed for object enrollment all of these features may be stored in database as an enrolled object model or utilized to generate an enrolled object model that is stored in database .

In an embodiment color feature extraction module comprises components for extracting a global color feature and a local color feature for a given input image. Thus the color features output by color feature extraction module may comprise a global color feature and or local color feature. It should be understood that regardless of which color feature extraction method s are used the results may be output as the color features from color feature extraction module . It should also be understood that one or a plurality of the color feature extraction method s described herein may be implemented in any combination by color feature extraction module .

Global color features are extracted over the entire image. One example of a global color feature is the average red R green G and blue B components for the entire image. Another example of a global color feature is the color histogram of the imager. In this case a hue histogram may be used for color feature extraction 

This histogram may be normalized by performing L1 or L2 normalization. The hue histogram is appealing because of its robustness to variations in shadows shading highlights illumination intensity and geometrical viewpoint affine transformations and partial occlusions. Another example of a global color feature uses the dominant primary colors of an input image. These dominant colors may be obtained using a statistical region merging procedure. Region merging is based on the idea that color variations within homogeneous regions should be smaller than between distinct regions. Considering a 4 neighborhood and given color images one way of measuring region similarity is to calculate the local gradient between pixels and then determine the maximum variation of the local gradients for each color channel. A statistical measure of similarity is used to merge adjacent pixels into regions and then adjacent regions into larger regions. The final result of this merging procedure is an image with color regions representing the dominant colors of the input image. A histogram built from the dominant colors of each of these segmented regions of the input image could then be calculated and used as the global color feature. In any case the color features output by color feature extraction module may comprise the results e.g. hue histogram s of any one or more of these examples for determining a global color feature.

Local color features are localized to a specific part or portion of the input image. As one example local color features may be extracted by dividing the image into sub regions and then applying the same calculation as that used for the global color feature in each sub region. In this case the image may be divided uniformly into sub regions and a single color histogram e.g. a hue histogram as calculated above may be calculated for each sub region. Accordingly the color features output by color feature extraction module may comprise the result e.g. hue histogram s for one or more sub regions of this example for determining a local color feature in addition to or as an alternative to the global color feature s .

In an embodiment texture feature extraction module extracts texture as a feature to characterize the visual specificities of an input image. The extracted texture features may be derived from those attributes which have been found to correspond to human perception contrast coarseness and directionality. Such texture features model repetitive patterns on the surface of an object. Examples include checked striped or polka dotted patterns on for example clothing ties or bags. These texture features are suitable for local and global description of visual texture characteristics. Local texture feature extraction may be implemented using the same strategy described above for local color features i.e. dividing the image into sub regions and performing texture feature extraction on one or more of the sub regions. It should be understood that regardless of which texture feature extraction method s are used the results may be output as the texture features from texture feature extraction module . It should also be understood that one or a plurality of the texture feature extraction method s described herein may be implemented in any combination by texture feature extraction module .

In an embodiment texture feature extraction module uses a tree structured wavelet transform for texture feature extraction. In such an embodiment a textured input image e.g. the normalized image outputted by image normalization module is decomposed with a two dimensional two scale wavelet transform into four sub images. Then the energy of each decomposed sub image is calculated. If the energy of a sub image is significantly smaller than other sub images decomposition is stopped in the region of the sub image. However if the energy of a sub image is significantly larger than other sub images the same decomposition procedure is applied to the sub image.

In an embodiment texture feature extraction module uses Gabor wavelets. For example texture feature extraction module may construct a multi channel filter bank by convolving the input image with a rotated and scaled Gabor elementary function. After post processing by a Gaussian filter of a size larger than that used for the corresponding maximum response Gabor filter by a predetermined factor the average energy or the maximum magnitude of a Gabor filter over each channel can be used to build a feature vector. The texture features output by texture feature extraction module may comprise this feature vector.

In an embodiment shape feature extraction module performs shape feature extraction based on contours polygons Hu moments and or other shape features that may be added to extend system capabilities. Accordingly the shape features output by shape feature extraction module represent visual characteristics that reflect the shape of an object in the input image. It should be understood that regardless of which shape feature extraction method s are used the results may be output as the shape features from shape feature extraction module . It should also be understood that one or a plurality of the shape feature extraction method s described herein may be implemented in any combination by shape feature extraction module .

In an embodiment shape feature extraction module extracts contours from the binary version of the input image using an edge detection operation e.g. Canny edge detection . Accordingly the shape features output by shape feature extraction module may comprise representations of the extracted contours or other features derived from the extracted contours.

In an embodiment shape feature extraction module approximates contours of an object by polygons. Shape feature extraction module may attempt to reduce the number of points in the approximation of each extracted contour while retaining the contour s salient features. In this case the shape features output by shape feature extraction module may comprise the approximations for each extracted contour.

In an embodiment shape feature extraction module may utilize Hu moments to characterize the shape of an object in the input image. The idea behind Hu moments is to combine different normalized central moments to create invariant functions that are invariant to scale rotation and reflection.

In an embodiment shape feature extraction module uses coefficients of the Angular Radial Transform ART as a region based shape feature in which pixels on the contour and in the interior of the shape of the object are considered during the feature extraction process. The shape is decomposed in complex valued two dimensional basis functions defined by the ART and separable along the angular and radial directions. The ART shape feature is defined as a set of normalized magnitude of complex ART coefficients and has been adopted by the International Organization for Standardization ISO International Electro technical Commission IEC as a shape descriptor in their multimedia description standard MPEG 7.

In an embodiment keypoint feature extraction module extracts features around keypoints. The first step in this process involves the detection of keypoints in the input image by keypoint detection module . For this purpose corners or homogeneous blobs may be used as keypoints. Methods which may be implemented by keypoint detection module to detect keypoints in an input image include without limitation the Features from Accelerated Segment Test FAST detector the Hessian detector the Harris detector the Harris Laplacian detector and the Hessian Laplace Detector. The FAST detector was introduced by E. Rosten et al. in Machine Learning for High speed Corner Detection in Proceedings of the European Conference on Compute Vision 2008 which is hereby incorporated herein by reference. The FAST detector compares pixels on a circle of 16 pixels around a candidate corner resulting in a detector that is up to thirty times faster than a Harris detector.

Alternatively or additionally keypoint detection module may implement localized keypoint feature extraction using Scale Invariant Feature Transform SIFT . SIFT is a method to detect and match robust keypoints. The uniqueness of SIFT is that it results in distinctive features that can be successfully matched between images with very different illumination rotation viewpoint and scale changes. Moreover it boasts high repeatability and a high matching rate in very challenging conditions. In contrast to corners detected using FAST SIFT results in features that are extracted around a small patch in the vicinity of detected keypoints.

In an embodiment once keypoint s have been detected by keypoint detection module keypoint feature description module encodes the content of the detected keypoint s into a suitable descriptor i.e. a feature to be included in the keypoint features output by keypoint feature extraction module to be used during matching. One embodiment of keypoint feature description module uses SIFT as described by D. Lowe in Distinctive Image Features from Scale Invariant Keypoints International Journal of Computer Vision Vol. 60 No. 2 pp. 91 110 2004 which is hereby incorporated herein by reference. A SIFT descriptor may be computed from the region around the keypoint s detected by keypoint detection module . The SIFT descriptor distinctively represents color and texture information in the regions around the detected keypoint s . In an embodiment the SIFT descriptor is a vector that represents the local distribution of the image gradients around the detected keypoint s . Orientation invariance may be achieved by rotating the gradient orientations relative to the keypoint orientation i.e. by encoding the image information in a set of localized gradient orientation histograms. The SIFT descriptor may be constructed by stacking all of the orientation histogram entries and provides a certain degree of invariance to variations in illumination.

In an embodiment keypoint feature extraction module may implement the Maximally Stable External Regions MSER as reported by J. Matas et al. in Robust Wide Baseline Stereo from Maximally Stable Extremal Regions in Proceedings of the British Machine Vision Conference pp. 384 393 2002 which is hereby incorporated herein by reference. This scale invariant feature detector is several times faster than SIFT. It uses Haar wavelets to approximate difference of Gaussian DoG filters and integral images for convolution which makes the image filtering process much more efficient.

In an embodiment keypoint feature extraction module implements keypoint feature detectors and descriptors using Speeded Up Robust Features SURF reported by H. Bay et al. in SURF Speeded Up Robust Features Computer Vision and Image Understanding Vol. 110 No. 3 pp. 346 359 2008 which is hereby incorporated herein by reference.

In an embodiment for the purpose of efficiency binary feature descriptors may be used as the output feature s of feature extraction module . Matching with these features involves comparisons using the Hamming distance bitwise XOR followed by a bit count e.g. instead of Euclidean distance and can be efficiently implemented using hardware support on modern computers. Invariance to changes in scale and rotation is supported when a local feature detector supporting the comparisons is coupled with binary descriptors.

In an embodiment keypoint feature description module may implement BRIEF feature descriptors that have been reported by M. Calonder et al. in BRIEF Binary Robust Independent Elementary Features in Proceedings of 11European Conference on Computer Vision pp. 778 792 2010 which is hereby incorporated herein by reference.

In an embodiment keypoint feature description module may implement Oriented FAST and Rotated Binary Robust Independent Elementary Features ORB feature descriptors that have been reported by E. Rublee et al. in ORB An Efficient Alternative to SIFT or SURF in Proceedings of International Conference on Computer Vision pp. 2564 2571 2011 which is hereby incorporated herein by reference.

In an embodiment keypoint feature description module may implement Binary Robust Invariant Scalable Keypoints BRISK feature descriptors that have been reported by S. Leutenegger et al. in BRISK Binary Robust Invariant Scalable Keypoints in Proceedings of IEEE International Conference on Computer Vision pp. 404 410 2012 which is hereby incorporated herein by reference.

In an embodiment keypoint feature description module may implement Fast Retina Keypoint FREAK feature descriptors that have been reported by A. Alexandre and al. in FREAK Fast Retina Keypoint in Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition pp. 510 517 2012 which is hereby incorporated herein by reference.

In an embodiment keypoint feature description module generates keypoint features using the robust LOcal Feature descripTor LOFT . First keypoints are detected by keypoint detection module using any of the methods described above individually or in any combination. Second a rectangular block of size M M is constructed in the neighborhood of each detected keypoint and centered on the respective keypoint. Third a histogram e.g. hue histogram is calculated for each region of the M M sub division and normalized to obtain a unit histogram via Lor Lnormalization. These histograms can then be combined into a local color feature that can be used for color matching. Accordingly keypoint feature extraction module can be used as color feature extraction module to generate a local color feature.

In an additional or alternative LOFT based embodiment each M M block centered on a keypoint is divided into a plurality of regions and for the plurality of regions in each M M block the average color is calculated resulting in an additional or alternative color feature that can be used for color matching.

In an additional or alternative LOFT based embodiment each M M block centered on a keypoint is divided into J bins and K orientations. A histogram e.g. hue histogram is calculated for each of a plurality of regions of the M M block and normalized with the magnitude of the variation of hue between adjacent angular slices resulting in a circular local color feature that can be used for color matching.

These LOFT based methods may be generalized to work for previously described texture features e.g. implemented by the texture feature extraction module by using the same techniques for generating local color descriptors to instead construct a local texture descriptor for texture matching.

These LOFT based methods provide robustness with respect to curvature and bending by calculating the inherent histogram in a log polar coordinate system. This makes the descriptor less sensitive to image shifts caused by deformation within the smaller inner bins than within the larger outer bins. This is based on the observation that given a query shape and a set of target shapes in a database with their corresponding matching points deformation of the query shape results in points on the query shape shifting away from their matching counterparts. The log polar histogram accommodates for this shift.

In an embodiment color feature matching module calculates the distance or similarity between two distributions one distribution representing the color in a query image and the second distribution representing color for object models enrolled in database .

In an embodiment color feature matching module may achieve color matching using histogram intersection.

In an embodiment color feature matching module may given two distributions Hand H respectively representing a distribution generated from the query image and the second distribution extracted from object models enrolled in database with N being the number of bins in the underlying histograms implement different similarity measures such as Chi square distance 

Other similarity measures that may be implemented for this purpose include the correlation distance metrics 

In an embodiment color feature matching module may measure similarity using the Earth Mover s Distance EMD . In simple terms EMD is a measure of the amount of work it would take to change the shape of a first histogram into the shape of a second histogram. EMD allows two histogram to be matched even if the shape of the histograms differ due to lighting variations that could cause other methods to fail.

In an embodiment texture feature matching module matches texture features to corresponding reference texture features in database via database management system module . When texture feature extraction module is implemented using the tree structured wavelet transform classification takes places in two phases a learning phase and a classification phase.

During the learning phase given m samples of the same texture each sample is decomposed with the tree structured wavelet transform and the normalized energy at its leaves is calculated. An energy map is generated for each texture by averaging the energy maps over all samples. This procedure is repeated for all textures.

During the classification phase an unknown texture is decomposed with the tree structure wavelet transform and the corresponding energy map is constructed. Then the first N leaf nodes in the energy map with the largest energy are selected as features and denoted X x . . . x . For a given texture in database the energy values in the same channel are selected m m . . . m . The distance between the feature to be matched and each references feature from database is calculated D d X m and the unknown texture j is assigned to i if D

For embodiments in which texture feature extraction module is implemented using Gabor wavelets a feature vector is constructed from the response of each spatial spatial frequency channel followed by a Gaussian filter to eliminate variations in the feature vectors within regions exhibiting identical colored texture properties. The principal component analysis PCA is applied to reduce the dimensionality of the feature space. Here as well similarity may be measured using the Mahalanobis distance.

An embodiment of shape feature matching module may implement a matching algorithm based on shape context. The goal is to match two shapes by finding the correspondence between point pon the first shape and point qon the second shape. For a point pon the first shape a coarse histogram hof the relative coordinates of the remaining n 1 points is calculated. This histogram is the shape context of p. The procedure for matching shapes is as follows 

Shape feature matching module may also be implemented using Hu moments to match two contours for similarity. An alternative embodiment of shape matching module may use Pairwise Geometrical Histogram PGH to match the contour extracted from the query object and the contour of a reference object in database derived from reference images.

In an embodiment efficient local feature matching module communicates with database management system module to match local keypoint features to reference features in database using Locality Sensitive Hashing LSH methods. LSH methods provide a significant increase in query time. Tree based similarity search methods that employ approximate similarity search techniques may also be utilized by efficient local feature matching module .

In an embodiment efficient local feature matching module uses a priori knowledge of where data i.e. reference features reside so that only a portion of database that contains potentially matching features is searched during the matching process. Hash tables data compartmentalization data sorting and database table indexing may be used as well by efficient feature matching module . This process is related to the object categorization or clustering stage e.g. performed by image categorization module of the object enrollment process. For a given visual query disqualified clusters are eliminated by checking the lower bound of distances in each cluster. Then only the resulting clusters are considered during the feature matching process e.g. performed by feature extraction module .

A parallel processing computing architecture may be employed to achieve fast searching of large databases. In this case the complete database or a portion of the database may be searched by partitioning the database across multiple Central Processing Units CPUs and or Graphics Processing Units GPUs .

In an embodiment a set of scalable methods are provided for matching reference features e.g. of enrolled object models in very large databases including algorithms that work with binary features from binary feature descriptors. An embodiment of efficient feature matching module could be implemented using a priority search k means tree. In such an embodiment leaf nodes are created with points in the dataset of keypoints. The points in the dataset are then partitioned at each level into K distinct regions using a k means clustering algorithm where K is the number of clusters. Afterwards the same method is recursively applied to the points in each region. The recursion is stopped when the number of points in a region is smaller than K.

Once the k means tree has been constructed in an initial traversal it is searched from the root to the closest leaf following the branch with the closest cluster center to the query point and adding all unexplored branches and the path to a priority queue. The priority queue is sorted in increasing distance starting from the query point and extending to the boundary of the branch being added to the queue. Following the above initial tree traversal the algorithm reiterates always starting from the top branch in the priority queue.

The priority search k means trees are not suitable for matching binary features from binary visual descriptors such as BRIEF ORB BRISK or FREAK. Indeed a priority search k means tree requires the points to be in a vector space where their dimensions can be independently averaged. For matching binary features the hierarchical clustering tree algorithm is very effective. In this algorithm a decomposition of the search space is attained by recursively clustering the input data set using random data points as cluster centers for non leaf nodes.

To scale indexing algorithms implementing efficient feature matching modules to very large databases the data may be distributed to multiple machines in a computing cluster where the nearest neighbor search may be performed using all the machines in parallel. To distribute the nearest neighbor matching on the computer cluster an implementation of Map Reduce may be undertaken using the message passing interface specification MPI .

In an embodiment after the features from the query object have been matched to their counterparts in database by efficient feature matching module geometric consistency verification module verifies that those matches occur in a geometrically consistent way. In other words the locations and scales of corresponding features should be related through a common geometrical transformation. The motivation for this process is that the set of keypoints alone does not contain the spatial relation between the query object and the reference object. Therefore a pair of objects that have the same set of keypoints but in a different geometrical configuration would have a high level of similarity. The sought spatial relation may be enforced by estimation of geometric transformation module . For this purpose the k most similar objects to a query object are tested for geometrical consistency by computing geometrical transformations in module using the coordinates of matching points. Transformations commonly used are affine transformations homographies and or the essential matrix between the query object and the most similar reference objects.

In this context outlier correspondences may be eliminated by elimination of outlier correspondences module . This elimination may be performed by finding a consistent geometric transformation as described above together with a large set of inliers supporting this geometrical transformation. A common approach to perform this computation in a robust fashion is through the use of a Random Sample Consensus RANSAC method to eliminate outliers. According to RANSAC the number of inliers to the transformation is counted. The reference object that achieves the largest number of inliers with the query object is then reported as the best match. The matching reference objects are sorted in descending order of the number of keypoints or regions which satisfy the geometrical transformation.

In an embodiment calculate matching confidence module calculates the confidence of this keypoint matching scheme using the ratio of inliers and outliers.

The above described feature matching methods e.g. performed by feature matching module are capable of identifying the query object from any background foreground clutter in the background or foreground and works without human intervention. Scalability is also available providing the ability to identify objects in digital media assets using a very large database e.g. millions of reference objects with a short response time. Moreover these feature matching methods offer robustness to affine transformations such as rotation translation scaling robustness to non affine transformation such as stretching and bending and or robustness to occlusions shadows shading reflections highlights variations in illumination intensity and image noise.

In an embodiment objects enrolled in database are associated with metadata. This metadata may comprise without limitation title description brand name associated keywords price unique product identification number product categories and or the like. Therefore in embodiments even in the context of visual search metadata could be used to further narrow down the search for the query object from a list of visually similar objects. For example following a search for a dress using visual features e.g. color texture shape etc. keywords such as red dress white polka dots sleeveless ruffles and belt could provide additional filtering. In other words search words can be derived from the features extracted from the query object and used in a query of metadata associated with reference objects in database that have been identified as visually similar to the query object. As a simple example if a color feature output by color feature extraction module represents the color red the keyword red can be used by feature matching module to search the associated metadata of reference objects. Alternatively or additionally keywords to be used to search metadata during matching may be input by a user.

The importance of a word in the metadata can be evaluated using the term frequency inverse document frequency TF IDF score. The TF measures the number of times a term appears in the metadata whereas the IDF lowers the weight of terms that appear very frequently and raises the weights of terms that appear rarely in the metadata. A TF IDF score or weight is calculated as w tf idf i.e. the frequency of term i within metadata dtimes the inverse of the frequency of the metadata that contain term i with respect to all the metadata considered. The weights have a tendency to filter out commonly occurring terms. A feature vector can be constructed using the TF IDF score of all terms in the description. The similarity of two items i.e. the search word and the metadata could be estimated using the cosine of their TF IDF vectors. A search for shoes could be carried out by using keywords such as high heels black leather open toe straps to only match shoes with a high degree of similarity to the sought item based on the TF IDF scores.

In an embodiment feature matching module produces for each feature a matching score to each matched reference object and an estimated weight of that feature in the query object. Therefore in the embodiment illustrated in for each matched reference object there is a set of score and weight results for color features a set of score and weight results for texture features a set of score and weight results for shape features and a set of score and weight results for keypoint features. In each set the score is a measure of the distance between the query object and a reference object within a particular feature space. These scores may be normalized in the range 0 1 . The weight on the other hand may be a measure of the importance of a given feature in the composition of the query object and may also be normalized in the range 0 1 .

Data fusion module may generate a final combined or fused score for example by a linear combination of weighted sum as shown below 

In an alternative embodiment data fusion module may calculate the final combined or fused score as follows 

In either case the final combined score generated by data fusion module is used to rank or order the matched reference objects by visual similarity. The object with the best score e.g. the lowest scoring value is ranked as the closest match.

In an embodiment the weights could be intelligently derived from the query image. For example for color images normalized entropy could be used as a measure of color weight. In the case that color feature description are represented by a 64 level global hue histogram the maximum possible entropy is 6 bits. Assuming an 8 bit dynamic range 256 levels in each color channel the average color could be computed over the entire image normalized by the dynamic range of the image e.g. 256 and serve as a measure of color weight.

In a similar manner texture weight could be estimated from the grayscale counterpart of the query image by converting it into a single channel image. Assuming a 256 level global histogram of this grayscale image the maximum possible entropy is 8 bits. Assuming an 8 bit dynamic range 256 gray levels the grayscale entropy may be normalized by the maximum dynamic range resulting in normalized entropy as a measure of texture weight.

In step the query image containing the selected object or the selected portion of an object is received e.g. by media analysis server as a visual query .

In step features are extracted from the query image e.g. by feature extraction module using visual attributes of the image such as color texture shape and or keypoints.

In step the features extracted from the query image are matched e.g. by feature matching module against reference features stored in a database e.g. database . The result of this matching process is a list of reference objects with corresponding confidence scores and weight values e.g. for each feature .

In step a combined matching score is calculated e.g. by data fusion module for each reference object from its corresponding confidence scores and weight values.

In step the matching reference objects are ranked in increasing order of this combined matching score. In an embodiment a low score indicates a smaller distance between the query object and the reference object i.e. a higher similarity. However it should be understood that in an alternative embodiment a high score may indicate a smaller distance i.e. a higher similarity.

In step the ranked list of visually similar objects e.g. list with associated information is returned. The associated information about each visually similar object may include without limitation one or more of the following items an identifier e.g. object name ID number classification group etc. the position the orientation the size the color and or the metadata associated with the object.

In an embodiment hotspot generator creates one or more hotspots corresponding to the one or more previously identified commercial objects. Where the content is a video each hotspot may comprise a video frame identifier a product object identifier and or a position within the identified video frame indicating where the hotspot should be rendered. Where the content is an image each hotspot may comprise a product object identifier and or a position within the image indicating where the hotspot should be rendered.

In an embodiment hotspot packaging module extracts a set of commerce related information for each of the one or more identified commercial objects from database . The information may include without limitation links to images video data and or metadata . This information may be packaged with the hotspot s generated by hotspot generator module to create a hotspot package.

In an embodiment hotspot rendering module embeds the hotspot package s generated by hotspot packaging module and each associated with a commercial object as a visual overlay in the original content of the digital media asset comprising the identified objects.

The result is a digital media asset that has been configured to respond to user action. This user action may comprise clicking on the graphical overlay rendered by hotspot rendering module and indicating the presence of a hotspot. This user interaction may take place on network device running interactive application e.g. via the content server .

In an example embodiment a pricing database e.g. within database may be accessed to identify appropriate sources to receive price information in accordance with a product classification of an identified object at issue. In general the product price sources may be various on line merchants of the relevant types of products. Other sources may also be used such as third party systems which are specifically designed to provide price comparisons of various specific products. The price information may be rendered in a hotspot embedded within a video or image containing the identified object that has been classified as the product.

In another example embodiment a consumer may be provided access e.g. via a rendered hotspot embedded within a video or image to various product reviews e.g. within database for an identified object representing a product. The reviews can be provided by prior users of the system e.g. consumers of the product . As part of the review process the user can rate or otherwise evaluate a purchase themselves and or the quality of a product. For example such a rating system can be based on five stars with five stars indicating an excellent product or purchasing experience. The user can rate not only the product but also the establishment or on line site where the product was purchased. Additional reviews can be retrieved from external repositories or specific content providers. Various product reviews can be maintained within a review database e.g. with database . The review database can comprise commercial or professional reviews and can also include comments or review submitted by users of the system.

Another aspect of embodiments may be to facilitate the purchase of various products corresponding to identified objects e.g. within a video or image . Various techniques can be used to enable purchase of the products. In a simple embodiment the user is provided with links e.g. via a rendered hotspot embedded within a video or image to one or more on line merchants or suppliers of the product at issue. By following those links the user can purchase the product using the merchant s on line facilities. Often several merchants may be available which supply the same product. In this case multiple links may be provided in the embedded hotspot. Alternatively the embedded hotspot may itself comprise a user interface for purchasing the product.

According to another embodiment a consumer may make an on line purchase of an item selected from results e.g. list displayed on network device obtained through a visual search. The consumer may indicate whether and how many of the item type to purchase. Assuming the consumer is registered with the e commerce site to provide transaction information such as name credit card number shipping address etc. the e commerce site completes the transaction. Transaction options may be available such as allowing the consumer to request more information to be sent to the consumer s personal computer email address having hardcopy information mailed to the consumer allowing the consumer to specify that the item be delivered to another party selecting the item as a gift obtaining information on related items etc.

The connection and information exchange between the client application on network device and media analysis server may be accomplished via standard Internet and wireless network software protocols e.g. HTTP TCP UDP WAP etc. and networks. However any suitable technique for exchanging information exchange may be used.

Likewise the connection and information between content interactivity synthesizer and network device running the client application may be accomplished via standard Internet and wired or wireless network software protocols and networks.

Various embodiments of the image identification calculations may also be implemented directly in custom hardware in forms such as Application Specific Integrated Circuits ASICs Field Programmable Gate Arrays FPGAs Programmable Logic Devices PLDs Digital Signal Processors DSPs Graphical Processing Units GPUs optical chemical biological quantum or nano engineered systems components and mechanisms.

Furthermore those of skill in the art will appreciate that the various illustrative logical blocks modules circuits and methods steps described in connection with the above described figures and the embodiments disclosed herein can often be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled persons can implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the application. In addition the grouping of functions within a module block circuit or step is for ease of description. Specific functions or steps can be moved from one module block or circuit to another without departing from the application.

Moreover the various illustrative logical blocks modules and methods described in connection with the embodiments disclosed herein can be implemented or performed with a general purpose processor a Graphics Processing Unit GPU a digital signal processor DSP an Application Specific Integrated Circuit ASIC Field Programmable Gate Arrays FPGAs or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor can be a microprocessor but in the alternative the processor can be any processor controller microcontroller or state machine. A processor can also be implemented as a combination of computing devices for example a combination of a DSP or a GPU and a microprocessor a plurality of plurality of microprocessors and one or more microprocessors in conjunction with a DSP core or GPU or any other such configuration.

Additionally the steps of a method or algorithm described in connection with the embodiments disclosed herein can be embodied directly in hardware in software module s executed by a processor or in a combination of the two. A software module can reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium including a network storage medium. An exemplary storage medium can be coupled to the processor such that the processor can read information from and write information to the storage medium. In the alternative the storage medium can be integral to the processor. The processor and the storage medium can also reside in an ASIC.

Examples of search by visual similarity in the context of image or video interactivity is discussed next.

In an embodiment selecting an item in the list of visually similar items in may generate the screen shown in . illustrates an example user interface conveying a detailed description for one of the items selected from list according to an embodiment. In this case the product Betsy Johnson Printed Pj has been selected from the list of items in . The result is a new graphical overlay comprising product name an image of the selected product the product price a link to facilitate the purchase of the product and a detailed product description . Graphical overlay may be overlaid on the digital media asset on or in the vicinity of the visually similar object e.g. in place of graphical overlay .

In an embodiment selecting link in graphical overlay in may generate the screen shown in . illustrates an example user interface conveying additional options available to a user according to an embodiment. User interface may comprise a graphical overlay e.g. in the vicinity of the object of interest in the image or video on the digital media asset may replace the digital media asset e.g. within interactive application or may be rendered separately from the digital media asset on the display e.g. in a different tab of interactive application or by a separate application initiated in response to selection of link . In any case user interface may comprise information related to the product in the digital media asset such as the product name the product image the product price a link providing an option to purchase the product a product review e.g. comprising the number of stars indicating the level of product quality or rating purchasing experience and or the number of reviews a toggle button for expanding or collapsing a list of visually similar items e.g. showing the name of each product on the list and its price and a link that provides the option to browse through more items that are visually similar to the product. In addition user interface may comprise options to filter visually similar items by one or more criteria such as price range location availability etc. User interface may also comprise a browsing area which enables browsing of items representing all of the objects e.g. from database that were matched to the object in the digital media asset e.g. by feature matching module . The user may scroll through the list in browsing area using navigation buttons and . Button is used to scroll the list left and button is used to scroll the list right. However it should be understood that other orientations e.g. a vertical orientation in which one button scrolls up and another scrolls down are possible. As illustrated each item in the list in browsing area may comprise an image of the product corresponding to a different visually similar object a short description of that product and a price for that product.

In an embodiment selecting an item in the list of visually similar items in may generate the screen shown in . illustrates an example user interface conveying a detailed description for one of the items selected from list according to an embodiment. In this case the product Etsy Vintage 1950 s Black Cat Eye Glasses has been selected from the list of items in . The result is a new graphical overlay comprising product name an image of the selected product the product price a link to facilitate the purchase of the product and a detailed product description . Graphical overlay may be overlaid on the digital media asset on or in the vicinity of the visually similar object e.g. in place of graphical overlay .

In an embodiment selecting link in graphical overlay in may generate the screen shown in . illustrates an example user interface conveying additional options available to a user according to an embodiment. User interface may comprise a graphical overlay e.g. in the vicinity of the object of interest in the image or video on the digital media asset may replace the digital media asset e.g. within interactive application or may be rendered separately from the digital media asset on the display e.g. in a different tab of interactive application or by a separate application initiated in response to selection of link . In any case user interface may comprise information related to the product in the digital media asset such as the product name the product image the product price a link providing an option to purchase the product a product review e.g. comprising the number of stars indicating the level of product quality or rating purchasing experience and or the number of reviews a toggle button for expanding or collapsing a list of visually similar items e.g. showing the name of each product on the list and its price and a link that provides the option to browse through more items that are visually similar to the product. In addition user interface may comprise options to filter visually similar items by one or more criteria such as price range location availability etc. User interface may also comprise a browsing area which enables browsing of items representing all of the objects e.g. from database that were matched to the object in the digital media asset e.g. by feature matching module . The user may scroll through the list in browsing area using navigation buttons and . Button is used to scroll the list left and button is used to scroll the list right. However it should be understood that other orientations e.g. a vertical orientation in which one button scrolls up and another scrolls down are possible. As illustrated each item in the list in browsing area may comprise an image of the product corresponding to a different visually similar object a short description of that product and a price for that product.

An operation of content interactivity synthesizer will now be described in more detail according to an embodiment.

In step content interactivity synthesizer generates one or more hotspots e.g. via hotspot generator module which may form a part of content interactivity synthesizer which in turn may form a part of media analysis server . As discussed elsewhere herein for a given digital media asset content interactivity synthesizer may generate a hotspot for each object detected in the digital media asset and matched to a list of identified or visually similar object s that is received from result ranking and management module . For instance result ranking and management module may output a list of identified or visually similar objects for each detected and matched object in the digital media asset. Thus each list represents the identified or visually similar objects that have been matched from database for a single object detected in the digital media asset. It should be understood that each list may represent all of the identified or visually similar objects that have been matched from database at or above a predetermined confidence level e.g. combined matching score a predetermined number e.g. the top 10 of the identified or visually similar objects that have been matched from database etc. Content interactivity synthesizer may receive the lists and generate a hotspot for each list i.e. for each object detected in the digital media asset .

In an embodiment each hotspot for a digital media asset may comprise a position of the hotspot. For a digital media asset that is an image this position represents the position within the image at which the hotspot is to be rendered. For a multi frame digital media asset this position represents the position within a frame of the digital media asset at which the hotspot is to be rendered.

In addition each hotspot may further comprise an object or product identifier. The object identifier identifies an object e.g. a product that has been identified as the object corresponding to the hotspot. Specifically as discussed above an object may be detected within the digital media asset e.g. by object localization server and identified as one of the objects enrolled in database e.g. by object identification server . Accordingly the object identifier may identify this object e.g. from database thereby associating the hotspot with a particular enrolled object e.g. from database . It should be understood that when the object detected in the digital media asset is a product the object identifier would identify this product e.g. from database .

In addition if the hotspot generated in step is for a multi frame digital media asset e.g. a video the hotspot may further comprise a frame identifier that identifies the frame within the multi frame digital media asset over which the hotspot is to be rendered.

In step content interactivity synthesizer packages each of the one or more hotspots generated in step into a hotspot package e.g. via hotspot packaging module . In an embodiment each hotspot package comprises a graphical user interface comprising information about the identified object and or visually similar objects. If the identified object is a product the graphical user interface may comprise information about that product and or information about related products. For example if the identified object corresponding to the hotspot package is a shirt the graphical user interface of the hotspot package may comprise information about the shirt and or visually similar shirts as well as inputs for facilitating the purchase of the shirt. Example of rendered hotspot packages are depicted in and discussed with respect to .

In step content interactivity synthesizer renders each hotspot and hotspot package generated in step with respect to the digital media asset e.g. via hotspot rendering module . In an embodiment rendering each hotspot comprises embedding a hotspot indicator as a visual overlay of the digital media asset at or near the position identified in the hotspot and corresponding to the identified object associated with the hotspot. The visual overlay for the hotspot may comprise a small or unobtrusive circular mark. In an embodiment rendering each hotspot package comprises embedding the hotspot package as a visual overlay of the digital media asset to be displayed at or near the position identified in the hotspot and corresponding to the identified object associated with the hotspot package.

Thus the digital media asset may comprise a visual overlay of a hotspot for each object identified or matched e.g. by media analysis server . Each hotspot may be selectable such that selecting a hotspot will result in the visibility or invisibility of a hotspot package corresponding to the hotspot. The hotspot may be capable of being toggled to render the hotspot package visible in response to one selection of the hotspot and render the hotspot invisible in response to a consecutive selection of the hotspot. In this manner a user of the digital media asset can open hotspot packages for objects for which he or she desires information e.g. product information about the object or visually similar objects and close hotspot packages for objects for which he or she does not desire information. An example of a digital media asset with one such hotspot package visible is depicted in whereas an example of a digital media asset with two such hotspot packages visible is depicted in .

In addition a navigation hotspot may be generated and rendered in steps to allow a user to navigate between all hotspot packages embedded in a digital media asset. An example of a navigation hotspot is illustrated as B in . As illustrated the navigation hotspot comprises a search box a snapshot of hotspots and a hotspot gallery . A user may type keywords into search box and the hotspot packages embedded in the image or frame being displayed may be searched to identify hotspot packages comprising or associated e.g. within metadata with the keywords. The search results may be displayed as snapshot . When no search is performed via search box snapshot may comprise all hotspot packages embedded in the image or frame being displayed. In addition hotspot gallery may comprise selectable thumbnails of the object associated with each hotspot. Hotspot gallery may be scrollable when the number of thumbnails exceeds the number that can be displayed within the boundaries of navigation hotspot B. Selection of a particular thumbnail within hotspot gallery may result in the hotspot package for the selected hotspot being toggled to visible or invisible similar to selection of the hotspot e.g. hotspots A B directly. The navigation hotspot may be capable of being toggled from visible to invisible and vice versa using for example tab A illustrated in . It should be understood that in multi frame digital media assets this navigation hotspot may or may not be rendered in frames in which no objects were detected or identified depending on the particular implementation.

In step a list e.g. list of identified objects from a given digital media asset e.g. image or video is loaded or otherwise received. For example the list may be received from result ranking and management module . This list comprises data representations of each of the objects that were detected in the digital media asset e.g. by object localization server and identified as or matched to enrolled objects e.g. by object identification server .

In step an asset template is generated from the original content of the digital media asset. This template may be generated in a markup language such as Hypertext Markup Language HTML eXtensible Markup Language XML eXtensible Hypertext Markup Language XHTML etc. The template serves as a container for the original content of the digital media asset and the functionalities to be added for interacting with objects within the content of the digital media asset.

In step an asset viewer e.g. image viewer video player etc. is embedded in the asset template. The asset viewer is configured to render the visual overlays of the hotspot and hotspot package on the original content of the digital media asset. In an embodiment the asset viewer is embedded in the asset template using a lightweight scripting language that allows a script to interact with the user or control the content of a browser e.g. Javascript .

In step metatag s for the interactive content are generated and inserted into the header of the asset template generated in step . In an embodiment the metatag s comprise one or more keywords and or a description of the content. The metatag s may be automatically generated and or manually input and may be available for content management e.g. searching .

In step the position for each hotspot and or hotspot package is calculated based on the positions of the identified objects from the list received in step . In an embodiment the list of identified objects received in step comprises the position of each of the identified objects in the original content and or the bounding box enclosing each of the identified objects in the original content. The position of the hotspot and or hotspot package corresponding to each of these identified objects is determined and recorded in the asset template based on its respective position and or bounding box. For example the position of each hotspot and or hotspot package may be calculated as the center of mass of the bounding rectangle enclosing the identified object corresponding to the hotspot and or hotspot package.

In step a hotspot template is generated for the previously determined hotspots. The hotspot template may be generated based on hotspots generated e.g. by hotspot generator module implementing the process depicted in step of from information in the list of identified objects and or visually similar objects e.g. list received in step e.g. from result ranking and management module . As with the asset template each hotspot template may be generated in a markup language such as HTML XML XHTML etc. It should be understood that the hotspot template generated in step may comprise a single hotspot template for all of the hotspots or separate hotspot templates for each individual hotspot. In either case the hotspot template s may be stored in a content database e.g. on content server in association with the asset template generated in the process depicted in .

In step a database e.g. database of object information is accessed based on an object identifier of each identified or matched visually similar object in the list e.g. list received in step in . Specifically the list of identified objects may comprise a unique object identifier for each identified or matched object.

In step for each object in the list e.g. list this object identifier may be used to retrieve object information associated with the object identifier from the database. In embodiments in which the identified or matched objects in the list are products the retrieved object information for each object may comprise product information such as product image s a product title a product description a product price information to facilitate purchasing the product and the like.

In step a hotspot package template is generated for the previously generated hotspot packages. The hotspot package template may be generated based on hotspot packages generated e.g. by hotspot packaging module implementing the process depicted in step of from information in the list of identified objects and or visually similar objects e.g. list received in step e.g. from result ranking and management module . For instance the hotspot packages may be generated from an aggregate set of information retrieved from a database e.g. database for a given identified object. As with the asset template and the hotspot template each hotspot package template may be generated in a markup language such as HTML XML XHTML etc. It should be understood that the hotspot package template generated in step may comprise a single hotspot package template for all of the hotspot packages or separate hotspot package templates for each individual hotspot package. In either case the hotspot package template s may be stored in a content database e.g. on content server in association with the asset template generated in the process depicted in . In addition or alternatively each hotspot package template may be linked to or otherwise associated with its corresponding hotspot.

In step the hotspot template s generated in step and the hotspot package template s generated in step are embedded into the asset template e.g. as generated according to the process depicted in . Accordingly all of the hotspot and hotspot package templates generated for a given image or video frame are embedded into the asset template. It should be understood that in embodiments in which the asset hotspot and hotspot package templates are implemented in a markup language e.g. HTML XML XHTML etc. embedding the hotspot template s and hotspot package template s may comprise nesting the markup language representing the hotspot template s and hotspot package template s into the markup language representing the asset template. In such embodiment the components of the hotspots e.g. position object identifier and or other attributes or parameters and hotspot packages e.g. image s title description price and or other attributes or parameters may be embedded as attributes or tags in the markup language representing the respective hotspots and hotspot packages.

In step each hotspot e.g. as generated in step in and or represented by a hotspot template generated in step in is overlaid over the content of the digital media asset e.g. image or video . As discussed above the hotspot may comprise a position e.g. x y offsets or coordinates within the digital asset corresponding to a position of the detected object to which it corresponds. Thus the visual representation of the hotspot e.g. a circular square triangular or other shaped visual overlay may be overlaid at or near that position. In instances in which the digital media asset comprises multiple frames the hotspot may comprise a frame identifier in addition to a position and the visual representation of the hotspot may be overlaid at or near that position within the identified frame of the digital media asset.

In step each hotspot package is made available for responding to actions performed on the visual representation of its associated hotspot. This process may involve nesting additional scripts in the markup language representing the hotspot package template s into the markup language representing the asset template to enable rendering of the hotspot package s once invoked by the rendered hotspot s . For instance a user operation e.g. a touch or click on the hotspot may toggle the visibility of the hotspot package as a visual overlay on the content of the digital media asset.

In step the interactive content comprising the original content of the digital media asset the overlaid hotspots and the available hotspot packages is transmitted to a content server e.g. content server for storage and or distribution e.g. to one or more network devices .

In step the content of the digital media asset is loaded for viewing or playback. This content may comprise the asset template with the embedded hotspot s and hotspot package s generated by the processes depicted in . The original content of the digital media asset may be rendered on a display e.g. of network device .

In step for each hotspot e.g. for each hotspot template a visual overlay e.g. a circular object is rendered on the displayed original content at the position indicated by the hotspot. For example the hotspot may be rendered according to its respective hotspot template embedded within the asset template. If the digital media asset comprises a plurality of frames e.g. a video the hotspot is rendered on the displayed content at the position indicated by the hotspot within the particular frame indicated by the hotspot. It should be understood that the position of the hotspot on the display content may be at or in a vicinity of an identified or matched object to which the hotspot corresponds within the content. For example the position of the hotspot may be determined or calculated based on the position of the corresponding object within the displayed content e.g. at the position or at an offset from the position .

In step a selection of a visual overlay rendered for one of the hotspots is received. This selection may be performed by a user for example touching e.g. if the display of network device comprises a touch panel or clicking e.g. if network device comprises a pointing input device the position at which the visual overlay for the hotspot is rendered.

In step in response to the selection of the visual overlay for the hotspot a hotspot package associated with the hotspot is invoked. Invocation of the hotspot package may comprise rendering a visual overlay of the hotspot package over the displayed content at a position indicated by the hotspot package or at or near the position of the associated hotspot. For example the hotspot package may be rendered according to its respective hotspot package template embedded within the asset template. Again it should be understood that the position of the hotspot package on the displayed content may be at or in a vicinity of an identified or matched object to which the hotspot package corresponds within the content or at or in a vicinity of the associated hotspot. For example the position of the hotspot package may be determined or calculated based on the position of the corresponding object within the displayed content e.g. at the position or at an offset from the position or based on the position of the associated hotspot e.g. at the position or at an offset from the position .

In step user interaction s with the visual overlay of the hotspot package may be received. In an embodiment the visual overlay of the hotspot package may be displayed as a frame mini webpage or other user interface e.g. generated in HTML including one or more inputs e.g. hyperlinks buttons checkboxes radio buttons text boxes etc. overlaid on the displayed content. The user interaction s may comprise one or more interactions with inputs within the user interface of the visual overlay such as the selection of buttons icons and or links e.g. hyperlinks to additional information about the identified object or visually similar objects scrolling toggling text entry etc. The user interaction s may enable the user to access object e.g. product descriptions prices reviews browse visually similar objects e.g. products initiate and or consummate a purchase of the object e.g. product etc.

In an embodiment an interactive video advertising platform interfaced with integral to or comprising media analysis server depicted in according to an embodiment is provided for an entity e.g. a company which sells branded products to register its entire collection of brand s or a portion of its brand collection onto a portal site. Media analysis server may then automatically detect and identify objects in pointed digital media assets based on personalized identification information provided by the entity.

Access to the dedicated interactive video advertising platform e.g. server may be granted through registration by a user representing the entity providing personalized identification information e.g. e mail address URL of the brand corporate website a list of URLs for online shops where the brand products are distributed to consumers etc. . Uploading this information may invoke a query to database management system module to retrieve products and or corporate created and or consumer created digital media assets e.g. images and video that correspond to the brand. In an embodiment this interactive content is readily available from a content repository e.g. content repository through content server .

It is possible that an entity would not have all of its commercial branded products available on the content repository. Thus an option may be provided to the user representing the entity to upload more digital media assets e.g. images and or videos that represent the branded product s . The addition of digital media assets to the content repository may trigger a batch processing of digital media assets as illustrated in according to an embodiment. Through this process commercial objects may be detected and identified in all uploaded digital media assets resulting in the generation of interactive content e.g. interactive content from the supplied original content of the uploaded digital media assets. This generated interactive content may then be stored on the content repository e.g. content repository and accessible to content server via database management system module .

In an embodiment the interactive video advertising platform identifies the entity s or brand s social media accounts followed by the publication of the generated and previously retrieved digital media assets e.g. image or videos representing the brands and or products to the identified social media accounts.

In an embodiment this brand registration service is a paid service and options may be provided for receiving payment from the registering entities e.g. via credit cards debit cards checking accounts etc. . Upon completion of the transaction for the brand registration service an entity may launch a campaign for example by a click of a button at the last stage of the brand registration process on the interactive video advertising platform.

In an embodiment the modules of object identification server are extensible to facilitate processing of digital media assets from specialized domains such as geographical information systems GIS biometrics e.g. biometric characteristics from DNA ear face facial thermogram hand thermogram hand vein fingerprint gait hand geometry iris palm print retina etc. education gaming entertainment medical imaging defense homeland security and or others. For example media analysis server may comprise modules for extending one or more of its basic components via a set of common interfaces. These common interfaces may include a feature extraction interface a recognition interface a data fusion interface an object tracking interface a scene change detection interface a digital media interface and an import export interface.

In an embodiment digital media interface enables extensible media analysis engine to receive digital media assets from digital media source s . In an embodiment digital media interface may comprise a separate interface for each type of digital media asset including an interface for images an interface for videos and an interface for custom digital media asset types.

Digital media assets can be represented by different visual characteristics. Feature extraction interface supports basic visual characteristics through pluggable dedicated modules such as module for color module for texture module for shape and module for keypoints. These modules may correspond to color feature extraction module texture feature extraction module shape feature extraction module and keypoint feature extraction module respectively in . Collectively the modules may form at least a portion of feature extraction module . In an embodiment extensibility may be provided by an application programming interface API that makes it possible to create new types of features e.g. by adding new modules or programmatically extend existing features e.g. for existing modules through for example object oriented programming.

In an embodiment recognition interface comprises pluggable modules that define how feature matching is performed e.g. by feature matching module . These pluggable modules may include without limitation an object classifier module that defines how the category of a given object is determined an image indexing module that defines how features are matched in a high dimensional feature space an object locator module that defines how objects are detected in images and or an object identifier module that defines how a given object is matched to a reference object in a database e.g. database of enrolled object models using image indexing module .

In an embodiment data fusion interface comprises pluggable modules that define how feature specific confidence values and weights are combined e.g. by data fusion module . As discussed elsewhere herein object identifier module which may form a part of feature matching module returns a weight and a confidence level for each matching feature. Data fusion interface allows different algorithms for fusing these weights and confidence levels to be integrated into extensible media analysis engine . For instance weight combiner module may define how the confidence levels are combined using the weights e.g. a weighted linear combination of the confidence level values to derive a combined matching score for a given detected object and reference object. Mixture of experts module may define a meta classifier that provides the weights that are used for the fusion of different scores e.g. the confidence values . In addition a custom module may be provided for a custom scheme for combining confidence level values and or using weights.

In an embodiment object tracking interface enables different object tracking algorithms to be interfaced with extensible media analysis engine e.g. to be utilized by object tracking module . For example module may define the mean shift algorithm for object tracking and module may define the Kanada Lucas Tomasi KLT tracker algorithm. In addition a custom module may define a custom object tracking algorithm.

In an embodiment scene change detection interface enables different scene change detection algorithms to be interfaced with extensible media analysis engine e.g. to be utilized by scene change detection module . As discussed elsewhere herein scene change detection is used to trigger the object matching process to improve efficiency. As an example module may define a scene change detection algorithm based on a hue histogram and module may define a scene change detection algorithm based on a two dimensional hue saturation histogram. In addition a custom module may define a custom scene change detection algorithm.

In an embodiment extensible media analysis engine communicates with external systems via import export interface . Import export interface may enable input and output through several different formats including without limitation XML files text files comma separated value CSV files binary large objects BLOBs and or custom file types for example for interfacing with a database e.g. Oracle Sybase Microsoft SQL Server IBM DB2 etc. . Import export interface may enable extensible media analysis engine to be used alongside a wide variety of applications including without limitation raw files database management systems digital media asset management systems visual search systems digital media asset tagging systems digital content distribution systems etc.

The described architecture of extensible media analysis engine and the underlying processes in an embodiment provide a set of interchangeable extensible and autonomous modules and algorithms that operate independently or collaboratively to produce results in the analysis process e.g. performed by media analysis server .

In an embodiment extensible media analysis engine provides functionalities for application developers in specialized domains to incorporate their own domain specific modules e.g. feature extraction module object classifier module image indexing module object locator module object identifier module etc. into extensible media analysis engine thereby customizing extensible media analysis engine e.g. implemented by media analysis server for the specialized domain. For example if the specialized domain involves fingerprint matching an application developer may plug module s designed to extract fingerprint features into feature extraction interface e.g. as custom module and plug module s designed to match extracted fingerprint features to reference features into recognition interface e.g. as object identifier module . Alternatively the application developer may program existing modules to do the required fingerprint extraction and matching via an API of extensible media analysis engine .

In an embodiment one or more including optionally all interfaces of extensible media analysis engine may be defined through an API using for example object oriented programming techniques support by object oriented programming languages e.g. C Java etc. . Extensibility may be achieved by deriving classes from an interface class for the module to be extended. For example to add functionalities for a new set of features to be used for feature extraction e.g. by feature extraction module in a specialized domain a new feature extraction class may be derived from the interface class for feature extraction and the basic and specific definitions and processing methods of this domain specific feature extraction module may be expanded using the same interface as the existing feature extraction modules e.g. modules in .

Extensible media analysis engine with its API and associated static libraries and dynamically linked libraries DLLs may provide application developers with a wide range of possibilities for developing applications directed to different domains.

The system preferably includes one or more processors such as processor . Additional processors may be provided such as an auxiliary processor to manage input output an auxiliary processor to perform floating point mathematical operations a special purpose microprocessor having an architecture suitable for fast execution of signal processing algorithms e.g. digital signal processor a slave processor subordinate to the main processing system e.g. back end processor an additional microprocessor or controller for dual or multiple processor systems a graphics processor or a coprocessor. Such auxiliary processors may be discrete processors or may be integrated with the processor .

The processor is preferably connected to a communication bus . The communication bus may include a data channel for facilitating information transfer between storage and other peripheral components of the system . The communication bus further may provide a set of signals used for communication with the processor including a data bus address bus and control bus not shown . The communication bus may comprise any standard or non standard bus architecture such as for example bus architectures compliant with industry standard architecture ISA extended industry standard architecture EISA Micro Channel Architecture MCA peripheral component interconnect PCI local bus or standards promulgated by the Institute of Electrical and Electronics Engineers IEEE including IEEE 488 general purpose interface bus GPIB IEEE 696 S 100 and the like.

System preferably includes a main memory and may also include a secondary memory . The main memory provides storage of instructions and data for programs executing on the processor . The main memory is typically semiconductor based memory such as dynamic random access memory DRAM and or static random access memory SRAM . Other semiconductor based memory types include for example synchronous dynamic random access memory SDRAM Rambus dynamic random access memory RDRAM ferroelectric random access memory FRAM and the like including read only memory ROM .

The secondary memory may optionally include an internal memory and or a removable medium for example a floppy disk drive a magnetic tape drive a compact disc CD drive a digital versatile disc DVD drive etc. The removable medium is read from and or written to in a well known manner. Removable storage medium may be for example a floppy disk magnetic tape CD DVD SD card etc.

The removable storage medium is a non transitory computer readable medium having stored thereon computer executable code i.e. software and or data. The computer software or data stored on the removable storage medium is read into the system for execution by the processor .

In alternative embodiments secondary memory may include other similar means for allowing computer programs or other data or instructions to be loaded into the system . Such means may include for example an external storage medium and an interface . Examples of external storage medium may include an external hard disk drive or an external optical drive or and external magneto optical drive.

Other examples of secondary memory may include semiconductor based memory such as programmable read only memory PROM erasable programmable read only memory EPROM electrically erasable read only memory EEPROM or flash memory block oriented memory similar to EEPROM . Also included are any other removable storage media and communication interface which allow software and data to be transferred from an external medium to the system .

System may also include an input output I O interface . The I O interface facilitates input from and output to external devices. For example the I O interface may receive input from a keyboard or mouse and may provide output to a display. The I O interface is capable of facilitating input from and output to various alternative types of human interface and machine interface devices alike.

System may also include a communication interface . The communication interface allows software and data to be transferred between system and external devices e.g. printers networks or information sources. For example computer software or executable code may be transferred to system from a network server via communication interface . Examples of communication interface include a modem a network interface card NIC a wireless data card a communications port a PCMCIA slot and card an infrared interface and an IEEE 1394 fire wire just to name a few.

Communication interface preferably implements industry promulgated protocol standards such as Ethernet IEEE 802 standards Fiber Channel digital subscriber line DSL asynchronous digital subscriber line ADSL frame relay asynchronous transfer mode ATM integrated digital services network ISDN personal communications services PCS transmission control protocol Internet protocol TCP IP serial line Internet protocol point to point protocol SLIP PPP and so on but may also implement customized or non standard interface protocols as well.

Software and data transferred via communication interface are generally in the form of electrical communication signals . These signals are preferably provided to communication interface via a communication channel . In one embodiment the communication channel may be a wired or wireless network or any variety of other communication links. Communication channel carries signals and can be implemented using a variety of wired or wireless communication means including wire or cable fiber optics conventional phone line cellular phone link wireless data communication link radio frequency RF link or infrared link just to name a few.

Computer executable code i.e. computer programs or software is stored in the main memory and or the secondary memory . Computer programs can also be received via communication interface and stored in the main memory and or the secondary memory . Such computer programs when executed enable the system to perform the various functions of the present application as previously described.

In this description the term computer readable medium is used to refer to any non transitory computer readable storage media used to provide computer executable code e.g. software and computer programs to the system . Examples of these media include main memory secondary memory including internal memory removable medium and external storage medium and any peripheral device communicatively coupled with communication interface including a network information server or other network device . These non transitory computer readable mediums are means for providing executable code programming instructions and software to the system .

In an embodiment that is implemented using software the software may be stored on a computer readable medium and loaded into the system by way of removable medium I O interface or communication interface . In such an embodiment the software is loaded into the system in the form of electrical communication signals . The software when executed by the processor preferably causes the processor to perform the inventive features and functions previously described herein.

The system also includes optional wireless communication components that facilitate wireless communication over a voice and over a data network. The wireless communication components comprise an antenna system a radio system and a baseband system . In the system radio frequency RF signals are transmitted and received over the air by the antenna system under the management of the radio system .

In one embodiment the antenna system may comprise one or more antennae and one or more multiplexors not shown that perform a switching function to provide the antenna system with transmit and receive signal paths. In the receive path received RF signals can be coupled from a multiplexor to a low noise amplifier not shown that amplifies the received RF signal and sends the amplified signal to the radio system .

In alternative embodiments the radio system may comprise one or more radios that are configured to communicate over various frequencies. In one embodiment the radio system may combine a demodulator not shown and modulator not shown in one integrated circuit IC . The demodulator and modulator can also be separate components. In the incoming path the demodulator strips away the RF carrier signal leaving a baseband receive audio signal which is sent from the radio system to the baseband system .

If the received signal contains audio information then baseband system decodes the signal and converts it to an analog signal. Then the signal is amplified and sent to a speaker. The baseband system also receives analog audio signals from a microphone. These analog audio signals are converted to digital signals and encoded by the baseband system . The baseband system also codes the digital signals for transmission and generates a baseband transmit audio signal that is routed to the modulator portion of the radio system . The modulator mixes the baseband transmit audio signal with an RF carrier signal generating an RF transmit signal that is routed to the antenna system and may pass through a power amplifier not shown . The power amplifier amplifies the RF transmit signal and routes it to the antenna system where the signal is switched to the antenna port for transmission.

The baseband system is also communicatively coupled with the processor . The central processing unit has access to data storage areas and . The central processing unit is preferably configured to execute instructions i.e. computer programs or software that can be stored in the memory or the secondary memory . Computer programs can also be received from the baseband processor and stored in the data storage area or in secondary memory or executed upon receipt. Such computer programs when executed enable the system to perform the various functions of the present application as previously described. For example data storage areas may include various software modules not shown that are executable by processor .

Various embodiments may also be implemented primarily in hardware using for example components such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . Implementation of a hardware state machine capable of performing the functions described herein will also be apparent to those skilled in the relevant art. Various embodiments may also be implemented using a combination of both hardware and software.

Furthermore those of skill in the art will appreciate that the various illustrative logical blocks modules circuits and method steps described in connection with the above described figures and the embodiments disclosed herein can often be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled persons can implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the application. In addition the grouping of functions within a module block circuit or step is for ease of description. Specific functions or steps can be moved from one module block or circuit to another without departing from the application.

Moreover the various illustrative logical blocks modules and methods described in connection with the embodiments disclosed herein can be implemented or performed with a general purpose processor a digital signal processor DSP an ASIC FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor can be a microprocessor but in the alternative the processor can be any processor controller microcontroller or state machine. A processor can also be implemented as a combination of computing devices for example a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

Additionally the steps of a method or algorithm described in connection with the embodiments disclosed herein can be embodied directly in hardware in a software module executed by a processor or in a combination of the two. A software module can reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium including a network storage medium. An exemplary storage medium can be coupled to the processor such the processor can read information from and write information to the storage medium. In the alternative the storage medium can be integral to the processor. The processor and the storage medium can also reside in an ASIC.

The above description of the disclosed embodiments is provided to enable any person skilled in the art to make or use the application. Various modifications to these embodiments will be readily apparent to those skilled in the art and the generic principles described herein can be applied to other embodiments without departing from the spirit or scope of the application. Thus it is to be understood that the description and drawings presented herein represent a presently preferred embodiment of the application and are therefore representative of the subject matter which is broadly contemplated by the present application. It is further understood that the scope of the present application fully encompasses other embodiments that may become obvious to those skilled in the art and that the scope of the present application is accordingly not limited.

