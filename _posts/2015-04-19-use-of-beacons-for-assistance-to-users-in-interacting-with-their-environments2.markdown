---

title: Use of beacons for assistance to users in interacting with their environments
abstract: Beacon-based guidance functionality is described herein that assists the user in navigating over a desired route within an environment, or otherwise interacting with the environment. The environment, in turn, is populated with a plurality of beacons having, in one implementation, respective non-overlapping ranges. The desired route traverses ranges associated with a route-specific set of beacons, from among the plurality of beacons. In one manner of operation, the beacon-based guidance functionality determines whether a user is within a range of one of the route-specific beacons. Based on that knowledge, the beacon-based guidance module can generate guidance information which directs the user towards a next waypoint in the route.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09652124&OS=09652124&RS=09652124
owner: Microsoft Technology Licensing, LLC
number: 09652124
owner_city: Redmond
owner_country: US
publication_date: 20150419
---
This application claims the benefit of U.S. Provisional Application No. 62 073 762 the 762 Application filed Oct. 31 2014. The 762 Application is incorporated by reference herein in its entirety

A user may rely on various conventional mechanisms in generating and executing travel plans and or in exploring his or her environment in a more spontaneous and unconstrained manner. For example a user may use a route selection application to generate a route for use in traveling between two locations. That same application may then guide the user as the user travels over the route through a series of prompts. The prompts may correspond to spoken and or displayed messages such as a message that verbally instructs the user to make a turn within a prescribed distance. There is nevertheless considerable room for improvement in these conventional mechanisms.

Various tools are described herein for assisting a user in interacting with physical and or virtual spaces. Such assistance for example may facilitate a user s navigation within the physical and or virtual spaces. As used herein navigation refers a user s purposeful movement through a space according to a predetermined plan and or movement that may reflect spontaneous decisions that do not necessarily conform to any predetermined plan. The tools may also assist a user s exploration within a space at any given moment in time e.g. corresponding to a process by which the user orients himself or herself in the space.

As a general principle the tools are designed to provide highly informative and timely assistance to the user in a user friendly manner thereby enriching the user s experience of the physical and virtual spaces through which the user moves but without unduly distracting the user from the actual task of interacting with the spaces. In view of these characteristics the tools may be successfully used even by users with vision impairments and or other handicaps. However the tools are general purpose in nature and thus may provide user friendly and unobtrusive guidance to any user in performing any task in which the user interacts within his or her environment.

The tools have various aspects which contribute to the above summarized performance. According to one aspect a space interaction SI module is described herein that includes a beacon based guidance module that assists the user in navigating over a desired route within an environment or more generally moving within and exploring the environment. The environment in turn is populated with a plurality of beacons having in one illustrative implementation respective non overlapping ranges. The desired route traverses ranges associated with a route specific set of beacons from among the plurality of beacons.

In one approach the beacon based guidance module is configured to determine whether a user is within a range of one of the route specific beacons to provide current location information when the user is within the range determine a next waypoint that the user is expected to reach based on predetermined journey information to provide next waypoint information determine direction information based on the current location information and the next waypoint information and generate audio information based on the direction information. The audio information once delivered to the user assists the user in reaching the next waypoint. In one case the audio information may be expressed at least in part as three dimensional e.g. directional sounds.

The above features contribute to the above goal of allowing the user to safely and efficiently move through his or her environment particularly in those situations in which the user cannot rely on other modes of determining his or her location e.g. based on the use of a satellite based navigation system . In addition the use of non overlapping beacon ranges according to one illustrative implementation provides an efficient mechanism for disambiguating the location of the user since the user cannot simultaneously exist within the ranges of two or more beacons at the same time.

The above approach can be manifested in various types of systems devices components methods computer readable storage media data structures graphical user interface presentations articles of manufacture and so on.

This Summary is provided to introduce a selection of concepts in a simplified form these concepts are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

The same numbers are used throughout the disclosure and figures to reference like components and features. Series numbers refer to features originally found in series numbers refer to features originally found in series numbers refer to features originally found in and so on.

This disclosure is organized as follows. Section A provides an overview of a system for assisting a user in interacting with a real and or virtual space. Section B describes different types of headset and handheld user computing devices that can be used in the system of Section A. Section C describes an illustrative user interface experience that may be provided by the system of Section A. Section D describes functionality for generating and using three dimensional sounds and other non three dimensional types of sounds. Section E describes beacon based guidance functionality for assisting the user in navigating through an environment that is populated with beacons having in one illustrative implementation non overlapping ranges. And Section F describes illustrative computing functionality that can be used to implement any aspect of the features described in the previous sections.

As a preliminary matter some of the figures describe concepts in the context of one or more structural components variously referred to as functionality modules features elements etc. The various components shown in the figures can be implemented in any manner by any physical and tangible mechanisms for instance by software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof. In one case the illustrated separation of various components in the figures into distinct units may reflect the use of corresponding distinct physical and tangible components in an actual implementation. Alternatively or in addition any single component illustrated in the figures may be implemented by plural actual physical components. Alternatively or in addition the depiction of any two or more separate components in the figures may reflect different functions performed by a single actual physical component. to be described in turn provides additional details regarding one illustrative physical implementation of the functions shown in the figures.

Other figures describe the concepts in flowchart form. In this form certain operations are described as constituting distinct blocks performed in a certain order. Such implementations are illustrative and non limiting. Certain blocks described herein can be grouped together and performed in a single operation certain blocks can be broken apart into plural component blocks and certain blocks can be performed in an order that differs from that which is illustrated herein including a parallel manner of performing the blocks . The blocks shown in the flowcharts can be implemented in any manner by any physical and tangible mechanisms for instance by software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof.

As to terminology the phrase configured to encompasses any way that any kind of physical and tangible functionality can be constructed to perform an identified operation. The functionality can be configured to perform an operation using for instance software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof.

The term logic encompasses any physical and tangible functionality for performing a task. For instance each operation illustrated in the flowcharts corresponds to a logic component for performing that operation. An operation can be performed using for instance software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof. When implemented by computing equipment a logic component represents an electrical component that is a physical part of the computing system however implemented.

The following explanation may identify one or more features as optional. This type of statement is not to be interpreted as an exhaustive indication of features that may be considered optional that is other features can be considered as optional although not explicitly identified in the text. Further any description of a single entity is not intended to preclude the use of plural such entities similarly a description of plural entities is not intended to preclude the use of a single entity. Further while the description may explain certain features as alternative ways of carrying out identified functions or implementing identified mechanisms the features can also be combined together in any combination. Finally the terms exemplary or illustrative refer to one implementation among potentially many implementations.

During his or her exploration of the space s the user may interact with the system via a user computing device of any type referred to as a simply a user device for brevity below and a headset or just the headset alone. The user device may correspond to any type of portable computing device having any form factor. For example the user device may correspond to a smartphone a tablet type computing device a laptop computing device a netbook type computing device a media consumption device such as a book reader type computing device or a music playing computing device a portable game device a wearable computing device such as eyewear goggles etc. and so on. In other cases not shown a user may carry and utilize two or more user computing devices such as a smartphone in combination with a tablet type computing device.

The headset may likewise correspond to any type of device for delivering audio information to the user . In one case for example the headset may deliver audio information using conventional speakers positioned over or in proximity to one or more ears of the user . In another case the headset may deliver audio information using a bone conduction technique. In a bone conduction technique the headset passes information to the user s eardrums via vibrations imparted to the bones of the user s head. A headset that uses bone conduction may not block the ear canals of the user and thus allows the user to hear other exterior sounds produced by the spaces through which he or she navigates this outcome may be desirable in some cases to increase the safety with which users interact with their physical environments particularly for the case of sight impaired users and the case of users who are unfamiliar with their surroundings.

According to one general manner of use the user may interact with the user device to receive information regarding his or her interaction with a space in primarily visual form. In addition or alternatively the user device can deliver information in non visual forms such as by producing haptic feedback cues e.g. vibration based cues . The user may primarily interact with the headset to receive audio information regarding his or her interaction with a space. That audio information may include spoken information other sounds etc. The system may also receive instructions from either the user device or the headset or both.

The user may also optionally interact with the system via one or more traditionally stationary computing devices such as a desktop computing device a game console device a set top box device and so on. For example the user may interact with the system using the other user computing device prior to a journey to create journey information that defines a particular route through a space. The user may then load that journey information on the user device and or the headset . Following the journey the user may again interact with the system via the other user computing device . For example the system may download information regarding a completed journey to the other user computing device allowing the user to review information regarding that journey at any time for any purpose.

In one case all functions associated with the system are performed by processing functionality provided by the above identified components namely the user device the headset and the other user computing device . In another case one or more remote processing resources may implement at least some aspects of the processing performed by the system such as those aspects which are particularly computation intensive in nature. For example the remote processing resources may include functionality for creating new journeys modifying existing journeys interpreting the user s spoken instructions and so on. In one case the remote processing resources may correspond to one or more server computing devices one or more data stores and so on.

The system may use any combination of communication conduits to couple the above described components together. For example the user device may interact with the headset via any wireless communication mechanism e.g. using BLUETOOTH communication and or any hardwired communication mechanism e.g. via a USB connection etc. . The user device and the headset may communicate with remote components of the system via a cellular connection a Wi Fi connection a hardwired connection etc. or any combination thereof.

Further although not shown the user device may interact with any remote position determination systems for the purpose of determining the position of the user device . The headset may perform the same function. The remote position determination mechanisms may correspond to any satellite based position determination system e.g. a GPS system terrestrial communication towers and so on. Still further the user device and or the headset may also interact with local beacons through any communication mechanism e.g. via BLUETOOTH communication Wi Fi communication etc. .

The remainder of this section Section A provides an overview of the functionality provided by the system . Later sections provide additional details regarding individual components of the system.

In general a user may use the system as a way of enriching the user s experience as the user moves through any type of space familiar or unfamiliar. In one scenario for instance the user may use the system as a guide in conducting a planned journey from a source location to a target location subject to a desired timetable. In other cases the user may use the system to provide assistance in exploring a space in a more open ended and spontaneous manner e.g. without a preplanned journey and or timetable. For example a user may use the system to provide assistance as he or she wanders through an unfamiliar city or meanders past the exhibits of a fair or museum the user may engage in this activity with no fixed itinerary and may alter his or her journey s course and objectives in a free form and spontaneous manner. In yet another case the user may use the system as a way of enlivening a familiar route through a familiar environment and so on. In yet another case a user may use the system in a hybrid manner of operation e.g. in which some aspects of the user s interaction conform to a prepared plan and other aspects are more open ended and spontaneous in nature.

Advancing to this figure shows one example in which a user uses the system of to take a preplanned journey. The user s illustrative experience in conducting this journey will be described below as a way of introducing the reader to the types of functions that the system may perform. Note that this example experience is presented in the spirit of illustration not limitation as noted above the system can be applied in a wide variety of other contexts in which a user interacts with his or her environment for any purpose.

In the non limiting case of assume that the user a man named John has created a journey prior to embarking on the journey although again this need not be the case . For example assume that the user has created a route to take him from his residence in London to a doctor s appointment in another part of the city. The user may have created a journey for this task because he is unfamiliar with the part of the city over which he is to travel. Or the user may have one or more handicaps which present various mobility related challenges. For example the user may have any form and degree of vision impairment. In this context the user may have created the journey to assist him in navigating the route even though he may be familiar with the route. In another case some entity other than the user may have created the journey on behalf of the user. In these cases the planned journey is defined by journey information which describes all aspects of the journey.

The planned route in the illustrated case of is defined by a series of transitional points w w w w and w or stations referred to as waypoints herein. For example the waypoint wmay correspond to the starting point of the user s journey while the waypoint wmay correspond to the destination of the user s journey. The waypoints wand wmay correspond to two intersections at each such intersection the user transitions from a first road to a second road. The waypoint wmay correspond to any station at which a user may change his mode of transportation. For example the user may travel to waypoint won foot. The waypoint wmay correspond to a shuttle station at which the user waits for the arrival of a shuttle which arrives per a predetermined schedule. The user may then continue to the waypoint won the shuttle . In other cases the mode of transportation may correspond to a train subway bus tram or any mode of private transportation e.g. private automobile bicycle etc. .

Hence the planned route may be conceptualized as having a series of segments s s s and s . The example of is a simplified case. In other cases a journey may include many more waypoints and associated segments. And that journey may encompass any combination of modes of transportation. In yet other cases a journey may be less complex than the one shown in e.g. including only a beginning and ending waypoint. And to repeat in other cases the journey may not be defined in advance.

A context as the term is used herein generally refers to the circumstance that confronts a user at any given time while conducting the journey or otherwise interacting with the environment. The circumstance in turn is governed by at least features of the environment with which the user may wish to interact the current time of day and day of week etc. and the goals that confront the user at the current time etc. For example at a time t the user is traversing the segment sin an attempt to travel from the waypoint wto the waypoint w. The context cof the user s journey is therefore defined at least in part by the user s current location along the planned journey and the user s effort to reach the waypoint wover the planned segment s. The user s context at other times will differ depending on the environment that confronts the user at those times coupled with the users respective local goals at that time.

With the above preliminary description of the planned route and the actual route now consider an illustrative user experience as the user travels from waypoint wto waypoint w. Assume in one example that the user carries at least the user device and wears the headset shown in . The user device is referred to below as a smartphone to simplify reference to this device although the user device can include any type of device mentioned above with reference to . In yet other cases the user may navigate with just the use of the headset that is by eliminating the use of the smartphone .

As a general principle the system exposes the user to relevant information at appropriate junctures along the path of the user as the user conducts his journey or otherwise interacts with the space. Instances of the information that may be automatically presented to the user in audible visual and or haptic form are referred to herein as items of interest IOIs . To perform this function the system determines the current context of the user at each moment of time. For instance the system senses the location and orientation and optionally the motion of the user at each particular time. The system can use any technology or combination of technologies to perform this task examples of which are provided below in the context of the description of . The system then identifies IOIs that are relevant to the user s current context. In some cases for instance the system can determine that an IOI is relevant to the user s current context because the user is within a prescribed distance of a physical object or region that is associated with the IOI where that distance is defined and stored in advance. The system then delivers information regarding those IOIs to the user. As will be described in greater detail below the user can also manually explore future contexts with which he will or may be confronted at later times in the journey allowing the user to prepare for those situations.

The system can provide the IOIs based on information extracted from various sources. For instance the system can determine the locations of roads natural features etc. from publically available map information such as map information provided by the BING map service provided by MICROSOFT Corporation of Redmond Wash. . The system can determine the locations of public and private entities e.g. stores government buildings etc. from any published directory information. The system can determine the occurrence of relevant events from various published sources such as public transportation schedule information public safety information. The system can determine user related information from various services such as one or more online social networking applications one or more calendar applications etc. and so on.

Each IOI pertains to a particular topic or experiential focus. The IOIs can be categorized in different ways along different explanatory dimensions. For instance IOIs of a first class directly map to physical objects or physical events in the space through which the user is moving or otherwise interacting. For example an IOI of this type may correspond to a store that lies in proximity to the user or an open manhole that lies in front of the user or a next waypoint that the user is within a prescribed distance of reaching etc. IOIs of a second class of IOIs do not necessarily have a direct physical counterpart in the environment. For example an IOI of this type may correspond to an advertisement that plays when the user approaches a bus stop. The advertisement may be associated with a space around the bus stop but otherwise is not a description of the bus stop per se.

Another virtual type IOI may correspond to a news headline that is brought to the user s attention as he approaches a coffee stand. The system may present information regarding that IOI to the user based on the premise that a user may wish to consume that IOI while drinking a cup of coffee. Another virtual type IOI may correspond to a weather report that is delivered to the user as the user leaves a subway station. The system may present information regarding that IOI to the user based on the premise that the user may wish to prepare for the weather which he is about to confront upon leaving the station. Another virtual type IOI may correspond to a message retrieved from a social network application as the user approaches his or her personal residence. The system may present information regarding that IOI to the user based on the premise that the user may wish to catch up on any messages sent by family members or friends prior to entering his home. Many other scenarios are envisioned in which the user receives some type of information and or opportunity to interact with functionality that is germane to the user s present circumstance yet might not serve to describe an actual object or event in the user s immediate vicinity.

Different IOIs of either the first real class or second virtual class described above can also be categorized based on their roles that they serve in the exploration of space. For example the IOIs may be classified along this dimension as warning IOIs journey IOIs contextual IOIs and available information IOIs etc. although such categories may not be strictly speaking mutually exclusive . The system sends information regarding warning IOIs to alert the user to occurrences that may affect the safety of the user during the journey such as an indication that there is an open manhole in front of the user . Warning messages are sharp and to the point. The system sends information regarding journey IOIs to alert the user to occurrences that may affect the progress of the user s journey such as an indication that a next waypoint is approaching or that a bus will be late . The system sends information regarding contextual IOIs to the user to alert the user to objects and events in the vicinity of the user whether real or virtual that may interest the user such as an indication that a coffee shop is ahead of the user on his journey . The system alerts the user to the existence of available information IOIs without automatically delivering them. A user may then choose to receive this information in an on demand manner or ignore it. For example a user may access the available information by tapping on an appropriate function button on the headset or through an information menu provided by the user device or using the equivalent voice command.

The system can deliver information regarding each IOI in any manner. For example for at least some IOIs the system can deliver the IOI by sending a telltale sound followed by a spoken message. The preliminary sound enables the user to tune in to hear the spoken announcement that is by directing attention to the spoken announcement. The sound also alerts the user as to the type of information that is to follow so that the user is better able to decide whether he or she will devote attention to it. For example users may choose to give heightened attention to journey information but devote less attention to contextual information e.g. regarding a store in the user s vicinity or a promotional offer pertaining to the user s current circumstance .

At any point in the journey the user can make a request to hear the information regarding one or more journey IOIs and or other types of IOIs that were most recently read aloud. That request can be made in various ways to be described below. If a warning has been read aloud within the last 30 seconds then the system will respond to the user s action by also repeating the warning followed by repeating the journey information. In some cases the user may request the system to repeat information but this information is no longer available. The system can play an appropriate sound to alert the user to this situation following by a spoken message information is unavailable at this time or the like.

As another general feature the system may automatically present information regarding some IOIs at times at which the user is not expected to be overwhelmed with other sensory information from the environment. For example the system may refrain from sending the user messages regarding the planned route until the user reaches a prescribed quiet zone along the planned route at which the user can safely and enjoyably consume the information. The system can store in advance the locations of regions that are considered quiet zones. 

Now more closely consider the user s particular experience while navigating over the space shown in . Assume that at time t the user is attempting to reach the second waypoint w. After sensing the location and heading of the user the system can send the user directions in visual and or audible form that assist the user in reaching the second waypoint w. Further the system can send the user information regarding the second waypoint witself when the user is within a prescribed distance from the second waypoint w. This will enable the user to make suitable preparations for any change in course that the second waypoint wmay entail. The above described information may be formulated primarily as journey IOIs.

In certain situations the user s actual journey may depart from the planned journey e.g. with respect to the route that is actually taken and or the timing of the user s traversal of the space. To address those situations the system can automatically or in an on demand manner determine whether the user s current situation will impact any remaining parts of the user s journey. For example the system can update the user s estimated time of arrival at a bus station and then determine whether the user will continue to arrive on time to catch a previously identified bus or shuttle. If the user s current situation impacts the user s journey in this manner the system can automatically re generate information that is used to assist the user in navigating within the space. That information may be expressed as a new set of IOIs. For example the system can advise the user to take a later bus or shuttle and can automatically make appropriate reservations send appropriate notifications and or make other arrangements . Further as noted above the user may explore the updated information in any manner e.g. by expressly requesting information regarding a waypoint to be encountered in the future etc.

The user may customize the behavior of the system in any manner with respect to contextual IOIs and or any other type of IOI . For example the user can specify the types of prompts that he wishes to receive along his route e.g. by indicating he would like to receive information regarding a first type of store but not a second type of store. The user may also specify the timing at which he would like to receive the information. For example the user can specify the maximum distance from his current location that should be considered when notifying him of the existence of contextual IOIs. The user may make these settings prior to embarking on the journey. In addition the user may dynamically change the type and quantity of information delivered by the system over the course of the journey e.g. by dialing back on the amount of information that is automatically provided by the system. The user may opt to reduce the amount of information because he finds it unnecessary or distracting at a particular juncture along the journey.

The system can present information regarding contextual IOIs in any application specific manner. For example in one case the system may announce a collection of contextual IOIs in the order in which they appear in front of the user from left to right or from right to left e.g. by essentially forming an angular sweep with the user positioned at the origin of the sweep or from front to back in terms of distance from the user or back to front etc. As noted above in one implementation the system can precede each announcement with a telltale sound that alerts the user that information regarding a contextual IOI is about to follow. The system may then describe the contextual IOI e.g. by providing audio information which announces Jane s Coffee Shop 150 feet. The system may alternatively provide different preliminary sounds associated with different types of establishments such as by providing a first type of preliminary sound for restaurants and a second type of sound for bus stops etc.

In some cases there may be a large quantity of contextual IOIs within the vicinity of the user at a current time. To cut back on the amount of information imparted to the user the system may consolidate this information into one or more summary messages such as by announcing Restaurants generally at 100 feet. The same holds true for any type of IOI. In general the system can determine whether a group of individual IOIs to be delivered to the user at a given time have at least one common characteristic. The system can then provide a summary message which announces the group of IOIs as a block or set rather than individually. In the specific example noted above the system determines that a group of contextual IOIs are located in the vicinity of the user and that these IOIs pertain to the same type of establishment. In another example the system may provide the summary warning IOI Numerous large potholes in the next 100 yards. 

The system may use other rules to provide announcements in an intelligent manner. For example some IOIs are relevant only if the user is in close proximity to these IOIs. For example a user may be interested in a park bench only if the user is within a few feet of this object. Hence the system may announce the presence of these kinds of objects only when the user is relatively close to these objects. But here again the user may customize the behavior of the system in this regard.

According to another illustrative feature the system may use three dimensional sounds to announce the presence of some types of IOIs such as some types of contextual IOIs. A three dimensional sound refers to a sound that the user perceives as emanating from a particular location or locations within physical space. In reality however the audio information is presented to the user via the headset and has no physical origin within the environment. As will be described below in Section D the system can achieve the above result through the use of Head Related Transfer Functions HRTFs in conjunction with different types of wideband audio sounds. In other implementations the system can use other techniques and technologies to create a three dimensional audio effect instead of HRTFs or in addition to HRTFs such as Ambiophonics Ambisonics wave field synthesis etc.

For example at the context c at time t the system may send a series of directional announcements to the user which identify at least three contextual IOIs IOI IOI and IOI . The system for instance can announce the presence of contextual IOIby sending a preliminary sound to the user which the user perceives as emanating from a particular location in space at which the entity associated with the contextual IOI is physically located. That is if the contextual IOI corresponds to Jane s Coffee Shop the message would direct the user s attention to the actual location of Jane s Coffee Shop. The system can also provide the description of this establishment using a three dimensional sound such as by announcing Jane s Coffee Shop 150 feet in a directional manner that is using three dimensional spoken information.

In addition or alternatively the system may send the user information regarding one or more information available IOIs. As explained above the system can perform this operation by sending a message which generally alerts the user to the existence of information that may pertain to the user s current context but without immediately announcing the particular substance of that information. The user may then manually ask the system to receive the information e.g. by making a more information instruction. Or the user may ignore the information.

All of the above information delivery features contribute to the goal of surfacing useful information to the user as the user traverses a space without overwhelming the user with too much information.

As another feature at any juncture along the user s path the system can compare the user s actual direction of travel with a desired direction of travel. The system can determine the user s actual direction of travel using any of the mechanisms described below in connection with . The system can determine the desired direction by determining where the user is expected to be headed at the present time. The system can then determine deviation information which determines an extent to which the actual direction of the user deviates from the desired direction. The system can then send the user information which attempts to steer the user along a desired path.

For example consider the user at time t. At this juncture the user is attempting to reach waypoint w. Assume that the user s actual direction is represented by arrow and the user s desired direction is represented by arrow . The system will send the user instructions that attempt to steer the user away from his current errant direction towards the desired direction.

In one implementation the system accomplishes the above goal using a three dimensional beat sound or other type of periodic sound. The three dimensional beat sound is any type of repeating sound such as a clicking sound that is perceived by the user as originating from a particular location or locations in the physical space. In the case of at the time t the system will deliver a three dimensional beat sound that appears to originate to the left of the user along his direction of travel. This will advise the user to the fact that a he is headed in a non optimal direction and b he should turn slightly left to achieve a more desirable trajectory.

The system can modulate the beat sound to achieve other effects. For example the system can change the tone and or periodicity and or volume and or some other aspect of the beat sound depending on the extent to which the user is currently headed in a non optimal direction. The user will interpret the beat sound as an indication of the extent to which he is headed in the wrong direction.

So far the description has mostly emphasized the ability of the system to automatically deliver information to the user at appropriate junctures along the user s path. In this mode of operation the user s encounter with the environment may constitute the events which trigger the delivery of information e.g. in the form of the above described different types of IOIs. In addition at any time the user may manually interact with either the smartphone or the headset to manually explore his environment and to obtain information regarding any of the IOIs described above. For example in a first manner of interaction the user may tap on a touch sensitive surface of the smartphone at any given time along the journey. In response to a single tap the system will announce top level information regarding the user s present context. For example at time t the system may respond to a single tap by announcing that the user is headed to waypoint w which may be considered a journey IOI. In response to a double tap the system will provide more detailed information regarding the present context. In response to a triple tap the system will provide instructions which allow the user to interact with the smartphone e.g. for the purpose of obtaining additional information regarding the context and to invoke various functions pertaining to the current context.

As another feature at any juncture the user may interact with the smartphone to activate one or more menus that are relevant to the user at a given time. For example at time t the user may perform a tap and hold gesture on the surface of the smartphone. In response the system may activate a menu associated with the user s present context. The user may then interact with the smartphone to explore the immediately presented menu or navigate to any other menu.

More specifically as will be explained in detail in Section C below the system may represent a collection of menus that are accessible through a set of workspaces. Each workspace has a fixed positional relationship with respect to each other workspace. The user may access desired information and or functionality by navigating to an appropriate workspace and associated menu.

According to one feature the user may perform all of the above screen interaction tasks with a single hand e.g. using the thumb of the hand that holds the smartphone. The system can also provide audible and or haptic feedback cues as the user interacts with the smartphone s touch sensitive surface. Collectively all of these features reduce to extent to which the user needs to divert his attention from the environment while interacting with the system . For example the user may keep his eyes on the road on which he is walking while interacting with the smartphone. A person having impairment of his sight can also successfully interact with the system due to the above summarized non visual characteristics.

As will be described in Subsection C.3 the user may also manually interact with the system via voice instructions. In addition or alternatively the user may manually interact with the system via input mechanisms provided by the headset as described in Section B . The system may provide yet other mechanisms by which the user may manually interact with the system .

The user may also invoke special modes for use in exploring his immediate environment. For example in response to activating an explore mode the system can determine the current focus of attention of the user which may correspond to the direction at which the user is presumed to be looking at the current time which in turn may be determined based on one or more orientation determination mechanisms . The system can then determine the contextual IOIs that are encompassed by or otherwise associated with a subspace formed around the user s direction of attention. The system can then read off these contextual IOIs by announcing these contextual IOIs using three dimensional sounds and or by sending visual messages for presentation on the user s smartphone etc.

Some of these contextual IOIs may pertain to real objects in the environments having respective real positions within the subspace. These IOIs serve principally to identify or mark the locations of these physical entities in physical space. Other contextual IOIs may be virtual in nature in that they pertain to the subspace given the user s current context but may not directly describe objects in that subspace. In other words these other contextual IOIs convey information and or an experience that relates to the user s current context beyond that of identifying the locations of physical entities.

To cite one example a contextual IOI of the virtual variety may correspond to a message Remember our fallen troops as the user approaches a military barracks. That message is related to the barracks but cannot be accurately said to simply mark the location of the barracks. The intent of that message is to create a mental association to enrich the user s experience as he approaches the barracks. Alternatively the contextual IOI in that circumstance may correspond to a song or an excerpt from an inspirational speech or a personal message that has particular meaning to the user as previously specified and loaded into the system by the user .

The same is true for other types of IOIs other than contextual IOIs . That is some IOIs serve mainly as labeling tags while other IOIs are intended to stimulate cognitive associations memories emotions etc. The latter group of IOIs is referred to as virtual herein in the sense that they pertain to a realm of associations experiences meanings etc. that is not a surface level transcription of events and objects in the environment. Such IOIs could alternatively be referred to as inferential suggestive relational etc. IOIs.

According to another feature in response to activating an orientation mode the system can perform a complete 360 scan around the user to identify all items of interest that are associated with a prescribed distance from the user. The system can also perform this 360 degree scan for successive levels in the vertical dimension e.g. to determine stores provided on different levels of a mall complex or the like. The user can customize the behavior of the explore mode and the orientation mode in any manner described above e.g. by changing the types of IOIs that are identified the dimensions of the space that is searched for the presence of the IOIs and so on. In addition the user may interact with the system to govern the manner in which the system reads off the IOIs.

Now assume that at time t the user spontaneously decides to veer off from the planned route to visit the store e.g. to purchase a sandwich. When in the store the context c of the user at this time pertains to a store environment. Hence the system may perform the same functions as described above but now in the context of the indoor environment of the store. For example the system can automatically determine contextual IOIs as the user traverses an isle of the store and announce those contextual IOIs to the user. For example upon approaching the dairy section of the store the user may receive a message that reads Milk cheese and yogurt 20 feet ahead. The system may send progressively more detailed information as the user draws closer to products that may interest him. Again some of the contextual IOIs can have a less direct correspondence with physical objects in the store e.g. as in a message that is delivered in the soup section which alerts the users of the presence of high sodium in many soups.

The user may also manually interact with the system within the store environment in any manner. For example the user may manually explore different menus associated with different products. The user may also use the smartphone to perform various transactions in the store environment such as purchasing an item researching an item etc.

In some implementations the system can determine the location of the user within the store by determining whether the user is within the range of one of a set of beacons which have been placed in advance at different locations within the store . As will be described in Section E the beacons may have non overlapping ranges.

Upon leaving the store the system may recalculate the user s journey to lead the user back to the planned route . For instance the system may provide the user with instructions that allow the user to reach the waypoint w associated with a shuttle stand. Upon reaching that waypoint the system may then deliver information that is relevant to the user at this juncture such as by announcing the expected time of arrival of the user s shuttle . That information may be delivered as one or more journey IOIs.

The system may continue to provide services to the user while he is travelling on the shuttle . For example the system may notify the user of the expected time of arrival at the ultimate destination i.e. waypoint w the user s doctor s office . The system can also provide other messages that may be of use to the user depending on the nature of the public or private transportation on which the user is traveling. For example when riding on a bus and approaching a final destination the system can alert the user to the existence of a high curb that he is expected to encounter upon exiting the bus. Further the system can with the user s permission alert the driver of the bus that a person requiring assistance will be disembarking at an upcoming bus stop.

In summary throughout the user s journey the user may receive a large quantity of information in audible form e.g. in the form of spoken messages other sounds three dimensional sounds and non three dimensional sounds etc. The system may use various techniques to manage the presentation of this information some of which have been already mentioned above such as the ability to dial back or dial up on the quantity of information that is delivered . This feature allows the user to receive the most relevant information in a timely manner without overwhelming the user with too much information.

For example the system may play back sounds during a journey subject to different rules to address the situation in which the delivery of one sound potentially interferes with the delivery of another sound. According to one illustrative rule the system will play the beat sound in a continuous loop to steer the user in a desired direction e.g. while walking. The system however may temporally disable this sound or reduce the volume of this sound compared to a normal state of this sound when any other sound is being played. This enables the user to hear the other sounds without interference from the beat sound which is considered a low priority sound.

According to another illustrative rule the system may unconditionally play sounds that represent interface related events such as flick gestures which change which menu or context is presented to the user. To avoid overloading the user with too much audio information these types of sounds may be designed to be short and distinct. A user may control the playback of these types of cues at least to some extent by temporally suspending his or her interaction with the system because no interaction cue will be produced if the user is not actively interacting with the system .

According to additional illustrative rules the system can prioritize navigational sounds by assigning the highest priority level to warning sounds e.g. for warning IOIs the next highest priority level to journey information e.g. for journey IOIs and the next highest level to contextual information e.g. for any type of contextual IOIs . In some cases the system will delay the delivery of information e.g. because more critical information is being played back. Further in some cases a delayed message will no longer be relevant by the time that the system is capable of presenting it e.g. because the user has moved to a new context in which the information is no longer relevant if so the system may refrain from presenting that information.

In conclusion the above described scenario is also useful in highlighting some of the advantageous technical effects of the system . Generally the system allows any user to receive guidance that serves different but related goals. First the system attempts to expose the user to the most useful information at any given time along his or her journey thus empowering the user to more effectively navigate within his or her environment or to achieve other objectives. Second the system enriches the user s experience of the environment beyond providing navigational assistance allowing the user to learn new information about the environment that may not be immediately apparent to the user without the use of the system in this regard the system allows the user to metaphorically delve beneath the surface of the environment to understand formerly hidden aspects and connections which pertain to the environment . Third the system attempts to provide this useful information to the user in a manner which minimizes the distractions placed on the user. The third goal is useful to provide a more enjoyable and useful experience to the user e.g. by allowing the user to maintain primary focus on his or her interaction with the real world not on the tools which he or she is using to interact with the real world. Stated in the negative the third goal attempts to reduce the anxiety that may occur by asking the user to interact with a cumbersome and complex tool to which the user would be expected to devote significant attention. The third goal also allows the user to efficiently and quickly access desired information in a safe manner without being overwhelmed at any given time with too much information.

A number of technical features contribute to the above summarized goals particularly with respect to the third goal. The features include but are not limited to a the use of a one handed interaction experience b the use of a user friendly menu structure that accommodates gestures that can be performed on a touch sensitive surface in a location agnostic manner to be described below c the use of a user friendly and easy to learn workspace structure which provides access to a safe home workspace to be described below d the use of multiple mechanisms to enter commands e.g. via the headset user device voice recognition etc. e the use of audio information and or haptic cues to convey information without unduly disrupting the user s focus on the journey f the use of three dimensional and non three dimensional sounds to help steer the user in a desired direction without inundating the user with complex instructions or to alert the user to the location of IOIs and so on.

The above advantages apply to any user of the system . The system may also be successfully used by people with any type of condition that impairs their ability to make journeys. These users may include users having partial or full loss of sight users with cognitive or other psychological impairments users having mobility related handicaps and so on. For these users the system acts as a virtual guide dog assisting the user in each stage of their journey in a safe manner or otherwise assisting the user in interacting with their environment whatever the user s goal happens to be at the moment. For these users in addition to the above summarized general benefits the system also allows the user to access information and guidance that would not otherwise be available to them thus potentially improving the mobility confidence and general quality of life of these users.

Advancing now to this figure shows a high level overview of computing functionality that may be used to implement the system of . The functionality of is shown in a device agnostic manner. In an actual implementation the functionality may be allocated for instance to any of the components introduced in or any combination of these components. For instance shows that the functionality includes a space interaction SI module . The SI module performs all or most of the functions described with respect to the scenario of . Some parts of the SI module may be implemented by the user device while other parts of the SI module may be implemented by processing components located on the headset . In addition or alternatively some parts of the SI module may be performed by the remote processing resources .

The SI module may receive input information from any combination of input mechanisms and may provide its output information for presentation on any output mechanisms . For example the input mechanisms may include one or more orientation determination mechanisms and or one or more motion determination mechanisms and or one or more position determination mechanisms and so on.

The orientation determination mechanism s determine the orientation of the device which incorporates these mechanism s . For example if housed by the user device the orientation determination mechanism s determine the three dimensional orientation of this user device . If housed by the handset the orientation determination mechanism s determine the three dimensional orientation of the headset . More generally stated the orientation determination mechanism s may determine the direction that the user is pointing his smartphone or turning his or her head on which the headset is placed . The motion determination mechanism s determine the nature and degree of movement of the device which incorporates these mechanism s . The position determination mechanism s determine the absolute and or relative position of the device which incorporates these mechanism s .

The mechanisms can be implemented using any combination of sensors including but not limited to magnetometers accelerometers gyroscopic sensors gravity based sensors torque sensors strain gauges flex sensors optical encoder mechanisms and so on. In addition some of the mechanisms may receive signals from external systems or sources. For example the mechanisms may include a sensor for determining the position of a device based on signals received from a satellite based navigation system e.g. a Global Positioning System GPS system . In addition or alternatively the mechanisms may include functionality for determining the position of a device by performing triangulation and or other processing based on signals received from plural external sources such as signals received from plural radio towers and or localized directional antennas etc. In addition or alternatively the mechanisms can include functionality for determining the position of a device using a dead reckoning technique. In addition or alternatively the mechanisms can include functionality for determining the position of a device by processing information from local beacons e.g. Wi Fi and or BLUETOOTH beacons etc. and so on.

The input mechanisms may also include any combination of manual input mechanisms . These mechanisms can include any of key input mechanisms touch sensitive input mechanisms e.g. a touch sensitive screen joysticks microphones e.g. for receiving voice instructions video cameras and or depth cameras e.g. for receiving free space gestures and so on. For example in the case of the user device may use a touch sensitive display screen as its primary manner of interacting with the user. For instance without limitation that touch sensitive display screen may incorporate a capacitive touchscreen mechanism that determines when a user touches and or hovers above the screen. The user device may also include a camera a microphone etc. The headset may include a microphone for receiving voice instructions together with one or more dedicated input mechanisms e.g. as implemented as buttons on the side of the headset to be described in greater detail in the next section .

The output mechanisms can incorporate one or more audio output mechanisms one or more display output mechanisms one or more haptic output mechanisms and so on. For example the audio output mechanism s can correspond to conventional speakers of any type. In addition or alternatively the audio output mechanism s can incorporate bone conducting audio devices e.g. as provided by the headset such as bone conducting transducers produced by AFTERSHOKZ LLC of Syracuse N.Y. The display output mechanism s may correspond for instance to a LCD type display e.g. as provided by the user device . The haptic output mechanism s may correspond for instance to a vibration producing mechanism e.g. as provided by the user device and or the headset etc. . A vibration producing mechanism may achieve a vibration effect using a rotating off balance weight and or by some other mechanism s .

The SI module may also interact with remote functionality that may be considered external to the SI module per se. For example the SI module may interact with a search engine for the purpose of conducting a search. For example the search engine may correspond to the BING search engine provided by MICROSOFT Corporation of Redmond Wash. In addition or alternatively the SI module may interact with a journey computation engine for the purpose of generating a new journey and or modifying an existing journey. In addition or alternatively the SI module may interact with a speech processing engine to interpret the spoken instructions made by the user. For example the speech processing engine may correspond to the CORTANA system provided by MICROSOFT Corporation of Redmond Wash. In other cases one or more aspects of the remote functionality may be incorporated in the SI module as native resources.

In addition the SI module may interact with any external information provided in one or more data stores . For example the external information may provide publically accessible map information transportation schedule information alert information business and personal directory information social network information calendar information etc. and so on. In some cases the SI module may interact with external sources e.g. external websites using application programming interfaces APIs provided by those external sources.

Now referring to this figure shows one implementation of the SI module which was introduced above. From a high level standpoint the SI module may include or may be conceptualized as including a plurality of sub components that perform different respective functions. Further some sub components may rely on the results generated by other sub components. An application interface module AIM allows the user to interact with any of the sub components. For example the application interface module may provide menu functionality which exposes various functions provided by the sub components.

Generally referring to the sub components from top to bottom the SI module may include various data stores for storing information that may be used by the SI module in performing its functions. For example a data store may store information which defines one or more journeys. For example the journey information may describe the waypoints in the journey and or any other information regarding the journey. A data store may store search results provided by a search engine those results may be produced upon direction of the user during the course of the journey or a user s more general interaction with a space. A data store may store a history of tabs created by the user in the course of the user s interaction with the SI module . A tab generally corresponds to a bookmark to a menu or other item of information and or functionality and or option. A user may create a tab when he or she visits that menu or other item or information and or functionality and or option in one implementation the data store initially contains no tabs when a user embarks on a journey and has not yet started to interact with the system . The system may use any manner of representing a collection of tabs e.g. as a list of tabs a radial menu of tabs etc.

The SI module can also provide various supporting modules that perform any type of support services. For example a setting module may allow a user to assign a value to any parameter which affects the operation of the system . A data store may store all such settings. The supporting modules may also include a portal for interacting with an external search engine to provide search results. Or the supporting modules may include a natively provided search engine.

A sound generation module performs various operations relating to the generation of sounds. For example the sound generation module may play particular sounds when various triggering circumstances are encountered. Some triggering circumstances correspond to actions made the user when interacting with the application interface module . Other triggering circumstances correspond to changes in the state of the system that are not directly caused by the user s interaction with the system . Other trigger circumstances correspond to events which occur during the course of a journey or more generally the user s interaction with a space and so on. A data store may store files which when played back produce the desired sounds. Subsection C.2 below provides additional information regarding different types of sounds that may be generated and the circumstances under which these sounds are played.

Some sounds are non three dimensional or non spatial in nature. In addition the sound generation module can produce three dimensional audio information. As described above the audio information is three dimensional in the sense that a user will perceive this information as emanating from one or more locations within a three dimensional physical or virtual environment. The sound generation module may create these sounds by transforming original sound information using one or more Head Related Transfer Functions HRTFs .

Similarly a haptic cue generation module can produce different types of haptic feedback experiences in different triggering circumstances. In one case the haptic cue generation module produces signals which produce vibration cues e.g. when delivered to the user device the headset and or some other device.

A path guidance module uses the sound generation module to generate the above described three dimensional periodic e.g. beat sound. The purpose of this periodic sound is to guide the user in a particular direction. The path guidance module produces this effect by determining the current actual heading of the user the desired heading and the difference between the actual and desired headings corresponding to deviation information . Then the path guidance module then leverages the sound generation module to produce an appropriate looping sound that is perceived by the user as originating from a particular direction. That is the sound generation module produces the looping sound based on the deviation information fed to it by the path guidance module . The user may respond to this sound by moving in the desired direction. In another case the beat sound may be perceived as travelling across a series of locations in physical space. The user may interpret this experience as an instruction to move in the direction of the traveling sound.

A beacon based guidance module provides assistance to the user in navigating within an indoor and or output space by means of the detection of signals emitting from a collection of beacons having in one illustrative implementation respective non overlapping ranges. Section E provides additional information regarding the operation of the beacon based guidance module . The beacon based guidance module may consult beacon information provided in a data store . The beacon information may describe the codes associated with the beacons that have been placed in the environment and their respective locations within the environment.

A relevant information determination RID module performs the general function of determining relevant information to present to the user at any given time. In context of the description of the RID module determines different types of items of information IOIs that have a bearing on the user s current context. To perform this task the RID module receives various contextual inputs that define the context of the user at the present time. These contextual inputs may describe the current location of the user the current heading of the user the current goals of the user and so on. The contextual input may also describe the environment itself such as objects and events associated with the environment both physical and virtual. Any such inputs may be mined from map information directory information social network information calendar information etc. The contextual inputs may also describe environmental factors which affect the user s interaction with the space such as public transportation information weather information etc. as obtained from any source s .

The RID module operates by determining whether it is appropriate to notify the user of any information at a given time based on the contextual inputs at the present time and based on various rules provided in a data store . The behavior of the RID module may also be defined by one or more parameters set by the user and stored in the data store . For example the RID module may determine whether there are any contextual IOIs in proximity to the user at the present time based on a depth range defined by the user. If such contextual IOIs exist the RID module can interact with the sound generation module and or menu functionality provided by the application interface module to notify the user of these contextual IOIs.

An exploration module and an orientation module perform respective services that may be invoked in an on demand manner by the user. As described with reference to the scenario of the exploration module determines any contextual IOIs that are associated with a subspace that lies in front of the user which in turn may be determined by the position of the user s head together with a setting parameter which defines the depth of investigation and a setting parameter which describes the span of the search space . To perform this task the exploration module leverages the services of the RID module . The exploration module then notifies the user of the contextual IOIs using three dimensional sounds displayed messages etc. The orientation module performs a similar task to the exploration module . But instead of investigating IOIs associated with a subspace that projects out in front of the user the orientation module can scan the entire three dimensional space which exists around the user at the present time.

The application interface module may include various components that interpret the input of the user e.g. to determine the nature of an instruction that the user is making. For example a gesture interpretation module may determine a gesture that the user has made by interacting with the touch sensitive screen of the user device or a free space gesture etc. The gesture interpretation module may perform this task by comparing the marks or touches or hovers or other behavior made by the user with a data store that identifies patterns associated with known gestures. If the behavior of the user matches a pattern associated with a particular gesture with a determined degree of matching confidence then the gesture interpretation module may conclude that the user has made that gesture.

A voice interpretation module may interpret spoken instructions by the user e.g. which may be received via a microphone on the user device and or the headset . In one case the voice interpretation module may correspond to a portal to the remote speech processing engine e.g. the CORTANA system . In another case the voice interpretation module may correspond to any type of native functionality for interpreting spoken utterances. In any event the agent which performs voice recognition can use any technology to perform this task such as Hidden Markov Model based technology neural network based technology etc.

A headset button interpretation module interprets the manner in which the user interacts with the input mechanisms of the headset to be described below . For example in some cases a set of buttons can perform different functions depending on the manner in which the user interacts with them e.g. depending on whether the user touches a button but does not press it or based on whether the user touches and releases the button one or more times or based on whether the user presses and holds the button etc. The headset button interpretation module maps the user s behavior to a particular instruction.

An action taking module may invoke an action based on the interpretations provided by the above described interpretation modules . For example in response to the interpretation the action taking module may invoke a menu close a menu transition between workspaces to be described below perform a function save a setting present an information item and so on.

Referring first to the headset this device may provide a frame made of plastic and or metal and or any other material. The frame may be flexible and may be secured to the user s head via tension in the frame which pushes laterally inward against the user s head and or through some other securing mechanism. The headset includes transducers which transmit vibrations to the bones of the user s head the bones of the user s head then transfer these vibrations to the user s eardrums where they are perceived as audio information. The use of a bone conduction type headset prevents the headset from occluding the user s ear canals and thereby allows the user to safely respond to other sounds in the environment. However alternatively the headset may include conventional speakers that are placed over or near the user s ears.

The headset may also optionally include a set of input mechanisms anywhere on its frame. The user may interact with the input mechanisms with one or more fingers of his or her hand or hands . Alternatively a separate device not shown may provide the input mechanisms and that separate device may communicate with the headset via wireless communication e.g. BLUETOOTH communication and or wired communication. shows that the input mechanisms include three buttons but more generally the input mechanisms can include any number of mechanisms and these input mechanisms can be placed on the frame in any manner. Further the input mechanisms can include other types of input mechanisms besides buttons such as wheel or knob mechanisms slider mechanisms etc. Alternatively or in addition the headset may incorporate one or more touch sensitive surfaces. For example different regions of the headset may incorporate different touch sensitive mechanisms and those regions may be associated with different respective functions.

The headset may include processing mechanisms which perform various tasks to be described below . The headset may include a compartment which houses those processing mechanisms. For example in one case the compartment lies in the back of the headset . However the processing mechanisms can be physically located at any location or locations on the headset . The processing mechanisms themselves may include one or more processing devices of any type memory etc. and or dedicated logic components e.g. one or more application specific integrated circuits ASICs etc.

The input mechanisms can initiate any operations when activated. For example without limitation the user may use the input mechanism to instruct the SI module to invoke the explore mode or the orientation mode described above . In addition or alternatively after hearing a summary of a certain topic e.g. the name of a contextual IOI a user may use the input mechanisms to instruct the SI module to provide additional information regarding the identified topic this instruction may be referred to as a more information instruction.

In addition or alternatively the user can use the input mechanisms to instruct the SI module to activate a listening mode or to stop the listening mode. In the listening mode the voice interpretation module processes the user s speech to determine whether the user has spoken an instruction.

In addition or alternatively the user can use the input mechanisms to instruct the SI module to repeat the most recent audio message that it has provided. In addition or alternatively the user can use the input mechanisms to request the SI module to repeat a set of previously delivered audio messages e.g. by starting with the most recent message and advancing back in time message by message for a predetermined number of prior messages. Such an instruction may be referred to as a rewind instruction.

Alternatively or in addition the user may use the input mechanisms to turn the three dimensional beat sound on or off and so on. Other implementations can use the input mechanisms to issue other instructions and or to omit one or more of the instructions set forth above.

The above functions or some subset thereof can be mapped to any number of respective buttons and or other input mechanisms in any manner. Further in some implementations the system may include a customization module that allows a user to define the mapping between input mechanisms and operations which the mechanisms invoke. Further as described above the same button can also perform two or more functions depending on the manner in which the user interacts with it. In one implementation the SI module may announce the function performed by a button when the user touches it but does not press it.

According to another feature the user may deactivate a function that is currently in active state by pressing the same button that was used to activate it. For example the user can stop the SI module from announcing information by pressing the same button that was used to ask the SI module to deliver this information etc. Alternatively or in addition the headset can incorporate a dedicated button which stops whatever function is currently being executed.

The user device may correspond to any of the types of portable devices described above such as a smartphone or a tablet type computing device. As described above a user may interact with the user device using a single hand and or optionally with two hands . The other user computing device may correspond to any type of traditionally stationary computing device such as a workstation computing device a game console a set top box device etc.

One or more communication paths may couple the user device with the headset . Such a communication path for example may correspond to a BLUETOOTH communication path a hardwired communication path e.g. a USB communication path etc. One or more communication paths may couple the other user computing device to the user device and or the headset . As described above one reason that a user may wish to establish a connection between the other user computing device and the user device and or the headset is to upload information to these devices. The communication paths can be implemented in any manner e.g. via any type of wireless and or wired communication path described above.

Other implementations of the system can use different types of headsets e.g. compared to the particular type of headset shown in . For instance in another implementation a headset can incorporate any of the above identified features including the input mechanisms together with a head mounted display HMD device of any type e.g. as physically implemented as eyewear e.g. goggles a helmet etc. The system may display any type of information using the head mounted display device. For example the system may display any of the menus and other information described in the next section Section C via the head mounted display device or some subset thereof. In another case the system may display computer generated information that is mixed with information associated with the real world with which the user interacts to thereby provide an augmented reality experience. For example the system can display descriptive tags in positional proximity to associated objects and events within the field of vision of the user. In another case the system can display directional prompts which assist the user in moving in a recommended direction and so on. Moreover the system can modify the type of information that is displayed to accommodate any visual impairments that may affect a particular user e.g. by displaying simplified and enlarged information for users with impairments in sight.

The system can achieve an augmented reality experience using any technology e.g. through the use of an optical mixer which displays computer generated information over the user s direct visual perception of the actual environment e.g. using partially reflective mirrors or the like . In another implementation the system can use a video camera to capture the actual environment together with an optical mixer which mixes video information from the video camera with computer generated information. In either case the system can determine the user s presumed field of view using one or more devices which detect the location of the user within the environment one or more devices which detect the position and orientation of the user s head and or one or more devices which detect the direction of the user s gaze etc.

In another case another type of wearable device besides a headset or in addition to a headset can perform any of the functions set forth above. For example the wearable device may correspond to a wrist mounted device an item of apparel etc.

To simplify the explanation the following description will assume that the user interacts with the headset of the basic type shown in although to repeat the headset can incorporate any number of supplemental features mentioned above such as a head mounted display . Further another type of wearable device can be used instead of a headset or in addition to a headset.

On the other hand the headset includes one or more audio output mechanisms such as a bone conducting audio mechanism a power source such as a battery and one or more communication mechanisms for communicating with the user device . In addition the headset may include any of the type of input mechanisms described above with reference to e.g. corresponding to the input mechanisms of . The communication mechanisms can transmit instructions invoked by the input mechanisms to the user device . The user device in turn can send audio information to the headset for presentation by the audio output mechanism s .

More specifically the headset in the case of may include a headset side SI module that performs any subset of the functions described above with reference to . In addition the headset may include any orientation motion and or position determination mechanisms . These mechanisms are generally referred to below as head sensing mechanisms because they determine the physical posture or movement of the user s head which may in turn reflect the direction of the user s focus of attention. In addition the headset may include any of the type of input mechanisms described above with reference to . In addition the headset may include a power source and one or more communication mechanisms for interacting with the user device if the system makes use of such a user device in this implementation which it need not . In addition the headset includes any type s of audio output mechanism s described above.

In one mode of operation the system of can make use of the head sensing mechanisms provided on the headset when they are provided and working properly. The head sensing mechanisms provided by the headset may be preferable to the counterpart sensing mechanisms provided by the user device because the head sensing mechanisms may more accurately reflect the orientation movement and or position of the user. Further the use of the head sensing mechanisms eliminates the need for the user to interact with the user device to register his or her orientation movement and or position.

In some cases the headset may optionally forward the information generated by the head sensing mechanisms to the user device . The user device may use this information to perform processing and then forward the results of its processing back to the headset . The headset may then transmit sounds to the user s ears via a bone conducting technique that convey the results. In another case the headset side SI module can perform this processing without the need for forwarding the information provided by the head sensing mechanisms to the user device . In still another case the headset side SI module can natively perform some operations and rely on the user device to perform other operations. For example the headset side SI module can rely on the user device to perform computationally intense operations such as the calculation of three dimensional sounds etc. because those operations can be more efficiently performed on the user device compared to the headset .

In another case the system may make use of the orientation determination mechanism s motion determination mechanism s and or position determination mechanism s provided by the user device when the counterpart components are not working properly on the headset .

In still another case the system may leverage the sensor readings provided by both the headset and the user device e.g. by using sensor readings of one device to identify obvious errors in the sensor readings of the other device and or to form an average of the two different versions of the sensor readings and so on.

In either of the implementations of at least some of the functions performed by the headsets e.g. and or the user devices e.g. can be alternatively or in addition performed by the remote processing resources of e.g. corresponding to cloud computing resources .

In the case of the user has mounted the user device using a mount onto the dashboard region of a vehicle. The user may be the driver of the vehicle as shown in or a passenger. A power cord may deliver power to the user device from a power outlet provided by the vehicle.

In summary the above features contribute to the above goal of allowing the user to safely and efficiently move through his or her environment. For instance the features provide a convenient way by which the user may activate various operational modes e.g. by interacting with the user input mechanisms of the headset without unduly distracting the user as the user moves through the environment e.g. without requiring the user to access and interact with a separate user device.

C. Illustrative User Interface Functionality for Facilitating Interaction Between Users and their Environments

This section provides illustrative details regarding one manner of operation of the application interface module of . To repeat the application interface module allows the user to interact with the various components of the SI module . To facilitate repeated reference to the application interface module this section will make reference to this component in abbreviated form as the AIM . The AIM may refer to a discrete component or to an aggregation of functions performed by two or more components in an actual implementation of the system .

The user interface experience in turn has different components or aspects. Subsection C.1 below provides details regarding one illustrative manner in which the AIM allows a user to interact with the SI module via a visual user interface presentation e.g. as provided by the user device . Subsection C.2 provides details regarding one illustrative manner in which the AIM may provide various sounds and haptic feedback cues to the user. Subsection C.3 provides details regarding one illustrative manner in which the AIM allows the user to interact with the SI module via spoken instructions.

Generally note that the following explanation describes the AIM in the context of a journey taken by a user through space e.g. as in the example of . But the AIM provides a similar service in other scenarios in which a user interacts with a space including the case in which the user wanders through the space without a prepared route for the purpose of exploring the space in an ad hoc manner. As a further note the user interface features described below are also general purpose in nature and thus can be applied in other contexts that are not necessarily related to a user s interaction with a space.

As another prefatory note the description sets forth many gestures interpreted by the AIM by describing gesture related actions performed by a user together with corresponding operations taken by the AIM in response to the gesture related actions. As more fully described above with reference to the AIM performs this task in each case by a detecting input information which describes a gesture performed by the user for example when the user makes a telltale flick gesture etc. b comparing the input information with stored patterns associated with known gestures to determine the particular gesture that the user has invoked and c executing the operations associated with the detected gesture. Explicit recitation of these individual operations is omitted in many cases below in the interest of brevity.

In one approach the AIM organizes a master workspace into a collection of smaller workspaces. The workspaces have positional relationships with respect to each other. This structure is beneficial because it allows the user to develop a mental picture of the organization of the application much like he or she would become familiar with the zones of a physical space through repeated encounters with its zones. This feature in turn allows the user to efficiently access the information and or functionality being sought.

For instance shows an example in which the AIM organizes a master workspace into five smaller workspaces. A home workspace serves as the central focus in the user s interaction while conducting a journey. For example as will be described below the home workspace presents information regarding the current context of the user at each particular point in the journey or any other interaction with a space. If the user has not yet started the journey the home workspace may present a default hub page.

A main workspace lies to the left of the home workspace and a nearby me workspace lies to the right of the home workspace . An information workspace lies to the top of the home workspace while a settings workspace lies to the bottom of the home workspace . The respective roles of these workspaces will be described in greater detail below.

The position of each workspace described above is set forth in the spirit of illustration not limitation. Other implementations can provide other placements of the workspaces. Further other implementations can vary the number of workspaces that are provided by the AIM . For example another implementation can provide another four workspaces by providing workspaces that are diagonally positioned relative to the home workspace .

In a similar manner the user may execute a flick left gesture on the home workspace to navigate to the nearby me workspace . The user may execute a flick down gesture on the home workspace to navigate to the information workspace and a flick up gesture on the home workspace to navigate to the settings workspace . The user can perform any of these gestures for example using the thumb of the hand that holds the user device e.g. by placing the thumb on the touch sensitive surface of the user device and flicking it in a desired direction. In a flick movement the user makes contact with the touch sensitive surface with one or more fingers moves the finger s for a distance across the surface and then removes the finger s from the surface all in relatively rapid succession as if flicking a physical object across a flat surface .

In one implementation in order to navigate to any peripheral zone the user is expected to first navigate to the home workspace . However in another implementation the AIM may allow the user to navigate from one peripheral region to another without first moving back to the home work space .

More generally in all examples in this section a dashed line circle on the surface of a display represents the point at which the user contacts the surface with a digit. In some cases the surface of the display will show two dashed line circles of different sizes. The larger of the two circles represents the location at which the user applied his or her digit while the smaller of the two circles represents the location at which the user removed his or her digit. In other cases not shown a user may perform at least one gesture that involves simultaneously touching the touch sensitive surface at two or more locations e.g. to execute a pinch type gesture . In other cases not shown a user may perform at least one gesture by hovering above the surface of the device without actually touching it. In other cases not shown a user may perform a free space gesture which may be detected by a video camera and or a depth camera etc.

As a final introductory note this section describes particular gestures menu structures and menu items all in the spirit and illustrative not limitation. Other implementations can vary any aspects of these user interface features. For example in another implementation not illustrated the AIM could allow the user to transition among workspaces using a tap type gesture or by drawing a particular shape on the surface of the display etc.

In the course of the user s journey the home workspace may present information regarding the user s current context although other information can also be presented in the home workspace during the journey upon request by the user . For example in the case of assume that the user is currently riding on a train. The home workspace may present a menu that focuses on the user s experience in the train. In the case of assume that the user is now walking down a particular street. As a default the home workspace will now present information regarding the user s experience on the street. For example the home workspace can present a map that shows the user s current location on the street. Alternatively the user may interact with the AIM to produce the type of menu shown in but where that menu will now contain menu items that pertain to the user s experience while walking on the street. By virtue of the above behavior the AIM surfaces the information that is most relevant to the user in his or her current context in the environment and presents that information within the home workspace the user can thus readily find and consume the most relevant information without having to hunt for it e.g. without having to navigate through a cumbersome menu structure. Such a feature also reduces the distractions placed on the user and thus contributes to the safety of the system .

The user may nevertheless interact with the AIM to activate information for presentation in the home workspace that does not pertain to the user s immediate surroundings. For example the user may activate a tab menu e.g. by activating a tab menu option to be described below. In response the AIM will present the tab menu shown in . The tab menu shows a collection of open tabs corresponding to previously opened menus. Some of these tabbed menus may correspond to information regarding prior journey segments that the user has already completed. The user may activate any such tab related menu item. In response the AIM may represent information regarding a prior journey step in the home workspace e.g. in the form of a menu or some other format. The menu structure may also allow the user to examine future journey steps upon request by the user that have not yet been encountered on the journey.

Further even after having embarked on a journey the user may instruct the AIM to return to the hub menu shown in e.g. so that the user may access the information and or functionality specified in that default hub menu.

As a further clarification in the example above the current context pertains to a physical location or segment in the user s overall journey. In other cases the home workspace can present context information regarding the user s exploration of a virtual space. For example the user can navigate within a hierarchy of products offered by a store. In that scenario the home workspace may present a menu that pertains to a group of products an individual product etc.

Generally speaking the above described types of menus shown in correspond to primary menus. The AIM displays these types of menus in the home workspace . In contrast the AIM presents secondary menus in the other workspaces that lie on the periphery of the home workspace . show illustrative secondary menus that may be presented in these peripheral workspaces.

For example shows a main menu for presentation in the main workspace . The main menu identifies actions that are commonly used when interacting with different aspects of the SI module . shows a settings menu for presentation in the settings workspace . The settings menu allows the user to change various parameters which affect the operation of the SI module . shows an information menu for presentation in the information workspace . The information menu presents convenient access to system information such as remaining battery life signal strength. The information menu also provides a convenient portal to notifications and other useful warning information and journey information such as real time updates from public transport services.

In the particular case of the overlay menu allows the user to change the level of contextual awareness provided by the SI module . For example through this menu the user can set a parameter value which determines the quantity of information that is forwarded to the user. In one case the user may opt to receive all information that is potentially relevant to his or her current context. In another case the user may opt to receive only warnings and other information items that are deemed of high importance.

Alternatively the user may make a double tab gesture. In response to detecting this gesture the SI module can present more detailed information regarding the user s current context again as a spoken message. For example upon a double tap the SI module can announce the menu items in the menu shown in .

Next assume that the user performs a triple tap gesture. In response to detecting this gesture the SI module can announce instructions that inform the user how to interact with the type of menu shown in if that menu in fact pertains to the user s current context. For example the instructions can inform the user how to activate the menu how to navigate within the menu how to select menu items within the menu and so on.

Advancing to this figure shows how a user may activate a menu and then how the user may subsequently interact with the menu. In one non limiting case the user may activate the menu shown in by making a tap and hold gesture anywhere on the surface of the touch sensitive display surface of the user device . For example as shown in a dashed lined circle indicates the location at which the user has tapped and held his or her thumb on the surface of the touch sensitive surface.

In response to detecting this gesture the AIM presents the menu . The menu is centered at the point corresponding to a display location on the surface at which the user touched the surface corresponding to a touch location . The user may find this feature useful because it eliminates the need to hunt for a particular element in a user interface presentation for the purpose of activating the menu . Rather the user may execute the tap and hold gesture anywhere on the surface. This feature in turn reduces the distractions placed on the user in using the system . But not all locations will result in the successful activation of the menu . For example if the user taps too close to the top or bottom of the surface the AIM may optionally present an error message to the user e.g. as a spoken message asking the user to repeat his or her tap and hold gesture closer to the middle of the surface.

The menus illustrated thus far have a particular uniform menu structure. The menu shown in has the same structure which will now be described below with the caveat that this structure is set forth in the spirit of illustration not limitation. Other implementations can adopt other menu structures and behaviors. For example this subsection will close with an example of an implementation which adopts a different menu structure compared to the menu of and which exhibits a different menu behavior. Further the menus described herein present their menu items in the form of linear lists but other menus can be used e.g. radial or pie menus etc. which present their menu items in other ways.

The menu structure shown in has two groupings of menu items separated by a marker item . More specifically a first grouping of menu items presents menu items that are particularly pertinent to the user s current context or task at hand. A second grouping of menu items presents menu items that are relevant to multiple types of menus that may be presented in different workspaces and therefore may be referred to as global or general menu items. For example the stop listening command which is a menu item in the second grouping is relevant across different menus and workspaces whereas the menu item item A which is a menu item in the first grouping may be chosen because it is particularly relevant to whatever task that the user is attempting to accomplish by interacting with the menu .

The second grouping however may omit certain global choices that are not relevant for a particular menu. For example if the user is already viewing a tabs menu the second grouping associated with that menu may omit a menu option that allows the user to access the tabs menu because the user is already viewing that menu .

Different menu items in either the first grouping or the second grouping invoke different operations when selected. For example a first type of menu item may invoke an action when selected. For example the menu item start listening when invoked instructs the SI module to start listening for the user s voice commands. A second type of menu item may present information when invoked such as battery status information etc. A third type of menu item may present an opportunity for the user to change a property value when that menu item is invoked. For example in some cases the user s activation of this type of menu item may activate an overlay menu. The user may then interact with the overlay menu to change a property value under consideration. In another case the user s activation of this type of menu item may directly change the property value e.g. by toggling a setting from an on status to an off status or vice versa. Still other types of menu items and associated invocation behaviors are possible.

The marker item displays the message release to dismiss. This message informs the user that he or she can release their finger from the surface of the touch sensitive surface without selecting any menu item assuming that is that the user releases his or her finger while it was positioned on the marker item and not some other menu item. In response to such a gesture the AIM can display the menu in its original deactivated state. The marker item in that menu bears the message tap and hold which invites the user execute a tap and hold gesture to reactivate the menu in its active state.

Advancing to assume that instead of deactivating the menu the user decides to scroll up through items in the first grouping . The user may perform this action by moving his or her finger in the upward direction while maintaining his or her finger on the touch sensitive surface. In response the AIM can produce the menu state shown in . In particular the AIM has responded to the user s gesture may moving the menu items in the downward direction which causes a first menu item to become highlighted instead of the marker item . To select this menu item the user may release his or her finger from the menu item . Alternatively the user may deactivate the menu by moving back to the marker item and then releasing his or her finger while positioned at that item. Or the user may scroll down in the opposite direction to the second grouping of menu items. The user can release his or finger while it is positioned on one of the menu items in the second grouping to select that menu item. A user may find the above user interface behavior beneficial because he or she can interact with the user device with one hand in a seamless and fluid manner with minimal distractions placed on the user as he or she moves through the environment. For instance in this implementation the user is not required to hunt for and select a series of command buttons or menu items doing so would require the user to give special attention to the user device .

In the specific example of assume that the user scrolls up to the top of the first grouping of items to select a more items menu item and then releases his or her finger on this menu item . In response the AIM presents the menu shown in . That menu provides another first grouping of menu items that represents a continuation of the first grouping shown in the previous menu that is while the first grouping in presents menu items and the first grouping of presents menu items and . More generally the menu structure may represent a complete list of menu items as a series of linked smaller lists. Two linked lists are shown in the example of but the complete list can be formed through any number of linked lists.

The user may return to the menu state shown in by scrolling up to the previous page menu item and then releasing his or finger from this item. Alternatively to execute this back instruction as shown in the user may move his or her finger to the periphery of the touch sensitive surface e.g. as indicated by the dashed line circle and then flick inward towards to the center of the surface. The user may execute the same operation by flicking inward from the opposite edge of the surface.

Although not shown in the menu may also give the user the option via an appropriate menu item to return to the first page in a series of cascaded menu lists e.g. rather than successively moving back through the linked lists by issuing a series of back instructions. Further although also not shown in the drawings each grouping may include an end of list marker item which designates the end of a grouping. The user can deactivate the menu by moving to this item and removing his or her finger while it is placed on that item.

More generally the user may execute the type of back gesture shown in on different types of menus which may produce different types of actions. For example a back action on a root primary menu presented in the home workspace may cause the user to exit the application associated with the SI module upon confirmation by the user that this is what he or she intends to do. A back action on a transient menu that is presented in the home workspace may cause the AIM to present whatever context menu was last presented in the home workspace . A back action on an overlay menu may sometimes result in the presentation of an underlying secondary menu in a secondary workspace. For example a back action on a particular settings related overlay menu e.g. as shown in may result in the display of an underlying settings menu e.g. as shown in in the settings workspace .

More specifically the home workspace shows the default tabs information shown in when there are no active tabs. The default tabs information may provide guidance on how the user may get started to create tabs. In contrast the home workspace may display the tabs menu shown in when there are active tabs. By default the tabs menu item having the first position in the list i.e. tab No. 1 corresponds to a current context.

Upon activation of the menu in the second implementation the AIM presents the first entry in the list near the middle of the user device s display surface. The user may then move up through the list by making one or more panning gestures in an upward direction or move down through the list by making one or more panning gestures in the downward direction. When the user reaches the end of the list in either direction the list repeats. That is when the user advances past the last item in the list the user will encounter as a next entry the first item. When the user advances past the first item in the list the user will encounter the last item. Due to this menu presentation strategy the second implementation may dispense with the use of multiple page lists as used in the first implementation. That is the user navigates through a single master list using one or more panning gestures. A user makes a panning gesture by placing one or more fingers in contact with the touch sensitive surface and dragging those finger s in a desired direction of panning the movement here is slower compared to a flick gesture.

As shown in a user may select a highlighted item in the list by making a short touch and drag gesture towards the periphery of the surface. The AIM will provide audible and or haptic feedback when it has interpreted the user s gesture as a request to select the highlighted item. Then the AIM will select the item when the user removes his or her finger from the surface of the user device . Alternatively instead of removing his or her finger the user may drag his or her finger back in the opposite direction to cancel the selection operation which the AIM will again confirm e.g. by providing an appropriate cue or cues .

As also indicated in the user may draw an up arrow shape or a down arrow shape on any menu to increase or decrease respectively a level of information that is provided to the user. For example the user may scale back on the quantity of contextual information by refraining from sending the user information regarding one or more of the least critical categories of information items such as by omitting contextual information but sending warning information and journey information .

The second implementation may also execute other new gestures compared to the first implementation although not specifically illustrated in the figures. For example the user can perform a tap and hold gesture on a first menu to transition from that menu to an actions menu where the actions pertain to actions that are relevant to the context associated with the first menu . The user may perform the same gesture on the actions menu to return to the original first menu. The tap and hold gesture in this case involves a longer hold action than the tap and hold gesture that is used to generally activate any menu.

As another new gesture the user may perform a vertical flick gesture while on the menu items of a menu. The AIM will interpret this action as a request to quickly advance through the menu items in either an up or down direction depending on the direction of the flick. As another new gesture the user may perform a horizontal flick gesture on a menu item in a menu to advance to the next menu item in the list. The user can make multiple such horizontal flick features to successively advance through the list one menu item at a time.

In either of the first or second implementations corresponding to the user may execute a gesture to place the user device in a pocket mode. When the user device detects that gesture it will ignore any subsequent gestures that the user may make with the exception of a gesture that has the effect of canceling the pocket mode. As the name suggestions the pocket mode is useful in those cases in which the user wishes to stow the user device in a pocket or some other compartment such as a purse bag etc. . When active the pocket mode prevents accidental touch contact and or movements by the user as being incorrectly interpreted as meaningful gestures. The gestures used to invoke and revoke the mode can correspond for instance to telltale swipe gestures etc.

The AIM can present various sounds by leveraging the use of the sound generation module . The AIM can also present various haptic feedback experiences e.g. vibration cues using the haptic cue generation module . The AIM can produce such sounds and vibrational cues in response to different types of triggering events. For example the AIM can present these sounds and haptic cues in response to certain changes in state in response to certain interface actions taken by the user such as navigation among menu items in response to certain events in a journey and so on.

The following listing describes representative sounds and associated haptic cues and the illustrative circumstances in which they are invoked.

The AIM can repeatedly play a Loading Busy sound while the SI module is in a state in which the user is unable to interact with it. The sound reassures the user that the SI module is working but it is presently busy performing an action. For example this sound may resemble a ping pong ball bouncing up and down the frequency of the bounce may increase as the processing nears completion and may end with a flourish type sound.

This sound indicates that a secondary menu presented in one of the dedicated secondary workspace zones has moved into focus and is now being presented instead of the current context menu in the home workspace. This sound may also convey the direction in which the user has performed the corresponding gesture. For example the sound may resemble a swoosh like a gust of wind in a respective direction. Further in one case the AIM may express this sound as a three dimensional sound.

This sound indicates that the current context menu has moved back into focus and is now being presented instead of a secondary menu in one of the dedicated workspace zones. This sound may also convey the direction in which the user has performed the corresponding gesture. For example the sound may resemble a swoosh that is the counterpart of the Transition To Zone Menu swoosh but moving in the opposite direction than the Transition To Zone Menu sound. Again the AIM may optionally express this sound as a three dimensional sound.

This sound indicates that a menu is now active and can be manipulated. This sound may be described as a fade in sound that terminates with a snap in type sound. In addition the AIM can present a short vibration cue that confirms the user s intention to produce a change in state. Further still the AIM may invoke a verbal cue that announces for example menu release to dismiss. That cue informs the user that the menu is active and that the user can deactivate the menu by releasing his or her finger from the marker item.

This sound indicates the menu has been deactivated and thus can no longer be manipulated without reactivating it. The sound may be described as a fade away type sound.

The AIM may play this short sound that indicates that a gesture was recognized but that it is otherwise invalid e.g. because it is currently not supported . This sound may resemble a thud followed by a soft two tone access denied style notification. The AIM may play a more specific Cannot Go Back sound to indicate that a back gesture has been performed but it cannot be performed e.g. because it is not possible to go further back.

This sound indicates that the user has moved to a menu item in a menu. The tone of this sound depends on the position of the menu item in the menu e.g. where the tone may increase as the user moves up the list and decrease as the user moves down the list. Further in one implementation the AIM can present different scales when traversing the first grouping of the menu items associated with the context specific items compared to the second grouping of menu items associated with the global items . In both cases the user may perceive the sounds that are produced as similar to the running of a hand over the keys of a piano but each sound in this case may be shorter that a piano tone and similar to the pop of a bubble.

Further the AIM can present a vibration cue upon traversing each menu item. The user may experience the vibration cues as similar to running a hand over a bumpy surface with each bump representing a different menu item. A user can consciously or subconsciously count the bumps to quickly get a general idea of his or her position within the list. Further the AIM can present an audio message upon advancing to a menu item that informs the user that the menu item is in focus and will be selected upon release of the user s finger. For instance the AIM can announce the menu item by giving its number in the list and then announcing a description of the item.

This short sound indicates when an action has resulted in a change to the state. It may be implemented as a short and precise sound with a flourish at the end. For instance the sound may resemble a bleep followed by a travelling grating sound that fades out towards its end. The AIM may also execute a short vibration cue that confirms the select action. Further the AIM can play a verbal cue that confirms that the item has been selected.

This short sound indicates that an overlay menu is now being presented on top of other content. This sound may resemble a fade in ascending type sound. The AIM can also provide an accompanying short vibration cue which further confirms that a change in state has taken place. Further the AIM can play a verbal cue that states that the overlay menu is now being presented.

This short sound indicates that a dialog e.g. an overlay menu has been closed and that the focus has returned to a previous context. The sound may resemble a fade out descending type sound.

This short and distinct sound indicates that the state of the current context menu has transitioned to present new content e.g. because the user has advanced to a new journey step etc. The sound may resemble a bleep followed by a subtle turnstile noise.

The above described sounds are representative and not exhaustive of the full suite of sounds that the AIM may provide. The following additional sounds for instance may be triggered upon certain actions performed by the user while interacting with the AIM a a To Actions sound to indicate that a menu of actions is presented based on a selection of a menu item made in a prior menu b a From Actions sound to indicate that an active menu is presented in response to the return from a menu of actions c an Actions Unavailable sound to indicate that an instruction to present actions for a particular menu has been recognized but that there are no actions associated with the particular menu d an Item Selectable sound that indicates that a menu item has been successfully marked for selection upon release of the user s finger e an Item Non Selectable sound to indicate that an item that was previously marked for selection upon release has now been successfully un selected and thus will no longer be selected upon release of the user s finger f an Item Not Selectable sound to indicate that a gesture to mark an item for selection has been recognized but the gesture is not applicable to the item under consideration g a Select But No Change sound to indicate that a selection has been made but that no change in focus is appropriate h a Back Success sound to indicate that a back gesture was recognized and the back action has been invoked i a Switch To Tabs sound to indicate that a switch to tabs gesture has been successfully recognized and that the tabs menu is now being presented j a Switch From Tabs sound to indicate that a switch from tabs gesture has been recognized and the prior menu that was presented prior to switching to the tabs menu has been restored k a Start Listening sound to indicate that the SI module is now listening for a user s voice commands l a Cancel Listening sound to indicate that the voice recognition mode has now been canceled m a Finished Processing sound to indicate that the application has completed receiving and processing a voice input n an Action Taken sound to indicate that an action has been taken as a result of a voice based input and so on. Further the AIM can also present any type of haptic cues that will accompany any of the sounds described above.

The following illustrative sounds may be triggered upon certain events that may occur in the course of the user s interaction with the headset a a Button Tap sound played prior to performing an action to indicate that a tap gesture has been recognized on a headset button b a Button Press sound played prior to performing any action to indicate that a press gesture has been recognized on a headset button c a Buttons Held sound played prior to performing an action that indicates that a hold gesture has been recognized on a headset button d a Cancel Action sound that indicates that any previous action invoked from a headset button has been cancelled e.g. for a request to stop announcing contextual IOIs as part of the orientation action and so on. Further the AIM can also present any type of haptic cues that will accompany any of the sounds described above.

The following illustrative sounds may be triggered upon certain events that may occur during the course of a journey a a Started Journey sound to indicate that a journey has been started and that navigation is now in progress b a Mode Switch sound that indicates that the user has switched his or her mode of transport e.g. from walking to train etc. c a Beat sound to directionally indicate the next point along the journey that the user is trying to get to as part of navigation d a Warning sound to indicate that warning information is about to be read aloud played to heighten the user s awareness and give the user time to tune in for that information e a Waypoint Reached sound to indicate that the user has reached a journey waypoint and that navigation information is about to be read aloud played to heighten the user s awareness and give the user time to tune in for that information f an Approaching Waypoint sound to indicate that the user is approaching a waypoint and that navigation information is about to be read aloud played to heighten the user s awareness and give the user time to tune in for that information g a Journey Update sound that is played to indicate that information regarding a change to the current journey is about to be read aloud played to heighten the user s awareness and give the user time to tune in for that information h a Contextual Information sound that indicates that contextual information is about to be read aloud i a Further Information sound that indicates that additional information is about to be read aloud and so on. Further the AIM can also present any type of haptic cues that will accompany any of the sounds described above.

Generally the above sounds and haptic cues further promote the goal of providing useful information to the user as the user interacts with his or her environment without unduly distracting the user. For example the sounds and haptic cues that the user hears while navigating a menu allow the user to interact with the menu without diverting his or her attention from the environment.

The system supports a voice recognition mode in which the user may issue commands to the system via spoken instructions e.g. in addition to manually interacting with the headset and or user device . Or the system can use the voice recognition mode as the sole mode in which the user interacts with the system . To perform recognition of the user s voice commands the system may rely on pre existing voice recognition technology e.g. the CORTANA system provided by MICROSOFT Corporation and or native voice recognition technology. illustrates the voice recognition technology as the speech processing engine .

As mentioned above the user can invoke the voice recognition mode via an appropriate command issued through the headset and or the menus of the user device . In other cases the user device may already be interacting with a voice recognition system e.g. CORTANA as part of its default manner of operation but not in the context of performing navigation. Here the user may interact with the SI module in the voice recognition mode by issuing a spoken command that is preceded by the name of the navigation application e.g. by speaking the command Soundscape start explore mode. If the user has already activated the voice recognition mode in the context of the navigation application the user can just say Start explore mode or the like.

The following list identifies illustrative operations that may be initiated in voice mode a the user may open an existing saved journey b the user may activate or reactive the orientation mode c the user may activate or deactivate the explore mode d the user may ask for more information about a particular topic e the user may request the cessation of all or some spoken messages f the user may make various parameter settings g the user may save journey information and so on.

Illustrative commands may correspond to a Create a route to destination x leaving at time y using only public transport z Find the closest coffee shop c Get me to a train station d What is the time e Increase volume f Remove restaurants from items of interest etc.

The speech processing engine may sometimes encounter a situation in which it understands the user s command but it determines that the command omits one or more items of necessary information. In response the speech processing engine can ask the user to supply the missing items of information.

In other cases the speech processing engine may encounter a situation in which it does not understand the user s command. In those cases the speech processing engine can ask the user to rephrase the command. If that is not successful the speech processing engine may present proposed interpretations of the user s utterance to the user e.g. based on the detection of keywords in the user s command and then ask the user whether any of the interpretations are correct. If that is not successful the speech processing engine may invite the user to enter an equivalent command via the headset and or the user device .

In block the AIM detects a second gesture performed by a user corresponding to an instruction to advance to a particular menu item among a collection of menu items in the menu. The second gesture for example may correspond to any of the types of scrolling or panning gestures described above e.g. with reference to . In block the AIM advances to the particular menu item in response to the second gesture.

In block the AIM detects a third gesture performed by a user corresponding to an instruction to select a particular menu item. The third gesture for example may correspond to the type of release gesture shown in or the type of pull to the side gesture show in . In block the AIM performs an operation in response to the third gesture. The operation may correspond to the invocation of an action the setting of a parameter the presentation of information etc.

In summary the above features allow the user to safely and efficiently move through his or her environment. For example the workspaces provide a user friendly way of surfacing the most relevant information to the user as the user moves through the environment. Further some of the features allow the user to interact with a user interface presentation without directing visual attention to that presentation and or without having to perform cumbersome and complex gestures that divert the user s attention from the physical task of interacting with the environment. For instance the user may perform some of the gestures with a single hand without looking at the user interface presentation.

As described in the introductory Section A the sound generation module of can generate non three dimensional non spatial sounds and three dimensional spatial sounds. A three dimensional sound is a sound which the user perceives as emanating from at least one location in physical space even though it has no actual origin in physical space. For specifically demonstrates the use of three dimensional audio information to create a perception of sound which emanates from a particular location within space. demonstrates the use of three dimensional audio information to create a perception of sound that moves across a series of locations within space.

In one implementation the sound generation module can produce a three dimensional sound using a library of Head Related Transfer Functions HRTFs e.g. as provided in the data store . An HRTF models the anatomical features a person s face that have a person specific bearing on the manner in which that person perceives the origin of sound in his or her environment. To produce a three dimensional sound the sound generation module can choose an HRTF that is appropriate for a particular person and for a particular sound location in space relative to the person s location . The sound generation module can then apply that HRTF to modify a non three dimensional sound e.g. a flat or non spatial sound to produce the three dimensional sound e.g. by convolving the non spatial sound with the chosen HRTF.

More specifically in one case the sound generation module can receive input information which describes a the non three dimensional sound to be played b the location in space at which the sound is to be perceived as originating and c optionally the identity of the user. The input information can describe the non three dimensional sound by providing reference information which identifies audio information in the data store and or may provide the audio information itself. For example the non three dimensional sound may correspond to a telltale tone and or a spoken message. Then for each ear of the user the sound generation module can a identify an HRTF associated with the location for the particular user under consideration and b apply the HRTF to the non three dimensional sound to produce the three dimensional sound. The input information that is provided to the sound generation module may originate from one or more of the other modules of the SI module described below e.g. corresponding to the path guidance module the relevant information determination RID module the exploration module the orientation module etc.

The sound generation module performs the above functions with respect to a library of HRTFs provided in a data store . In one implementation the SI module can store HRTFs in the data store that have been prepared for particular individuals and which thus take into account the particular anatomical features of these people . For example the HRTFs for each person can be obtained by measuring the physical characteristics of each person s face. That task in turn can be performed in manual fashion e.g. by using physical distance measuring tools by using a depth camera e.g. by using the KINECT system provided by MICROSOFT Corporation of Redmond Wash. and so on. More specifically the set of HRTFs that is generated for a particular person includes an HRTF for each location of sound in space with respect to the position of the user s head and for each of the user s two ears. Different implementations may entrust different individuals to create these HRTFs e.g. acoustic engineers system administers end users etc. .

In a second implementation the sound generation module can store separate groups of HRTFs that perform well for different respective groups of people. The system can then invite an end user to choose the set of HRTFs that is perceived as producing the most realistic three dimensional sounds. For instance a user may make this choice in a set up phase by asking the system to produce the same three dimensional sound s using different HRTFs the user may choose the sound s and the corresponding HRTF that produce the most desirable result. In a third implementation the sound generation module can store a single set of HRTFs that have proven suitable for a large population of users even though these HRTFs are not customized for any particular person or group of people. Still other techniques may be used to generate and or select suitable HRTFs.

In one implementation the sound generation module generates three dimensional sounds that convey wideband audio information. Wideband audio information is audio information that includes a wide distribution of audio frequencies e.g. in one implementation in the range of 300 Hz to 3.4 kHz. Further the individual three dimensional sounds are expressive in nature to further emphasize their directionality. For example one such three dimensional sound may resemble a gust of wind blowing from left to right or from left to right. Other properties of the audio information besides its HRTF based directionality may contribute to its expressiveness such as by providing variations in volume tone reverberation looping frequency for repeating sounds etc. All of these factors contribute to a realistic perception that sound is originating from a particular location or locations in physical space.

As also described in Section A different modules may leverage the use of three dimensional sounds for different purposes and in different modes of operation. These modules include the path guidance module the RID module the exploration module the orientation module etc. To simplify description each such module is sometimes described as producing three dimensional sounds. In actuality each module produces the sound by generating the above described input information which it feeds to the sound generation module the sound generation module then uses the input information to produce the actual three dimensional sounds.

With reference to first consider the operation of the path guidance module . As already described the path guidance module determines the current direction in which the user is headed. The path guidance module can make this assessment based on any of the information provided by the system s orientation determination mechanism s motion determination mechanism s position determination mechanism s and so on. In one case for instance the path guidance module can determine the direction in which the user s head is pointed e.g. based on an orientation determination mechanism in the headset and then use this information as a proxy for the direction in which the user is presumed to be headed. Alternatively or in addition the path guidance module can determine a series of locations that the user has recently traversed. The path guidance module can use this information to project a direction that the user appears to be headed. Alternatively or in addition the path guidance module can determine the direction in which the user is purposely pointing his or her user device .

The path guidance module can also determine the desired direction of the user. The path guidance module can determine the desired direction based at least the current location of the user and the location of the next waypoint. The path guidance module can also take into consideration map information when determining the desired direction. For example the map information may reveal that the user s path is restricted in various ways e.g. by the course of a roadway obstacles of any type etc. The path guidance module can use all of this information to determine the direction that the user should be headed to ultimately place the user on course to reach the next waypoint in the most efficient manner possible.

Based on the actual direction and the desired direction the path guidance module can then determine the extent to which the user may be deviating from the desired direction e.g. to provide deviation information. The path guidance module can then use the deviation information to generate a three dimensional sound that will have the effect of steering the user in a desired direction e.g. by feeding the deviation information to the sound generation module as part of the input information.

To clarify the above description consider the illustration of . Here the user is located at a current location . The next waypoint w is located at target location . The user s current actual direction is denoted as direction . The user s desired direction corresponds to that vector which connects the current location of the user with the target location although as noted above this need not be the case in all situations e.g. due to an obstruction which blocks a direct route from the current location to the target destination . The deviation information may correspond to the angular difference between the current direction and the desired direction .

In response to the above determinations the path guidance module can produce a three dimensional sound using the sound generation module which appears to originate from the location . In other words the user is currently headed off to the left of the desired direction . The user will hear the directional cue emanating off to the right. The user will perceive that cue as urging the user to veer towards the right to correct the direction of his or her path. If the user starts moving in a direction that is too far to the right of the desired direction the path guidance module may produce a three dimensional sound using the sound generation module off to the left of the user. The user may continue on essentially following or pursuing the direction of the perceived three dimensional sound. When the user is headed in the correct direction the three dimensional sound will be perceived as emanating directly in front of the user the user then heads straight for that sound.

As noted above the path guidance module can use the sound generation module to create a three dimensional sound that is periodic in nature e.g. corresponding to a beat sound. For instance the beat sound may correspond to a repeating single tone or a two tone or n tone clip clop sound and so on. In addition or alternatively the three dimensional sound can appear to move in the direction that the user is being urged to move e.g. from left to right or right to left etc.

Moreover the path guidance module can provide input information to the sound generation module which has the effect of varying one or more audio characteristics of the beat sound depending on an extent to which the user s actual current direction deviates from the desired direction . For instance the path guidance module in conjunction with the sound generation module can generate a beat sound having a first tone and or a first looping frequency when the user s current deviation from the ideal path is within a first threshold. The path guidance module in conjunction with the sound generation module can generate a beat sound having a second tone and or a second looping frequency when the user s current deviation is outside the first threshold but within a second threshold and so on. The path guidance module can classify the user s deviation into any number of such deviation categories each being associated with a particular beat sound.

In the particular example of the path guidance module defines a first range of deviations from the desired direction and a second range of deviations from the desired direction the second range being larger than the first range . The current direction falls within the second range and therefore the path guidance module in conjunction with the sound generation module will generate a beat sound that is appropriate for this range .

In actual practice the path guidance module is useful in many circumstances but may be perceived as most useful in two circumstances. In a first case a user may reach an intersection at which he or she is expected to turn in a particular direction among a fixed set of available options. Or the user may reach a fork in the road in which he is expected to choose one of the possible paths. The path guidance module in conjunction with the sound generation module can provide unambiguous guidance to the user by playing a three dimensional beat sound that the user perceives as resolving his or her navigation choices. For example when standing in an intersection the user will hear the beat sound leading him or her to the left or right for example . When standing at a fork in the road the user will perceive the beat sound as leading him or her down the correct fork in the road.

In a second scenario the user is moving across a more open space in which he or she is given more latitude to gradually veer off course compared to the case in which the user is walking down a well defined road or path . Here the path guidance module may give the user incremental nudges in the manner illustrated in to keep the user on a proper heading.

In another use scenario the relevant information determination RID module can determine at each point along a user s path and for each corresponding current context the location of relevant items of interest IOIs such as contextual IOIs. The RID module can then use the sound generation module to generate a three dimensional sound for each IOI. The user will perceive that sound as originating from the actual location of the corresponding physical entity in physical space that is assuming that the IOI corresponds to a physical entity having a discrete physical location in space. For example the RID module in conjunction with the sound generation module can generate the following audio information for a particular IOI that has a counterpart physical entity in the user s vicinity 3D sound Jane s Coffee Shop restaurant 30 feet ahead. In other words in this case the audio information includes a three dimensional sound followed shortly thereafter by a spoken message. The user will perceive the three dimensional preliminary sound as emanating from a physical location of Jane s Coffee Shop. The spoken message may also be presented as a three dimensional sound although it need not be . The spoken message could alternatively include additional or fewer fields of information or could be omitted entirely.

In other cases an IOI may have a general origin in space rather than a pinpoint location. For example the IOI may correspond to an advertisement that is associated with a general area that in turn corresponds to a bus stop. The RID module in conjunction with the sound generation module can generate a sound for that IOI that is perceived by the user as emanating from the general area of the bus stop or from the center of that area etc. That type of IOI may be considered virtual insofar as it is not intended to describe the bus stop but operates as an audio attribute of the bus stop.

In another implementation the audio information can also include a verbal cue which specifies the directional location of the IOI relative to the user. That directional cue can be expressed broadly e.g. by specifying that Jane s Coffee Shop is ahead of the user and to the right or more narrowly e.g. by specifying that Jane s Coffee Shop is located at 10 o clock relative to the current direction of the user . In addition or alternatively the RID module in conjunction with the sound generation module can provide different three dimensional sounds for different respective categories of IOIs e.g. by playing a first type of sound for restaurants and a second type of sound for restroom facilities etc.

Both the exploration module and the orientation module leverage the above described behavior of the RID module e.g. in the explore mode and the orientation mode respectively. As described in Section A the user may enter a command to expressly activate and deactivate these modes e.g. via manual interaction with the headset and or the user device and or via a voice instruction.

More specifically shows the illustrative behavior of the exploration module operating in the explore mode. In this mode once the explore mode is activated by the user the exploration module determines the current location of the user and the current focus of interest of the user. For example the exploration module can use any position determination mechanism s on the headset and or on the user device to determine the location of the user at the present time e.g. using a GPS sensor etc. . The exploration module can use any orientation mechanism s on the headset and or on the user device to determine the direction at which the user appears to be orienting his body. For example the orientation mechanism s on the headset can determine the direction at which the user s head is pointed which can be used as a proxy indication of the user s focus of interest. Alternatively or in addition the system may instruct the user to point the user device in the direction that matches his or her focus of interest in that case the orientation mechanism s of the user device will correctly reflect the user s intended focus of interest. Still other techniques can be used to assess the user s presumed focus of interest.

Next the exploration module defines a search space of interest. In one case the exploration module may define the search space as a volume of any shape that is centered on the user s presumed focus of interest. For example the volume may correspond to a wedge shape volume which originates from the user s current location and which is bisected by the user s presumed focus of interest. System defined and or user defined parameter settings may specify the depth of the volume the angular span of the volume the width of the volume etc.

Next the exploration module may use the RID module to search the volume for the presence of IOIs of interest such as but not limited to contextual IOIs. The RID module can perform this task by consulting one or more data stores which provide information about IOIs together with any parameter settings which define the user s particular interests for example the user may have indicated that he or she is by default interested in restaurant information with the exclusion of coffee shop information . In some cases an IOI may be associated with the search space because it has a counterpart physical entity that is physically located in the search space. In other cases an IOI may be associated with the search space because of some other nexus established in any other way. For example an administrator or an end user himself or herself can manually specify in advance that the area near the exit of a subway station is to be associated with a weather report IOI and so on.

Finally the exploration module can generate three dimensional audio messages which announce the presence of the IOIs that have been identified. The exploration module can generate these messages in any manner described above by leveraging the sound generation module . For example as described above the audio information may correspond to an introductory sound followed by a spoken message which announces the IOI. The exploration module in conjunction with the sound generation module can read off these IOIs in an order such as in clockwise order across the volume or counterclockwise order and or by increasing or decreasing distance relative the location of the user etc.

Overall the user may interact with the exploration module by rotating his direction of attention by pivoting about his current location in stages. At each stage the user may wait to hear the IOIs that lie within or are otherwise associated with the search volume thus defined by his current presumed attention of focus prior to moving on to a new focus of attention. For example in one case the swath encompassed by a user s direction of interest is 45 degrees. The user could conduct a complete inventory of IOIs around him by turning to the northeast the southeast the southwest and the northwest in succession. At each orientation the user will hear the IOIs that are encompasses in that quadrant of interest.

According to another illustrative feature the user may select an IOI in any manner after hearing it announced. For example the user may issue the voice instruction take me there or more information after hearing about Jane s coffee shop. Or the user may select the IOI by turning his or her focus of attention towards the perceived location of the IOI e.g. based on the perceived location of the three dimensional sound which announces this IOI . The user may perform this task for instance by turning his head or body or user device directly towards the perceived location of the IOI. Or the user may select the IOI via an appropriate menu provided by the user device . Or the user may make an appropriate selection via a headset button after hearing the IOI being announced etc. In response to the selection of the IOI in whatever manner made the SI module can present additional information regarding the IOI or provide instructions on how to reach the IOI etc. The SI module can also confirm the user s selection by increasing the volume of the three dimensional sound that announces the presence of the chosen IOI or by otherwise making that sound more prominent in the user s experience.

Finally the orientation module operates by determining the current location of the user in the manner described above. It then defines a three dimensional volume around the user. For example the three dimensional volume may correspond to a cylinder or sphere or box or rectangle for example with the user located at the center of the volume. The orientation module then uses the RID module to identify the set of IOIs that exist within the volume if any. The orientation module then uses the sound generation module to read off the IOIs in any order.

For example in the orientation module defines a three dimensional volume having the shape of a cylinder with the user positioned at its center at a current location . The orientation module uses the RID module to identify the IOIs that lie within the cylinder or are otherwise associated with the space defined by the cylinder. Then the orientation module reads off the IOIs in any order. For example consider a user who visits a mall having three floors. Assume that the user is standing in the open air atrium on the first floor of the mall. The orientation module can read off the IOIs that it finds on a floor by floor basis e.g. floor z floor z and then floor z. The orientation module can announce the IOIs on each floor in any order such as a clockwise order a counterclockwise order etc. The user may then optionally select any IOI in any of the ways described above.

As closing comment the above explanation set forth the use of three dimensional sounds to announce the presence of contextual IOIs such as restaurants etc. But the SI module can use three dimensional sounds to announce the presence of any types of IOIs such as warning IOIs and journey IOIs. For example the SI module can generate a warning regarding a pothole in the road that appears to emanate from the location of the pothole. As another clarification the SI module can also deliver many types of IOIs in a flat or non spatial manner. For example the SI module can produce another warning IOI that has no directivity to generally notify the user that the road on which he is currently traveling is slick due to rain.

Further the above description was predicated on the use of sound that the user perceives as originating from locations in physical space. In addition or alternatively the haptic cue generation module can generate vibration cues which convey directional information. For example the headset can include two or more vibration mechanisms e.g. a first vibration mechanism on the left side of its frame and a second vibration mechanism on the right side of its frame . The haptic cue generation module can activate either the first or second vibration mechanism to provide instructions to the user to turn left or right respectively. This type of instruction can include additional gradations by including additional vibration mechanisms. The same effect can be achieved by activating different vibration mechanisms on the user device e.g. by activating a left side vibration mechanism to provide a cue to turn left and a right side vibration mechanism as a cue to right. The vibration mechanisms can be coupled to yet other devices or parts of the user s body.

In one case the SI module can apply the process as a background service as the user traverses a route. For example the RID module can alert the user to the existence of IOIs when the user draws sufficiently close to the locations associated with these IOIs as governed by any distance based parameter setting.

In another scenario as indicated in block the exploration module may apply the above general process by using the RID module to identify a set of IOIs that are associated with a subspace to which an attention of the user is current directed or presumed to be currently directed. In another scenario as indicated in block the orientation module can apply the above general process by using the RID module to identify a set of IOIs that are associated with an entire space around the user at the current time without reference to the user s current focus of interest because the user is now interested in the complete volume of space around him or her at the moment . The user can invoke and suspend the readout of the exploration module or the orientation module by issuing appropriate instructions e.g. via the headset or the user device .

As a final topic the above examples were based on the simplifying assumption that the position of the virtual sound producing entity e.g. the virtual sound source in space is stable relative to the location of the listener at least over the course of the delivery of the three dimensional sound. This may be a valid assumption in many cases but it may not hold true for all situations.

For example consider the following scenarios. In a first case a user may listen to a three dimensional sound that describes or otherwise relates to a fixed position entity as the user passes the entity in a vehicle of any type e.g. on a train . For example the user may listen to a message that describes Seattle s Space Needle tower as the user is traveling as a passenger in a car in any direction relative to the Space Needle. In a second case the may listen to a three dimensional sound that describes or otherwise relates to a fixed position entity as the user stands still but nevertheless moves his or her head about such that the position of the user s left and right ears are changing relative to the location of the fixed position entity. In a third case a user may listen to a three dimensional sound that describes or otherwise relates to an in motion entity relative to a fixed position of the listener. For example the user may listen to a message that announces the arrival of an airplane that is moving down a runway while the user remains at a fixed position in the airport s terminal. In a fourth case both the location of an IOI and the user may be in motion during the delivery of the audio information.

Further note that an IOI need not always correspond to a physical entity such as the Space Needle or a moving airplane. For example a moving virtual IOI may correspond to a virtual billboard which delivers a spatial message that the user perceives as moving down the street towards the entrance of a nightclub enticing the user to enter that establishment. Further everything that is set forth herein with respect to sounds associated with IOIs applies equally to other sounds such as the periodic beat sound.

To address any of the above situations the SI module can dynamically update the three dimensional audio information that it is producing over the course of its delivery to reflect at each instance of time the relative position between the user s head and the IOI e.g. the location of the Space Needle or the moving airplane in the examples cited above . The SI module can perform this task in different ways. In one non limiting approach the SI module can perform the following operations in an iterative process over the delivery of the audio information a determine the position of the user s head relative to the IOI to provide relative position information b select an HRTF per ear based on the relative position information c convolve whatever audio information is to be delivered at the current time by the selected HRTF to produce a three dimensional sound that is suitable based on the relative position information.

To cite one specific example consider a message that takes five seconds to deliver to the user. The SI module can determine for each second of that delivery the relative position between the user s head and the IOI. The SI module can use that information to produce a three dimensional sound for each second of the message s delivery that is based on the relative position between the user s head and the IOI at that instance of time.

More generally the above iterative processing can be performed at different update rates. The update rate may depend on the rate at which the relative position between the listener and the IOI changes. That is the SI module can update the HRTFs at a relatively quick rate for large relative changes to the relative position information and at a slower rate for slower changes. The update rate may also take into consideration the amount of processing resources that are available in the system to update the three dimensional sound information.

The SI module can also apply various techniques that are designed to expedite the above dynamic processing which may represent a large processing burden on its resources . For example in some cases the system can pre compute a dynamic sound for at least one predicted trajectory each such trajectory defines some type of expected relative movement between a listener and the IOI due to a the movement of the listener in space or b the movement of the IOI in space or c the movement of both the listener and the IOI. For example assume that a train passes by an IOI at generally the same speed each day. In this case it is possible to pre calculate a three dimensional sound per ear that is predicated on a dynamically changing HRTF. That three dimensional sound takes into account a presumed progression of the user s head as the train moves down the tracks and may be based on the simplifying assumption that the user s head has a predetermined fixed orientation as the user moves down the tracks. The SI module can launch that three dimensional sound when it detects that a listener s position reaches a predetermined triggering location relative to the IOI such as a location along the tracks that is a predetermined distance from the IOI.

The above manner of operation can be expanded to account for different scenarios. For example the system can pre compute different dynamic three dimensional sounds to take account for different train speeds different fixed head orientations e.g. indicating whether the user is looking straight ahead during the delivery of the message or looking out the window etc. and so on.

The above situation can also be extended to a more general setting in which the movement between the user and an IOI can be decomposed into a number of possibilities allowing for the pre calculation of respective dynamic three dimensional sounds for these possibilities. The SI module can then launch any pre calculated sound when it determines that the user s current context matches one of the predetermined possibilities.

In another case the SI module can dynamically perform processing to predict the movement of the user relative to the location of an IOI e.g. on the basis of the user s current heading. The SI module can then dynamically pre compute a three dimensional sound based on a projected trajectory of the user relative to the IOI.

Still other techniques can be used to expedite the computation of dynamically changing three dimensional sounds. In some implementations for instance the SI module can draw on the enhanced processing capabilities of the remote processing resources e.g. by leveraging parallel processing performed by those resources .

In summary the above features contribute to the above stated goal of allowing the user to efficiently safely and enjoyably move through his or her environment. For instance the features provide the user with different categories of information as the user traverses the environment using different modes of presentation. The functionality presents this information to the user in a user friendly manner that avoids overwhelming the user with too much information at any given time. The use of three dimensional sounds further enhances the ability of the user to understand the nexus between the information that is provided and objects regions and events in the environment.

The following section provides additional information regarding the operation of the beacon based guidance module . To repeat the beacon based guidance module provides a strategy for guiding the user through a space by leveraging the use of beacons having defined respective ranges. The space may correspond to an indoor space e.g. a space defined by the interior of one or more buildings or structures of any type. Alternatively or in addition the space may correspond to an outdoors space.

The beacons may emit electromagnetic radiation of any type such as radio waves infrared waves etc. In other cases the beacons may emit sound waves e.g. in the ultrasound range . Further in some cases the beacons may generate signals according any protocol or combination of protocols such as the BLUETOOTH protocol the Wi Fi protocol etc. For example without limitation the beacons may correspond to BLUETOOTH Low Energy BLE beacons. Further a beacon can have a range of any desired depth to best suit the target environment in which it is deployed. In some indoor settings for example beacons are chosen having relatively short ranges e.g. ranges of one or more meters. In some cases a beacon s stated range may be based on the implicit or explicitly stated assumption that its signals are to be detected by a particular type of user device such as a particular smartphone or a particular class of user devices such as a particular class of smartphones which have known and stable signal receiving characteristics.

In yet another case each beacon is a passive device such as a passive RFID that may be interrogated by the user device or some other interrogating device when that device is within a prescribed distance to the beacon. That prescribed distance corresponds to the range of the device. However to facilitate explanation the remaining description will assume that each beacon actively emits a signal.

In one implementation each beacon possesses a code which defines it identity. The beacon may constantly or periodically or in an on demand manner emit signals that announce its code and or any other application specific information which may be detected by receiving devices within the range of the beacon. For example consider a BLE beacon that has a range of about one meter. A receiving device that lies within that range may detect the beacon s signal and read its particular code.

In the example of an indoor and or outdoor environment is characterized by a collection of corridors and obstacles e.g. obstacle obstacle and obstacle . This scenario involves pre populating the environment with a collection of beacons e.g. beacons b b . . . b e.g. by placing the beacons at every intersection of two or more corridors. The outmost dashed line circle surrounding each beacon represents the range of the beacon. As noted above a receiving device that lies within the range of a beacon can successfully detect the beacon otherwise the receiving device will have no knowledge of the beacon. Note that in one illustrative implementation the ranges of the beacons do not overlap. This characteristic is advantageous because it eliminates any ambiguity as to the location of the user at any given time. In other words the user s device cannot simultaneously detect two or more beacons because the ranges of these beacons do not overlap and the user cannot simultaneously exist at two places at the same time. In some environments the non overlapping nature of the beacon ranges may be ensured by also taking into consideration the nature of the user device or class of devices that will be receiving the signals emitted from the beacons.

As a first preliminary step the locations of all beacons in the environment of may be loaded in the data store . The data store may also store the codes associated with those beacons.

As a second preliminary step the system can use any route planning tool to generate a route through the environment of . The route planning tool can apply any algorithm to perform this task. For example the routing planning tool can express the search space defined by the corridors and obstacles as a graph having a plurality of nodes and links. That is the links represent corridors and the nodes represent intersections of corridors. The routing planning tool can then use any algorithm such as the well known Dijkstra s algorithm to find the most efficient path through the graph. Once the route is defined the beacon based guidance module can identify a subset of beacons referred to herein as route specific beacons that will be traversed by the user in traveling the route and store the identities of those beacons. In the case of the user is expected to encounter all of the beacons but this is generally not the case to be clarified below .

Now assume that the user embarks on the route using the above described system as guidance. The system may provide real time guidance to the user in the manner described above via the headset and or the user device . In this case however the system uses a different technique to determine the user s current location at each time compared to techniques described above.

More specifically the beacon based guidance module constantly scans the environment to determine whether the user is in the range of any beacon. If so the beacon based guidance module identifies the current location of the user as the current location of the beacon that has been detected. In other words the beacon based guidance module knows a priori the code associated with the detected beacon and its location in the environment. If the user s headset or user device detects the presence of that beacon based on the code conveyed by the detected signal then the beacon based guidance module may make the assumption that the user has the same location as the detected beacon.

At this juncture the beacon based guidance module can leverage the services of the path guidance module to direct the user in the desired direction using a three dimensional sound and or based on other guidance information such as a non three dimensional sound displayed instructions etc. . For example assume that the user s user device detects that it is in the range of beacon b. The path guidance module will determine based on the predetermined journey information that the next waypoint along the user s journey corresponds to the beacon b. The path guidance module can then generate a three dimensional sound which the user will perceive as originating from the right side of the user which serves to direct the user towards the next waypoint e.g. beacon b . The user will interpret this cue as an instruction that he or she should turn to the right. The user will continue in this direction until he or she encounters another beacon e.g. beacon b at which time the direction of the user may be updated.

In some cases a user may deviate in an unexpected manner from a planned path in such a manner that he or she falls outside the range of the beacon that he or she was expected to encounter next. represents one manner of addressing this situation. The strategy there is to increase the number of beacons along the planned route of the user which has the effect of increasing the frequency at which the user s current position is assessed and hence decreasing the potential that the user may wander off route e.g. to the extent the he or she falls outside the range of a beacon that he or she is expected to encounter . Note that shows only those beacons that the user is expected to traverse on the planned route. But the environment may include additional beacons that are not shown that the user is not expected to traverse.

The opportunity for a user to go significantly astray is not great in e.g. because the user s choices are significantly restricted by the obstacles. Nevertheless a user may become confused and take a wrong turn causing him to leave the planned route. Or the user may purposely decide to deviate from the planned path. For example at the intersection associated with beacon bin the user may take a left turn rather than a right turn. The user may therefore eventually encounter an out of path beacon not shown which he is not expected to encounter.

The beacon based guidance module can address this situation in different ways. In one case the beacon based guidance module can inform the user that guidance can no longer be provided to the user because the user appears to have wandered off track and it is no longer possible to determine the heading and intent of the user. The beacon based guidance module can also query the user whether he or she intends to pursue the original defined path through the environment.

In another case if sufficient information can be obtained regarding the current location heading and intent of the user the beacon based guidance module can direct the user back onto the planned route. For example the beacon based guidance module can determine the direction that the user appears to be currently headed although it appears to be wrong by forming a trajectory based on a set of the user s most recent known positions. The beacon based guidance module can then use that heading information together with information regarding the user s current position if known to steer the user back onto the planned route. Or the beacon based guidance module can query the user to determine whether he or she intends to still pursue the planned route or select another route.

In any scenario the beacon based guidance module can also rely on other evidence of the current location and heading of the user in addition to information provided by the BLE beacons when that information is available. For example the beacon based guidance module can collect that information from GPS sensor sources dead reckoning techniques etc. In other cases it is assumed that at least some of these additional sources are not available or are not reliable.

Again a route planning tool may generate a route through the space based on any input objective and using any route planning algorithm for this purpose. The beacon based guidance module then determines the beacons that the user is expected to traverse as he or she travels the planned route . These beacons are represented as solid black beacon symbols.

During the actual traversal of the route the beacon based guidance module performs the same function as described above. That is when the beacon based guidance module determines that the user has entered the range of a beacon along the expected path such as beacon b then it accepts the location of that beacon as the current location of the user. It then recalculates the heading of the user and updates the three dimensional beat sound and or other guidance information to guide the user towards the next beacon e.g. beacon b .

The risk that the user will veer off track in the case of is greater than the case of because the user is given more degrees of freedom in which the err. describes one way of addressing this situation. Here the beacon based guidance module defines the beacons represented as solid black beacon symbols that the user may encounter along a planned route in a more general manner e.g. by also encompassing neighboring beacons that lie to either side of the most optimal route . The beacon based guidance module may continue to provide guidance to the user if he or she wanders into the range of these neighboring beacons under the assumption that the user is still attempting to adhere to the planned route but has veered only slightly off course. The beacon based guidance module may only generate an error condition when the user wanders beyond the boundaries associated with outermost neighboring beacons.

In another implementation the beacon based guidance module can form a postulate as to the desired destination of the user based on the beacons that the user has already encountered along his or her path thus far. For instance the beacon based guidance module can form a trajectory based on the locations of the beacons encountered thus far. The beacon based guidance module can then determine a likely intermediary or final destination to which the trajectory points e.g. by extending the trajectory along its current direction. The beacon based guidance module can then ask the user whether he or she wishes to pursue a path toward the assumed destination. If so then the beacon based guidance module can thereafter provide the same type of navigational assistance described above that helps the user reach the identified destination.

In any of the cases described herein the beacon based guidance module can also take into consideration historical information regarding the user s previous travel habits and or historical information regarding the travel habits of others with respect to a specified environment . That information when available may provide further evidence of the user s intent in reaching a desired destination.

In block the beacon based guidance module determines based on the particular beacon signal whether the user is within a range of one of the route specific beacons this operation yields current location information when the user is within the range. In block the beacon based guidance module determines a next waypoint that the user is expected to reach based on the predetermined journey information to provide next waypoint information. In some cases that next waypoint may correspond to the next beacon along the user s predetermined journey. In block the beacon based guidance module next determines direction information based on the current location information and the next waypoint information. The direction information reflects a direction that the user is advised to travel to reach the next waypoint. In block the beacon based guidance module generates audio information and or other guidance information based on the direction information. In block the beacon based guidance module delivers the audio information to the user e.g. as a three dimensional beat sound. The audio information assists the user in reaching the next waypoint. The beacon based guidance module can perform the above functions with the assistance of the path guidance module and the sound generation module .

In summary the above features contribute to the above stated goal of allowing the user to safely and efficiently move through his or her environment particularly in those situations in which the user cannot rely on other modes of determining his or her location e.g. based on the use of a satellite based navigation system . In addition the use of non overlapping beacon ranges according to one illustrative implementation provides an efficient mechanism for disambiguating the location of the user since the user cannot simultaneously exist within the ranges of two or more beacons at the same time.

In the above description of a first implementation the assumption was made that the user device and or the handset receives at any given time a signal transmitted by either zero beacons or a single beacon but not plural beacons. In other implementations the above characteristic can be relaxed in different ways.

For instance in a second implementation the beacon based guidance module will conclude the user device and or the handset is within range of a particular beacon if it receives a signal from that beacon having a signal strength that is above a prescribed threshold. But unlike the first implementation the beacon based guidance module may also simultaneously receive weaker signals from one or more other beacons in the environment where the strength of each of those signals is below the prescribed threshold. In this scenario the environment is populated with beacons having positions such that at any given time the beacon based guidance module will receive either 1 no signal having a signal strength that is above the threshold or 2 just one signal having a signal strength that is above the threshold. In practice the second implementation functions in the same manner as the first and offers the same benefits e.g. by providing a binary mechanism for disambiguating the location of the user at any given time assuming that the user is within the range of one of the beacons. The beacons in the second implementation may therefore be considered as functionally or effectively non overlapping due to the above behavior. And accordingly any reference to non overlapping as used herein is to be understood as encompassing both the case in which the beacons have ranges that literally do not overlap as well as the case in which the ranges may be considered non overlapping because the user device and or headset can at most receive a signal from one beacon having a signal strength above the prescribed threshold.

In a third implementation the beacon based guidance module may receive at any given time signals from any number of beacons having any arbitrary signal strengths. The set of signals and strengths at a particular location defines signal profile information for that location. In a preliminary operation the beacon based guidance module can store signal profile information for each navigable location in the environment e.g. constituting information regarding the signals and their respective strengths at that location. Collectively the stored information constitutes a profile map of the environment. During navigation the beacon based guidance module can determine the signals that it is receiving at a given time at a given location to provide current signal profile information. Then the beacon based guidance module can use the current signal profile information as a key to find the location having the closest matching signal profile information. That location defines the probable location of the user at the given time. In some environments the strengths of the signals emitted by the beacons and or the ability to detect those signals may vary over time for various environment specific reasons. The beacon based guidance module can address this issue by comparing normalized versions of the signal profile information. Note that the positions of the beacons in the third implementation need not meet the non overlapping constraints associated with the first or second above described implementations.

The computing functionality can include one or more processing devices such as one or more central processing units CPUs and or one or more graphical processing units GPUs and so on.

The computing functionality can also include any storage resources for storing any kind of information such as code settings data etc. Without limitation for instance the storage resources may include any of RAM of any type s ROM of any type s flash devices hard disks optical disks and so on. More generally any storage resource can use any technology for storing information. Further any storage resource may provide volatile or non volatile retention of information. Further any storage resource may represent a fixed or removable component of the computing functionality . The computing functionality may perform any of the functions described above when the processing devices carry out instructions stored in any storage resource or combination of storage resources.

As to terminology any of the storage resources or any combination of the storage resources may be regarded as a computer readable medium. In many cases a computer readable medium represents some form of physical and tangible entity. The term computer readable medium also encompasses propagated signals e.g. transmitted or received via physical conduit and or air or other wireless medium etc. However the specific terms computer readable storage medium and computer readable medium device expressly exclude propagated signals per se while including all other forms of computer readable media.

The computing functionality also includes one or more drive mechanisms for interacting with any storage resource such as a hard disk drive mechanism an optical disk drive mechanism and so on.

The computing functionality also includes an input output module for receiving various inputs via input devices and for providing various outputs via output devices . Illustrative input devices include a keyboard device a mouse input device a touch sensitive input device a digitizing pad one or more video cameras one or more depth cameras a free space gesture recognition mechanism one or more microphones a voice recognition mechanism any movement detection mechanisms e.g. accelerometers gyroscopes etc. and so on. One particular output mechanism may include a presentation device and an associated graphical user interface GUI . Other output devices include a printer a model generating mechanism a tactile output mechanism an archival mechanism for storing output information and so on. The computing functionality can also include one or more network interfaces for exchanging data with other devices via one or more communication conduits . One or more communication buses communicatively couple the above described components together.

The communication conduit s can be implemented in any manner e.g. by a local area network a wide area network e.g. the Internet point to point connections etc. or any combination thereof. The communication conduit s can include any combination of hardwired links wireless links routers gateway functionality name servers etc. governed by any protocol or combination of protocols.

Alternatively or in addition any of the functions described in the preceding sections can be performed at least in part by one or more hardware logic components. For example without limitation the computing functionality can be implemented using one or more of Field programmable Gate Arrays FPGAs Application specific Integrated Circuits ASICs Application specific Standard Products ASSPs System on a chip systems SOCs Complex Programmable Logic Devices CPLDs etc.

According to a first aspect a computing device is described for assisting the user in interacting with a space. The computing device includes a space interaction module for performing functions that assist the user in moving over a desired route within an environment. The environment is populated with a plurality of beacons having respective non overlapping ranges and the desired route traverses ranges associated with a route specific set of beacons from among the plurality of beacons. The space interaction module includes a beacon based guidance module that is configured to determine whether a user is within a range of one of the route specific beacons to provide current location information when the user is within the range determine a next waypoint that the user is expected to reach based on predetermined journey information to provide next waypoint information determine direction information based on the current location information and the next waypoint information and generate guidance information based on the direction information. The guidance information once delivered to the user assists the user in reaching the next waypoint.

According to a second aspect the environment corresponds to at least in part an indoor environment provided by an interior of at least one building.

According to a fourth aspect the guidance information is audio information the audio information guiding the user in an appropriate direction to reach the next waypoint.

According to a fifth aspect the computing device delivers the audio information to the user via a headset.

According to a sixth aspect the space interaction module has access to stored information which identifies locations of the plurality of the respective beacons. Each such beacon emits a beacon signal having a beacon code the beacon code identifying the beacon which has emitted the beacon signal. The beacon based guidance module is configured to determine the current location information which identifies the current location of the user by determining a particular beacon code associated with a particular beacon signal that has been received by the computing device and then by determining a particular beacon associated with that particular beacon code and the location of that particular beacon.

According to a seventh aspect the beacon based guidance module is configured to ignore any beacon signal that is emitted from a beacon that is not a member of the route specific set of beacons.

According to an eighth aspect the route specific set of beacons encompasses just the beacons that lie on the desire route.

According to ninth aspect the route specific set of beacons encompasses beacons that lie on the desired route together with one or more neighboring beacons that lie adjacent to the desired route.

According to a tenth aspect a method is described for assisting a user in navigating within a space. The method includes receiving a particular beacon signal by a computing device which operates at a current location within an environment. The environment is populated with a plurality of beacons having respective non overlapping ranges and a desired route traverses ranges associated with a route specific set of beacons from among the plurality of beacons. The method further includes determining based on the particular beacon signal whether the user is within a range of one of the route specific beacons to provide current location information when the user is within the range determining a next waypoint that the user is expected to reach based on predetermined journey information to provide next waypoint information determining direction information based on the current location information and the next waypoint information generating guidance information based on the direction information and delivering the guidance information to the user. The guidance information assists the user in reaching the next waypoint.

According to an eleventh aspect the environment mentioned in the tenth aspect corresponds to at least in part an indoor environment provided by an interior of at least one building.

According to a twelfth aspect the environment mentioned in the tenth aspect corresponds to at least in part an outdoor environment.

According to a thirteenth aspect the computing device delivers the guidance information to the user via a headset.

According to a fourteenth aspect the beacons are located at intersections of pathways within the environment.

According to a fifteenth aspect the beacons are also located at intermediary locations along at least one pathway in the environment.

According to a seventeenth aspect the above summarized method in the tenth aspect determines the current location information by identifying a particular beacon code associated with the particular beacon signal identifying based on the particular beacon code a particular beacon that is associated with the particular beacon code and identifying a location of the particular beacon based on stored information which identifies the beacon codes and respective locations of beacons within the environment.

According to an eighteenth aspect the method according to the tenth aspect further includes the operation of ignoring any beacon signal that is emitted from a beacon that is not a member of the route specific set of beacons.

According to a nineteenth aspect a computer readable storage medium is described for storing computer readable instructions. The computer readable instructions implement a beacon based guidance module when executed by one or more processing devices. The computer readable instructions include logic configured to receive a particular beacon signal by a user using a computing device which operates at a current location within an environment. The environment is populated with a plurality of beacons having respective non overlapping ranges. Further a desired route traverses ranges associated with a route specific set of beacons from among the plurality of beacons. The computer readable instructions further include logic configured to identify a particular beacon code associated with the particular beacon signal logic configured to identify based on the particular beacon code a particular beacon that is associated with the particular beacon code logic configured to identify a location of the particular beacon based on stored information which identifies the beacon codes and respective locations of particular beacons within the environment to provide current location information logic configured to determine a next waypoint that the user is expected to reach based on predetermined journey information to provide next waypoint information logic configured to determine direction information based on the current location information and the next waypoint information and logic configured to generate guidance information based on the direction information the guidance information assisting the user in reaching the next waypoint.

According to a twentieth aspect the computer readable instructions further include logic configured to ignore any beacon signal that is emitted from a beacon that is not a member of the route specific set of beacons.

A twenty first aspect corresponds to any combination e.g. any permutation or subset of the above referenced first through twentieth aspects.

According to a twenty second aspect one or more computing devices and or one or more headsets are provided for implementing any of the first through twenty first aspects.

According to a twenty third aspect a system is provided for implementing any of the first through twenty first aspects.

According to a twenty fourth aspect one or more computer readable storage mediums are provided that include logic that is configured to implement any of the first through twenty first aspects.

According to a twenty fifth aspect one or more means are provided for implementing any of the first through twenty first aspects.

Further in closing the functionality described herein can employ various mechanisms to ensure that any user data is handled in a manner that conforms to applicable laws social norms and the expectations and preferences of individual users. For example the functionality can allow a user to expressly opt in to and then expressly opt out of the provisions of the functionality. The functionality can also provide suitable security mechanisms to ensure the privacy of the user data such as data sanitizing mechanisms encryption mechanisms password protection mechanisms etc. .

Further although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

