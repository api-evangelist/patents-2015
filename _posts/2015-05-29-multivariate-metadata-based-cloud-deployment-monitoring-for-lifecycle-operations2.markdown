---

title: Multivariate metadata based cloud deployment monitoring for lifecycle operations
abstract: Techniques are described for metadata-based monitoring of lifecycle operations on software deployments. In one embodiment, a set of metadata is stored in volatile or non-volatile store. The set of metadata may include a plurality of signatures and map a first signature of the plurality of signatures to a first status identifier for a first benchmark of a particular operation. A first set of log data that is associated with one or more software deployments is monitored for occurrence of the first signature. Based, at least in part, on the monitoring, a status of the first benchmark with respect to the first set of one or more software deployments is determined. Report data that indicates the status of the first benchmark is then generated and displayed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09626271&OS=09626271&RS=09626271
owner: ORACLE INTERNATIONAL CORPORATION
number: 09626271
owner_city: Redwood Shores
owner_country: US
publication_date: 20150529
---
This application is related to Provisional Appln. 62 056 073 filed Sep. 26 2014 entitled Method and System for Implementing Efficient Classification and Exploration of Data the entire contents for each of which is hereby incorporated by reference as if fully set forth herein.

The present disclosure relates generally to techniques for managing lifecycle operations and more specifically to a computer implemented metadata based approach for monitoring and reporting status information for lifecycle operations across one or more software deployments.

The approaches described in this section are approaches that could be pursued but not necessarily approaches that have been previously conceived or pursued. Therefore unless otherwise indicated it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.

Administrators are often responsible for performing various lifecycle operations such as patching upgrade and cloning of complex software deployments. Over the course of a lifecycle operation an administrator may run a set of tasks or commands one after another until the operation is complete. In order to avoid mistakes that could negatively impact performance or require significant time to correct administrators tend to perform one lifecycle operation at a time and to continuously monitor the progress of the operation. Some lifecycle operations take several hours or even days to complete. In some cases an administrator may spread a lifecycle operation across multiple work shifts until the operation is complete.

Concerns over lifecycle management scalability generally do not arise in on premise datacenter environments that have low deployment to administrator ratios. As an example some datacenter environments have a one to one deployment to administrator ratio where an administrator handles a single deployment at a time. In such environments an administrator may manually perform the lifecycle operation and monitor its progress in front of a computer console until the operation is complete without worrying about other deployments. As the number of deployments per administrator increases however this approach does not scale well and lifecycle operations may become backlogged until the administrators have time to work on them.

In cloud deployments each administrator may be responsible for dozens or hundreds of service deployments. With such a high deployment to administrator ratio it is not scalable for administrators to perform and continuously monitor one lifecycle operation at a time. Therefore administrators generally perform multiple lifecycle operations in parallel when responsible for managing a large number of deployments. However switching between multiple lifecycle operations for various deployments may be a tedious and error prone process. Delays and errors in performing lifecycle operations may negatively impact delivery schedule and increase costs for service deployments.

One approach administrators may use to help manage various deployments is to write each complex lifecycle operation into one large script that sends emails during and after each of the sub tasks in the lifecycle operation. When these scripts are invoked emails are sent notifying the administrator of completion of the sub tasks as well as anomalies that may have been encountered. Administrators may thus scan through the emails on a routine basis to monitor the lifecycle operation s progress. This approach may help keep an administrator apprised of current progress of various lifecycle operations. However administrators may not be able to handle the volume of emails as dozens or hundreds of service deployments go through lifecycle operations at the same time. As the deployment to administrator ratio increases the number of emails generated by the scripts per hour may be greater than the processing bandwidth of an administrator.

Another approach administrators may use to help monitor the progress of a lifecycle operation is to install custom software. According to this approach the administrator writes custom software to monitor and report on the progress of a lifecycle operation. This approach may be time consuming and cumbersome as it involves the administrator writing new custom code each time a new lifecycle operation is created or a new version of the software is deployed.

In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the disclosure. It will be apparent however that the present invention may be practiced without these specific details. In other instances structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present invention.

In various embodiments computer systems stored instructions and technical steps are described for monitoring lifecycle operations within software systems. Various embodiments provide a flexible and customizable metadata based approach through which lifecycle operations may be monitored and managed. The metadata based approach described herein may accommodate new lifecycle operations with little to no change in the underlying logic for monitoring an operation s progress. Rather than writing complex code an administrator or other user may define custom metadata objects to guide the monitoring process. The metadata based approach may also provide a highly scalable solution to lifecycle monitoring in cloud deployment environments where a large number of services are managed by a relatively small number of administrators. The monitoring and reporting of lifecycle operations may be tailored in such a way as to streamline their performance and facilitate processing of benchmark events.

According to one embodiment monitoring metadata is maintained by a lifecycle management system. The monitoring metadata that is maintained may generally comprise a set of signatures that may be used to track and report the progress of a lifecycle operation. The metadata may further comprise a signature to status mapping that maps a respective signature from the set of signatures to a status of a benchmark of the lifecycle operation. During runtime the lifecycle management system may read a respective signature from the metadata and monitor a set of log files associated with one or more software deployments for an occurrence of the signature. Based on such monitoring the lifecycle management system may determine the status of an operation benchmark with respect to one or more software deployments. The lifecycle management system may further generate and cause report data to be displayed where the report data indicates the status of the benchmark of the particular operation with respect to the one or more software deployments.

According to another embodiment the lifecycle management system maintains report metadata that indicates how to generate and present report data. For example the report metadata may define which operation benchmarks to display and an order in which they are displayed. During runtime the lifecycle management system may process signature matches for the benchmarks defined in the report metadata and display the status of the benchmarks in the indicated order. The administrator may specify one or more filter criteria to further select and refine which benchmarks are displayed.

Hosts to represent a set of one or more network hosts and generally comprise software deployments to agents to and log data to . Hosts to are communicatively coupled with operation management services and may send receive messages according to one or more communication protocols. Example communication protocols that may be implemented include without limitation the hypertext transfer protocol HTTP secure shell SSH and or other communication protocols of the internet protocol suite.

A software deployment in this context refers to an instance of a particular software application or set of interrelated software applications that are jointly managed. Example software deployments may include without limitation an instance of a database management system middleware applications customer relationship management systems CRMs operating systems enterprise resource planning ERP systems and application software. A software deployment may be an instance of a standalone software product package or an instance of software that is delivered as part of a service offering in a cloud deployment model. A deployment that is part of a cloud offering is also referred to herein as a service instance . Example cloud deployment models for a service instance may include without limitation software as a service SaaS infrastructure as a service IaaS database as a service DBaaS and platform as a service PaaS .

Agents to periodically or continuously send data collected from software deployments to to operation management services . The data collected by agents to may generally comprise data that identifies attributes of the corresponding software deployment and the status of a lifecycle operation if any that is being performed for the software deployment. As an example agents to may collect and send data that may include or otherwise identify without limitation one or more of the following attributes 

Although only one software deployment and one agent are illustrated per host the number of software deployments agents and log files per host may vary from implementation to implementation. For example multiple deployments agents and or log files may be provisioned on a single host. Furthermore in a clustered environment a single software deployment may logically be spread across a plurality of hosts. As an example a deployment may comprise a cloud database with multiple sub target database nodes spanning more than one compute node with each database node running a database server.

Operation management services generally comprise collection logic modelling logic reporting logic and control console . Each of these logic units is configured to perform a distinct set of functions with respect to managing lifecycle operations performed for software deployments to . The term logic as used herein includes computer or electrical hardware component s firmware a non transitory computer readable medium that stores instructions and or combinations of these components configured to perform one or more functions or actions and or to cause one or more functions or actions from another logic method and or system. Logic may include a microprocessor controlled by executable code a discrete logic e.g. ASIC an analog circuit a digital circuit a programmed logic device a memory device containing instructions that when executed perform an algorithm and so on. Logic may include one or more gates combinations of gates or other circuit components. Where multiple logic units are described it may be possible to incorporate the multiple logic units into one physical logic component. Similarly where a single logic unit is described it may be possible to distribute the single logic unit between multiple physical logic components.

Collection logic is configured to collect or otherwise receive data from agents to and to store the collected data in repository . Collection logic may collect data on a continuous or periodic basis depending on the particular implementation. As an example one or more of agents to may send collected data to collection logic in response to detecting occurrence of an event. As another example one or more of agents to may send collected data every few minutes or on some other periodic basis.

Modelling logic provides interfaces and logic for generating and saving meta models for monitoring and reporting. For example modelling logic may allow a user to create monitoring meta models and or reporting meta models as described in further detail below. Modelling logic may store the meta models in repository and or distribute the meta models to hosts to

Reporting logic generates and causes display of report data in accordance with the reporting meta models. Reporting logic may continuously or periodically generate and display updated report data to keep an administrator or other user apprised of the current progress of a lifecycle operation. Reporting logic may also provide logic for searching and filtering the displayed report data. For example reporting logic may receive filter criteria and in response adjust the report data that is displayed.

Control console provides a user interface that allows a user to monitor and administer locally or from a remote network location the lifecycle management processes described herein. The user interface may comprise without limitation a graphical user interface GUI an application programming interface API a command line interface CLI or some other means of interacting with a user. A user in this context may include without limitation an application or a human user such as a system administrator.

Repository comprises computer data storage that stores data for operation management services . For example repository may include without limitation a set of one or more disks memories storage appliances storage servers databases and or other non transitory volatile or non volatile storage mediums. Repository stores data that is used by operation management services including without limitation monitoring metadata reporting metadata event data and report data .

Monitoring metadata stores monitoring meta models or other metadata that indicates what should be monitored during a lifecycle operation. In an embodiment monitoring metadata includes a plurality of signatures for a lifecycle operation where each respective signature of the plurality of signatures is mapped to a status of a benchmark. The signatures may further be mapped to other attributes which may include without limitation location data that specifies one or more file locations of log file s to be monitored for the respective signature deployment identification data that identifies a deployment to which the signature applies and or lifecycle operation identification data that identifies an operation to which the signature applies.

Reporting metadata stores reporting meta models or other metadata that describes how to generate and display report data. For example reporting metadata may define the monitoring benchmarks for which signature pattern matches are processed and reported. Reporting metadata may further define one or more reporting characteristics which may include without limitation ordering data that specifies an order in which the benchmarks are processed for reporting purposes and or notification configuration data that specify conditions for reporting event alerts.

Event data stores data that identifies target events within software deployments to . A target event in this context refers to occurrence of a signature pattern within a log file. The occurrence of the signature may be matched to a benchmark status e.g. skipped started success failure not applicable an event start time and or an event end time.

Report data stores data indicating the progress of a lifecycle operation for one or more of software deployments to . For example report data may be used to display an ordered set of events from event data in accordance with reporting metadata .

The development lifecycle of a software deployment may include without limitation the following stages pre deployment deployment and post deployment. Each of these stages may entail one or more lifecycle operations. As an example the pre deployment phase may include operations for backing up data and provisioning software components. During the deployment and post deployment phases a software deployment may undergo various lifecycle operations such as upgrades patches and or configuration changes.

A lifecycle operation may be associated with one or more sub tasks which are herein referred to as operation benchmarks or milestones . For example a provisioning operation may involve installation of various software components where the installation of each separate software component represents a separate sub task. Similarly an upgrade operation may involve upgrading a plurality of software components with the upgrade of each separate software component corresponding to a separate sub task. The upgrade operation may further involve validation of one or more of the software components to determine whether they are functioning correctly. Lifecycle management system may track and report on the progress of such benchmarks as described in further detail below.

In one embodiment the plurality of signatures in the metadata are determined by automatically clustering and categorizing log files according to the techniques described in 62 056 073 that describes generating signatures. In a particular embodiment the plurality of signatures in the metadata comprise an automatically or manually selected subset of the signatures produced according to the techniques from 62 056 073. The signatures may be automatically selected and mapped to corresponding status identifiers that represent corresponding benchmarks of particular operations. In one embodiment the automatic selection and mapping is done using machine learning by applying results from a training set of manually classified log items or signatures to a larger set of log items to be automatically classified. The manually classified items or signatures are matched to corresponding signatures or are the signatures themselves from the set of automatically generated signatures according to the techniques of 62 056 073. Machine learning techniques such as techniques that model data using support vector machine techniques or other classification techniques or use decision trees or some other modeling data structure are applied to the manually classified log items or signatures to learn which signatures typically represent which status identifiers. Log items matching those signatures are then automatically classified using the matched status identifiers. The mapping between log items and status identifiers can change over time as additional information becomes available about incorrect classifications and or additional manual classifications.

As previously indicated lifecycle management system may maintain a set of monitoring metadata to determine which benchmarks to track for a given lifecycle operation. In one embodiment monitoring metadata includes a set of one or more meta models for monitoring where a meta model defines a set of signatures for various benchmarks of a particular lifecycle operation. A signature in this context may be a string value a sequence of characters or some other pattern that may be used to uniquely identify the status of a particular benchmark. For example the signature may correspond to a unique pattern that is included in a log message upon occurrence of an event. A signature may include one or more wildcard characters to cover a range of string value that map to the same status.

An operation benchmark may be associated with one or more signatures where each signature maps to a particular status. Examples of a benchmark status may include without limitation 

Monitoring metadata may map a signature to location data that indicates which logs to monitor for occurrence of the signature. As an example reporting metadata may specify a filename and or path for a log file or set of log files. The location data may include one or more wildcard characters to search a plurality of locations and log files for a signature.

Monitoring metadata may further map a signature to a location of a start time and or end time associated with the corresponding status. For example reporting data may include data identifying the location of a timestamp that indicates the time that a benchmark was skipped started or completed. In one embodiment the time information may be extracted from the log file that contains the signature.

Row includes the following metadata FA HCM UPGRADE 11.1.9.0.0 fsnadmin upgrade 11.1.9.0.0 ST1 11.1.9.0.0 ruplitesaas output logs ruplitesaasautocorrect root.log Bootstrap Root RUPLite for SAAS Bootstrapping autocorrect root Started Started where FA HCM corresponds to the deployment type UPGRADE corresponds to the lifecycle operation type 11.1.9.0.0 corresponds to the release version of the deployment fsnadmin upgrade 11.1.9.0.0 ST1 11.1.9.0.0 ruplitesaas output logs ruplitesaasautocorrect root.log Bootstrap Root corresponds to the file location of the logs to monitor Bootstrap Root corresponds to the benchmark to which the signature applies RUPLite for SAAS Bootstrapping autocorrect root Started corresponds signature pattern and Started corresponds to the status of the benchmark. The above metadata may be processed such that during an upgrade of Fusion Apps HCM FA HCM to 11.1.9.0.0 software if the file fsnadmin upgrade 11.1.9.0.0 ST1 11.1.9.0.0 ruplitesaas output logs ruplitesaasautocorrect root.log contains a pattern string RUPLite for SAAS Bootstrapping autocorrect root Started the benchmark Bootstrap Root is marked as Started 

Rows and map different signatures to different statuses for the same benchmark. The metadata of row may be processed such that during an upgrade of Fusion Apps HCM to 11.1.9.0.0 software if the file fsnadmin upgrade 11.1.9.0.0 ST1 11.1.9.0.0 ruplitesaas output logs ruplitesaasautocorrect root.log contains a pattern string RUPLite for SAAS Bootstrapping autocorrect root Completed the benchmark Bootstrap Root is marked as Completed . Row is similar with the exception that if the same file includes the signature RUPLite for SAAS Bootstrapping autocorrect root Completed with failures the task Bootstrap Root is marked as Failed .

Metadata may include a plurality of other signatures that are not illustrated for the purpose of brevity. For example an upgrade operation for an HCM system may include without limitation benchmarks such as upgrading one or more application tiers performing backups performing downtime checks installing new components and or upgrading language packs. For each respective benchmark metadata may map a set of one or more signatures to a status of the respective benchmark. The metadata and signatures specified therein may vary by release version deployment type and or lifecycle operation. As new versions of software are released and benchmarks evolve over time new meta models may be added or the signatures of an existing meta model may be updated.

In one embodiment lifecycle management system maintains a set of reporting metadata to determine how to report benchmark events for a lifecycle operation. The reporting metadata may comprise a set of one or more reporting meta models. Each reporting meta model may be associated with a different respective lifecycle operation and type of software deployment and may specify a set of one or more attributes for reporting benchmarks of the respective lifecycle operation and software deployment. Example attributes that may be specified by a reporting meta model may include without limitation benchmark identification data that specifies which benchmarks to report ordering information that indicates an order for reporting the benchmarks and notification preferences for generating sending and or displaying the report data.

Modelling logic may provide an interface through which an administrator or other user may create reporting meta models. For example modelling logic may provide through a GUI or some other interface a list of benchmark operations defined by a monitoring meta model for a particular lifecycle operation. An administrator or other user may then provide input to select a subset of benchmarks for reporting the progress of the lifecycle operation. In response to the selection modelling logic may generate or update a reporting meta model to include data that identifies the subset of benchmarks. Benchmarks that are not included in the selected subset are not included in the generated reports during runtime.

Modelling logic may further provide an interface through which a user may control notification alerts. A notification alert in this context may be an email or some other message that notifies the user of the occurrence of an event. In one embodiment the user may specify triggers for generating and sending a notification. As an example the user may select to be notified upon the completion of a benchmark but forego notifications when a benchmark has been started or skipped. In response modelling logic may store the notification preferences within reporting metadata and during runtime reporting logic may trigger notification based on the saved preferences. The reporting metadata thus allows the administrator to customize alert notifications to control the volume information that is reported during a lifecycle operation.

In one embodiment a reporting benchmark may be distinct from an operation benchmark. For one reporting benchmark there may be one or more operation benchmarks. From each host one or more log files may be scanned to determine the operation benchmarks. Once all the operation benchmarks are gathered the reporting benchmarks are applied. For example there may be a particular set of host machines where each host in the set of hosts has a log message indicating the completion of a particular task. The log message from the different host machines gives the status of a different operation benchmark. Once the operation benchmarks are gathered the start time and end time of a reporting benchmark may be computed where the start time is the earliest start time of an operation benchmark from a plurality of operation benchmarks associated with the reporting benchmark and the end time is the last time associated with a status among the plurality of benchmarks. The status success failure skipped associated with the reporting benchmark may be based on a plurality of operation benchmarks. If all operation benchmarks succeed for example then the reporting benchmark may be reported as a success. If one operation benchmark failed or skipped then the reporting benchmark may also be reported as having failed or skipped even if other operation benchmarks of the plurality of operation benchmarks associated with the reporting benchmark have succeeded.

When a lifecycle operation is performed for one of software deployments to the corresponding agent monitors for the occurrence of target signatures within operation log data. A target signature in this context is a signature that is included in an associated monitoring meta model. As an example if software deployment is an instance of Fusion Apps HCM software and has been upgraded to a release version of 11.1.9.0.0 then agent may monitor log messages at the location fsnadmin upgrade 11.1.9.0.0 ST1 11.1.9.0.0 ruplitesaas output logs ruplitesaasautocorrect root.log for the signatures specified in rows to of . The agent may report signature matches if any to operation management services .

At step the agent collects log messages from the software deployment during the lifecycle operation. In order to collect log messages the agent may identify the file location of the log files from the instantiated meta model. The agent may then search these locations for log messages. The agent may collect the log message on a periodic or continuous basis depending on the particular implementation.

At step the agent compares signatures from monitoring metadata to the collected log messages. For example the agent may iterate through a set of signatures included in the instantiated monitoring meta model and compare each respective signature to the collected log messages to determine if any of the log messages includes a string value or other pattern that matches the respective signature.

At step the agent determines based on the comparison whether a target signature matches any patterns within the log messages. If a match is found then the process continues to step and the agent sends event data to the central server for each signature that was matched. Otherwise the process skips to step and the agent determines that there are no matches for the signature.

At step the agent determines whether the pattern matching for the operational monitoring benchmarks defined in the monitoring metadata are complete. If not then the process returns to step and continues monitoring log messages for the occurrence of signatures. The agent may then update the status information for a benchmark is a signature is subsequently detected in a log file. Once the pattern matching for all monitoring benchmarks is complete at step the reporting benchmark updates may also be completed and the monitoring process with respect to the particular software deployment may end.

The monitoring process described above may be concurrently applied to a plurality of different software deployments. As an example agent may monitor a first lifecycle operation performed for software deployment while agent monitors a second lifecycle operation for software deployment . Each of the agents may thus independently report benchmark events to operation management services .

The monitoring meta model instantiated by each active agent may vary based on the type of software deployment that is being monitored and the type of lifecycle operation being performed. For instance if agent is monitoring the upgrade of a first type of deployment e.g. a CRM agent may instantiate a first meta model that includes a first set of signatures. Agent may then monitor log data for the first set of signatures during the upgrade operation. During this time agent may concurrently monitor the upgrade of a second type of deployment e.g. an ERP which may involve instantiating a second meta model that includes a different set of signatures than the first meta model. Agent monitors log data for the second set of signatures included in the second meta model. This process may continue for other agents as well with each agent independently monitoring a different lifecycle operation on different software instances.

In one embodiment operation management services acts as a central servicer for agents to . As each agent independently processes log messages during a lifecycle operation event data is sent to operation management services . Report logic may process the event data to generate a centralized report that indicates the progress of each of the lifecycle operations being performed on software deployments to

Steps to of depict an example process for generating a centralized report according to an embodiment. At step the central server receives event data from the agents for the respective lifecycle operations being monitored. The event data that is received may indicate the occurrence of a pattern matched to a task status and its start or end time.

At step the central server correlates events based on benchmark and time. For example the central server may build a first index that sorts the events according to benchmark and another index that sorts the events based on start or end time. As previously indicated different signatures may be correlated to the same event. For example a different signature may be used for different respective statuses of an event. Across different types of deployments different signatures may be used for the same benchmark status. Thus a first signature may be used to indicate success of a benchmark for a first type of deployment while a different signature may be used to indicate success for the same benchmark for a different type of deployment.

At step the central server generates an ordered report of benchmarks in accordance with the reporting metadata. The status of benchmarks that are not specified in the reporting metadata may be omitted in the report. The central server may determine the status of the benchmarks for reporting based on the event correlations described above.

At step the central server displays the status of the benchmarks on a monitoring dashboard. In one embodiment each software deployment that is being monitored is displayed as a separate row in a monitoring dashboard. The status of various benchmarks may be displayed as column values for each software deployment. The central server may allow the administrator to look at each of the deployment s lifecycle operation the start and end times the progress of each stage and any failures from a single dashboard.

A benchmark may span across different release versions and different types of software deployments. The status of benchmark for example is reported for ERP HCM CRM and GSI deployments. The status indicates the date and time and outcome of the particular benchmark. In some cases a benchmark may not be applicable to a particular service instance. For example the status of benchmark and benchmark are left blank for the CRM deployment with a release version of 11.1.8.0.0 as they are not applicable to this service instance.

In one embodiment reporting logic allows the user to refine the report data that is displayed in real time. To refine the report the user may input a set of one or more filter criteria for organizing the report data. Example filter criteria may include without limitation 

According to one embodiment the techniques described herein are implemented by one or more special purpose computing devices. The special purpose computing devices may be hard wired to perform the techniques or may include digital electronic devices such as one or more application specific integrated circuits ASICs or field programmable gate arrays FPGAs that are persistently programmed to perform the techniques or may include one or more general purpose hardware processors programmed to perform the techniques pursuant to program instructions in firmware memory other storage or a combination. Such special purpose computing devices may also combine custom hard wired logic ASICs or FPGAs with custom programming to accomplish the techniques. The special purpose computing devices may be desktop computer systems portable computer systems handheld devices networking devices or any other device that incorporates hard wired and or program logic to implement the techniques.

For example is a block diagram that illustrates a computer system upon which an embodiment of the invention may be implemented. Computer system includes a bus or other communication mechanism for communicating information and a hardware processor coupled with bus for processing information. Hardware processor may be for example a general purpose microprocessor.

Computer system also includes a main memory such as a random access memory RAM or other dynamic storage device coupled to bus for storing information and instructions to be executed by processor . Main memory also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor . Such instructions when stored in non transitory storage media accessible to processor render computer system into a special purpose machine that is customized to perform the operations specified in the instructions.

Computer system further includes a read only memory ROM or other static storage device coupled to bus for storing static information and instructions for processor . A storage device such as a magnetic disk optical disk or solid state drive is provided and coupled to bus for storing information and instructions.

Computer system may be coupled via bus to a display such as a liquid crystal display LCD or a light emitting diode LED display for displaying information to a computer user. An input device including alphanumeric and other keys is coupled to bus for communicating information and command selections to processor . Another type of user input device is cursor control such as a mouse a trackball or cursor direction keys for communicating direction information and command selections to processor and for controlling cursor movement on display . This input device typically has two degrees of freedom in two axes a first axis e.g. x and a second axis e.g. y that allows the device to specify positions in a plane.

Computer system may implement the techniques described herein using customized hard wired logic one or more ASICs or FPGAs firmware and or program logic which in combination with the computer system causes or programs computer system to be a special purpose machine. According to one embodiment the techniques herein are performed by computer system in response to processor executing one or more sequences of one or more instructions contained in main memory . Such instructions may be read into main memory from another storage medium such as storage device . Execution of the sequences of instructions contained in main memory causes processor to perform the process steps described herein. In alternative embodiments hard wired circuitry may be used in place of or in combination with software instructions.

The term storage media as used herein refers to any non transitory media that store data and or instructions that cause a machine to operate in a specific fashion. Such storage media may comprise non volatile media and or volatile media. Non volatile media includes for example optical disks or magnetic disks such as storage device . Volatile media includes dynamic memory such as main memory . Common forms of storage media include for example a floppy disk a flexible disk hard disk solid state drive magnetic tape or any other magnetic data storage medium a CD ROM any other optical data storage medium any physical medium with patterns of holes a RAM a PROM and EPROM a FLASH EPROM NVRAM any other memory chip or cartridge.

Storage media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between storage media. For example transmission media includes coaxial cables copper wire and fiber optics including the wires that comprise bus . Transmission media can also take the form of acoustic or light waves such as those generated during radio wave and infra red data communications.

Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor for execution. For example the instructions may initially be carried on a magnetic disk or solid state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system can receive the data on the telephone line and use an infra red transmitter to convert the data to an infra red signal. An infra red detector can receive the data carried in the infra red signal and appropriate circuitry can place the data on bus . Bus carries the data to main memory from which processor retrieves and executes the instructions. The instructions received by main memory may optionally be stored on storage device either before or after execution by processor .

Computer system also includes a communication interface coupled to bus . Communication interface provides a two way data communication coupling to a network link that is connected to a local network . For example communication interface may be an integrated services digital network ISDN card cable modem satellite modem or a modem to provide a data communication connection to a corresponding type of telephone line. As another example communication interface may be a local area network LAN card to provide a data communication connection to a compatible LAN. Wireless links may also be implemented. In any such implementation communication interface sends and receives electrical electromagnetic or optical signals that carry digital data streams representing various types of information.

Network link typically provides data communication through one or more networks to other data devices. For example network link may provide a connection through local network to a host computer or to data equipment operated by an Internet Service Provider ISP . ISP in turn provides data communication services through the world wide packet data communication network now commonly referred to as the Internet . Local network and Internet both use electrical electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link and through communication interface which carry the digital data to and from computer system are example forms of transmission media.

Computer system can send messages and receive data including program code through the network s network link and communication interface . In the Internet example a server might transmit a requested code for an application program through Internet ISP local network and communication interface .

The received code may be executed by processor as it is received and or stored in storage device or other non volatile storage for later execution.

The techniques described herein are implemented using one or more processing solutions examples of which include distributed systems clustered computing systems and cloud computing systems. In an embodiment operation management services are part of a cloud computing system. A cloud computing system implements one or more of cloud storage cloud processing cloud communication and any other kind of cloud computing service. Further cloud computing systems may operate under a pay for what you use as you use it model under a fixed subscription model etc. In this embodiment any part or the whole of the functionality attributed to system or to other entities within this description is controllable via an interface that is exposed at a cloud computing system.

In the foregoing specification embodiments of the invention have been described with reference to numerous specific details that may vary from implementation to implementation. The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. The sole and exclusive indicator of the scope of the invention and what is intended by the applicants to be the scope of the invention is the literal and equivalent scope of the set of claims that issue from this application in the specific form in which such claims issue including any subsequent correction.

