---

title: Network control system for configuring middleboxes
abstract: Some embodiments provide a method for configuring a logical middlebox in a hosting system that includes a set of nodes. The logical middlebox is part of a logical network that includes a set of logical forwarding elements that connect a set of end machines. The method receives a set of configuration data for the logical middlebox. The method uses a stored set of tables describing physical locations of the end machines to identify a set of nodes at which to implement the logical middlebox. The method provides the logical middlebox configuration for distribution to the identified nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09558027&OS=09558027&RS=09558027
owner: NICIRA, INC.
number: 09558027
owner_city: Palo Alto
owner_country: US
publication_date: 20150112
---
This application is a continuation application of U.S. patent application Ser. No. 13 678 485 filed Nov. 15 2012 now published as U.S. Patent Publication 2013 0132536. U.S. patent application Ser. No. 13 678 485 claims the benefit of U.S. Provisional Application 61 560 279 entitled Virtual Middlebox Services filed Nov. 15 2011. U.S. patent application Ser. No. 13 678 485 and U.S. Provisional Application 61 560 279 are incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers middleboxes e.g. firewalls servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual network components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L2 domain but L2 domains cannot scale to large sizes. Furthermore retaining user isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments provide a network control system that allows a user to specify a logical network that includes one or more middleboxes as well as logical data path sets. The user specifies 1 a network topology including logical forwarding elements e.g. logical routers logical switches and middlebox locations within the network 2 routing policies for forwarding traffic to the middleboxes and 3 configurations for the different middleboxes. The network control system of some embodiments uses a set of network controllers to distribute both flow entries that implement the network topology and the middlebox configurations to host machines on which managed switching elements and distributed middleboxes operate as well as to centralized middlebox appliances operating outside of the host machines.

The network controllers in some embodiments are arranged in a hierarchical manner. A user enters the topology and configuration information into a logical controller or an input translation controller that passes the information to a logical controller as a set of records. The logical controller communicatively couples to a set of physical controllers with each physical controller in charge of distributing the configuration data to one or more host machines. That is each host machine is assigned to a particular physical controller that acts as the master for that host machine. The logical controller identifies which host machines need to receive the configuration then passes the appropriate information to the physical controllers that manage the identified host machines. Before exporting the records to the physical controllers the logical controller translates the flow entry data. In some embodiments the middlebox configuration data is not translated however.

The physical controllers receive the information perform additional translation for at least some of the data and pass the translated data to the host machines i.e. to the managed switching elements and middleboxes on the host machines . As with the logical controllers in some embodiments the physical controllers translate the flow entries destined for the managed switches but do not perform any translation on the middlebox configuration data. The physical controllers of some embodiments do however generate additional data for the middleboxes. Specifically because the distributed middlebox applications elements operating on the host machine e.g. as daemons or applications may perform several separate middlebox processes for different tenant networks the physical controllers assign a slicing identifier to the particular configuration for the middlebox. This slicing identifier is also communicated to the managed switching element which adds the identifier to packets destined for the middlebox in some embodiments.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

In the following detailed description of the invention numerous details examples and embodiments of the invention are set forth and described. However it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.

Some embodiments provide a network control system that allows a user to specify a logical network that includes one or more middleboxes e.g. firewalls load balancers network address translators intrusion detection systems IDS wide area network WAN optimizers etc. as well as logical data path sets. The user specifies 1 a network topology including logical forwarding elements e.g. logical routers logical switches and middlebox locations within the network 2 routing policies for forwarding traffic to the middleboxes and 3 configurations for the different middleboxes. The network control system of some embodiments uses a set of network controllers to distribute both flow entries that implement the network topology and the middlebox configurations to host machines on which managed switching elements and distributed middleboxes operate as well as to centralized middlebox appliances operating outside of the host machines.

The network controllers in some embodiments are arranged in a hierarchical manner. A user enters the topology and configuration information into a logical controller or an input translation controller that passes the information to a logical controller as a set of records. The logical controller communicatively couples to a set of physical controllers with each physical controller in charge of distributing the configuration data to one or more host machines. That is each host machine is assigned to a particular physical controller that acts as the master for that host machine. The logical controller identifies which host machines need to receive the configuration then passes the appropriate information to the physical controllers that manage the identified host machines. Before exporting the records to the physical controllers the logical controller translates the flow entry data. In some embodiments the middlebox configuration data is not translated however.

The physical controllers receive the information perform additional translation for at least some of the data and pass the translated data to the host machines i.e. to the managed switching elements and middleboxes on the host machines . As with the logical controllers in some embodiments the physical controllers translate the flow entries destined for the managed switches but do not perform any translation on the middlebox configuration data. The physical controllers of some embodiments do however generate additional data for the middleboxes. Specifically because the distributed middlebox applications elements operating on the host machine e.g. as daemons or applications may perform several separate middlebox processes for different tenant networks the physical controllers assign a slicing identifier to the particular configuration for the middlebox. This slicing identifier is also communicated to the managed switching element which adds the identifier to packets destined for the middlebox in some embodiments.

In addition a middlebox attaches to the logical router . One of ordinary skill in the art will recognize that the network topology represents just one particular logical network topology into which a middlebox may be incorporated. In various embodiments the middlebox may be located directly between two other components e.g. directly between the external network and logical router e.g. in order to monitor and process all traffic entering or exiting the logical network or in other locations in a more complex network.

In the architecture shown in the middlebox is not located within the direct traffic flow either from one domain to the other or between the external world and the domain. Accordingly packets will not be sent to the middlebox unless routing policies are specified e.g. by a user such as a network administrator for the logical router that determine which packets should be sent to the middlebox for processing. Some embodiments enable the use of policy routing rules which forward packets based on data beyond the destination address e.g. destination IP or MAC address . For example a user might specify e.g. through a network controller application programming interface API that all packets with a source IP address in the logical subnet switched by logical switch and with a logical ingress port that connects to the logical switch or all packets that enter the network from the external network destined for the logical subnet switched by the logical switch should be directed to the middlebox for processing.

The logical network topology entered by a user e.g. a network administrator is distributed through the network control system to various physical machines in order to implement the logical network. The second stage of conceptually illustrates such a physical implementation of the logical network . Specifically the physical implementation illustrates several nodes including a first host machine a second host machine and a third host machine . Each of the three nodes hosts at least one virtual machine of the logical network with virtual machine hosted on the first host machine virtual machines and hosted on the second host machine and virtual machine hosted on the third host machine .

In addition each of the host machines includes a managed switching element MSE . The managed switching elements of some embodiments are software forwarding elements that implement logical forwarding elements for one or more logical networks. For instance the MSEs in the hosts include flow entries in forwarding tables that implement the logical forwarding elements of network . Specifically the MSEs on the host machines implement the logical switches and as well as the logical router . On the other hand some embodiments only implement logical switches at a particular node when at least one virtual machine connected to the logical switch is located at the node i.e. only implementing logical switch and logical router in the MSE at host .

The implementation of some embodiments also includes a pool node that connects to the host machines. In some embodiments the MSEs residing on the host perform first hop processing. That is these MSEs are the first forwarding elements a packet reaches after being sent from a virtual machine and attempt to perform all of the logical switching and routing at this first hop. However in some cases a particular MSE may not store flow entries containing all of the logical forwarding information for a network and therefore may not know what to do with a particular packet. In some such embodiments the MSE sends the packet to a pool node for further processing. These pool nodes are interior managed switching elements which in some embodiments store flow entries that encompass a larger portion of the logical network than the edge software switching elements.

Similar to the distribution of the logical switching elements across the hosts on which the virtual machines of network reside the middlebox is distributed across middlebox elements on these hosts . In some embodiments a middlebox module or set of modules resides on the host machines e.g. operating in the hypervisor of the host .

As stated the network control system of some embodiments is used to configure the distributed forwarding elements MSEs and middleboxes. Each of the three hosts is assigned to a particular physical controller that receives flow entries for the MSEs and configuration information for the middleboxes performs any necessary translations on the data and passes the data to the elements on the hosts.

While illustrates only one logical network implemented across the hosts some embodiments implement numerous logical networks e.g. for different tenants across the set of hosts. As such a middlebox element on a particular host might actually store configurations for several different middleboxes belonging to several different logical networks. For example a firewall element may be virtualized to implement two or more different firewalls. These will effectively operate as two separate middlebox processes such that the middlebox element is sliced into several virtual middleboxes of the same type .

In addition when the MSE on the host sends packets to the middlebox some embodiments append e.g. prepend a slice identifier or tag on the packet to identify to which of the several virtual middleboxes the packet is being sent. When multiple middleboxes are implemented on the same middlebox element for a single logical network e.g. two different load balancers the slice identifier will need to identify the particular middlebox slice rather than just the logical network to which the packet belongs. Different embodiments may use different slice identifiers for the middleboxes.

In some embodiments these slice identifiers are assigned by the network control system. Because for distributed middleboxes such as middlebox the middlebox element will generally only receive packets from the MSE on that host the slice identifiers can be assigned separately for each middlebox element by the physical controller that manages the particular middlebox element and MSE . In the case of a centralized middlebox i.e. a separate physical appliance to which all of the MSEs send packets a single slice identifier will be used for the virtual middlebox operating on that appliance. In some embodiments a physical controller that manages the appliance assigns this identifier and then distributes this through the network control system to the other physical controllers in order for the other physical controllers to pass the information to the MSEs.

The above illustrates examples of the implementation of logical middleboxes in a network of some embodiments. Several more detailed embodiments are described below. Section I describes the network control system of some embodiments for configuring a network in order to implement a logical network that includes a firewall. Section II describes the architecture of a network controller of some embodiments. Next Section III describes packet processing between two virtual machines when the packet passes through a middlebox. Finally Section IV describes an electronic system with which some embodiments of the invention are implemented.

As described above some embodiments use a network control system in order to provision middleboxes and managed switching elements for a managed network. In some embodiments the network control system is a hierarchical set of network controllers with each level in the hierarchy performing different functions in the provisioning of the managed switching elements and middleboxes.

In some embodiments the middlebox configuration interface is actually a part of the input translation controller in this figure the two are shown separately as they receive different inputs and include different APIs for communicating with the user. In some embodiments each of the controllers in the network control system has the capability to function as an input translation controller logical controller and or physical controller. That is each controller machine includes the necessary application stack for performing the functions of any of the different controller types but only one of those application stacks is used at any time. Alternatively in some embodiments a given controller may only have the functionality to operate as a particular one of the types of controller e.g. as a physical controller . In addition different combinations of controllers may run in the same physical machine. For instance the input translation controller middlebox configuration interface and the logical controller may run in the same computing device with which a user interacts.

Furthermore each of the controllers illustrated in and subsequent is shown as a single controller. However each of these controllers may actually be a controller cluster that operates in a distributed fashion to perform the processing of a logical controller physical controller or input translation controller.

The input translation controller of some embodiments includes an input translation application that translates network configuration information received from a user. For example a user may specify a network topology such as that shown in which includes a specification as to which machines belong in which logical domain. This effectively specifies a logical data path set or a set of logical forwarding elements. For each of the logical forwarding elements the user specifies the machines or other elements that connect to the logical switch i.e. to which logical ports are assigned for the logical switch . In some embodiments the user also specifies IP addresses for the machines.

For example a user might enter a network topology such as that shown in with machines connected to logical switches a logical router connecting the two logical switches and one or more middleboxes connected to ports of the logical router as well. As shown in the flow generation column the input translation controller translates the entered network topology into logical control plane data that describes the network topology. In some embodiments the logical control plane data is expressed as a set of database table records e.g. in the nLog language . An entry in the control plane describing the attachment of a particular virtual machine to the network might state that a particular MAC address B is located at a particular logical port X of a particular logical switch.

The middlebox configuration interface receives middlebox configuration input from a user. In some embodiments each different middlebox e.g. middleboxes from different providers different types of middleboxes may have a different API particular to the middlebox implementation. That is the different middlebox implementations have different interfaces presented to the user i.e. the user will have to enter information in different formats for different particular middleboxes . As shown in the middlebox data generation column of the user enters a middlebox configuration which is translated by the middlebox API into middlebox configuration data.

In some embodiments the middlebox configuration data as translated by the configuration interface is also a set of records with each record specifying a particular rule. These records in some embodiments are in a similar format to the flow entries propagated to the managed switching elements. In fact some embodiments use the same applications on the controllers to propagate the firewall configuration records as for the flow entries and the same table mapping language e.g. nLog for the records.

While this figure illustrates the middlebox configuration data being sent to the logical controller some centralized middleboxes of some embodiments are only accessible through a direct interface with the middlebox device. That is rather than entering a configuration that is sent to the logical controller and distributed through the network control system the user enters a configuration directly into the middlebox device. In such a case the user will still need to enter routing policies to send packets to the middlebox as part of the network topology configuration. In some such embodiments the network control system will still generate slicing data i.e. virtualization identifiers for the middlebox as described below. On the other hand in some embodiments the user configures the slicing data for the middlebox and either the middlebox or the user provides this information to the network control system.

In some embodiments each logical network is governed by a particular logical controller e.g. logical controller . With respect to the flow generation for the managed switching elements the logical controller of some embodiments translates the logical control plane received from the input translation controller data into logical forwarding plane data and the logical forwarding plane data into universal control plane data. In some embodiments the logical controller application stack includes a control application for performing the first translation and a virtualization application for performing the second translation. Both of these applications in some embodiments use a rules engine for mapping a first set of tables into a second set of tables. That is the different data planes are represented as tables e.g. nLog tables and the controller applications use a table mapping engine to translate between the data planes. In some embodiments both the control application and virtualization application use the same rules engine to perform their translations.

Logical forwarding plane data in some embodiments consists of flow entries described at a logical level. For the MAC address B at logical port X logical forwarding plane data might include a flow entry specifying that if the destination of a packet matches MAC B forward the packet to port X.

The translation from logical forwarding plane to physical control plane in some embodiments adds a layer to the flow entries that enables a managed switching element provisioned with the flow entries to convert packets received at a physical layer port e.g. a virtual interface into the logical domain and perform forwarding in this logical domain. That is while traffic packets are sent and received within the network at the physical layer the forwarding decisions are made according to the logical network topology entered by the user. The conversion from the logical forwarding plane to the physical control plane enables this aspect of the network in some embodiments.

As shown the logical controller converts the logical forwarding plane data to a universal physical control plane while the physical controllers convert the universal physical control plane data to a customized physical control plane. The universal physical control plane data of some embodiments is a data plane that enables the control system of some embodiments to scale even when it contains a large number of managed switching elements e.g. thousands to implement a logical data path set. The universal physical control plane abstracts common characteristics of different managed switching elements in order to express physical control plane data without considering differences in the managed switching elements and or location specifics of the managed switching elements.

For the example noted above attachment of MAC B to logical port X the universal physical control plane would involve several flow entries. The first entry states that if a packet matches the particular logical data path set e.g. based on the packet being received at a particular logical ingress port and the destination address matches MAC B then forward the packet to logical port X. This adds the match over the logical data path set the conversion from a physical port to a logical port to the forwarding entry that performs its analysis in the logical domain. This flow entry will be the same in the universal and customized physical control planes in some embodiments.

Additional flows are generated to match a physical ingress port e.g. a virtual interface of the host machine to the logical ingress port X for packets received from MAC A as well as to match logical port X to the particular egress port of the physical managed switch for packets sent to MAC A . However these physical ingress and egress ports are specific to the host machine containing the managed switching element. As such the universal physical control plane entries include abstract physical ports i.e. a generic abstraction of a port not specific to any particular physical host machine to logical ingress ports as well as for mapping logical egress ports to generic physical egress ports.

The middlebox configuration data on the other hand is not converted by the logical controller in some embodiments while in other embodiments the logical controller performs at least a minimal translation of the middlebox configuration data records. As many middlebox packet processing modification and analysis rules operate on the IP address or TCP connection state of the packets and the packets sent to the middlebox will have this information exposed i.e. not encapsulated within the logical port information the middlebox configuration does not require translation from logical to physical data planes. Thus the same middlebox configuration data is passed from the middlebox configuration interface to the logical controller and then to the physical controllers and .

In order to distribute the physical control plane data as well as the middlebox configuration data the logical controller has to identify which of the host machines and thus which of the physical controllers need to receive which flow entries and which middlebox configuration information. In some embodiments the logical controller stores a description of the logical network and of the physical implementation of that physical network. The logical controller receives the one or more middlebox configuration records for a distributed middlebox and identifies which of the various nodes will need to receive the configuration information.

In some embodiments the entire middlebox configuration is distributed to middlebox elements at all of the host machines so the logical controller identifies all of the machines on which at least one virtual machine resides whose packets require use of the firewall. In general the identified machines are the hosts for all of the virtual machines in a network e.g. as for the middlebox shown in . However some embodiments may identify a subset of the virtual machines in the network if the network topology is such that the middlebox will never be needed at certain host machines. Some embodiments make decisions about which host machines to send the configuration data to on a per record basis. That is each particular rule may apply only to a subset of the virtual machines e.g. only packets originating from a particular virtual machine or subset of virtual machines and only hosts running these virtual machines need to receive the record.

Similarly the logical controller identifies which nodes should receive each flow entry in the physical control plane. For instance the flow entries implementing the logical switch are distributed to the hosts and but not the host in .

Once the logical controller identifies the particular nodes to receive the records the logical controller identifies the particular physical controllers that manage these particular nodes. In some embodiments each host machine has an assigned master physical controller. Thus if the logical controller identifies only first and second hosts as destinations for the configuration data the physical controllers for these hosts will be identified to receive the data from the logical controller and other physical controllers will not receive this data . For a centralized middlebox the logical controller needs only to identify the single physical controller that manages the appliance implementing the middlebox. When the centralized middlebox is implemented as a cluster e.g. as a set of resources a master backup cluster etc. each of the middlebox appliances in the cluster will receive the configuration data. The middleboxes in the cluster are all managed by a single physical controller in some embodiments while in other embodiments different physical controllers manage different middleboxes within a cluster.

In order to supply the middlebox configuration data to the hosts the logical controller of some embodiments pushes the data using an export module that accesses the output of the table mapping engine in the logical controller to the physical controllers. In other embodiments the physical controllers request configuration data e.g. in response to a signal that the configuration data is available from the export module of the logical controller.

As stated each of the physical controllers and is a master of one or more managed switching elements e.g. located within host machines . In this example the first physical controller is a master of the managed switching elements at host machines and while the second physical controller is a master of the managed switching element at host machine . In some embodiments a physical controller receives the universal physical control plane data for a logical network and translates this data into customized physical control plane data for the particular managed switches that the physical controller manages that need to receive the data as the physical controller may also manage additional managed switching elements that do not receive the data for a particular logical network . In other embodiments the physical controller passes the appropriate universal physical control plane data to the managed switching elements which includes the ability e.g. in the form of a chassis controller running on the host machine to perform the conversion itself.

The universal physical control plane to customized physical control plane translation involves a customization of various data in the flow entries. While the universal physical control plane entries are applicable to any managed switching element because the entries include generic abstractions for any data that is different for different switching elements the customized physical control plane entries include substituted data specific to the particular managed switching element to which the entry will be sent. For instance the physical controller customizes the physical layer ports in the universal physical control plane ingress and egress port integration entries to include the actual physical layer ports e.g. virtual interfaces of the specific host machines.

As shown in the physical controllers and pass information to both the managed switching elements and the middleboxes on their assigned host machines. In some embodiments the middlebox configuration and the physical control plane data are sent to the same database running on the host machine and the managed switching element and middlebox module retrieve the appropriate information from the database. Similarly for a centralized middlebox the physical controller passes the middlebox configuration data to the middlebox appliance e.g. to a database at the middlebox for storing configuration data .

The customized physical control plane data passed to the managed switching element includes attachment and slicing information to enable the managed switching element to send packets to the middleboxes in some embodiments. This slicing data as shown by the middlebox slice information generation column of is generated within the physical controller and also sent to the middlebox in some embodiments along with the physical controller. Because the middlebox configuration is used to virtualize a middlebox instance within the distributed middlebox element the middlebox element may have multiple separate middlebox processes running at once e.g. for different tenant networks for different logical middleboxes within a single tenant network .

Essentially the slicing information is a tag for the managed switching element to add to packets that it sends to the middlebox. The tag indicates to which of the potentially several processes being run by the middlebox the packet should be sent. Thus when the middlebox receives the packet the tag enables the middlebox to use the appropriate set of packet processing analysis modification etc. rules in order to perform its operations on the packet. Some embodiments rather than adding slicing information to the packet either define different ports of the managed switching element for each middlebox instance and essentially use the ports to slice the traffic destined for the firewall in the distributed case or connect to different ports of the centralized appliance to differentiate between the instances in the centralized case .

In order to send the slicing data to the managed switching element as part of the customized physical control plane data in some embodiments the physical controller adds flow entries specifying slicing information particular to the middlebox. Specifically for a particular managed switching element the flow entry may specify to add the slicing tag for a particular middlebox which may be e.g. a VLAN tag or similar tag to a packet before sending the packet to the particular middlebox based on a match of the port connecting to the middlebox.

The attachment information in some embodiments includes flow entries that enable the managed switching element to send packets to the middlebox. In the distributed middlebox case with the middlebox in the same physical machine as the managed switching element the middlebox and the managed switching element negotiate a software port abstraction through which packets will be transferred in some embodiments. In some embodiments the managed switching element or the middlebox element pass this information up to the physical controller enabling the physical controller to use the information in the customized physical control plane data i.e. using the specific software port for the customized physical control plane entries .

For centralized middleboxes some embodiments provide tunneling attachment data to both the managed switching element and the middlebox. The middlebox in some embodiments will need to know the type of tunnel encapsulation various host machines will use to send packets to the middlebox. In some embodiments the middlebox has a list of accepted tunneling protocols e.g. STT GRE etc. and the chosen protocol is coordinated between the managed switching element s and the middlebox. The tunneling protocol may be entered by the user as part of the middlebox configuration or may be automatically determined by the network control system in different embodiments. The physical controller will also add the tunnel encapsulation information to the customized physical control plane flow entries in order for the managed switching element to encapsulate packets properly for sending to the middlebox.

Upon receiving the customized physical control plane data from the physical controller a managed switching element performs a translation of the customized physical control plane data into physical forwarding plane data. The physical forwarding plane data in some embodiments are the flow entries stored within a forwarding table of a switching element either a physical router or switch or a software switching element against which the switching element actually matches received packets and performs actions on the packets based on those matches.

The middlebox receives its configuration data from the physical controller and in some embodiments translates this configuration data. The middlebox configuration data will be received in a particular language to express the packet processing analysis modification etc. rules through a control plane API of the middlebox. The middlebox distributed and or centralized of some embodiments compiles these rules into more optimized packet classification rules. In some embodiments this transformation is similar to the physical control plane to physical forwarding plane data translation. When a packet is received by the middlebox it applies the compiled optimized rules in order to efficiently and quickly perform its operations on the packet.

As shown in the middlebox also translates the slicing information into an internal slice binding. In some embodiments the middlebox uses its own internal identifiers different from the tags prepended to the packets in order to identify states e.g. active TCP connections statistics about various IP addresses etc. within the middlebox. Upon receiving an instruction to create a new middlebox instance and an external identifier that used on the packets for the new instance some embodiments automatically create the new middlebox instance and assign the instance an internal identifier. In addition the middlebox stores a binding for the instance that maps the external slice identifier to the internal slice identifier.

The routing policy is entered by the user in order to indicate which packets the logical router should send to the middlebox. When the middlebox is located on a logical wire between two logical forwarding elements e.g. between a logical router and a logical switch then all packets sent over that logical wire will automatically be forwarded to the middlebox. However for an out of band middlebox such as that in network topology the logical router will only send packets to the middlebox when particular policies are specified by the user.

Whereas routers and switches will normally forward packets according to the destination address e.g. MAC address or IP address of the packet policy routing allows forwarding decisions to be made based on other information stored by the packet e.g. source addresses a combination of source and destination addresses etc. . For example the user might specify that all packets with source IP addresses in a particular subnet or that have destination IP addresses not matching a particular set of subnets should be forwarded to the middlebox. In this specific case the routing policy specified by the user routes traffic with a source IP in subnet A and an ingress context of logical Port L i.e. coming from the logical switch A to the middlebox. While the source IP address would be enough to route packets to the middlebox D the ingress port prevents packets coming back from the middlebox from being sent to the middlebox again i.e. a never ending loop . The packets from the middlebox will have a different ingress port and therefore will not be routed by the policy .

As shown the routing policy is sent as logical control plane data to the logical controller e.g. from an input translation controller . In this example the control plane entry states Send packets with source IP address in subnet A received at ingress Port L to middlebox D at Port K . This L3 logical routing control plane entry combines the routing policy with the network topology of the middlebox being located at Port K. As shown the logical controller first converts the logical control plane entry to logical forwarding plane entry . As described in some embodiments a table mapping rules engine at the logical controller performs this conversion. That is the entry is a first database table record which is mapped via the rules engine to the entry . The logical forwarding plane entry is a flow entry in the format of match action stating If source IP matches A and Ingress match Port L forward to Port K . Because the network performs forwarding in the logical plane the flow entry sends packets to a logical port of the logical router.

Next the logical controller translates the logical forwarding plane entry into a set of universal control plane entries . The first entry is the forwarding entry in the universal physical control plane stating If match L3 C and source IP match A and Ingress match Port L forward to Port K . This entry adds a match over the L3 router to the forwarding entry ensuring that a packet acted upon by the flow entry is not part of a different logical network.

In addition the logical controller adds ingress and egress port integration entries and . Because these entries are part of the universal physical control plane the physical port information in these entries is generic. These entries include an ingress port integration entry that maps packets received via a software port connected to the middlebox in the host machine to the logical ingress Port K. Similarly the egress port integration entry maps packets forwarded to Port K to the software port connected to the middlebox. As this port may have different specific identifiers in different host machines at the universal control plane level it is represented using a generic abstraction of such a port. The universal physical control plane will include various other entries such as ingress mapping to map packets received from a generic virtual interface at which VM1 is located to an ingress port on the L2 A logical switch and a L2 forwarding entry that maps packets with a destination not on the L2 A logical switch to the egress port connected logically to the L3 C logical router.

As with the logical control plane to logical forwarding plane the logical controller performs the second conversion using a table mapping rules engine. In some embodiments the first conversion is performed by a control application within the logical controller while the second conversion is performed by a virtualization application. In some such embodiments these two applications use the same rules engine.

Next the logical controller identifies which physical controllers in the network control system should receive the universal physical control plane entries. For instance the ingress and egress port integration entries at the L2 level for a particular virtual machine will only need to be sent to the physical controller that is the master of the node on which the particular virtual machine is hosted. The entries for the L3 router are sent to all of the machines in some embodiments and therefore the logical controller identifies all of the physical controllers that receive any universal control plane data for the logical network to receive the entries . On the other hand in some cases the middlebox may only be implemented at a subset of the nodes e.g. because the middlebox will only need to process traffic at those nodes and therefore these entries are only exported to the physical controllers managing nodes in the subset.

Upon receiving the entries the physical controller one of several physical controllers to receive the entries performs the universal physical control plane to customized physical control plane conversion. As shown the entry stays the same as there is no information in the flow entry that requires specification. On the other hand the generic port abstraction in the ingress and egress port integration entries and are converted into the specific software port of the MSE to which the middlebox connects for entries and .

As shown in some embodiments after negotiating the software port connection with the middlebox the MSE passes the middlebox attachment port information up to the physical controller . The physical controller then uses this information as an input to its table mapping engine that converts the universal physical control plane records into customized physical control plane records. The physical controller passes this information to the MSE which converts the customized physical control plane entries into physical forwarding plane entries in its forwarding tables . These forwarding tables are used by the MSE to match against received packets and perform actions e.g. forwarding encapsulation etc. on the packets. While not shown here additional flow entries are generated by the physical controller to add the slicing tag to the packet before sending the packet over the software port to the middlebox.

As shown the logical controller receives the rule e.g. as a database table record . The logical controller identifies the particular nodes that should receive the rule . In some embodiments this is all of the nodes that implement Middlebox D which may be all of the nodes implementing the logical forwarding elements of the network. In some embodiments however the logical controller parses the routing policies for the middlebox to determine that only a subset of the nodes implementing the logical forwarding elements need to implement the middlebox i.e. based on its placement and function in the logical network . Based on the identified nodes the logical controller identifies a set of physical controllers to which it distributes the rule .

This set of physical controllers includes the illustrated physical controller to which it exports the rule . As at the logical controller the physical controller does not perform any transformation or at least only minimal transformation on the rule . However as the new configuration entails starting up a new virtual middlebox on the middlebox application running on the host machine e.g. a middlebox daemon the physical controller assigns a slicing identifier for the middlebox instance.

The physical controller distributes the rule and the slicing identifier to the middlebox on the same host machine as the MSE . The middlebox generates a new middlebox instance and converts the rule along with other configuration rules received from the physical controller into a compiled set of data plane rules for the middlebox instance. In some embodiments these data plane rules act effectively as a hardcoded set of forwarding tables for packet processing in the middlebox. In addition the middlebox adds an entry to its slice binding table . For each instance the middlebox creates its own internal identifier and stores a binding of this internal ID to the packed slicing ID assigned by the physical controller in the slice binding table . Furthermore as shown the middlebox would have contracted with the MSE in the host machine to create the software port for transferring packets between the two modules.

As shown a second logical controller receives the rule e.g. as a database table record . The logical controller identifies the particular nodes that should receive the rule as in the previous example. In this case the node with middlebox and MSE hosts virtual machines for both the first and second networks and so the same physical controller that manages that node receives the rule .

Because the Middlebox H requires the creation of a new virtual middlebox on the middlebox application the physical controller assigns a new different slicing identifier for the middlebox instance. The physical controller then distributes the rule and the slicing identifier to the middlebox on the host machine. The middlebox creates a new middlebox instance and converts the rule along with other configuration rules received from the physical controller into a new set of data plane rules for the newly created middlebox instance.

In addition the middlebox adds another entry to its slice binding table . As stated above the middlebox creates its own internal identifier and stores the binding of this internal ID to the assigned slicing ID from the physical controller in its slice binding table . This way when a packet is received from the MSE the middlebox can remove the slicing identifier and match that identifier with its internal ID in order to use the appropriate set of middlebox rules to process the packet. In addition when the middlebox creates new states e.g. for new TCP connections it uses the internal identifier to associate the state with a particular instance.

The above example relates to a distributed middlebox. For a centralized middlebox the user enters the configuration in the same manner through an interface designed for the middlebox . The logical controller identifies only a single physical controller that manages the middlebox or the host machine at which the middlebox is located if implemented as a single virtual machine and exports the configuration rules to this physical controller. The physical controller assigns a slice identifier for the configuration as the centralized middleboxes may also be virtualized between several networks.

Whereas in the distributed case only one managed switching element sends packets to a particular distributed middlebox element in the centralized case numerous different MSEs will need to receive the slicing identifier. As such in some embodiments the physical controller managing the centralized middlebox sends the slicing identifier to the logical controller that manages the particular network which distributes this slicing identifier e.g. in the form of flow entries that add the identifier to packets destined for the middlebox to all of the nodes that may send packets to the middlebox via their managing physical controllers . In other embodiments the logical controller assigns the slicing identifier for the middlebox and distributes this information to all of the nodes via the managing physical controllers.

In addition the network control system sets up tunnels between the centralized middlebox and the various managed switching elements that send packets to the middlebox. The tunneling information may be entered by a user at the input translation controller interface or automatically generated by the logical controller with knowledge of the different tunneling protocols supported by the middlebox. In the conversion to the physical control plane the logical controller adds tunnel encapsulation flow entries that add or remove tunnel encapsulation from the packet. These entries can then be customized at the physical controller level to account for the particular ports and encapsulation used at each different managed switching element.

The above section describes a network control system that includes several different types of network controllers. illustrates example architecture of a network controller e.g. a logical controller or a physical controller . The network controller of some embodiments uses a table mapping engine to map data from an input set of tables to data in an output set of tables. The input set of tables in a controller include logical control plane LCP data to be mapped to logical forwarding plane LFP data LFP data to be mapped to universal physical control plane UPCP data and or UPCP data to be mapped to customized physical control plane CPCP data. The input set of tables may also include middlebox configuration data to be sent to another controller and or a distributed middlebox instance. The network controller as shown includes input tables a rules engine output tables an importer an exporter a translator and a persistent data storage PTD .

In some embodiments the input tables include tables with different types of data depending on the role of the controller in the network control system. For instance when the controller functions as a logical controller for a user s logical forwarding elements the input tables include LCP data and LFP data for the logical forwarding elements. When the controller functions as a physical controller the input tables include LFP data. The input tables also include middlebox configuration data received from the user or another controller. The middlebox configuration data is associated with a logical datapath set parameter that identifies the logical switching elements to which the middlebox to be is integrated.

In addition to the input tables the control application includes other miscellaneous tables not shown that the rules engine uses to gather inputs for its table mapping operations. These miscellaneous tables include constant tables that store defined values for constants that the rules engine needs to perform its table mapping operations e.g. the value 0 a dispatch port number for resubmits etc. . The miscellaneous tables further include function tables that store functions that the rules engine uses to calculate values to populate the output tables .

The rules engine performs table mapping operations that specifies one manner for converting input data to output data. Whenever one of the input tables is modified referred to as an input table event the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more output tables.

In some embodiments the rules engine includes an event processor not shown several query plans not shown and a table processor not shown . Each query plan is a set of rules that specifies a set of join operations that are to be performed upon the occurrence of an input table event. The event processor of the rules engine detects the occurrence of each such event. In some embodiments the event processor registers for callbacks with the input tables for notification of changes to the records in the input tables and detects an input table event by receiving a notification from an input table when one of its records has changed.

In response to a detected input table event the event processor 1 selects an appropriate query plan for the detected table event and 2 directs the table processor to execute the query plan. To execute the query plan the table processor in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables. The table processor of some embodiments then 1 performs a select operation to select a subset of the data values from the record s produced by the join operations and 2 writes the selected subset of data values in one or more output tables .

Some embodiments use a variation of the datalog database language to allow application developers to create the rules engine for the controller and thereby to specify the manner by which the controller maps logical datapath sets to the controlled physical switching infrastructure. This variation of the datalog database language is referred to herein as nLog. Like datalog nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments nLog provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance in some embodiments nLog only allows the AND operator to be used in any of the declaratory rules.

The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping rules engine that is referred to as the nLog engine.

Some embodiments designate the first join operation that is performed by the rules engine for an input event to be based on the logical datapath set parameter. This designation ensures that the rules engine s join operations fail and terminate immediately when the rules engine has started a set of join operations that relate to a logical datapath set i.e. to a logical network that is not managed by the controller.

Like the input tables the output tables include tables with different types of data depending on the role of the controller . When the controller functions as a logical controller the output tables include LFP data and UPCP data for the logical switching elements. When the controller functions as a physical controller the output tables include CPCP data. Like the input tables the output tables may also include the middlebox configuration data. Furthermore the output tables may include a slice identifier when the controller functions as a physical controller.

In some embodiments the output tables can be grouped into several different categories. For instance in some embodiments the output tables can be rules engine RE input tables and or RE output tables. An output table is a RE input table when a change in the output table causes the rules engine to detect an input event that requires the execution of a query plan. An output table can also be an RE input table that generates an event that causes the rules engine to perform another query plan. An output table is a RE output table when a change in the output table causes the exporter to export the change to another controller or a MSE. An output table can be an RE input table a RE output table or both an RE input table and a RE output table.

The exporter detects changes to the RE output tables of the output tables . In some embodiments the exporter registers for callbacks with the RE output tables for notification of changes to the records of the RE output tables. In such embodiments the exporter detects an output table event when it receives notification from a RE output table that one of its records has changed.

In response to a detected output table event the exporter takes each modified data tuple in the modified RE output tables and propagates this modified data tuple to one or more other controllers or to one or more MSEs. When sending the output table records to another controller the exporter in some embodiments uses a single channel of communication e.g. a RPC channel to send the data contained in the records. When sending the RE output table records to MSEs the exporter in some embodiments uses two channels. One channel is established using a switch control protocol e.g. OpenFlow for writing flow entries in the control plane of the MSE. The other channel is established using a database communication protocol e.g. JSON to send configuration data e.g. port configuration tunnel information .

In some embodiments the controller does not keep in the output tables the data for logical datapath sets that the controller is not responsible for managing i.e. for logical networks managed by other logical controllers . However such data is translated by the translator into a format that can be stored in the PTD and is then stored in the PTD. The PTD propagates this data to PTDs of one or more other controllers so that those other controllers that are responsible for managing the logical datapath sets can process the data.

In some embodiments the controller also brings the data stored in the output tables to the PTD for resiliency of the data. Therefore in these embodiments a PTD of a controller has all the configuration data for all logical datapath sets managed by the network control system. That is each PTD contains the global view of the configuration of the logical networks of all users.

The importer interfaces with a number of different sources of input data and uses the input data to modify or create the input tables . The importer of some embodiments receives the input data from another controller. The importer also interfaces with the PTD so that data received through the PTD from other controller instances can be translated and used as input data to modify or create the input tables . Moreover the importer also detects changes with the RE input tables in the output tables .

The above sections describe in detail the creation of flow entries and middlebox configurations for a logical network using a network control system. This data is then used to process and forward traffic within the physical implementation of the network e.g. by matching packets to flow entries in the managed switching elements applying the middlebox rules to packets etc. .

The VM sends a packet to the MSE e.g. through a virtual interface within the host machine . The packet has a source MAC and IP address corresponding to VM and a destination IP address and if available destination MAC address corresponding to VM . The MSE begins by executing the L2 flow for logical switch A the logical switch to which VM attaches which includes mapping the physical ingress port the virtual interface to a logical ingress port then performing a logical L2 forwarding decision to send the packet to the logical router. This logical router is also implemented by the MSE so sending the packet only involves a resubmit of the packet.

The MSE then executes the L3 flow for the logical router. In some embodiments the forwarding tables include a flow entry to forward the packet to the logical switch B to which the VM attaches based on the destination IP address. However the L3 forwarding tables also include a higher priority entry to forward the packet to the middlebox based on a user entered routing policy e.g. based on the source IP address and or the logical ingress port or other data . Thus the L3 forwarding decision is to send the packet to the middlebox . Before sending the packet over the software port negotiated between the two software elements the MSE adds a slice tag to the packet to identify the correct middlebox instance. In some embodiments the MSE prepends this tag into a particular field of the packet header.

The middlebox receives the packet and removes the slice tag in order to identify the correct middlebox instance for processing the packet. With the correct instance identified through the slice binding table the middlebox performs its processing. This may involve modifying the source IP for S NAT modifying the destination IP for a load balancer determining whether to drop or allow the packet for a firewall or other process. After performing its processing on the packet the middlebox sends the packet back to the managed switching element.

In some embodiments the middlebox sends a new packet back to the MSE. Because the MSE may receive packets from multiple middlebox instances over the same port in some embodiments the middlebox adds the slice tag to the packet before sending it over the software port. At the MSE the flow entries map the packet back to the logical L3 router i.e. to the ingress port of the L3 router connected to the middlebox and then execute the L3 forwarding. Because the logical ingress port is now the port connecting to the middlebox the routing policy to send to the middlebox is not executed and the destination IP address results in a forwarding decision to the logical switch B. The logical forwarding decision at this switch is based on the destination MAC address which results in a forwarding decision to the VM . The forwarding decision is used to encapsulate the packet and also maps to a particular physical port of the host machine over which the packet is sent after adding tunneling encapsulation .

The packet traverses the network via the tunnel in order to arrive at the MSE of the second host . After removing the tunneling the MSE reads the egress context removes it and delivers the packet to the VM . This maps to a virtual interface within the host machine such that the packet arrives at the VM .

One of ordinary skill will recognize that this example is only one of numerous possible packet processing examples involving a distributed middlebox. If the destination VM was located in the host along with the source VM the packet would go to the MSE to the middlebox back to the MSE and then to the destination VM without ever leaving the physical machine. As another example if the middlebox only receives duplicate packets then the MSE would send the received packet to the second host and a duplicate packet to the middlebox which would not send out a packet after its processing was finished.

While the example of illustrates the processing involved in sending a packet to a distributed middlebox conceptually illustrates sending a packet through a centralized middlebox located outside of any of the host machines. As shown in this figure the VM in the host again sends a packet to the VM in the host . Upon receiving the packet from the source VM the MSE executes the L2 flow ingress mapping to the logical ingress port connected to the VM then logical forwarding to the logical L3 router . At the L3 router the routing policies again send the packet to the middlebox which in this case is not located within the host . As such the MSE adds the slice tag to the packet in the same way as if the middlebox was distributed but also adds tunneling encapsulation to the packet in order to send the packet over the physical network to the centralized middlebox .

The centralized middlebox maps the slice identifier to one of its virtual middleboxes i.e. using its slice binding table then performs the middlebox processing. Examples of centralized middleboxes include in some embodiments firewalls which may also be distributed intrusion detection systems which are passive middleboxes that receive duplicate packets and WAN optimizers for performing various data compression techniques before sending packets out over a WAN as well as other middleboxes. After performing its middlebox processing the middlebox sends a new packet to a pool node.

In some embodiments each centralized middlebox sends all of its packets to a particular pool node or different pool nodes for different middlebox instances because the middlebox may not have the functionality to perform the logical forwarding required to send the packet to its destination. Thus the middlebox encapsulates the packet for the tunnel and in some embodiments adds the slice tag to the packet then sends it to the pool node . The pool node maps the packet to the correct logical router and uses the destination IP address of the packet to make a forwarding decision to the logical switch to which the destination VM connects. The logical L2 flows in the pool node forward the packet to the destination machine and this egress context is used to encapsulate the packet. The pool node then adds the tunnel encapsulation for transport over the physical network and sends the packet to the host. . The MSE reads this egress context removes it and delivers the packet to the VM in the same manner as in .

Many of the above described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium also referred to as computer readable medium . When these instructions are executed by one or more computational or processing unit s e.g. one or more processors cores of processors or other processing units they cause the processing unit s to perform the actions indicated in the instructions. Examples of computer readable media include but are not limited to CD ROMs flash drives random access memory RAM chips hard drives erasable programmable read only memories EPROMs electrically erasable programmable read only memories EEPROMs etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.

In this specification the term software is meant to include firmware residing in read only memory or applications stored in magnetic storage which can be read into memory for processing by a processor. Also in some embodiments multiple software inventions can be implemented as sub parts of a larger program while remaining distinct software inventions. In some embodiments multiple software inventions can also be implemented as separate programs. Finally any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments the software programs when installed to operate on one or more electronic systems define one or more specific machine implementations that execute and perform the operations of the software programs.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash memory device etc. and its corresponding drive as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieves instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices cameras e.g. webcams microphones or similar devices for receiving voice commands etc. The output devices display images generated by the electronic system or otherwise output data. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD as well as speakers or similar audio output devices. Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself. In addition some embodiments execute software stored in programmable logic devices PLDs ROM or RAM devices.

As used in this specification and any claims of this application the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification and any claims of this application the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

