---

title: Method and system for building an integrated user profile
abstract: A system and method are provided for adding user characterization information to a user profile by analyzing user's speech. User properties such as age, gender, accent, and English proficiency may be inferred by extracting and deriving features from user speech, without the user having to configure such information manually. A feature extraction module that receives audio signals as input extracts acoustic, phonetic, textual, linguistic, and semantic features. The module may be a system component independent of any particular vertical application or may be embedded in an application that accepts voice input and performs natural language understanding. A profile generation module receives the features extracted by the feature extraction module and uses classifiers to determine user property values based on the extracted and derived features and store these values in a user profile. The resulting profile variables may be globally available to other applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09564123&OS=09564123&RS=09564123
owner: SoundHound, Inc.
number: 09564123
owner_city: Santa Clara
owner_country: US
publication_date: 20150505
---
This application claims the benefit of U.S. Application No. 61 992 172 filed on May 12 2014 entitled Method and System For Building An Integrated User Profile naming inventors Joel Gedalius Jun Huang Kiran Garaga Lokeswarappa and Bernard Mont Reynaud which is hereby incorporated by reference.

This invention relates to voice and text processing and more particularly to methods and systems that extract user profile information from speech and text input for use in advertisement targeting and other applications.

A variety of computer based services could be provided more effectively if the service provider could tailor the service to the preferences of a particular user. For example a fact of the contemporary Internet is that its users are frequently exposed to advertising yet users may be annoyed by these advertisements ads especially when these advertisements are irrelevant to the user s interests. To support choosing advertisements judiciously known or presumed characteristics and preferences of a user are collected in a user profile. Hence a user profile is a collection of user characteristics and preferences.

User profiles are populated with information provided directly from the user such as the user s name and location. However service providers need more detailed information to personalize interactions with a user.

Software service providers have attempted to build user profiles that contain as much information as possible about the user in order to tailor the content presented to the user and or its mode of delivery. Users may balk at providing anything more than minimal data completing questionnaires is burdensome and users do not want to give up their privacy by disclosing the requested personal data. Thus collecting additional user information is best performed without requiring additional effort for the users and with a limited invasion of a user s privacy. One technique service providers have used is monitoring user web browsing activity and keystrokes. However this is seen by many as too invasive. As a result marketers still look for effective yet acceptable ways to assess user characteristics and preferences in order to present more effective marketing messages.

A user profile includes of a set of properties that describe user characteristics and preferences. One aspect of the present disclosure is a method for building a user profile using speech based information after receiving the user s permission . A system module may receive speech as input extract speech features from those inputs and produce a text transcription of the input speech from which language based features may also be gathered. The extracted features may be used to determine information about the user for inclusion in the user s profile. Extracted features may be used in isolation or detected patterns in a combination of extracted features may be used to infer user characteristics. Simple examples include the pitch of a user s voice may provide a clue as to the user s age and or gender. An accent may provide a clue as to a user s ethnicity or home location. The variety and sophistication of vocabulary used may indicate a level of education and or English proficiency and subject matter discussed may provide insight into socio economic status.

A user profile augmented in this way may be used by a variety of applications that tailor user interactions based on profile information. One kind of application that benefits from a rich user profile is one that selects advertising that would be relevant to the user.

The system module described above performs speech recognition and natural language understanding. In an implementation the information added to the user profile may be used by any application that personalizes user interaction. In another implementation a feature extraction module may be included within a particular application that accepts natural language as input. The resulting user profile information may be shared with other applications or may be private to that particular application.

Examples of the relevance of user profile information abound. If a user searches for music an online ad system can use of the estimate of the user s age to select for display a music ad that may be of interest to a person of the user s age. Gender based and age based ad targeting is applicable to many types of products however the age or gender of users is not always entered explicitly into the system. An intelligent system can try to infer the probable age or gender of a user based on a statistical analysis of user inputs. In the case of clothing and many other products using socio economic status information may be essential in addition to using information about age and gender. If a user s socio economic status SES is known the information can be used to select ads that marketers have aimed at a specific demographic.

The technology disclosed herein is directed to detecting user characteristics based on attributes of their speech including acoustic features such as pitch or speed acoustic phonetic features such as accent or linguistic features such as use of grammar and vocabulary. A feature extraction module working in conjunction with a user profile generator may create a richer set of property values representing user characteristics or add confidence to property values inferred in other ways. An application benefiting from an enriched user profile need not use natural language interfaces. An advertisement application can use the profile information to increase effectiveness for advertisers. Applications can also benefit from the additional property values by improving the overall experience of users by adjusting the content and the style of the system output to the specific user and increase effectiveness for advertisers.

Also the additional user profile information may aid applications with natural language interfaces to be tuned or customized to a particular user. For example once a user s accent has been inferred this information can be used in a car navigation system to address the user with voice instructions bearing a similar familiar accent or the choice of words may also be based on a regional or geographical constraint e.g. Australian English . In addition accent detection allows the adaptation of a speech recognizer s acoustic model improving the reliability of understanding the user. Naturally inferred region culture or ethnicity must be performed with high confidence to avoid a user s negative reaction to the system making incorrect assumptions. The joint use of accent and language clues can improve reliability. More broadly it is most effective to identify a user as belonging to a group of people sharing various combinations of regional accent regional language educational level and SES.

Advertisement selection applications are well suited for using profile data inferred from speech because if a wrong inference is made the result is that the selected ads are less likely to appeal to the user but the user is unlikely to be offended by a mistargeted ad.

Features The word feature is an umbrella term broadly used in pattern recognition speech recognition machine vision and other fields of perceptual processing. It often refers to a distinctive attribute quality or property of an object or frequently of a small area within a larger object. In a software implementation features may be represented by complex data structures or simply by numeric or symbolic data values. In the context of this disclosure the term primary features applies specifically to data computed directly from the user s audio input. A speech recognition application typically computes primary features but also extracts additional features which become part of the application s extracted feature set. Yet more features may be derived in one or more steps from the extracted feature set creating an extended feature set. illustrates feature categories of primary interest. The different types of features are presented below and the manner in which they are computed is discussed later.

Acoustic features Common acoustic features found in speech audio may be represented by the Mel frequency cepstral coefficients MFCC . They are computed by well known techniques first the Short Term Fourier Transform STFT is used to process audio input into a sequence of spectral frames then further processing of every frame s magnitude spectrum yields the MFCCs which serve as input to subsequent modules of the speech processing system.

Phonetic features The most common format for representing phonetic information is a phonetic sequence. A speech recognition system may identify several possible phonetic sequences for a user utterance. Weights may be applied to phonemes within each sequence according to some method of assigning probability. A score for each alternative phonetic sequence may be computed and the most likely phonetic sequence may be selected based on the score. In some cases multiple phonetic sequences are kept at the same time often as a phoneme lattice. In addition acoustic phonetic information including phoneme length and HMM state info is available and can also contribute to accent identification and detection. Articulatory phonetic information includes a variety of secondary features place and manner of articulation vowel placement and more.

Textual features A textual feature comprises a word a sequence of words or alternative sequences of words. Many speech recognition systems build a word lattice that collects the most likely word sequences in one data structure.

Linguistic features Linguistic features include words assembled into grammatical structures which may be parse trees or syntax trees.

Semantic features In this context semantic features focus on the meaning of individual words or phrases as opposed to that of sentences meanings which may be expressed by way of table driven or statistical associations with concepts from one or more domains of discourse.

Classifier a function taking extracted and or extended features as input and assigning a value to a user profile property. A classifier function is a software module that is trained as known in the fields of machine learning and pattern recognition. The training data includes known associations between features usually called a feature vector and corresponding user characteristics usually called the ground truth . After training a classifier can accept a feature vector as input and map it to the most probable property value for that input.

Feature extraction module receives speech input User Speech and analyzes the speech input. In an implementation the feature extraction module may also transcribe the speech into text then may analyze the text for linguistic features. The analysis results are provided to the profile generation system. The Profile Manager manages the User Profile and provides a programmatic interface for reading and writing profile property values. The profile generation system interacts with profile manager to retrieve the previously assigned property values and to add new profile property values into the user profile . Applications such as Profile Subscriber Application may retrieve profile property values for personalizing user interaction from the augmented user profile through interaction with the profile manager . In an implementation the profile subscriber application may be an advertisement selection application that selects an advertisement that is expected to be of high relevance to the user. Context Info identifies the user and or user context including user supplied information system supplied information such as location or time zone and user characteristics inferred through observing user behavior over time. For example a user ID or other way of identifying the user may be input into the profile generation system . Some of the context info may also be stored in the user profile for example the user ID. However the context info may contain contextual information not stored in the user profile .

In an implementation such as shown in the feature extraction module may operate independently of any vertical natural language based application. The feature extraction module in this configuration is referred to herein as a stand alone implementation. In such an implementation the recognized language may be very broad so as to be relevant across many subject domains. When the feature extraction module is not part of that is not included in a particular vertical application the user needs to be prompted to talk or other sources of user speech or text may be relied on for the purpose of generating user profile information.

In another implementation the feature extraction module may be included in a particular application accepting speech as input. User speech may be obtained naturally by the user interacting with the application such as asking a question which the application attempts to answer. In addition the language recognized by the feature extraction module may be more constrained and specific to the subject matter of the vertical application.

In an implementation the speech input is a sequence of utterances. For purposes of speech recognition human speech is processed in units or segments called utterances which represent a single meaning An utterance may be a word a group of words a sentence or even multiple sentences. Application may process each utterance independently from previous user utterances. For example an utterance may include all the words spoken before a pause lasting more than some threshold of time. A user utterance user speech can be received by any conventional means for receiving audio input such as a microphone. Alternatively a user utterance can be generated remotely and transmitted to the system. In other implementations audio may be recorded and played back into a device connected to the system.

The profile generation system receives feature data extracted by the speech application and also has access to context information including the user s unique ID. The textual features and linguistic features may be provided to the profile generation system for an entire utterance when the speech recognition application completes the processing of the utterance whereas acoustic features may be provided to the profile generation system at a pre configured time interval even before the utterance is completed .

The extracted feature set is available as input to the profile generation system which may use parts of it as the basis for creating an extended feature set as explained in . The extended feature set is used by the user profile generation module to compute the likely values of certain user properties. For example the average spectral shape and average pitch are part of the extended feature set and they allow statistical inferences to be made about the user s age and or gender. The user profile generation module is further discussed with . Inferred property values are stored in the user profile using the profile manager . User profile may also include profile properties whose values are not inferred from speech input but determined by the context info such as a user s age gender and other survey data voluntarily provided by the user behavioral data collected by a tracking system and any other information available to the system.

Each user characteristic may be represented in the user profile by a property value pair applicable to an individual user. Profile instances provide values for the properties representing user characteristics. Some property values are numerical such as a user s age in years. Other property values although they may be encoded numerically are a selection among an enumerated set of discrete choices Gender M or F for Age instead of a number which is more accuracy than available the system could use categories of age. In one implementation Age categories might for example consist of five distinct groups 13 and under 14 18 19 35 36 50 and over 50 years. The use of categories is limited by the ability of the system to reliably make necessary distinctions from the extended features. For example age differences may become more difficult to determine for subjects older than 50 so the upper category may be defined to reflect that granularity.

The user profile may be available both to the speech recognition application and to other applications such as profile subscriber applications . User profile may be stored persistently to a hard disk drive or other non transitory computer readable storage medium. Storage persistency allows user profile data to be created during a session and expanded or modified as new information is received during the same session or in later sessions.

A profile subscriber application such as an online advertisement application may retrieve user profile information through an application programming interface API and use the information retrieved from the user profile to select ads of potential interest to the user. The answer generation module may also retrieve user profile information via an API or any other means of inter process communication to help with generating a more accurate personalized answer. For example a user s age gender or level of education may guide the answer generation module to choose between alternative ways to compose an answer.

The answer from the speech recognition application may be merged with the selected ad in Merge and a Multimedia Output may be provided to the user. This merge process can occur for example when two or more outputs share the screen space as is common in online advertising within a search engine. The merge module may be driven by a screen layout for visual material such as text graphics and video and by preference parameters regarding procedures for audio and video playback such as whether audio or video will be playing spontaneously or wait for the user to press the play button whether video will play with the audio muted etc. The precise techniques used by the answer generation module and the profile subscriber applications to negotiate their share of the screen and speakers are not essential to this disclosure. In a pure audio that is a screen less implementation recorded ads could be mixed into the flow of audio from an application.

An advantage of the cooperation between the speech recognition application and the profile generation system is that features extracted by the feature extractor module of the speech recognition application and used within the application can also be used for profile generation thereby positively affecting the feature quantity quality and speed of augmenting the user profile. This implementation also provides the profile generation system with an abundance of user utterances received under natural circumstances. In a stand alone implementation a speech mini application such as an embedded transcription application may compute a primary feature set . In such an implementation the speech application may be hidden inside the system and not directly contribute to the visible multimedia output . In another implementation the primary feature set may be computed by a standalone speech front end module with some feature detector which extracts at least acoustic features but perhaps not all of the features shown in . When only a subset of the features is extracted and present in the feature set subsequent processing will only apply to the available information.

Speech input for the asynchronous or standalone system can be obtained in a number of ways. In one embodiment audio input from a completely separate speech application is made available to the profile generation process but the speech application and profile generation are otherwise be entirely independent beyond the shared audio input. Alternatively a collection of utterances can be gathered and forwarded to a standalone profile generation system where the input can be processed in batches or recordings of the user s voice can be provided resulting for example from recording sources such as collected voicemails or dictations. These and many other variations lie within the scope of the present disclosure.

Speech features may include the acoustic features phonetic features textual features linguistic features and semantic features of a user utterance. Acoustic features are those related to pronunciation pitch and tonal quality of the voice. These are the features of a person s voice that allows one to recognize their voice even when the content of their speech may be gibberish.

Whereas acoustic features may be obtained directly from an utterance the other features may be obtained indirectly and or in a non linear fashion. The states of determining phonetic textual linguistic and semantic features may be maintained in parallel over time. A global likelihood optimization may affect the extraction of these primary features. In this context global means that the optimization affects all of the non acoustic features phonetic textual linguistic and semantic. Likelihood refers to a probabilistic evaluation of alternative feature representations enabling prioritization or selection among alternatives. For example if a phonetic feature of a user utterance is determined to be the phoneme sequence R IY D using the CMU phoneme set the corresponding textual feature representation may be the word read or reed. The likelihood of each of each of these words being correct may be determined based on context semantic features . For example if the subject matter of discourse is an orchestra or instrumental music then reed might be the favored transcription but if the subject matter is a library or bookstore read might be the favored transcription. The likelihood for each word may also be determined based on global frequency of word use textual features . For example the word read is used more often than reed in general speech. Also the syntax could be used to favor reed if the grammar expects a noun or read if the grammar expects a verb linguistic features .

A large number of models and knowledge sources including language models and tri state HMMs for phonemes may all contribute to the determination of the most likely interpretation of the user utterance. Each model relies on constraints. For example a dictionary may list possible pronunciations for each word as a set phoneme sequences. The selected phonetic features and textual features are bound by this constraint. Linguistic constraints may take the form of statistical language models SLMs or stochastic grammars or a mix of both. Both types of language model affect the likelihood of word sequences and thus the results from this optimization. Even semantic constraints may be brought to bear on this joint optimization which results in the integration of constraints from different levels of the system. Selection of one feature representation may affect the likelihood of another feature representation. Thus all non acoustic features may be optimized in lock step. Each feature selected in the context of the other features may be processed by the profile generation unit within a similar time span.

The extracted feature set comprises more than simple sequences of elements such as frames states phonemes or words but also may include relationships such as alignments or mappings between the elements at successive levels sufficient to derive additional information. For example when a speech transcription contains a certain word in the textual features the extracted feature set also delimits the specific phoneme subsequence from the phonetic features that matches this word in the transcription and each phoneme in turn can be mapped to the sequence of states or frames that it spans. Such a cross referencing capability between words and phonemes and acoustic features is often useful for deriving extended features the generation of extended features is discussed later. In a preferred implementation the entire set of features extracted by the feature extraction module is visible and available to the profile generation system upon completion of the processing of an utterance assuming such a defined boundary exists or after a pause of sufficient length or at the end of each phrase or sentence or paragraph whatever small grouping unit is appropriate for the application.

There are a variety of ways in which the extracted primary features may be provided to the profile generation system when the features are available for further processing. In an implementation in which the speech recognition application and the profile generation system are tightly coupled the features may be pushed to the profile generation system by invoking an API provided by the speech recognition application or the profile generation system may pull the features using a callback interface. In an implementation in which the speech recognition application and the profile generation system are more loosely coupled the extracted features may be pushed by the speech application into a buffer i.e. a queue and pulled from the queue by the profile generation system. The buffer may reside in a shared memory area that both the feature extraction module and profile generation system can access.

Once received in the profile generation system the extracted features may be stored along with extended features for the same user over time. Storing features over time may require persistent storage across user utterances sessions and across different sources of features such as multiple feature extraction modules all of which contribute their results. As mentioned above user profile data should be specific to an individual and accordingly statistics should be gathered on an individual basis. In many cases user login information supplied as part of the context info is sufficient to retrieve the presumed identity of a user. In this manner a user profile can be associated with a specific user and maintained and updated over time.

Alternatively a non registered user may only be known to persist within a session or collection of sessions and the user profile will by necessity be based on a smaller number of interactions. In either case standard speaker identification techniques may be applied to detect speaker changes done to address the undesirable case in which the identity of the user speaker unexpectedly changes mid session. Such a speaker change reduces the validity of the statistics being gathered and their applicability to the current user. When possible a combination of both approaches context info and speaker identification techniques to retrieving the user s identity will support the persistence and reliability of user profile information by restricting the statistics to be computed based on speech features attributed to the same user.

In the absence of speaker changes the extended feature set is associated either with a known user in which case statistics may extend over multiple sessions or with the current user during a single session. This limits the scope of the user profile to the active session.

The Feature Derivation module and feature Accumulation module of the profile generation system use as input the extracted primary feature set to generate an extended feature set . In this context derivation refers to a variety of simple computations that may combine some part of the data in the extracted feature set with some other part thereof. For example a dictionary may attach properties to a word such as its frequency of use in the general population relating to its degree of sophistication or its regional character. Average word length and number of words per sentence may be derived from the transcription to estimate vocabulary levels and reading levels. Other examples of such derivations include calculating the duration for a phoneme syllable word or pause looking up the properties of a phoneme in a phoneme table to derive its primary articulatory features such as place and manner of articulation for consonants or placement for vowels and phrase structure complexity may be derived from extracted linguistic features. These are just a few of the many simple feature derivations that can be performed. The basic point is that derived features are simple quantities typically numeric associated with a very short time span. Derived features are added to the extended feature set .

Derived features may be included in the extended feature set as well as provide input to the feature accumulation module . Feature accumulation is closely linked with derivation. Many of the accumulated features are based on derived features. Feature accumulation refers to the process of adding and tabulating information across successive time spans be that frames phonemes syllables words phrases or entire sentences. Accumulation may be applied directly to features already present in the extended feature set however accumulation more often is applied to derived features. Accumulation includes generating statistics. The types of statistics accumulated may include means and variances histograms from which percentiles and medians may be obtained or other known statistics. Feature accumulation extends the feature set across a larger time span. Example accumulated features include mean syllable duration mean and variance duration of the AY phoneme from the CMUP phoneme set or the frequency of use for a multiple of pronunciations of a particular word distribution of pronunciations of the word .

Some examples follow additional details on the construction of extended features will be given later in the sections devoted to specific properties in the user profile.

As mentioned earlier features such as MFCC frames or spectral frames that are among the acoustic features may be included in the extended feature set in their raw state. Global statistics over such features such as the average spectrum may be accumulated. The average spectrum that is frequency spectrum related to pitch can be useful for a simple approach to age and gender classification as well as for more elaborate statistics or classification based on the same features.

Phonetic features such as phoneme sequences or lattices can be processed to derive extended features such as phoneme length or articulatory phonetic features such as the place and manner of articulation for consonants vowel placement and other articulatory features that may be used for determining value for the accent property in the user profile.

Textual features are primarily word transcriptions or lattices that can similarly be processed to derive through analysis with dictionaries and tables the extended feature set needed for language based profile generation such as frequency of use and regional character. The resulting extended features deriving from the textual features can include the average word length and the number of words per sentence.

When a feature extraction model uses an actual grammar as opposed to a plain SLM it generates parse trees from which it can extract measurements like the number of nodes or the depth of every node. Hence extended linguistic features may be extracted derived and accumulated.

Certain profile characteristics may be computed from one feature source with the result passed straight through to the integrated user profile . In other cases a profile property value is determined by correlating newly derived accumulated extended features with previously established property values or across speech based and language based features. For example for young people age is correlated with both reading level and with education level. Thus classifiers may constrain profile property values in relation to other property values to improve the accuracy of determining these values. Similarly there are correlations between English proficiency education level SES and the presence of regional accent. Secondary classifiers may integrate information further. In principle user profile property values may be determined based on a combination of extended features coming both from speech profile characteristics and language profile characteristics . A variety of ensemble methods may be used to achieve the necessary integration.

The reason for dividing acoustic and phonetic features from linguistic features in this implementation pertains to the kinds of characteristics that can be derived from these features. Speech profile characteristics derived from acoustic and phonetic features are independent of the language profile characteristics derived from linguistic features the first relates to audible characteristics regarding pronunciation such as accent that is how a user speaks while the second is based on transcribed text from the audio to characterize the user s proficiency speaking English what the user says . Other implementations may use features from both sets and to classify the speech and language profile characteristics together or from only one feature set or .

The speech profile generation module is explored in more detail in . illustrates speech based classification and profile generation according to an implementation of the invention. In the acoustic and phonetic features of are split into separate acoustic features and phonetic features . The speech profile generation module includes a combined age and gender classifier using acoustic features as input and an accent classifier using phonetic features as well as acoustic features as inputs. These classifiers produce speech profile characteristics for integration into the user profile .

It is possible to recognize the accent of a foreign or native speaker from samples of a user s speech and to add a value to an accent property to the user s profile . In one exemplary implementation the values for the accent property for English may include Native English and Non Native English. Subcategories of Native English include Australian English British English Scottish English and Southern American and subcategories of Non Native English include an Indian Subcontinent accent a Chinese accent a French accent and others. In some implementations certain subcategories could be further divided. For example the Indian Subcontinent accent can be derived from the Tamil accent and Kannada accent.

The accent classifier can employ a variety of classification techniques such as the Na ve Bayesian Classifier Hidden Markov Model HMM Support Vector Machine SVM or Deep Neural Networks DNN to classify the user accent based on prosodic features including stress intonation and rhythm including pauses derived from primary acoustic features. One can also use model combination techniques such as Bayesian Model Averaging or other ensemble techniques to improve the accent detection accuracy. These classifiers may be adapted on a continuous basis based on new data.

An accent classifier may use many features for identifying and classifying an accent. Articulatory prosodic and phonetic factors may all play a role in analyzing utterances to identify accents. Language skills developed by pre pubescent children include a speaking style involving the features of phoneme production articulation tongue movement or other vocal tract physiological phenomena that governs accent as well. Normal speech production includes a sequence of movements in the vocal tract. These movements are different accounting for different accents. A person with a foreign accent can be said to exhibit deviations in speech articulation. A modified articulation model based on parameters such as the place and manner of articulation can be used to model the articulation deviations of accents. We may use deviations in neutral versus accented word production as distinctive features for accent classification including but not limited to front vowel position central vowel position back vowel position bilabial position labiodental position dental position alveolar position velar position glottal position and first and second order deviation of articulation positions.

Patterns of intonation as well as lexical stress and rhythm are prosodic features of speech production. Speakers with accents differ from native speakers in their continuative intonations. Other acoustic and prosodic features to model the change in speech production due to accent including autocorrelation lags log area ratios line spectral pair frequencies fundamental frequency format location and bandwidth short time energy of the speech signals slope of continuative intonations mean and standard deviation of the syllable rate mean and standard deviation of the syllable rate of the speakers and pause duration between the syllables.

Extraction of any and all of these features that contribute to accent classification can be derived from the acoustic features and phonetic features that make up extracted feature set . Associations between such features and various national or regional accents can be used to build a classifier as set out in for example Ghinwa Choueiter Geoffrey Zweig and Patrick Nguyen 2008 An Empirical Study of Automatic Accent Classification In Proc. ICASSP 08 Vol. 1 pp. 4265 4268 to map a user s features to an accent characteristic. For example statistics concerning the phonetic realization of words are correlated to accents. This fact can be exploited by machine learning models. Additional techniques may be used to improve accent classification for example Gaussian Tokenization or discriminative training with a Maximum Mutual Information criterion could be applied. Finally ensemble classifier methods can then help create a stronger classifier from many independent ones.

Age and Gender properties are closely related in terms of acoustic expression. As a result the illustrated implementation employs a single age and gender classifier to perform both age and gender classification. Alternate approaches using separate independent classifiers may be adopted.

A number of acoustic features can help identify the characterization of age and gender. For example speaker articulation differences vocal tract differences and vocal fold differences together may indicate a particular user s age and gender. A number of suitable designs for classifying age and gender based on speech are available in the art. In one example a machine learning model such as a Deep Neural Network DNN can capture the underlying mapping between acoustic features such as MFCC and internal articulation parameters such as articulator position manner of articulation etc. The derived articulation features may be used as input to train age gender and accent classifiers. A number of similar approaches have been proposed. These include the use of a Gaussian Mixture Model based on MFCC a Support Vector Machine based on GMM mean supervectors or an SVM. Acoustic features useful for gender and age classification include 1 the vocal tract features derived from MFCC coefficients 2 the modulation cepstrum in which slow and fast varying factors can be selected to extract relevant spectral features of different speakers 3 the pitch or the fundamental frequency including mean and standard deviation and corresponding statistics of the slope of the extracted pitch contour 4 the harmonics to noise ratio and 5 the magnitude of spectral peaks in each frame as detected in a short time Fourier analysis of the input signal.

Prosodic features also apply to age and gender classification. Physiological studies show that the speech generation process is affected by age in many ways. For instance the vocal tract length is increased by a lowered glottis position and pulmonary function may be reduced together with stiffer vocal folds. Prosodic features found useful for age and gender classification include the following 1 short time energy and its first and second order statistics 2 duration features on the phoneme level a measurement of how much faster or slower a phoneme is spoken by a user compared to its mean duration and 3 duration features on the word level the measurement of how much faster or slower the word in a given interval was spoken as compared to their mean user specific word rate .

The age and gender classifier also recognizes the possibility that user context information may already contain age and gender information in which case that information may be fed through to the speech profile characteristics and needs no classification step. The following discussion addresses the case where age and gender are not given in context information .

Age and gender may be classified based on acoustic features in the extended feature set derived and accumulated from primary acoustic features either by classifying age independently from classifying gender or by classifying age and gender inter dependently. In other words there may be cross property value constraints for the age and gender properties. The gender property may have a value of Male M or Female F and the age property value may be expressed using the following categories 

It is difficult to identify the gender of children so a value of a combined Age Gender profile property may only indicate a child without an indication of gender.

In an alternative implementation the age and gender profile properties could be classified in an application specific manner or could change based on improvements in classification algorithms availability of more data or both. For example the adult age categories could be further subdivided.

The language profile generation module is explored in more detail in . illustrates language based classification and profile generation according to an implementation of the invention. The computations used to classify linguistic features are much simpler than for classifying acoustic and phonetic features. Language based features may be syntactic represented by text and text processing is not as computationally expensive as audio processing algorithms. The system uses language profile generation module to generate language profile characteristics . Language profile generation receives input from linguistic features of the extended feature set particularly those derived or accumulated from textual features and linguistic features . Specific features may include average sentence length and average word complexity as measured by their length lookups in stored tables of word complexity and or a simple function of the word s rank in a frequency ordered dictionary.

Several formulas exist that can power a reading level classifier education level classifier and English proficiency classifier . In some English speaking countries or regions there is a strong correlation between speech patterns and socio economic status SES . In one implementation information from reading level education level and English proficiency classifiers as well as semantic features may be used as input to a SES classifier . A reading level classifier generates a numeric value derived from analyzing a body of text which approximates the minimum American grade level education that one would need to understand the text. Many such formulas have been developed including Flesh Kincaid Grade Level Dale Chill Formula and Fry Readability Graph . These tools generally infer a readability rating of some given text based on its mean sentence length mean syllables per word and whether the words it contains are classified as easy or hard . In a typical implementation of this system language classifiers may be based on one or more of these formulas to estimate the readability rating of user utterances. The value can be interpreted as an American school grade level such as the value 8.0 for grade 8 or it can be used to estimate directly the attained education level of the user. Hence reading level and education level may have similar values and in one implementation the reading level classifier and education level classifier may be one and the same or otherwise grouped together.

An estimation of likely educational level can in turn be used to make statistical inferences about a user s age based on the likely age of a person with that education level. In some instances if the actual age of the user is otherwise known through the context information or from the speech based profile characteristics any disparity between the readability of the user s input and his age can be used to infer other information about the user. For example a very high readability score relative to the user s age can potentially provide signals that the user has high aptitude. Depending on the system however a user may have been deliberately using simple language and short sentences when interacting with the system leading to an estimate of the user s reading level much lower than the user s true reading level. Thus design and weighting e.g. usefulness of the classifier to an advertiser of the classifiers categorizing a user s education may take into account the anticipated language of the user speech in the speech recognition application .

There are several simple formulas that an English language proficiency classifier can apply to user utterances. The percentage of utterances that are grammatically correct could be evaluated by using a parser for a precise English grammar as opposed to a more forgiving parser. Another method may be to determine the number and variety of unique sentence or phrase structures collected from the user such as looking for variations in tense person and use of prepositional phrases. Yet another method may be to determine the number of unique words uttered by the user. These features are in contrast with the reading level classifier based on formulas traditionally used in the school system. However depending on the system a user may deliberately use simple utterances of similar form in an attempt to ensure that the system will recognize his utterances based on previous experiences with this or similar systems.

The interaction of the profile manager with profile subscriber application s answer generation and multimedia output remain unchanged from the speech based system in .

User interface input devices may include a keyboard pointing devices such as a mouse trackball touchpad or graphics tablet a scanner a touch screen incorporated into the display audio input devices such as voice recognition systems microphones and other types of input devices. In general use of the term input device is intended to include all possible types of devices and ways to input information into computer system or onto communication network .

User interface output devices may include a display subsystem a printer a fax machine or non visual displays such as audio output devices. The display subsystem may include a cathode ray tube CRT a flat panel device such as a liquid crystal display LCD a projection device or some mechanism for creating a visible image. The display subsystem may also provide non visual display such as via audio output devices. In general use of the term output device is intended to include all possible types of devices and ways to output information from computer system to the user or to another machine or computer system.

Storage subsystem stores programming and data constructs that provide the functionality of some or all of the modules described herein including the logic to create inferred queries for use as query suggestions according to the processes described herein. These software modules are generally executed by processor alone or in combination with additional processors.

Memory used in the storage subsystem can include a number of memories including a main random access memory RAM for storage of instructions and data during program execution and a read only memory ROM in which fixed instructions are stored. A file storage subsystem can provide persistent storage for program and data files and may include a hard disk drive a floppy disk drive along with associated removable media a CD ROM drive an optical drive or removable media cartridges. The modules implementing the functionality of certain embodiments may be stored by file storage subsystem in the storage subsystem or in additional machines accessible by the processor.

Bus subsystem provides a mechanism for letting the various components and subsystems of computer system communicate with each other as intended. Although bus subsystem is shown schematically as a single bus some embodiments of the bus subsystem may use multiple busses.

Computer system can be of varying types including a workstation server computing cluster blade server server farm or any other data processing system or computing device. Due to the ever changing nature of computers and networks the description of computer system depicted in is intended only as a specific example for purposes of illustrating the preferred embodiments. Many configurations of computer system are possible having more or fewer components than the computer system depicted in .

In one implementation a method is described that assigns values to user profile properties based on analyzing user speech. The method includes receiving and storing extracted features representing one or more features of user speech observed over a first time period deriving and storing derived features based on the received extracted features and aggregating accumulated features by computing statistics based on extracted features and derived features that are stored during a second time period that is longer than the first time period. Values for one or more user profile properties are assigned based on the accumulated features derived features and extracted features. The user profile property values are stored in a user profile.

This method or other implementations of the technology disclosed can each optionally include one or more of the following aspects. Two features extracted within the same time period may be mutually constrained. The extracted features may be received from more than one feature extractor. The extracted features may include multiple of a cepstrum a spectrogram and a phoneme lattice. Values may be assigned to user profile properties including age gender accent reading level education level English language proficiency and socio economic status SES . A user supplied value for a user profile property may be used to constrain a value assigned to a different speech related user profile property.

Other implementations may include a non transitory computer readable storage medium storing instructions executable by a processor to perform a method as described above. Yet another implementation includes a system including a non transitory computer readable storage medium and a processor operable to execute instructions stored in the non transitory computer readable storage medium to perform a method as described above.

In another implementation a method is described that determines values of user profile properties based on textual input. The method includes receiving a textual input creating a parse tree by processing the textual input against a grammar and deriving linguistic features from the parse tree and mapping the linguistic features to one or more profile property values. Optionally a linguistic feature is derived that includes an indication of use of grammar or vocabulary.

The speech recognition architecture just described is exemplary many variants and alternative implementations exist that a person in the art will recognize

The specification has described a method and system for generating a detailed user profile through analysis of audio speech text and context inputs. Those of skill in the art will perceive a number of variations possible with the system and method set out above. These and other variations are possible within the scope of the claimed invention whose scope is defined solely by the claims set out below. The description is made with reference to the figures. Preferred implementations are described to illustrate the disclosure not to limit its scope which is defined by the claims. Those of ordinary skill in the art will recognize a number of equivalent variations in the description that follows.

