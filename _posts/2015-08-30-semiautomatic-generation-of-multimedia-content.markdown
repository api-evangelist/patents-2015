---

title: Semi-automatic generation of multimedia content
abstract: A method for multimedia content generation includes presenting to a user text that will serve as audio narration in a video clip, and a collection of media items to be selectively included in the video clip. Instructions, which associate one or more selected media items with corresponding elements of the text, are received from the user. The video clip is generated automatically, such that the selected media items appear in the video clip in synchronization with the corresponding elements of the text in accordance with the instructions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09524751&OS=09524751&RS=09524751
owner: WOCHIT, INC.
number: 09524751
owner_city: New York
owner_country: US
publication_date: 20150830
---
This application is continuation in part of U.S. patent application Ser. No. 14 170 621 filed Feb. 2 2014 which is a continuation in part of U.S. patent application Ser. No. 13 874 496 filed May 1 2013 which claims the benefit of U.S. Provisional Patent Application 61 640 748 filed May 1 2012 and U.S. Provisional Patent Application 61 697 833 filed Sep. 7 2012. The disclosures of all these related applications are incorporated herein by reference.

The present invention relates generally to multimedia generation and particularly to methods and systems for semi automatic generation of multimedia content.

An embodiment of the present invention that is described herein provides a method for multimedia content generation including presenting to a user text that will serve as audio narration in a video clip and a collection of media items to be selectively included in the video clip. Instructions which associate one or more selected media items with corresponding elements of the text are received from the user. The video clip is generated automatically such that the selected media items appear in the video clip in synchronization with the corresponding elements of the text in accordance with the instructions.

In some embodiments presenting the text includes laying the text on a timeline and receiving the instructions includes enabling the user to position the selected media items on the timeline in proximity to the corresponding elements of the text. In an embodiment the instructions received from the user associate each selected media item with a respective element of the text selected from a group of elements consisting of a word a part of a word a space between words and a punctuation mark.

In some embodiments automatically generating the video clip includes estimating respective times at which the elements of the text will appear in the audio narration in the video clip and inserting the corresponding media items into the video clip at the estimated times. In an embodiment the method further includes estimating based on the estimated times durations for which the selected media items will appear in the video clip and presenting the estimated durations to the user.

In some embodiments presenting the text and receiving the instructions include interacting with the user over a screen of a mobile communication device. Interacting with the user may include displaying a portion of the text with a corresponding subset of the media items on the screen and in response to input from the user scrolling to display a different portion of the text and a different subset of the media items. Additionally or alternatively interacting with the user may include displaying on the screen a portion of the text and a corresponding subset of the media items that span a given time duration and in response to input from the user zooming to display a different portion of the text and a different subset of the media items that span a different time duration.

There is additionally provided in accordance with an embodiment of the present invention apparatus for multimedia content generation including a user terminal and a processor. The user terminal is configured to present to a user a text that will serve as audio narration in a video clip to further present to the user a collection of media items to be selectively included in the video clip and to receive from the user instructions which associate one or more selected media items with corresponding elements of the text. The processor is configured to automatically generate the video clip such that the selected media items appear in the video clip in synchronization with the corresponding elements of the text in accordance with the instructions.

There is further provided in accordance with an embodiment of the present invention a computer software product the product including a tangible non transitory computer readable medium in which program instructions are stored which instructions when read by a processor cause the processor to present to a user a text that will serve as audio narration in a video clip to further present to the user collection of media items to be selectively included in the video clip to receive from the user instructions which associate one or more selected media items with corresponding elements of the text and to automatically generate the video clip such that the selected media items appear in the video clip in synchronization with the corresponding elements of the text in accordance with the instructions.

The present invention will be more fully understood from the following detailed description of the embodiments thereof taken together with the drawings in which 

Embodiments of the present invention that are described herein provide improved methods and systems for generating multimedia content. In the disclosed embodiments a video generation system receives textual input for which a video clip is to be generated. The textual input may comprise for example a short article relating to entertainment business technology general news or other topic. The system generates a video clip based on the textual input using a semi automatic human assisted process that is described in detail below.

The video clip generation process is mostly automatic and reverts to human involvement only where human input has the strongest impact on the quality of the video clip. As a result the time and cost of generating video clips are reduced to a minimum while still producing highly professional clips. Moreover the disclosed techniques generate video clips virtually in real time shortening the turnaround time needed for presenting breaking news to end users.

In some embodiments the video generation system analyzes the textual input for example using contextual analysis algorithms so as to extract descriptive metadata. The system queries various media databases using the extracted metadata so as to retrieve media assets that are likely to be related to the textual input. Media assets may comprise for example video and audio excerpts still images Web page snapshots maps graphs graphics elements social network information and many others. The system ranks and filters the media assets according to their relevance to the textual input and presents the resulting collection of media assets to a human moderator.

The task of the moderator is largely editorial. The moderator typically selects media assets that will appear in the video clip and correlates one or more of them in time with the textual input. In some embodiments the presentation times of at least some media assets are set automatically by the system.

In some embodiments audio narration of the textual input is not yet available at the moderation stage and the moderator uses an estimation of the audio timing that is calculated by the system. The system thus receives from the moderator input which comprises the selected media assets and their correlation with the textual input. Moderation typically requires no more than several minutes per video clip.

Following the moderation stage the video generation process is again fully automatic. The system typically receives audio narration of the textual input. The audio narration is typically produced by a human narrator after the moderation stage and possibly reviewed for quality by the moderator. The system generates the video clip using the audio narration and the selected media assets in accordance with the moderator input. The system may include in the video clip additional elements such as background music and graphical theme. The video clip is then provided as output optionally following final quality verification by a human.

As noted above the methods and systems described herein considerably reduce the time and cost of producing video clips. In some embodiments the disclosed techniques are employed on a massive scale for converting a large volume of textual articles into video clips using a shared pool of moderators and narrators.

In the example of system receives the textual inputs from a client system and returns the video clips to the client system. A video generation system of this sort may be used for example for providing a publisher with video clips based on textual articles received from the publisher. System communicates with client system over a communication network e.g. the Internet. In alternative embodiments however system may obtain textual inputs from any other suitable source and deliver video clips to any other suitable destination. System can thus be used in a variety of business models and modes of operation.

The details of the video generation process performed by system will be explained in detail below. Generally system communicates over network with one or more media databases DBs so as to retrieve media assets that are related to the textual input. The media assets are also referred to as media items and may comprise for example video and or audio excerpts still images Web page snapshots maps graphs graphical elements social network information and many others. Media DBs may comprise for example content Web sites social network servers or any other suitable database.

System presents the textual input and the corresponding automatically retrieved media assets to a human moderator using a moderator terminal . The figure shows a single moderator for the sake of clarity. A real life system however will typically use multiple moderators for handling multiple textual inputs and video clips simultaneously. Moderator reviews and selects media assets that will be included in the video clip and arranges the media assets so as to correlate in time to the timing of the textual input. The moderator thus produces moderator input which is fed back to system over network .

In addition to moderator input system further receives audio narration of the textual input in question. The audio narration is produced by a narrator using a narrator terminal and provided to system over network . Although the figure shows a single narrator for the sake of clarity a real life system will typically use multiple narrators. Based on moderator input and audio narration system automatically produces video clip . Video clip is delivered over network to client system . In some embodiments the automatically generated video clip is verified by moderator before delivery to client system . Audio narration is also optionally verified for quality by moderator .

In the example of system comprises an interface for communicating over network and a processor that carries out the methods described herein. The system configuration shown in is an example configuration which is chosen purely for the sake of conceptual clarity. In alternative embodiments any other suitable system configuration can be used.

The elements of system may be implemented using hardware firmware such as in an Application Specific Integrated Circuit ASIC or Field Programmable Gate Array FPGA using software or using a combination of hardware firmware and software elements. In some embodiments processor comprises a general purpose processor which is programmed in software to carry out the functions described herein. The software may be downloaded to the processor in electronic form over a network for example or it may alternatively or additionally be provided and or stored on non transitory tangible media such as magnetic optical or electronic memory.

In various industries it is becoming increasingly important to generate video clips with low cost and short turnaround time. For example news Web sites increasingly prefer to present breaking news and other stories using video rather than text and still images. Brands may wish to post on their Web sites video clips that are relevant to their products. Publishers such as entertainment Web sites may wish to publish topic centered video clips. Multi Channel Networks MCNs may wish to create video clips in a cost effective way for blog content.

System generates video clips using a unique division of labor between computerized algorithms and human moderation. The vast majority of the process is automatic and moderator is involved only where absolutely necessary and most valuable. As a result system is able to produce large volumes of high quality video clips with low cost and short turnaround time.

Processor analyzes the textual input at an input processing step . Typically processor applies contextual analysis to the textual input so as to extract metadata that is descriptive of the subject matter and content of the article in question. Using the extracted metadata processor generates one or more search queries for querying media databases . In some embodiments processor summarizes the textual input e.g. to a certain target length and then performs contextual analysis and query generation on the summarized article.

At a data retrieval step system queries media databases over network using the automatically generated search queries so as to retrieve media assets . The media assets may comprise any suitable kind of media items such as video excerpts audio excerpts still images Web page snapshots maps graphs graphics elements and social network information.

The retrieved media assets are all likely to be relevant to the textual input since they were retrieved in response to search queries derived from the textual input. Nevertheless the level of relevance may vary. Processor assigns relevance scores to the media assets and filters the media assets based on the scores at a filtering step . The filtering operation typically comprises discarding media assets whose score falls below a certain relevance threshold.

When processing video assets processor may assign relevance scores to specific parts of a video asset not only to the video asset as a whole. For example the video asset may be previously tagged by moderators to identify portions of interest or it may comprise a time aligned transcript of the video that enables processor to identify portions of interest.

The output of step is a selected collection of ranked media assets which are considered most relevant to the textual input. System presents this output to moderator over network using terminal . At this stage human moderation begins.

In an embodiment moderator initially critiques the textual input and the selected media assets at a verification step . In an embodiment the moderator verifies whether the textual input indeed answers the editorial needs of the system. For example the moderator may verify whether the article content is interesting enough to justify generation of a video clip. The moderator may validate and optionally edit the textual input before it is provided to narrator for narration.

Additionally or alternatively moderator may proactively search for additional media assets that were not retrieved automatically by system and add such media assets to the collection. The moderator may also validate the article topics that were suggested by the system and fix them if necessary. In an embodiment moderator rates the media assets that were suggested by system . The rating can be used to train the system improve its enrichment mechanisms and enhance the automatic asset retrieval process. Further additionally or alternatively the moderator may critique and or modify the textual input and or the automatically selected media assets in any other suitable way.

The moderator selects the media assets that will actually be included in the video clip and correlates them with the textual input at an asset configuration step . Typically system estimates the expected duration of voice narration of the textual input even though the actual narration is not available at this stage and indicates the expected duration to the moderator. This indication helps the moderator determine the number and types of media assets he should select.

If the total duration of the media assets chosen by the moderator is smaller than the expected duration of the narration system may abort the process altogether or attempt to find additional media assets possibly assets that have been filtered out at step .

Moderator may perform additional filtering of the media assets on top of the filtering performed by processor based on editorial considerations. For example the moderator may prefer media assets that are likely to be more attractive to the client. Within video assets the moderator may mark particular segments to be included in the video clip e.g. specific phrases or sentences from a long speech.

When a video asset comprises an audio soundtrack the moderator may configure the use of this audio in the video clip. For example the moderator may decide to use the original audio from the video asset as foreground audio or as background audio in the video clip or discard the original audio and use only the video content of the video asset.

In an embodiment the moderator indicates that a specific media asset is to be synchronized with a specific component of the textual input e.g. with a specific word. This indication will later be used by processor when scheduling the media assets in the final video clip.

Additionally or alternatively moderator may configure the media assets to be included in the video clip in any other suitable way. The output of the human moderation stage is referred to herein as moderator input denoted in or user input that is fed back to system over network .

At a narration input step system receives audio narration of the textual input from narrator . The narrator may divide the textual input into segments and narrate each segment as a separate task. In embodiment of the audio narration is received after the moderation stage. In alternative embodiments however the audio narration may be received and stored in system at any stage before generation of the final video clip.

In some embodiments system processes the audio narration in order to improve the audio quality. For example the system may automatically remove silence periods from the beginning and end of the audio narration and or perform audio normalization to set the audio at a desired gain. In some embodiments moderator reviews the quality of the audio narration. The moderator may approve the narration or request the narration to be repeated e.g. in case of mistakes intolerable audio quality such as background noise wrong pronunciation or for any other reason.

At this stage processor automatically constructs the final video clip at a clip generation step . Processor generates the video clip based on the moderator input the audio narration and the media assets selected and configured by the moderator. Processor may use a video template e.g. a template that is associated with the specific client. The final video clip generation stage is elaborated below.

In an embodiment moderator validates the final video clip at a final validation step . The moderator may discard the video clip altogether e.g. if the quality of the video clip is inadequate. After validation the video clip is provided to client system . The flow of operations shown in is depicted purely by way of example. In alternative embodiments any other suitable flow can be used.

In some embodiments processor constructs the final video clip by scheduling the selected media assets over a timeline that is correlated with the audio narration. Scheduling of the media assets is performed while considering the constraints given by the moderator step of with regard to synchronization of media assets to words or other components of the narrated text.

In some embodiments processor produces a timing estimate for the narration. The timing estimate gives the estimated occurrence time of each word or other component in the audio narration. In some embodiments processor derives the timing estimate from the textual input independently of the actual audio narration. In many cases the timing estimate is produced before the audio narration is available. Processor may use any suitable process for producing the timing estimate from the textual input. An example process is detailed in U.S. patent application Ser. No. 13 874 496 cited above. In other embodiments the audio narration is already available to processor when producing the timing estimate. In these embodiments the processor may derive the timing estimate from the audio narration rather than from the textual input. The output of this estimation process is narrated text with time markers that indicate the timing of each word or other component.

In some embodiments processor divides the narrated text into segments. The borders of each segment are either the start or end points of the entire narrated text or the estimated timing of media segments that include foreground audio e.g. video assets that will be displayed in the final video clip with the original audio and without simultaneous narration . Processor then schedules media assets separately within each segment.

In the present example the moderator instructed that a video asset is to be synchronized to a particular word and therefore occur at a time T on the timeline. Times T and T mark the beginning and end of the entire narrated text respectively. The moderator has also decided that the original audio track of this video asset will be used as foreground audio in the final video clip. Therefore there is no narration track to be played during the playing time of this video asset.

In this example processor divides the narrated text into two segments denoted S T T and S T T . The video asset in question is scheduled to appear between the two segments. Within each segment processor schedules the media assets that will appear in the segment.

Typically each type of media asset has a minimal and a maximal allowed duration and therefore not all combinations of media assets can be scheduled in each segment. For example if the duration of segment S is estimated to be four seconds and the minimal duration of a still image asset is configured to be two seconds then no more than two still images can be schedule in this segment.

In some embodiments processor selects media assets for each segment by calculating multiple possible permutations of media asset scheduling and assigning each permutation a score. The score of a permutation is typically assigned based on factors such as 

Additionally or alternatively processor may use any other suitable criteria for calculating the scores of the various scheduling permutations.

For a given segment processor schedules the media assets in accordance with the permutation having the best score. In some embodiments processor also schedules video template components such as visual effects and transitions between successive media assets.

In some embodiments processor applies a supervised learning algorithm to perform the automatic media asset scheduling i.e. automatic timeline generation process. The features for training such a model can be derived from the contextual metadata of the article and the narrated text. The target feature i.e. examples of correct and or incorrect placement of a media asset in a given segment can be derived from feedback of moderator . In the training stage the scheduling process is assisted by the moderator. After training processor can generate the timeline in a fully automatic manner based on the trained model.

In various embodiments processor may schedule the audio in the video clip in different ways. For example processor may choose background music for the video clip depending on the contextual sentiment of the textual input possibly in conjunction with predefined templates. Processor typically receives as input a list of audio tracks The audio narration of the textual input the background track or tracks effects for transition between media assets raw audio of the media assets e.g. original audio that is part of a video asset . Processor adds the audio tracks to the timeline including transitions between the different audio tracks. Transition rules between audio tracks are typically applied based on the applicable template e.g. by performing cross fade between different tracks.

Processor typically performs video rendering based on the selected visual assets e.g. template related visual objects video assets still images maps Web pages and transitions and audio assets e.g. audio narration background music effects and natural sounds from video assets according to the generated time line. Rendering may also be performed automatically using an Application Programming Interface API to a suitable rendering module. An optional manual validation step may follow the rendering process.

Term user however may refer to moderator or to any other user. An alternative system configuration in which the user is a personal user who generates video clips using a mobile application on a mobile communication device is addressed further below.

The user terminal of enables the user e.g. moderator to select media assets to be included in the video clip and to synchronize them with specific elements of the textual input. These actions correspond for example to step above. The specific GUI features of are shown purely by way of example and any other suitable GUI features can be used for this purpose.

As explained above system generates the video clip on the basis of textual input referred to below simply as text . At some stage of the process the text is narrated and this narration is inserted as audio in the clip. In the present example the text or a portion thereof is displayed on a timeline at the bottom of the screen.

A window at the top of the screen displays the collection of media assets that were retrieved by system based on the text. These media assets may comprise for example still images video and audio excerpts Web page snapshots maps graphs graphics elements social network information and many others.

The user terminal GUI enables the user to select a media asset and associate the selected media asset with a desired element of the text. In the present example the user is able to select a media asset in window and drag and drop the media asset at a desired position on timeline . By positioning a media asset above a certain element of the text the user instructs system to associate and time synchronize the media asset and the text element.

Processor generates the video clip in accordance with these instructions i.e. inserts each media asset into the video clip in synchronization with the corresponding element of the text.

For example an image A has been dragged and dropped by the user above the word and of the text. By performing this action the user instructs system that image A should be displayed at the time the word and is played in the audio narration. Similarly an image B has been placed by the user over the word competition of the text. A video excerpt C has been placed over the word pavilions and an image D has been placed over the word be. The GUI typically underlines or otherwise marks the text element with which the media asset is associated.

Using the user terminal GUI the user may associate each media asset with various types of text elements such as for example with a word a part of a word a space between words a punctuation mark or any other suitable element of the text. The association instructs system to synchronize the appearance of the media asset with the narration of the corresponding text element in the video clip.

In some embodiments processor of system estimates the timing of the audio narration based on the input text. In other words processor estimates the respective times at which the text elements e.g. words parts of words spaces or punctuation marks will appear in the audio narration and thus in the video clip. Based on this time estimation processor is able to estimate and present to the user various timing figures relating to the selected media assets.

In one example embodiment processor estimates and displays the time duration for which a given image will appear in the video clip. For example image A is estimated to appear for a duration of 3.00 3.63 seconds in the video clip. Processor typically estimates this time duration by assuming that image A will first appear in the clip when the word and is played and will be displayed until the word has is played at which time the next image has to be inserted . Thus the presentation duration of image A is therefore the time difference between playing of the words and and has in the audio narration.

Unlike still images the time duration of video excerpts is predefined. Except C for example will appear for a duration of 7.15 seconds in the video clip. In some embodiments when the user attempts to place a video excerpt on the timeline the GUI may verify that the predefined duration of the excerpt does not overlap another media asset that has already been placed on the timeline. If an overlap is found the GUI may prevent the excerpt from being dropped at the attempted position and or prompt the user.

In some embodiments the user terminal marks the media assets in window that have already been selected and positioned on timeline . In the present example previously selected media assets are labeled pinned in window . This marking prevents the user from unintentionally selecting the same media asset more than once in the same video clip.

The configuration of above refers mainly to a system that produces a mass of video clips using multiple moderators and narrators for various client systems. In alternative embodiments the disclosed techniques can be implemented in a mobile communication device environment for allowing a personal user to generate video clips for his own use of for sharing with others.

In such embodiments the same personal user typically plays the roles of both the moderator and the narrator. The functionality of the user terminal is typically carried out by an application running on the user s mobile device e.g. smart phone or tablet computer. The application interacts with the user using the available input and output devices e.g. a touch sensitive screen of the device. The functionality of processor may be carried out in the mobile device or in a server with which the mobile device communicates.

In some embodiments a variant of the user terminal GUI is implemented using the touch sensitive screen of the mobile device. Typically however the mobile device screen is considerably smaller than the screen of a desktop terminal and therefore the GUI typically differs from that of .

For example in some embodiments the user terminal mobile application in this example displays only a portion of the timeline with the corresponding media assets at any given time. By interacting with the touch sensitive screen the GUI enables the user to scroll forward and backward along the timeline.

As another example the user terminal may enable the user to zoom in and zoom out using simple gestures so as to increase or decrease the time duration of the displayed portion of the timeline. Zooming out enables the user for example to view the entire timeline and scroll to times of interest. Zooming in enables the user for example to place media assets on the timeline and correlate them with text elements with high precision.

It will be appreciated that the embodiments described above are cited by way of example and that the present invention is not limited to what has been particularly shown and described hereinabove. Rather the scope of the present invention includes both combinations and sub combinations of the various features described hereinabove as well as variations and modifications thereof which would occur to persons skilled in the art upon reading the foregoing description and which are not disclosed in the prior art. Documents incorporated by reference in the present patent application are to be considered an integral part of the application except that to the extent any terms are defined in these incorporated documents in a manner that conflicts with the definitions made explicitly or implicitly in the present specification only the definitions in the present specification should be considered.

