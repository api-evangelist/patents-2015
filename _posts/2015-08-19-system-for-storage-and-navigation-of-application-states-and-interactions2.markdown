---

title: System for storage and navigation of application states and interactions
abstract: Complex collaboration or decision support applications perform complex design or planning tasks, often with the input of large groups of people over long periods of time. The combination of time and complexity can often obscure past actions, making it difficult to remember the factors that influenced earlier stages in the planning task. This is especially true if the task involves many people and different people work at different times. The application state navigation system provides an application-independent mechanism that allows operators to walk back through the history of the application in order to better understand (or remember) the application actions that were taken in the past and the triggers for those actions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09575640&OS=09575640&RS=09575640
owner: Accenture Global Services Limited
number: 09575640
owner_city: Dublin
owner_country: IE
publication_date: 20150819
---
This application is a continuation of U.S. application Ser. No. 13 480 624 filed May 25 2012 now allowed which is a continuation of U.S. application Ser. No. 11 409 538 filed on Apr. 21 2006 which claims the benefit of U.S. Provisional Application Ser. No. 60 764 004 filed on Jan. 31 2006 the entire contents of all of these applications which are hereby incorporated by reference.

A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyright rights whatsoever.

This invention relates to tracking and display of the history of an evolving application. In particular this invention relates to a system through which an operator may navigate in time through application states actions taken on the application and the triggers for those actions to view the application at selected temporal points in the past.

Groups of people often use complex collaboration and decision support tools over long periods of time to perform complex design and planning actions and decisions which drive an application e.g. a chemical plant process configuration or a complex material distribution chain to its current state. The combination of time and complexity can often obscure past actions and decisions making it difficult to remember the factors that influenced earlier stages in the planning. This is especially true if the task involves many people and different people work at different times.

Thus in the past applications resulting from the complex design and planning tasks had a relatively opaque history. Any understanding of how an application arrived at its current state was often limited by individual often inaccurate memories hastily taken notes and poor documentation. Accordingly it was very difficult to unravel the history of the design and planning steps leading to the current state of the application.

For these and other reasons substantial challenges existed in providing an understanding of how and why decisions were made who made the decisions and how those decisions impacted the current state of the application. These limitations hindered the further development improvement and changes to the application as well as attempts to understand how an application arrived at its current state. The limitations also greatly increased the difficulty of effectively and efficiently introducing new people to development or planning roles for the application.

At the same time the amount of data underlying the history of the application presents significant display manipulation and presentation challenges. In particular distributed data gathering and processing systems allow the collection storage and manipulation of large amounts of data including real time data. Real time data is data that is updated and processed at the same rate at which it is received. Real time data may originate in a variety of sources. Examples include sensor networks in systems or environments to be monitored and radio frequency identification tag RFID tracking systems for inventory or assets. These and other related technologies have given organizations access to a wealth of real time information about all aspects of the organization s operation. These aspects include for example consumer behavior competitor behavior employee productivity supply chain activity manufacturing shop floor activity and so on.

For large organizations this information can be extremely valuable for making decisions or developing insights. In the aggregate this information may reveal patterns or trends not otherwise immediately apparent. When processed over time this information may reveal developments that may be used for future prediction. Gathering and managing large amounts of data can provide a new view of system status or operation.

However the enormous volume of data and the density of inter connections can make it difficult to easily visualize this information on standard workstations. A conventional workstation is based on personal computer technology and generally includes a processing device and a user interface including for example a display screen a mouse or other input device. While multimedia capabilities can enhance the presentation of information the small size and limited features of the conventional device make meaningful presentation of the information difficult.

Furthermore the complexity of the data and the decision making processes necessitate a shared view and collaboration among multiple experts. Even with a large screen and multimedia capabilities only a few people can see and interact with a conventional workstation simultaneously. If manipulation of a mouse or keyboard is required only a single person can control the workstation while a limited number of other participants view the process.

One solution to both issues is to give experts access to an immersive environment with which they can view share and physically interact with the information. In one example such an environment is made up of large high resolution displays personal digital assistants PDAs and three dimensional 3 D displays along with alternative interaction modalities such as touch enabled screens 3 D mouse data gloves etc.

Due to constraints with both technology and form factor such an environment requires a system that can distribute one logical application across multiple computers and display systems that make up the immersive environment. Such a system must handle distribution of both the visual display and the user interactions.

Adequate systems to provide the necessary capabilities have not been available. Currently there is no standard architecture language or protocol for building applications that span multiple and possibly heterogeneous computers multiple displays possibly of different form factors and multiple interaction modalities. Such applications have to be created from scratch with the application developer managing interaction among multiple computers multiple displays and multiple interaction modalities. Some commercially available system are typically built using expensive video processors that allow information and graphical data to be displayed on a screen but not necessarily as one cohesive application. Examples are available from Jupiter.com and are priced in the hundreds of thousands of U.S. dollars.

A need has long existed for a system which facilitates the understanding review and design of complex applications and which addresses the difficulties associated with displaying the potentially extensive volume of data underlying the history of the applications.

Given this backdrop of complex planning design and planning leading to complex applications a navigation system provides an application independent mechanism that allows operators to walk back through the history of the application. The navigation system thereby allows the operator to better understand or remember the actions that were taken in the past and the triggers for those actions. Because the actions determine how the application reached any given state at any given time the navigation system allows the operator to obtain the insight necessary to understand not only the present state of the application but also the manner in which the application changed over time and why. In addition a display image generated by coordinated display devices may provide an extensive graphical interface for conveying the application history actions taken on the application contemporaneous environmental data or any other data useful for understanding the history of the application.

The navigation system may include a display formed from multiple image tiles and a temporal selection interface element. The navigation system stores historical data concerning the application and its development over time. As one example a historical action database may store action records optionally distinguished by the operator responsible for the action which influenced the application. The action records may include application action data defining an action taken on the application e.g. modify a material transportation route at an action time an operator identifier which identifies an operator responsible for the action taken and an application action timestamp representing the action time. As another example a historical state database may store state records. The state records may include application state data defining an application state of the application at a state time and an application state timestamp representing the state time. As a third example a historical environment database may store environment data defining an environmental state of the application at an environment state time an application environment timestamp representing the environment state time.

A processor in the navigation system executes programs stored in a memory of the navigation system. For example an interface input program may obtain a temporal selection e.g. one week ago from the temporal selection interface element. A temporal index program may initiate database searches based on the temporal selection. As examples the temporal index program may initiate an action search of the historical action database which results in retrieved application action data a state search of the historical state database which results in retrieved application state data and an environment search of the historical environment database which results in retrieved environment data.

An output program may then render an application representation according to the retrieved application state data. Alternatively or additionally the navigation system may propagate or recreate the application state given the retrieved application action data. The navigation system may also display the application action data for review. In addition the navigation system may output the retrieved environment data to provide local or remote audiovisual context for the application state.

Regarding the tiled output display for presenting the application history and by way of introduction only the presently disclosed embodiments provide a system and method for distributed information processing and interaction. In one general embodiment application processors respond to one or more software applications to produce outputs such that the respective outputs together form a system output. The individual processor outputs may be a display on a display device operation of an actuator such as unlocking an electronic lock or changing information in a database. The processor outputs may be as simple as a change in output data from the processor. Each change or output produced by a processor is a change in the local state of the respective processor as well as a change in the overall system state. Each change in state of an application processor is reported in a state change message to a state server.

The state server operates to synchronize the local states of the respective processors. The state server reflects received state change messages from one application processor to all other application processors in the system. Also any system inputs are reported to the state server which again reflects the state change to all application processors. In this way the application processors are maintained in synchronous states. Output changes in one processor that should be reflected in the outputs of other processors are done so automatically and seamlessly by reporting the change to the state server and reflecting the change to all other processors.

In a specific embodiment each application processor drives a display device which produces one or more image tiles of a tiled output display. Each processor uses the state change messages from the state server to update state information for the entire display even though the processor is responsible for displaying only a designated portion of the display. The assigned designated portion of the display is the context of the processor. An input device detects user interaction with the display similar to mouse movements and mouse clicks but in one embodiment hand motions in front of the display. Detected user interactions are reported to the state server and information about the user interactions is reflected from the state server to the application processors which produce the image. The image is updated based on the information from the state server.

Other systems methods features and advantages of the invention will be or will become apparent to one with skill in the art upon examination of the following figures and detailed description. It is intended that all such additional systems methods features and advantages be included within this description be within the scope of the invention and be protected by the following claims.

The elements illustrated in the Figures interoperate as explained in more detail below. Before setting forth the detailed explanation however it is noted that all of the discussion below regardless of the particular implementation being described is exemplary in nature rather than limiting. For example although selected aspects features or components of the implementations are depicted as being stored in memories all or part of systems and methods consistent with the application state navigation system and method may be stored on distributed across or read from other machine readable media for example secondary storage devices such as hard disks floppy disks and CD ROMs a signal received from a network or other forms of ROM or RAM either currently known or later developed.

Furthermore although specific components of the application state navigation system will be described methods systems and articles of manufacture consistent with the application state navigation system may include additional or different components. For example a processor may be implemented as a microprocessor microcontroller application specific integrated circuit ASIC discrete logic or a combination of other type of circuits or logic. Similarly memories may be DRAM SRAM Flash or any other type of memory. Flags data databases tables and other data structures may be separately stored and managed may be incorporated into a single memory or database may be distributed or may be logically and physically organized in many different ways. Programs may be parts of a single program separate programs or distributed across several memories and processors. Furthermore the programs or any portion of the programs may be instead be implemented in hardware.

The present embodiments provide a software architecture and a uniform abstraction layer that allows application developers to create data intensive and collaborative applications without explicit concern for how the application might be distributed across multiple displays and computers within the immersive environment. This architecture is implemented through high level primitives that include five mechanisms that a divide an application or set of applications across multiple machines and multiple form factors b provide mechanisms for aligning output portions such as display segments on different computers and or different output devices such as projectors c provide means for collecting user interactions from multiple input devices which may be controlled by multiple machines so that the application has a synchronized view of user interactions d provide a communication synchronization mechanism to allow multiple data intensive and collaborative applications to execute and cooperate within a single computing environment and e provide user interface constructs that allow multiple users of the application to interact with each other within the immersive environment.

The software architecture in accordance with these embodiments provides a uniform abstraction layer for constructing data intensive and collaborative applications for immersive environments. The uniform abstraction layer frees application developers from managing low level details of the environment thereby dramatically increasing their productivity and application quality. The innovation also allows application developers in developing data intensive and collaborative applications for immersive environments.

The architecture is built to support applications and the review of application history in a distributed immersive multi user environment an environment that is not supported by typical desktop operating constructs. Therefore the architecture is built to shield a developer from the complexities of the environment and allow them to write the end user applications through a uniform abstraction layer. That layer assumes that any application might reside on more than one physical computer. It therefore provides transparent support for segmenting the application and synchronizing the segments. The developer creates the application as if it were a single process on a single computer. The architecture also assumes that several applications might be cooperating within a single environment or multiple locations. It therefore provides the messaging and synchronization mechanisms needed to maintain the overall environment even across multiple platforms and modalities. Developers need only to make sure that each different application understands different states. Furthermore the architecture assumes that there can be several input mechanisms and that one or more of the input mechanisms might allow multiple simultaneous inputs from one or more users. The architecture provides the end applications with a common synchronized view of all inputs. The developer needs only to map those inputs to state changes based on application rules which are in turn synchronized by the architecture. Finally the architecture assumes that some end displays might be composed of multiple tiled projectors. In this case the architecture supplies the software facilities needed to precisely align and blend the edges of multiple tiles. This may be done by the user at run time but this complexity is hidden from the application developer. Details for each component are listed below 

Networked Messaging Component The architecture automatically connects all applications to the centralized server that controls synchronization and messaging. The complexities of networking are hidden from developers and the architecture passes messages directly to the application. As a result there is no distinction between local interactions such as from an attached keyboard and networked interactions such as from a networked control device . Also the developer handles messages in a manner that is very similar to a Windows Message Map but the architecture handles distributing the results of the message map to all computers. Messages can originate from applications or from dedicated user interface devices such as touch screens or data gloves.

Networked Time Synchronization The architecture also ensures that all applications and or application fragments have a common synchronized time. This is crucial to animation and state management. In addition to the common time data the architecture also supplies an interpolator class that can be used to manage state animations and any other time based data. Interpolator objects can be assigned start and end times start and end values and interpolation functions that determine how the values change over time. Once created the interpolators supply time synchronized values to animations and other time sensitive features.

Application Fragmentation Application developers can write single applications that place and animate objects on a single very large virtual canvas. Host computers each have a small data file that specifies what area of that virtual canvas they control. This is generally determined by the capabilities of the computer and the physical placement the screen. When the application runs on multiple machines the architecture uses the data file to draw the appropriate portion of the canvas. The previous two features ensure that all fragments are synchronized and present themselves as one logical application.

Application Coordination In some environments especially those with different hardware platforms several individual applications as opposed to one fragmented application might need to interact to provide a single working environment. This is also supported by the architecture. The architecture uses the Networked Messaging Component to message environmental state changes to all registered applications. Each application can respond according to its own criteria.

Screen Alignment The architecture supports the use of tiled projected images to present display output. One inherent problem with tiled projection is that adjacent projectors must be carefully aligned and edge blended to create a single contiguous display space. The architecture uses information in the data file mentioned above to determine whether or not the image must be aligned and how the alignment must be done. If alignment is necessary the architecture loads a file with user supplied alignment data and performs the necessary image warping on the application output. This feature can be extended to color correction between projectors. The details of the end display device are shielded from the application developer.

High Level Primitives The architecture shields developers from the complexities of the underlying graphics technology with a set of common high level primitives. These primitives represent common objects such as 2D objects 3D objects lines webcams pictures video sound text graphs and binary data objects.

Scene Encapsulation Finally application developers take advantage of the above features by creating instances of Controller objects that encapsulate all objects data and logic within a given scene. The architecture will display the output of one controller at a time making a controller nearly synonymous with application or episode . However controllers can also be nested and otherwise used to place objects into individual batches. This helps application developers distribute development efforts across several people. It also helps to increase overall graphics performance as different batches can be updated at different frequencies if necessary.

Referring now to the drawing shows a system for distributed information processing and interaction and which supports navigation of complex application history. The system includes a state server a plurality of application processors or output devices and an input output I O controller with a plurality of input devices . The system is an embodiment of an architecture that spans multiple heterogeneous computer devices multiple output devices and multiple modalities. The system responds to inputs received from the input devices to produce effects on the output devices . The effect produced may be identical for one or more output device or the effect produced may be different depending on the effect and the particular output device. As will be described in further detail below the effect produced on any one output device will depend on the nature of the output device and effect.

Generally the state server maintains the state or states of the system and ensures that the output devices are synchronized. The state server communicates with the output devices by sending messages preferably with a predetermined message protocol. The state server reflects all messages it receives about the system back to all appropriate devices in the system in particular back to all the output devices . The state server receives input signals from the input devices and sends messages to the appropriate output devices to synchronize the states of the output devices in a manner to be discussed in greater detail below. The input devices may include any sort of sensor or data gathering device. The output devices may include any sort of audio or video or other output device or system including a personal computer PC such as a table PC a personal digital assistant PDA or laptop computer or other specialized device or processor. The output devices most generally include any type of actuator for causing an interaction or result in the environment.

The embodiment of will be discussed in the context of an exemplary system in which a display image is controlled on a display surface by the system . The display image is made up of a plurality of display devices . In the exemplary system the display devices are video projectors under control of one or more respective application processors arranged to project a display on panels of a glass wall forming the display surface . Any suitable number of projectors and glass panels may be used to form the display image. In the example of a 2 4 array of projectors together form the display image on a similar array of glass panels . In other embodiments other numbers of projectors and other numbers of glass panels may be used. In still other embodiments other types of video display devices may be substituted such as flat panel displays plasma displays or cathode ray tubes. The principles described herein should not be limited by any particular technology by may be applied to the widest variety devices.

In the exemplary embodiment of the input devices include one or more cameras a data glove a pointing device and an infrared sensor . As is indicated in the drawing other input devices similar to these or of other types may be added to the system . A camera is a device which captures a video image. The image may be single images in any suitable format including digital formats such as jpeg referring to a standard data format advanced by the Joint Photographic Experts Group or gif Graphics Interchange Format or other or the video images may be a motion picture image such as a data file encoded according to a standard such as that proposed by the Motion Picture Experts Group MPEG or one of its variants or other standard. A data glove is an input device for virtual reality and other data processing devices in the form of a glove which measures the movements of the wearer s fingers and transmits them to the data processing device or computer. Sophisticated data gloves also measure movement of the wrist and elbow or other body part movements may be measured as well. A data glove may also contain control buttons or act as an output device e.g. vibrating under control of the computer. The user usually sees a virtual image of the data glove and can point or grip and push objects. The pointing device may be any data entry and manipulation device such as a computer mouse or pen tablet. The infrared sensor may be any sensing device that responds to heat energy or radio emissions from a device to produce an image.

The I O controller serves as a data gathering interface between the system and the input devices . The I O controller therefore is configured to receive signals from the respective input devices and provide any necessary signals in return. For example for receiving a jpeg format file from a camera the I O controller receives a jpeg formatted file from the camera and stores the file providing appropriate network handshaking signals to the camera . In another example the format may be streamed data from another camera or from the data glove . The I O controller provides the necessary handshaking signals to the input device and stores the received data for viewing or immediately provides the data to the state server . The I O controller receives data from the input devices in the format provided by each respective input device and formats the data to the format required by the state server and the output devices .

In the exemplary embodiment in which a display image is controlled on a display surface by the system the input devices include sensing devices to detect a user s hand or other body part adjacent the display surface and acting as a pointing device to manipulate a portion of the image. For example in one embodiment the input devices include a set of cameras positioned adjacent the display surface to detect position and movement of a user s hand and in conjunction with the I O processor to provide signals to the state server indicating the position and motion of the user s hand. The signals are conveyed to the respective application processors and used to produce an appropriate display or change in the display. For example if the hand motion should be interpreted as a left click right click or other mouse actuation of the display on the display surface each respective application processor will respond appropriately by processing the hand motion and if appropriate reflecting the hand motion in the display portion controlled by the respective application processor.

As noted the output devices in this embodiment include one or more display devices arranged to display respective display portions of an output display of the system. To accomplish this the glass panels of the display surface and their associated projectors are tiled so that together they form a composite image. Each image produced by each projector is unique and is produced under control of the application processor associated with the display device.

In general the application processors are general purpose data processing systems customized by particular application software to perform a particular function in the system . The application processors are coupled by data links to the state server . These may be wireline or wireless data links or any suitable data communication devices. The application processors may include or have associated therewith particular hardware components for producing a particular output actuation. One exemplary actuation for a general purpose output device is opening an electrically controlled lock at a remote location. In such an application the application processor cooperates with a servo motor or other device to mechanically open the lock. In general then each application processor is configured to process an application program and is responsive to context information. Each respective application processor is in data communication with an associated respective output device for controlling the output device in providing an output portion of the system output. The system output is the aggregate of the output portions of all the output devices .

In the particular exemplary embodiment of the application processors are customized for processing respective display portions of an output display of the system . Accordingly the application processors receive data and instructions in the form of messages from the state processor and provide control signals to one or more associated projectors . shows an exemplary software architecture of an application processor. The software architecture includes core components and an application layer . The core components are aspects of the software architecture that are shared by all developers preparing software applications for the application processor . Examples include input output drivers storage and retrieval routines and other operating system components. The application layer includes functions routines and other components which are customized by each developer of software for the application processor to create a unique application behavior for the application processor.

The application layer includes a message handler and one or more objects . The message handler communicates with the state server . in particular the message handler receives messages from the state server which define changes in the system for processing by the respective application server. Further the message handler sends messages to the state server which define changes in the system produced at the respective application server and which should be reflected to other application servers . The messages are preferably formatted in accordance with a predefined data transmission protocol including for example a header with source and destination addresses and a payload including data defining a change of state in the system. However any suitable data transmission format may be substituted.

The objects are instantiated by the developer of the application layer . Objects are discrete items that can be selected and maneuvered such as an onscreen graphic. In object oriented programming objects include data and the procedures necessary to operate on that data. Operation of the application processor under control of the application layer may cause the objects to be generated and to operate. In the illustrated example the objects include their own message handlers to communicate with other portions of the application layer and the state server . Messages communicated by the objects may receive updates to the system state that affect in individual object or may contain information about a changed detected or produced by the object that should be reflected by the state server to the other application processors .

In accordance with the present invention the architecture exemplified by supports a shared state among multiple processors such as the application processors and multiple applications active on the processors such as the application layer . Operation of the state server ensures that the states of the application processors are synchronized. In other words the state server operates to ensure that all state changes are reflected to all processors as simultaneously as possible.

Synchronization in the present context means most generally that the state of each application processor is identical to the extent that the states include the same information. For application processors running local copies of the same application program the states are identical when each copy of the application has identical data. There may be variations in the respective applications. For example in the specific example described herein two or more application processors produce portions of an image on a display. Each processor is assigned to display a designated portion referred to herein as its context of the overall image. However each processor maintains the data necessary to produce the entire image not just its context. Data which are not part of the context are updated based on processing of the application and based on synchronization information received from the state server so that the local copy of the application is synchronized or contains the same data as other copies which have other contexts.

In the present application synchronization is not just synchronization in time. All synchronization involves a common framework. Synchronization relates to the local state or states of the application processors.

In a first example time synchronization involves the server sending a message to all applications specifying the current time for example because the server accessed the system clock which may be considered a time sensor. In response to this message some applications on some of the application processors might simply update an animation. Others might perform some logic to map the new time to a higher level state change e.g. to an indication that it is time for a train to arrive and use the architecture to reflect that new state to the other applications. Other applications on other processors may not have any time dependent aspects and so have no response to the message. Different applications respond to the reported state change appropriately.

In a second example a train has a global positioning system GPS receiver which forms a position sensor and an application processor of the type described herein. The train and its application processor send a message reporting the GPS position data as the new state of the train. The message is received at the state server and is reflected to several other rail yard applications. For example in a rail yard command center the information in the message effects a change in a display of train position. In a computer room with data processing equipment the message is handled with a database entry. In the rail yard another application on another processor opens a gate.

In a third example a touch screen sensor detects the location of a touch and reports the location to the state server. The server sends information about the state change to all applications in the system. One application maps that state change to a higher level state a button has been pressed. Another application maps the touch to a different higher level state there is human activity in the command center. Again each application receives the same state update message and responds appropriately.

In each of these examples time is handled in conceptually the same way as any other state change. Touch motion and any other sensor input to the system such as swiping an identification card in a card reader or using a fingerprint reader are reported to the state server and are all handled the same way appropriately for each application processor in the system. State changes are not necessarily handled homogeneously. One application might simply update a display. Another might actuate a robot.

Referring again to in the most general applications state changes are effected from a variety of sensors and or program logic and are not limited to user interactions. Any information about a change of state is passed to the state server which mirrors or re transmits the information to others in the system including the application processors . The application processors or any other output devices receive the updated state information from the state server and respond appropriately. The response might be to update data stored at the output device . The response might be to change a displayed image or program state. The response might be to provide some suitable actuation. In the most general case the response of each output device is tailored to the nature of its operation or function and the updated state information.

In the particular application illustrated in in which the output devices include projectors under control of application processors state changes are effected from the input devices or program logic operating on an application processor . In response to an input signal detected by the I O processor the state server updates its state information and mirrors or re transmits the information to others in the system including the application processors . That is each application processor is configured to dispatch an update message to the state server in response to processing an application program to produce a change in the image forming the respective display portion produced by the application processor. The state server reflects the message to other application processors . The application processors receive the updated state information from the state server and respond appropriately. In this specific application of the response of each application processor is to change the display produced on its associated display screen .

In this particular embodiment the cameras positioned adjacent the display surface operate as input detectors positioned to detect user activations occurring adjacent to one of the display devices formed by the display panels . In response to the detected user activations the input detector or camera produces user activation signals. For example the signals might be digital data defining the location in x and y coordinates where a user s hand is located at the surface of the display panels . The I O processor is in data communication with the input detectors formed by the cameras and the state server to convey information about the user activation signals. For example the I O processor may simply provide the x and y coordinate data or the data may be enhanced or formatted with other information.

Further in the particular embodiment described herein each of the application processors runs substantially the same program or identical application programs. Each application processor maintains its own copy of the state of the system updated in response to state messages received from the state server . Based on its local copy of the system state each application processor displays in predefined display portions on separate display devices an output display of the application programs. The processing of the applications is synchronized by the state server operating as a synchronizer. In this system each respective application processor controls a respective display device for displaying a respective display portion of the output display on the respective display device .

When a user interaction such as a hand near a display panel is detected at a respective application processor associated with the respective display device the display is updated on the respective display portion. The state server as synchronizer conveys information about the user interaction to each of the respective application processors each of which has all of the system data. However each of the application processors is responsible for displaying its respective display portion. Therefore an application processor updates its display portion only if the user interaction affected that display portion. For example if a user moves a hand near the left most panel of a 2 4 panel display in which each panel is controlled by its respective processor the user interaction represented by the hand movement is reported by the state server to all processors. However only the processor controlling the left most panel changes the display produced on that panel to reflect the user interaction. All other displays remain unchanged although all other processors have identical data matching. Each processor maintains its own context and updates its display only if the context has changed requiring an update. The context information defines what display portion of the overall display is produced by the processor. In the same way if a user interaction requires a change in the display to more than one display panel e.g. because a new application window is opened on the display the state of the system will reflect the changes to each respective context of each application processor that is affected. Because of the change within the context of the application processor the processor s display is updated.

In the process is implemented as computer code in this example a C code portion entitled RWExecutable.cpp. The code portion first creates an instance of an object called CRWApplication block . The code portion then calls the newly created object block . As is indicated by the line in control then transfers to a code portion which is the instantiation of CRWApplication. After processing of the code portion the instance of CRWApplication is destroyed block by code portion .

Code portion is an instantiation of the C object CRWApplication. This object handles setting up the graphics environment for applications in the system of . This object also initializes and maintains a list of all CRWController defined in the application as will be described in further detail below. Contents include CRWScreenAlignmentObject in case the application needs to be displayed through a multi projector system.

The code portion or application implements the flow diagram illustrated in . At block the application reads any files containing machine specific settings. Examples of such settings include defining which slice of the respective display portion this machine is responsible for processing. More broadly the machine specific settings define what output device is to be controlled by the machine. The output device may be a display device an actuator or any other appropriate device.

At block the application sets up the DirectX environment for any required graphics video networking or audio capabilities. DirectX is a library for accessing several low level functions such as networking graphics input audio and video. The system of the illustrated embodiment uses the graphics library most extensively but the networking library is used for communication.

At block the application creates and initializes one or more CRWController objects. A controller object is a list of objects and the logic to send and receive messages from them. At block the application creates an object called CRWScreenAligner. This is an object that allows for use of a multi projector tiling system in which the display of the application is spread seamlessly across a number of projected areas. By distorting the display output that is actually sent to the projector this object corrects for the distortion of the projector minimizing the seams between projection areas. The exact character of the correction is entirely user configurable through this object. At block the application initializes a networking object. This object allows the application processor to communicate over a network with other devices including the state server.

At block the application checks for quit messages or other data indicating the application should be terminated. If no such a message is received at block any pending message is transmitted from the application processor to the state server. At block an object called Render is called to render an image on the display. Control remains in a loop including blocks until a quit message is received. In response to a quit message control proceeds to block where the application sends terminating messages to the state server to discontinue communication with the state server. At block the application cleans up by destroying all instances of CRWController objects which have been created by the application. At block all DirectX resources are released and the application terminates.

Block instantiates an interface called CRWBaseController. CRWBaseController is an interface that defines the basic properties of a Controller object. Controllers contain a list of objects derived from CRWBaseObject and the logic to send and receive messages from them. CRWBaseObject is an interface that defines the properties of a general object such as a piece of text a CRWSimpleTextObject a picture a CRWTextured2DObject or a webcam feed a CRWWebCamObject . Objects are created and maintained by Controllers. Objects have the ability to initialize to draw their own contents every frame and they can also respond to messages from the Controller to write an application in the system. The developer must write a class that inherits from and implements the CRWBaseController interface. The developer fills the Controller with objects and by specifying logic of the objects they define what the contents and the behavior of the application will be. Specifically the CRWBaseController interface allows the application processor to learn the extents of the sub region or respective display portion of the entire output display or application geometry and set up extents data to store this information block . These extents are also referred herein as the context for this application processor. Also block the CRWBaseController interface creates a graphics texture on which to draw controller contents.

Block creates and initializes all objects for the application. In response to block an object is initialized. At block an object CRWBaseObject is called. As noted above CRWBaseObject defines the functionality of a general object for instance the general idea that objects are rendered . Pictures videos etc. CRWVideoObject and others as described herein inherit from and extend the base object for instance defining how that particular type of object is rendered . Thus CRWBaseObject defines the basic set of commands all objects understand Initialize Render etc. and each object type defines how objects respond to those commands. This allows the controller to send commands to objects without needing to understand what they are exactly. The controller operates in terms of CRWBaseObjects but each specialized object responds with specialized behavior.

The object is created and initialized at block . However in block if the application developer has indicated that this object will never be shown in the sub region or respective display portion for which the copy is responsible creation of the object is suppressed.

Referring again to block following initialization of the CRWBaseObject at block at block any resource specific to the current object are initialized. A first example is a CRWTextured2DObject which is a rectangle in two dimensional space defined by two corner points. The rectangle can have a simple color or it can hold an image a texture . Available options include setting the color value of the object rotating it resizing it setting its position changing its transparency. Other similar examples include the following 

CRWLineObject which is a line in two dimensional space defined by two endpoints. The thickness and type of the line can be set in addition to all the properties and settings that apply to CRWTextured2DObjects.

CRWMagLine which is a polygon in two dimensional space defined by four corner points. This allows more flexible geometry than a CRWTextured2DObject and is used mostly for visually connecting zoomed areas to unzoomed areas.

CRWWebCamObject which is a rectangle in two dimensional space defined by two corner points. The object displays a continuous feed of images fetched from a server or webcam.

CRWVideoTexture which is an object that handles taking video from a file and writing it to a texture and the associated operations on that file such as decompression.

RWVideoObject which is similar to a CRWTextured2DObject but holds a video clip see CRWVideoTexture instead of a still image.

CRWCompoundObject which is an object that contains and manages a collection of objects and draws the collection to a 2D texture which can then be drawn onscreen. Compounded in this way the objects in the collection can all share a coordinate system separated from the global coordinate system. Changing the location scale rotation color etc. of the compound object causes a proportionate change for all sub objects.

CRWSimpleTextObject which is a chunk of text. Settings include size and shape of bounding rectangle font height and width color and transparency and some formatting options e.g. left and right justification .

CRWSpriteScatter which is a collection of points in space like pushpins on a map. The scale of the collection can be set by the user to any value. Each point in the collection can have a certain size and optionally hold an image. All the points have the same image in this context the images are called sprites . Positions of the points can be read from a bitmap image allowing the developer to position them visually.

CRWVideoSpriteScatter which is similar to the CRWSpriteScatter but each point can hold a video image instead of a still image.

CRWStencilController. Stencils are used to define where objects are and aren t drawn. CRWStencilController is an object that defines a stencil and the various objects which are subject to that stencil. The object contains two lists objects to be used as stencils and drawn objects.

CRW3DObject which is analogous to a CRWTextured2DObject but its vertices occur in three dimensional space and there can be many of them. Object knows how to load vertex lists from a file and do three dimensional lighting and transformations.

RW3DRenderObject which is an object that contains and manages a collection of three dimensional Objects in a scene and draws that scene to a two dimensional texture which can then be drawn onscreen.

RW3DBillBoardObject which is an object that creates a two dimensional object in a three dimensional world and dynamically updates its orientation so that it always faces the viewer.

CRWDataBundle an object for holding data in row and column format. Data can be inserted functionally by the user or can be loaded from a properly formatted text file. Data contents can also be written to a formatted text file. CRWDataBundles can be used to provide a set of data for display by an implementer of the CRWChart interface such as CRWPieChart .

CRWChart an abstract class or interface that defines the properties and methods which all chart objects have in common including their data underpinnings.

CRWPieChart a pie chart object implementing the CRWChart interface. Data contents are defined with a CRWDataBundle.

CRWLineChart a line chart object implementing the CRWChart interface. Data contents are defined with a CRWDataBundle.

CRWBarChart a bar chart object implementing the CRWChart interface. Data contents are defined with a CRWDataBundle.

CRWScatterPlotChart a scatter plot chart object implementing the CRWChart interface. Data contents are defined with a CRWDataBundle.

CRWToggle an object that acts as a button when placed in the context of the touchscreen messaging system. The object listens for touch messages within a coordinate range which corresponds to its boundaries when triggered it raises some preset message which is passed to the server and rebroadcast to all machines on the network alerting everyone to the state change. The object knows its own state and also controls how often it can be triggered.

CRWWhiteboardObject an object that holds a collection of lines or dots arranged on a two dimensional surface. These lines or dots represent marks on a whiteboard. Marks can be added to or subtracted from the whiteboard by the user of object.

CRWValueInterpolatorObject an object which acts like a stopwatch. It is initialized with a start time a duration and a starting and stopping value. Over that duration of time it takes the start value and changes it into the stopping value i.e. it interpolates the value . Objects and Controllers use this object to help them do animation smoothly.

CRWBezierInterpolatorObject which is similar to CRWValueInterpolatorObject but uses a specialized Bezier function to interpolate.

At block a routine Render was called for all CRWControllers. In this case control proceeds to the developer s code at block . There the applications controller updates any Controller data needed to correctly draw the frame block and instantiates a CRWBaseController object to render the frame block . The CRWBaseController instantiation iterates through all objects in the controller and calls a Rendero method for each object block . This causes all objects to be drawn.

At block messages were transmitted between the state server and the Controller. In this case control proceeds to the developer s code . The code processes a DistributeCommandMessage or a DistributeKeyMessage. At block the CRWBaseController for the message type is called. This object iterates through all objects in the Controller and calls the appropriate object to handle the message for each object block . Thus the object at block calls a corresponding CRWBaseObject method block and processes specific responses associated with some high level objects block .

After the developer s code processes block at block all system messages are processed to create the details of the behavior of the system.

From the foregoing it can be seen that the presently disclosed embodiments provide system and method for distributed information processing and interaction. In one general embodiment application processors respond to one or more software applications to produce outputs the respective outputs together forming a system output. Each change in state of an application processor is reported in a state change message to a state server which reflects the message to all other application processors in the system. Also any system inputs are reported to the state server which again reflects the state change to all application processors. In this way the application processors are maintained in synchronous states. Output changes in one processor that should be reflected in the outputs of other processors are done so automatically and seamlessly.

Each application processor may drive a display device which produces one image tile of a tiled output display. An application processor may be any data processing device including as examples desktop or laptop computer systems PDAs or any other processing device. Furthermore an image tile may extend over multiple displays. Each processor uses the state change messages from the state server to update state information for the entire display even though the processor is responsible for displaying only a designated portion of the display. The assigned designated portion of the display is the context of the processor. An input device detects user interaction with the display similar to mouse movements and mouse clicks but in one embodiment hand motions in front of the display. Detected user interactions are reported to the state server and information about the user interactions is reflected from the state server to the application processors which produce the image. The image is updated based on the information from the state server.

The repository includes a historical action database a historical state database and a historical environment database . A data processor executes a database management program which stores data in the databases after receipt at the communication interface . More specifically the historical action database may store action records . Each action record may include application action data an operator identifier and an application data timestamp . More fewer or different types of data may be present in the action record however. The application action data represents an action taken on an application at a given action time. The operator identifier provides an identifier e.g. an alphanumeric string of the operator responsible for the action. If multiple operators are jointly responsible for an action the action record may include an operator identifier for each responsible operator. The application data timestamp identifies the time at which the operator took the action on the application the application time .

The application action data may vary widely in type form and content to represent any action taken on or with respect to an application. Examples of actions for a materials shipment routing application include assigning a material to a transportation vehicle e.g. a train truck or plane changing the delivery schedule of materials changing the availability of a transportation vehicle changing the factories or plants at which material is needed changing the material needed at the factory changing the time or date the material is needed adding a new factory product material supplier material transportation vehicle or transportation route changing the maximum minimum or expected speed of a delivery vehicle changing an expense of one or more types of transportation material or product setting or changing the products manufactured by a factory or the volume manufactured by a factory annotating a display shown on the display surface and changing labels tags or other descriptors for the factories stores transportation vehicles or materials.

Other types of application actions may also be monitored tracked and recorded. As examples such actions may include selecting a part of a map on the display surface zooming in or out to view additional detail on the map causing a report to be displayed on the display surface or scrolling through information on the display surface . Additional examples include moving a pointer e.g. a mouse pointer clicking a button entering a keyboard keystroke touching the display surface or dragging a pointer e.g. a finger on the display surface .

In general the operator acts which qualify as application actions may be pre configured selected at run time or changed while the application executes. For example action preference settings in a memory in the system may identify those operator acts which are considered application acts. Then as the operator acts the system may determine whether the act qualifies as an application action to be stored in the historical action database .

Thus when operator action data arrives the processor may compare the operator action data e.g. including an action identifier with the preference settings to determine whether the database management program should save the operator action as an application action in the historical action database . In other implementations the operator actions may be filtered prior to transmission to the application history repository . In that case the application history repository may save each operator action in the historical action database or may perform additional filtering using local action preference settings in the memory .

In addition to the storing the actions taken with respect to the application the application history repository also stores application states. In particular the historical state database stores state records . The state records include application state data which represents an application state at a given state time and an application state timestamp which represents the state time i.e. the time at which the application state was captured as represented by the application state data .

The current application state may be reflected in the variables which specify the current characteristics or attributes of any part of the application. Thus the application state data may represent the application state as an application snapshot in time including as examples the number type position speed and current cargo of transportation vehicles the number type location and products output by factories current delivery schedules or expectations for one or more products the quantity type and cost of materials currently needed at a factory current projections of material shortfalls or excesses and the workers currently assigned to each factory or transportation vehicle. The application state may also include tabular data such as amounts costs and credits relating to the entities shown or manipulated in the application.

Application state may also include data received by the system e.g. over a network . One example of such network data includes web page data displayed with or integrated into the application. Other examples are symbols e.g. map symbols annotations or emendations made to or added to the application shown on the display surface . Another example is a real time cloud layer received from an external weather reporting system and overlaid on a map incorporated into the application on the display surface .

The discussion below relates to environment data which reflects the context in which the application exists. The distinction between environment data and application state is a flexible one. Additional or different distinctions or dividing lines between distinctions may be determined and implemented in the application history repository . Thus for example a door lock sensor may provide environment data relating to the individuals in the room with the display surface . On the other hand an application which displays a door lock interface which may be manipulated by the operator may have an application state which includes the operation and status of the door lock and the doors it controls. In one implementation a distinction between environment data and application state may be made based on the core functionality of the application. Environment data may then reflect what is happening around the application while the application state may reflect what the application is actually doing processing or displaying.

The historical environment database stores contextual data. In particular the historical environment database stores environment records . The environment records may include environment data and an application environment timestamp . The application environment timestamp represents the time and or date at which the environment data was captured.

The environment data provides the context in which application actions occur and in which an application state exists. The context may be local or remote. Examples of environment data include audio capture or video capture of the environment in which the application is developed studied used or modified. The environment data may be gathered from multiple locations e.g. in a distributed development environment which may be nearby e.g. in different rooms of the same building or geographically dispersed e.g. in an office in Los Angeles Chicago and New York . Additional examples of local environment data include the number identity and characteristics of people in the area e.g. in a planning room the length of time one or more people have been working on the application the room temperature noise levels weather conditions and other environment data. Other environment data may arise from context provided by biosensors which read and report operator biodata such as heart rate blood pressure brain wave activity weight height temperature or any other biodata.

The environment data may also include information from other information sources which are not necessarily directly involved with the application itself. This additional environment data may be timestamped for storage and later retrieval and playback. The other information sources may represent web sites free or subscription electronic databases news groups electronic news feeds journal article databases manual data entry services or other information sources.

Examples of other information sources as shown in include government publication information sources online price information sources financial report information sources and local and national news information sources . The information sources may also include one or more blog online discussion or USENET information sources analyst report information sources product review information sources and trade press information sources . The information sources are examples only and the repository may receive environment data from any other information source.

Generally these sources include local and national new stories available on the Internet or privately paid subscriptions services such as Lexis Nexis. Company financial reports may also be available on the corporate websites or on government reporting websites. Pricing information of products is frequently available online and the sources of information may be monitored for changes and introduction of new information. Additional sources of information include government publications product reviews analyst reports and the trade press as well as less credible sources of information such as web blogs and online community discussion bulletin boards or chat rooms.

Returning to the communication interface receives the historical data relating to the application displayed on the display surface through the communication network . As examples the communication interface may receive operator action data application state data and environment data . The environment data may be provided by the sources of local and remote context as described above. The operator action data may be provided by the system which detects and responds to operator interaction with the display surface on which the application is displayed. For example the operator actions may result from operator input through the input devices or from other input devices such as speech recognition systems in communication with the system .

The application state data may also be provided by the system . In particular the system may at any given time provide a snapshot of the current application state and deliver the state data to the repository . The application state data may represent the variables which specify the current characteristics or attributes of the whole application or any part of the application. For example in response to an operator request to save the application state the system may serialize each application object to provide a binary representation of each object. The binary representation from which the application objects may be recreated may then be provided as application state data to the repository .

As noted above the repository may tag each application action represented by an action record with an operator identifier . As a result the history of the application is tracked not only in the global sense but also with respect to potentially many individual actors. In other words the operator identifiers allow the repository to distinguish between multiple operators and identify which operator was responsible for any given application action. For complex applications on which large groups of people work the operator identification can be a very beneficial additional piece of information to have when reviewing the application history. The operator identification data may be included with the operator action data .

The strategic planning room incorporates identification technologies which offer progressively more information about operator identity and their application actions which influence the application state. One way to distinguish the identification technologies is to consider them part of categories such as a presence category a usage category and an input category. The identification technologies may be categorized in other manners however.

With regard to the presence category the operator identification information conveys that a given operator was present during an application action and the time that they were present. As one example a key card door lock or other door or lock sensor may provide the identity of an individual in the strategic planning room and the time when the card was swiped to enter the room . Operator identification updates may be issued whenever the key card door lock detects a key swipe.

As another example an operator may wear a badge or other indicia which may be sensed. The badges transmit an ID signal to one or more detectors placed around the room . In one implementation the badge is part of an infrared or RFID badge system. The detectors may be positioned on the ceiling or walls of the room . The badge emits a unique operator identifier in the infrared or RF spectrum. The detectors may report to the information processing system or the application history repository as operator identification data the detected operator identifier and any additional information available such as which part of the room the operator is in e.g. based on a triangulation or other location technical employed by multiple detectors .

The identification technologies in the usage category may provide information about operator identity for example when it is known that the operator was present and when there is a greater degree of certainty about when. Generally more evidence is available that the operator actually generated an application action. As one example a localized tagging system may be employed.

The localized tagging system may include electronic badges and sensors . The badges and sensors operate at power levels defining a localized area for example 0 48 inches 0 36 inches 0 24 inches or 0 12 inches from the display surface . The tagging system may operate in a wide band spectrum to support high bandwidth communication of operator identification data to the repository with greater accuracy resolution and frequency than more coarse sensor systems. Accordingly the tagging system may detect not only whether an operator is in the planning room but which operators are near the display surface itself. Even when it is not possible to determine precisely which operator was responsible for an application action the sensors may provide a list of operator identifiers who may have taken the application action and who may have witnessed supported or contributed to the application action.

The identification technologies in the input category may provide information about the operator identity the time of the application action taken by the operator and the particular application action taken by the operator. These identification technologies are better suited to specifically tying an application action to a specific operator. Examples are given below.

As one example the operator may carry an identifying input device such as a stylus which the system may distinguish from other styluses or other input devices. The stylus of each operator may be distinguished by color shape size or other characteristic. As a result the cameras may capture characteristics of the stylus e.g. the stylus shape or color which permits a correspondence to be determined between the operator known to use the stylus and the application action. The identifying characteristics of the stylus may then be correlated with an operator identity and may be communicated to the repository . Alternatively or additionally the stylus may include an RFID tag or other electronic identifying mechanism which communicates an operator identifier to the system or to a sensor connected to the system.

As another example the information processing system may receive application actions from other input mechanisms such as a laptop PDA desktop computer cellular phone. These input mechanisms may incorporate an authentication system e.g. a username password or public key private key system . When the laptop for example connects to the information processing system e.g. through a WiFi Ethernet or other network connection the information processing system may authenticate and identify the operator of the laptop or the laptop itself. As a result any application action taken or submitted through the laptop may be identified as caused by the operator associated with the laptop or identified as originating from the laptop itself .

As shown in the repository may communicate with the information processing system . In response to an operator request to navigate through the history the application the information processing system may query the databases in the repository for historical action data application state data and environment data. Alternatively the request for application navigation may come from another source e.g. directly from the operator with the repository responding to the request and providing the historical data to the information processing system .

In one implementation the information processing system generates a temporal selection interface element. The operator may use the temporal selection interface element to navigate through application history. shows an example of the display surface on which the information processing system generates a temporal selection interface element .

In this case the temporal selection interface element is a scrollbar rendered on the display surface . In other implementations the temporal selection interface may include a voice recognition system menu selection mouse wheel or any other input mechanism. The temporal selection interface allows the operator to provide one or more temporal selections i.e. a time and or date at which to view application history.

The temporal selection interface element adheres to a usage semantic . As shown in the usage semantic establishes that the center of the scrollbar represents the present state of the application that scrolling to the left navigates back in time and that scrolling to the right navigates the application forward in time. In response to a temporal selection e.g. one week ago one hour from now or now obtained from the temporal selection interface element the information processing system generates an application representation on the display surface .

The application representation is a rendering of the application at the time selected by the temporal interface element . To create the application representation the information processing system may initiate queries of the databases in the repository . The queries may specify the temporal selection resulting in retrieved application state data application action data and environment data.

When the application state data includes a snapshot of the application state at the time specified by the temporal selection or within a pre selected operator or system selected time threshold the information processing system may render the application representation from the retrieved application state data. When the application state data does not include a snapshot of the application at the specified time the information processing system may start with application state data close in time to the specified time. The information processing system may then apply the subsequent operator actions specified by the retrieved application action data to arrive at the application state at the specified time. Alternatively the information processing system may start at the beginning of the application and re apply each application action until the specified time to reach the application state to render on the display surface .

In addition the information processing system may playback the operator actions and playback or convey the environment that existed during each operator action and or application state. Thus for example as the information processing system applies an operator action the information processing system may display the application action. As examples the information processing system may show a mouse cursor move to a selection menu and pick an option or may print a text message which describes the application action and its parameters. In addition the information processing system may output the retrieved environment data. For instance the information processing system may output audio or video data capture from the planning room contemporaneous with the application actions optionally supplemented by contemporaneous news stories or other context information obtained from the information sources shown in .

Referring again to that Figure shows a navigation example in which the operator has provided a temporal selection which specifies a time near the beginning of the application development. As a result the application representation shows the application state near the beginning of the application e.g. three component inventory levels with connections representing dependencies on other components . As shown in the navigation example the component inventory levels and represent full stock for two components while the inventory level shows that there is no inventory yet for the third component.

As another example the information processing system may predict transportation vehicle locations by extrapolating their current speed consulting established travel schedules or using other factors. As yet another example the information processing system may predict the number of factories which will be in existence depending on historical manufacturing growth rates. The application representation shows the predicted future state of the application at the time represented by the temporal selection in response to the predictive modeling growth forecasting extrapolation interpolation or other predictive techniques.

Alternatively the information processing system may communicate the present state of the application to a separate predictive modeling system along with the future temporal selection . The predictive modeling system may then apply any predictive modeling growth forecasting extrapolation or interpolation techniques to the present state. The resulting predicted future application state to the information processing system for rendering on the display surface or for storage as a future application state in the application history repository .

In an operator navigation interface system receives navigation requests and temporal selections and coordinates display of the application state with the information processing system . In other implementations the hardware and software shown in may be incorporated into the state server or may be implemented on other processing systems in communication with the information processing system and or application history repository .

The navigation interface system includes a memory a processor and a display . The navigation interface also includes a communication interface which communicates over the network with the application history repository and the information processing system . The navigation interface system receives a temporal selection from an application operator from the information processing system or from another source.

More specifically the interface input program running in the memory obtains the temporal selection . The interface input program may be a device driver for example which receives mouse keyboard or touchscreen input. As other examples the interface input program may be a voice recognition system or a message receiver which receives operator input data from the information processing system . In the latter case the interface input program may receive messages from the information processing system that indicate that the operator has made a temporal selection using the temporal selection interface on the display surface .

In response to the temporal selection the temporal index program initiates searches of the databases in the application history repository . The temporal index program specifies the temporal selection for the searches. The searches may include one or more of any combination of an action search of the historical action database which results in retrieved application action data a state search of the historical state database which results in retrieved application state data and an environment search of the historical environment database which results in retrieved environment data .

The output program provides the retrieved data to the information processing system . The information processing system may then render application representations on the display surface based on the temporal selection as described above. It is not necessary that the display surface be used to display the application history. Instead any other display such as the display may display all or part of the application history including the application state application actions and environment data. The output program may also assist the information processing system to convey the application history. For example the output program may include audio and or video coder decoders to reproduce audio and video context data which is contemporaneous with any application action or application state.

The temporal index program initiates an action search of the historical action database Act . Retrieved application action data results. The temporal index program also initiates a state search of the historical state database Act . Retrieved application state data results. In addition the temporal index program initiates an environment search of the historical environment database to obtain retrieved environment data Act .

The output program communicates the retrieved search data to the information processing system Act . The information processing system may then recreate and render the application state on the display surface . However as noted above the output program or other logic in the navigation interface system may output all or part of the application history including the application state application actions and environment data.

The action preference settings may be established based on individual or combined operator preferences or profiles. Furthermore other parameters in the information processing system application history repository application navigation system or navigation interface system may be established or modified depending on the particular operator or operators working with the application. For example each operator or group of operators may specify which actions will be considered application actions for that operator or group of operators . To that end the memory may include multiple sets of preference data established on the basis of individual operators or groups of operators. Other examples of parameters which may vary depending on operator profile include the amount type and time span of environment data played back for each application action the rate at which historical application actions are displayed and or applied to reach a specified prior or future application state and the threshold for determining when an application state record matches a specified temporal selection e.g. an application state record with a timestamp within 12 hours of the specified temporal selection will be considered a match .

Operator profiles may be considered a specific example of user profiles. A user profile may establish system configuration parameters for an individual in a management role supervisory role operator role or any other role. The systems and or may then provide historical application navigation responsive to the user profile. For example an individual in a management role may request application navigation. The systems and or may then filter historical application data according to the profile established for the management individual.

The action preferences may specify the application actions e.g. as noted above with regard to the preference settings which will be recorded for a particular type of user e.g. an operator or a management individual responsible for the application action. The playback preferences may specify which application actions and or environment data are to be provided when a particular type of user initiates an application navigation. Additional preferences may be established to govern the behavior of any other aspect of the systems and or with respect to any particular type of user. As an additional example the preferences may specify the level of historical detail to provide based on the time time or review or other context of a requested application navigation. As examples the preferences may specify a very detailed level of review in which each application action and all available environment data are played back a summary review which in which only application actions are played back or may specify any other level of review.

The application navigation technology may be used in conjunction with techniques for determining positions of multiple targets on a planar surface as described below in conjunction with a multi target detector system. These techniques may be used to distinguish and identify inputs e.g. touch pen or other inputs on the display surface to accurately detect operator inputs. The navigation system may then respond to the inputs by navigating the operator through application state and interaction history as described above. Alternatively or additionally the navigation system may use the multi target detector system discussed below to distinguish and identify inputs to the display surface in order to help resolve and specify application actions taken by an operator.

There are a variety of applications for a multi target detector system. For instance in a command center where a large interactive map or other data is displaying real time information multiple users may want to work with the map at the same time selecting objects on its surface. Prior camera based target systems limited to detecting single targets such as a single touch pen or the like required each user to take turns touching the screen in order to properly detect the user s single touch. With additional touch inputs prior systems may have undefined or undesirable effects. By contrast the system described below is not so limited. A single user or multiple users may touch the screen multiple times and the location of the touches may be detected properly even with a camera based touch system.

Consumer applications for the multi target detector system include for example a public kiosk in a movie theater lobby that displays information about currently showing movies in a tabular format some yards wide a sort of digital billboard. Multiple independent users may walk up and browse this information simultaneously. The multi target detector system allows the touchable computed area to be treated as an active surface available to all users a surface not requiring users to acquire an input channel such as a mouse or a pointing tool in order to use it. The multi target detector system thereby supports a new class of valuable multi user applications.

The multi target detector system may further include one or more visual sensors . The visual sensors may comprise cameras such as CCD based cameras or other image acquisition devices. The CCD based cameras may capture electromagnetic information such as from the visible and or infrared spectra. An example of a CCD camera is Lumenera Lu275. The image captured by the CCD based cameras may be single images in any suitable format including digital formats such as jpeg referring to a standard data format advanced by the Joint Photographic Experts Group or gif Graphics Interchange Format or other or the video images may be a motion picture image such as a data file encoded according to a standard such as that proposed by the Motion Picture Experts Group MPEG or one of its variants or other standard. Other visual sensors may include infrared sensors which may be any sensing device that responds to heat energy or radio emissions from a device to produce an image.

One example of a configuration of the multi target detector system comprises two or more cameras that look along the viewing surface with the cameras fields of view parallel to that surface . An exemplary camera based system is disclosed in U.S. Patent Application No. 2005 0077452A1 U.S. application Ser. No. 10 312 983 published on Apr. 14 2005 which is incorporated by reference. A section of black tape may be placed along the bottom and sides of the surface so that without any touch present the portion of the camera that senses the image near the surface is black. In effect the camera may see only a black strip when no touch is present. When a target such as a user s finger or elbow a pen or the like is proximate to or touches the surface the finger viewed against the black tape registers a change or an occlusion in the field of view of the camera which may be subsequently processed. For example the occlusion may comprise a bright spot within the field of view within the camera. Thus the cameras may acquire images of the multiple simultaneous targets on the planar surface .

The information from the cameras may be sent to the multi target state detector . The multi target state detector therefore is configured to receive signals from the visual sensors and provide any necessary signals in return. For example for receiving a jpeg format file from a camera the multi target state detector receives a jpeg formatted file from the camera and stores the file providing appropriate network handshaking signals to the camera. In another example the format maybe streamed data from another camera. In either case the format may comprise data indicative of the images of the multiple simultaneous targets on the surface .

As discussed more fully below the multi target state detector may sense the state of a single target on the surface or the states of multiple targets on the surface . As discussed above a target may comprise a finger a pen or other object that is proximate to or touches the planar surface . The state of the target may comprise any characteristic of the target such as position of the target size of the target or movement of the target e.g. general movement up down left right or speed .

The states of the targets may be sent to an application device for processing. The application device may include an input interface that receives the determined states of the targets from the multi target state detector . Further application program may use the states of the targets as inputs for control or historical navigation of the program. The application program may then send its output to a display program . For example the display program may display onto the surface an image or other rendering. The image displayed may include a single image or a series of image tiles of a tiled output display as discussed above. Thus the multi target detector system may detect position and movement of a user s hand and in conjunction with the application device indicate the position and motion of the user s hand. For example if the hand motion should be interpreted as a left click right click or other mouse actuation of the display on the surface the application device will respond appropriately by processing the hand motion and if appropriate reflecting the hand motion in the display portion controlled.

Multi target state detector may receive input from visual sensor s via a visual sensor interface . As discussed in more detail below the input from the visual sensor interface may comprise data such as that shown in . Multi target state detector may further include a hard disk drive for data storage and an external disk drive for reading from or writing to a removable external disk . The removable disk may be a magnetic disk for a magnetic disk driver or an optical disk such as a CD ROM for an optical disk drive. The hard disk drive and external disk drive may be connected to the system bus by a hard disk drive interface and an external disk drive interface respectively. The drives and their associated computer readable media provide nonvolatile storage of computer readable instructions data structures program modules and other data for the multi target state detector . Other types of computer readable media which can store data that is accessible by a computer such as magnetic cassettes flash memory cards random access memories read only memories and the like may also be used. Though depicts RAM as storing the state detector programs any one of any combination of or all of the hard disk external disk ROM or RAM may store the state detector programs .

The system may track multiple targets at the same time. Initially there may be no targets registered in the system. When new targets arrive the targets may be added one at a time to a list of known active targets and tracked independently of other pre existing targets. The multi target detector system may notify either constantly or periodically any connected client software such as application device of the state of all tracked targets. The application device may comprise any program adapted to accept such state information such as for example a command center or movie kiosk application program. Given the state information developers of application programs are allowed to provide software behaviors that reflect these multiple input streams such as moving two objects independently on different parts of the screen based on independent user inputs.

In one aspect of the invention the multi target detector system first determines potential positions. The potential positions may include both positions corresponding to real targets e.g. targets corresponding to an actual touch and positions corresponding to ghost targets e.g. targets not corresponding to actual targets . The potential positions are then analyzed to determine which are positions corresponding to real targets and which are positions corresponding to ghost targets.

Using the occlusion from Camera A a ray may be calculated that is indicative of passing through a potential target. For example one of the occlusions as shown in is . Using a look up table the occlusion may be correlated to ray shown in . For example the output of CCD camera shown in may include a set of pixels such as 1 to 1 600 corresponding to the image captured. If pixel number in a CCD camera is above a predetermined brightness indicating an occlusion the look up table may be accessed to determine the ray correlated to pixel number . The ray may be represented in a variety of ways such as using two points along the ray x1 y1 x2 y2 or a point along the ray and a slope x y m .

The look up table may be generated at any time during operation such as during an initialization process. For example when the visual sensor is a CCD camera the pixels generated by the camera may be correlated to a particular ray. As shown in the narrow band of data transmitted may correspond to a discrete number of pixels such as 1600 pixels. Each of the pixels or groups of pixels may be assigned to a ray. In order to compile the look up table during initialization various pre determined points on the planar surface may be touched such as by pasting a white object to a point on the planar surface . The corresponding pixels that light up may then be recorded and assigned to the pre determined points on the planar surface . Using this data the rays corresponding to each pixel or a group of pixels may be determined. If the cameras shift relative to the screen the system may need to be re initialized or calibrated in the manner as noted above.

An occlusion may be selected in the secondary camera such as Camera B for analysis Act . For example occlusion may be selected. For the secondary camera using the occlusion from Camera B a ray may be calculated that is indicative of passing through a potential touch Act . For occlusion ray passes through touch . The intersection of the two rays primary ray and second ray is calculated Act . The intersection may be calculated by mathematically determining for the two rays whether there is an intersection point. The intersection point is considered the potential position. Subsequent analysis determines whether the potential position is a position for a real target or a position for a ghost target. In the case of the subsequent analysis will determine that the potential position is a position for a real target. In the case of two real targets there are four potential positions generated two positions corresponding to real targets and two positions corresponding to ghost targets. As shown in the two real targets are and . The two ghost targets are shown in as and . The system may also determine the positions for more than two real targets such as three four five or six real targets.

There are several ways to determine whether a potential position is a position for real or ghost target as shown in which continues the flow diagram started in . One way is to determine whether the potential position is within the boundary of the planar surface Act . If the intersection point is within the bounds the intersection may be a position corresponding to a real target. Therefore the intersection point is added to a global list of all possible targets Act . The global list of targets is subject to further analysis as discussed below with reference to and . In the intersection point of rays and occurs at target which is within the bounds of the planar surface . Therefore the potential position is added to the global list of all potential targets.

The multi target state detector then determines whether there are any other bright spots in the field of view of the secondary camera Act . In for example there is one more occlusion . Therefore the next spot is selected Act . shows a schematic of the determination of the rays and the intersection point for occlusion for Camera A and occlusion for Camera B. As discussed above ray is the ray that corresponds to occlusion . Ray is the ray that corresponds to occlusion . shows the intersection point of the two rays which may be mathematically calculated as discussed above. The intersection point designated as in is outside the bounds of the planar surface . Therefore in accordance with Act the intersection point is deemed not to be a position corresponding to a real target and will not be added to the global list for further analysis.

The output from the primary camera is then analyzed to determine if there are any other occlusions Act . If so the next occlusion is selected Act and the process is repeated. For example shows the determination of the rays for a second occlusion from Camera A. The second occlusion corresponds to ray . As discussed above occlusion corresponds to ray . The two rays result in an intersection point . Intersection point is within the bounds of the planar surface and is therefore added to the global list of possible targets according to Act . However the intersection point corresponds to a ghost target because it does not correspond to the presence of a physical target at that intersection point. Subsequent processing discussed in further analyzes whether the potential positions correspond to a real target or a ghost target.

After all of the occlusions of Camera B are analyzed other cameras may be analyzed. For example if a third camera such as Camera C is used the methodology may iterate through similar analysis with Camera A selected as the primary camera and Camera C selected as the secondary camera. After all of the secondary cameras are analyzed it is determined whether any camera has not been selected as the primary camera Act . For example if Camera B has not been selected as the primary camera it is selected Act and the process is repeated. In this manner all of the potential positions may be determined.

The list of potential positions may be analyzed to determine which are positions corresponding to real targets and which are positions corresponding to ghost targets. As discussed above the analysis may be based on a variety of factors. For example the determination as to whether a potential position corresponds to a real target may be based on the history including the position and or speed history of a previous target. In one aspect the history may be compiled one target at a time whereby only one additional target may be added to the history in a current frame of processing. In this manner the system first sees no targets on the planar surface then one target on the surface and then multiple targets on the surface . Using this sequence when more than one target is seen on the surface the history which includes at least one target may be used to detect the multiple targets currently on the planar surface as discussed in more detail below.

An expected position may be derived in the current frame for the selected ancestor target Act . The expected position may be determined in a variety of ways such as based on the position of the ancestor target and instantaneous velocity of the ancestor target in the previous frame. A distance may be determined between the ancestor target s position in the previous frame and each of the potential positions corresponding to the potential targets Act . Potential targets may be discarded whose distance from the ancestor target s position in the previous frame is greater than a predetermined threshold Act . The predetermined threshold may be a function of several variables such as the framerate of the camera the resolution of the screen and or assumptions of how fast the target will move. For example if the framerate of the camera is higher the threshold may be lower. Moreover if the CCD camera provides pixel information the threshold may be a function of the real physical resolution of the screen. For instance if one assumes that a person would move their finger no more than 1 inch per frame i.e. approximately 1 100th of a second and the screen has a resolution of 50 pixels inch then the threshold would be approximately 50 pixels.

For the non discarded potential targets determine the distance between the ancestor target s expected position according to Act and each non discarded possible target Act . The potential target whose position is closest to the expected position is designated as a child of the ancestor target Act . This completes and extends the ancestor child relationship which may extend over many camera frames. This process is repeated for any other ancestor targets Act with the next ancestor target selected Act . Act is shown in which continues the flow diagram started in . 

Further rays formed between the child target and its associated ancestor target may be calculated Act . For example a child target has associated with it a set of coordinates for its potential position such as xC and yC. Similarly an ancestor target has associated with it a set of coordinates for its position in a previous frame such as xA and yA. A ray may be calculated that includes both sets of coordinates xC yC xA yA . Analysis of the rays may assist in determining whether a potential target is a real target. For example if two child targets share the same rays the target whose ancestor child relation goes farther into the past the child with the longer history trumps and the other child is deleted as a probable ghost. As another example if a child target shares any rays with an orphan which may be a new intersection point that has not been successfully matched with any ancestors the child target trumps and the orphan may be deleted as a ghost.

The multi target state detector determines whether any child targets share the same rays Act . If so the child target with the longer history is kept on the list and the other child target is deleted as a ghost Act . Moreover the multi target state detector determines whether any child target shares the same ray with an orphan Act . If so the child target is kept on the list and the orphan is deleted as a probable ghost Act . The remaining child targets are thus designated as real intersections Act .

In order to determine whether a target has been removed such as a user lifting his finger from the screen it is determined whether any ancestors are not matched with a child Act . Further it is determined whether a timeout has occurred Act . If yes the ancestor target is removed from the list Act and the removed ancestor is designated as target up Act . Depending on the sampling rate the timeout may comprise a predetermined expiration period such as a specific number of milliseconds or specific number of frames. The timeout may be selected such that the target is given a chance to reappear after a very brief pause such as due to vibration or sensor error. The timeout period may be measured in a predetermined number of frames that the target is no longer registered. For example the timeout period may include 3 frames or approximately 3 100 of a second if the frames run at 100 frames per second that the target is no longer registered. As another example the timeout period may include 1 frame.

The multi target state detector may determine whether any possible targets on the list of possible targets have not been matched to an ancestor Act . Act is shown in which continues the flow diagram started in . This indicates that a new target i.e. an orphan who does not have an ancestor has been acquired. For example the user may have just put his finger on the planar surface . The unmatched possible target is moved into the list of real targets Act and designated as target down Act .

Distances are determined between each entry on the list of real targets Act . If any distance is less than a predetermined minimum Act the entry from the list of real targets whose history is shorter is removed Act . The predetermined distance acts as a measure of the same target moving from frame to frame such as the same finger moving from one frame to the next . The predetermined minimum may be selected based on the current frame rate of the CCD camera. For example if the frame rate is low e.g. 20 frames per second the amount of time a person may move his her finger in one frame is greater than a higher frame rate. Therefore the predetermined minimum which acts as part of the pruning process may be larger for cameras that are slower. For a camera that operates at 100 frames per second the predetermined minimum may be 100 pixels selected for an approximately ten foot surface of 4096 pixels .

This sequence removes potentially inadvertent targets close to a more established target. For example a user may touch a part of the screen with his finger and inadvertently touch the screen with the cuff of his shirt. To remove or prune these inadvertent targets a minimum distance between eligible targets is established so that some entries from the list may be pruned. The targets with the longer history such as those maintaining longer ancestor child relationships are preferred to younger ancestor child relationships. This may implement the heuristic that the firmest and longest lasting targets on the planar surface are the one most important to users.

While various embodiments of the invention have been described it will be apparent to those of ordinary skill in the art that many more embodiments and implementations are possible within the scope of the invention. Accordingly the invention is not to be restricted except in light of the attached claims and their equivalents.

