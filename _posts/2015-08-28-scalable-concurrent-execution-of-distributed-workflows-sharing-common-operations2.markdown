---

title: Scalable concurrent execution of distributed workflows sharing common operations
abstract: Examples perform concurrent execution of distributed workflows sharing common operations by a plurality of nodes, such as execution of recovery plans for disaster recovery of virtual machines operating on and off premises. Concurrent execution of identical operations that were part of a previously executed workflow are prevented, by evaluating the source of the workflow and whether the workflow has previously been initiated by that source. The disclosure is scalable to allow for new nodes to be included.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563478&OS=09563478&RS=09563478
owner: VMware, Inc.
number: 09563478
owner_city: Palo Alto
owner_country: US
publication_date: 20150828
---
Some existing systems execute concurrent workflows on distributed nodes with shared operations between the workflows. For example disaster recovery of virtual machines VMs operating in cloud environments requires a high level of coordination between nodes which do not have direct knowledge of the existence or state of the other nodes associated with the workflow. A synchronization mechanism allows for mutual exclusion on any shared operations.

Some existing solutions utilize lock based synchronization to effectuate the execution of concurrent workflows on distributed nodes with shared operations however it is difficult for a lock based synchronization system to respond to dynamic scaling of concurrency when new nodes are added. For example some of the existing lock based synchronization systems are a bottleneck for scalability and are undesirable in a distributed cloud environment. Another existing approach is to replicate the processing functions and to use protocols for achieving consensus between processors on a network of unreliable communication channels. However that method sometimes results in failing over the same VM multiple times which is not acceptable for disaster recovery workflows.

One or more examples described herein perform concurrent execution of a plurality of distributed workflows including workflow templates by a cluster of management nodes. Execution includes receiving by one of the cluster of management nodes one of a plurality of workflow templates and a recovery plan. The management node generates an initiator identifier ID associated with the current execution of the workflow. The same Initiator ID is associated with each operation in an execution of the workflow. The management node passes the Initiator ID to an intermediate node along with the request to execute a workflow operation on an object in the recovery plan. Execution further includes performing upon a plurality of objects the plurality of operations in the workflow template associated with the objects for the accessed at least one of the plurality of workflow templates. Upon failure of an operation if the failed operation was previously performed the Initiator ID of the previous execution of the operation is compared to the Initiator ID of the current execution of the operation. If the recovery plan of the current execution of the operation matches the recovery plan of the previous execution of the operation the subsequent operations in the received workflow are performed otherwise the received workflow is terminated or modified to skip certain operations.

This summary introduces a selection of concepts that are described in more detail below. This summary is not intended to identify essential features nor to limit in any way the scope of the claimed subject matter.

Examples of the disclosure include a scalable system capable of performing multiple potentially conflicting operations on multiple objects such as virtual machines VMs simultaneously or near simultaneously. The operations may be associated with any function.

In some examples such as disaster recovery a group of objects such as VMs is associated with a recovery plan. The disclosed system is a highly scalable service capable of performing multiple workflows on different recovery plans concurrently. These recovery plans may share the recoverable objects e.g. VMs so it is possible for two plans to attempt the same operation on the same object from which only the first attempt to perform the operation succeeds.

The group of objects described above is in some examples not static or permanent. In that example the group of objects is not required to be associated with a recovery plan. In some examples a group of objects is created on demand to run a specific workflow not associated with a recovery plan. In that example once the workflow is done the grouping is discarded. If the workflow fails then the user in some examples reruns the same workflow for the same group of objects similar to rerunning a recovery plan. In alternative examples a recovery plan is created just to run a workflow then discarded.

However the failure of a particular operation does not always mean that recovery or other type of workflow or operation is not possible. Under some conditions the failure may be ignored for the purposes of the given workflow. For example if the workflow was initiated by the same recovery plan as a previous workflow this is considered a re run of the same workflow. In this example the system ignores the failure and continues with the workflow operations optionally skipping already completed operations.

If the workflow was initiated by a different recovery plan the error was likely caused by concurrent execution of the two plans. In this example the VM may have been recovered successfully but with different settings. The disclosed system skips the remaining operations in the recovery plan that encountered the error for this VM and displays a warning to the user because a positive outcome is not guaranteed. However the workflow execution is still considered successful. If the initiator of the workflow is not recognized the workflow fails.

One or more examples orchestrate a plurality of objects such as VMs during workflows. Site Recovery Manager Air SRM Air by VMware Inc. is an example of the disclosed system. One example of a workflow managed by the present system is disaster recovery from the on premises datacenter to the cloud and back. The disclosed system runs in the cloud for example vCloud Air by VMware Inc. and allows customers to use the cloud based datacenter as a disaster recovery site. In another example the coordinated workflow is migration planned failover disaster recovery forced failover test cleanup reprotect test failover deactivate or reverse replication from one datacenter to another. For example vSphere Replication to the Cloud by VMware Inc. may provide the replication technology for replicating running VMs to and from the cloud. However aspects of the disclosure are not limited to disaster recovery or any specific product or implementation.

Examples described herein attempt to execute sequences of operation across multiple nodes simultaneously or near simultaneously. If an operation fails on a node the individual node evaluates the failure in some examples and based on the evaluation attempts to execute the remaining operations or aborts the remaining operations. All of the nodes report their results.

Aspects of the disclosure further enable increasing speed in executing concurrent workflows across distributed nodes. Under the disclosed method operations which are initiated by the same source and previously performed are not re performed. This frees up processing resources for the execution of operations which have not yet been performed. This aspect of the disclosure also reduces the processor load. Because fewer extraneous operations are executed the load on the processor is reduced freeing the processor to perform other operations.

Each cluster is populated by a plurality of nodes. The management cluster contains a plurality of management nodes . In some examples the management nodes are SRM Air nodes by VMware Inc. The management nodes manage in some examples disaster recovery applications. Although in the example of the management nodes are located on the pod in other examples the management nodes are not located on the pod . Each management cluster is in some examples backed by a database cluster containing multiple nodes. The compute cluster contains several compute nodes . Compute nodes are described in some examples as compute management nodes. In some examples the compute cluster also contains a compute server such as the vSphere Replication Management Server VRMS by VMware Inc. which manages replication to and from the datacenter . The pod also contains intermediate clusters which contain a plurality of intermediate nodes such as vSphere Replication Service VRCS or vCloud Director VCD cells by VMware Inc. The intermediate nodes in some examples are not necessary. In other examples the intermediate nodes are cloud service provider nodes. In some examples the management node calls into the intermediate node which in turn calls into the compute node which ensures that the operations are performed to recover the objects.

In some examples the intermediate nodes disseminate workflow operations to the compute cluster . Within the compute cluster the individual workflow operations are executed by a compute node . The compute nodes in some examples transmit responses or reports back to the intermediate nodes or to the intermediate clusters . In some examples the responses indicate that a workflow operation is successfully completed by the compute node that the workflow operation was previously initiated an error or other warning. All servers and infrastructure components in a pod are shared among all nodes objects or tenants assigned to this pod . Each pod is in some examples designed to handle hundreds of tenants running thousands of VMs simultaneously.

However only one recovery plan recovers the VM others skip the appropriate recovery operations and consider the VM as successfully recovered. The relationship of VMs to workflow templates is not always one to one. In one example a VM is associated with a recovery plan based on the location of its physical resources e.g. all VMs sharing the same underlying hardware are associated with one recovery plan and the same VM is associated with a recovery plan based on its project association e.g. all VMs sharing related tasks . In another example multiple recovery plans are used in hierarchical recovery. Specifically one recovery plan recovers all VMs in a datacenter one recovery plan recovers a particular cluster one recovery plan recovers a particular application etc.

In the example of the associations of VM VM and VM are illustrated with arrows connecting those VMs with their associated workflow templates . Specifically VM is associated with the workflow template included in the workflow specified for Palo Alto VMs however VM is also associated with the workflow template included in the workflow specified for Project A VMs . The associations for VM and VM are also illustrated by arrows. While not similarly illustrated each other VM is associated with a workflow based on its geographic association and its project association.

Although the above example and are drawn to geographic and project based associations VMs or other objects are grouped in other examples in a plurality of other ways. Recovery plans are in other examples created based on resource allocation tasks users mechanical constraints backup schedules etc. Further a VM or other object is in some examples only associated with one recovery plan or any combination of recovery plans .

At an individual management node begins to perform the next operation associated with the workflow template of that VM . If at the operation is successfully performed then the management node continues to perform all of the operations in the sequence at until execution of the workflow template is completed at .

However if the operation is not successfully completed at the management node at evaluates whether the operation had previously been performed. In some examples the operation is previously performed if the initiation of the workflow is a re run if the management node failed before completing the workflow template or if a message indicating that the intermediate node completed the workflow operation was not received by the management node which initiated the workflow . If the operation was indicated as previously performed the management node determines if the previous execution of the current operation was performed by the same recovery plan at . If the previous execution of the current operation was performed by the same recovery plan the management node continues to perform the operations in the sequence at until the execution of the workflow template is complete at .

If at the management node discovers that the operation was not successful evaluated at and also that the operation was not previously performed the execution of the workflow template is terminated and the management node returns an error to the user .

In some examples not illustrated if a recovery plan workflow fails the user does not rerun the same workflow. In some examples where the recovery plan workflow fails the user runs a different workflow what would perform a similar function and would perform similar operations for the same object or other VM . For example if the user started a planned failover workflow and then half way through it fails for a VM the user may determine that he no longer cares about the protected site and runs a forced failover workflow for the same recovery plan instead. In that example for the forced failover workflow only operations at the recovery site create failover image power on customize etc. would be performed but not the operations at the protected site sync power off etc. . If the planned failover failed at one of the steps that are shared with the forced failover the forced failover workflow in that example picks up where planed failover left off and continues.

However if the management node discovers that the operation was not successful evaluated at and also that the operation was previously performed at then the management node evaluates if the previous execution of the current operation was performed by the same recovery plan at . If the previous execution of the current operation was performed by the same recovery plan at then the management node returns to the decision loop illustrated in blocks and and described above.

At when evaluating whether the current execution of the current operation was performed by the same recovery plan as the previous execution of the current operation if the management node determines that the two recovery plans are not the same then the execution of the current workflow template is terminated by the management node at . Upon terminating the execution of the current workflow template at the management node also returns a warning to the user .

The above operations are performed by an individual management node . However a plurality of management nodes or a management cluster simultaneously performs method as described above. Method is illustrated and discussed in more detail in .

At the management node starts executing the operations in the order specified by the workflow template for each object in parallel across all the objects . The operations illustrated in are duplicated for each object.

If all of the objects report completion in executing their associated workflow templates at the workflow ends at . However if not all objects have reported completion at the workflow ascertains whether the objects which failed to report completion have returned an error raised a warning or made any other report at . If some report has been made by one of the objects failing to complete their associated workflow templates at then the workflow ends. Otherwise the workflow continues to wait for responses from the objects until the incomplete objects have timed out at . In some examples the duration of the wait is defined by a user a policy or based on statistical averages of previous execution times for workflow templates . The wait time is for example defined as the average wait time plus one standard deviation. In examples where the wait time is defined by average historic wait times the wait time is adjusted as additional information is accumulated. After all objects have reported completion raised a warning or returned an error the workflow is ended at .

At the management node receives its workflow template from the user and the Initiator ID for the workflow . At the management node instructs the intermediate node to perform the next operation in the workflow template . The current Initiator ID is passed to the intermediate node together with the command request. The intermediate node instructs the compute node to perform the operation passing the Initiator ID together with the operation request. If the operation has not been previously performed for the object the compute node performs the operation and stores the Initiator ID. If the operation is successfully performed at the management node continues to execute the next operation in the sequence of operations in the workflow template until all operations in the workflow template are completed at . Once all of the operations are completed the management node completes the workflow template and in some examples sends a report to the user reporting that it has completed the requested workflow template .

However if at an operation is not successfully performed the management node determines whether the operation was previously performed at . If at the management node determines that the operation which failed at was not already performed then the management node stops performing operations in the workflow template and returns an error. Alternatively if the operation was previously performed at the management node queries the Initiator ID associated with the last successful execution of that operation . The Initiator IDs are in some examples stored in a database e.g. Zookeeper from Apache Software Foundation table via an API or any other logical storage system. If the recovery plans that initiated the previous and the current execution of the operation are the same then the workflow is a re run of the same workflow template for the same recovery plan . The management node verifies that all operations in the workflow template are completed at completes any unexecuted operations in the workflow template and subsequently completes the workflow template at .

Alternatively if at the recovery plan that initiated the previous execution of the operation does not match the recovery plan of the current execution of the operation then the remaining operations in the workflow template are not performed and a warning is returned to the user at .

In some examples it is possible for two recovery plans to attempt the same operation on the same underlying object. In that example only the first attempt to perform the operation succeeds. However the failure of the other attempt to perform a particular operation does not always mean that the other recovery plan or other type of workflow requested on the management node ends with failure. Under some conditions the failure to perform the operation is ignored for the purposes of the given workflow .

Object 1 performs operation 1 . Upon attempting to perform operation 2 object 1 discovers operation 2 is already done by another recovery plan. Consequently object 1 skips the subsequent operations and produces a warning.

Object 2 performs operation 1 . Upon attempting to perform operation 2 the operation succeeds or object 2 discovers operation 2 was completed earlier by the same recovery plan. This is a re run and management node 2 continues to execute the subsequent operations .

Object 3 performs operation 1 . Upon attempting to perform operation 2 the operation fails and object 3 determines that operation 2 was not performed previously. In this case object 3 returns an error.

The sequence begins when the workflow is initiated. In some examples the management node initiates the workflow . In other examples the workflow is initiated from outside the management node . The management node associates an Initiator ID with the workflow and executes the workflow templates upon each object in this example a VM . The objects upon which the workflow templates are executed are in some examples state machines. Although the operations are described as performed by the objects it is understood that in some examples the management node executes the workflow while the compute nodes intermediate nodes or other components perform the individual operations upon the objects to achieve the desired state. In the example of each object attempts and successfully performs operation 1 .

Object 1 attempts to perform operation 2 and it fails to do so. Upon failing to perform operation 2 the management node evaluates whether operation 2 was previously performed and if so which recovery plan had previously performed the operation e.g. by evaluating the Initiator ID of the execution of the operation . The management node discovers that operation 2 was previously performed and initiated by a different recovery plan . Object 1 skips the subsequent operations completes the workflow template with a warning and the management node reports completion of the workflow template to the user with a warning.

Object 2 attempts to perform operation 2 and it fails to do so. Upon failing to perform operation 2 the management node evaluates whether operation 2 was previously performed and if so which recovery plan had previously performed the operation e.g. by evaluating the Initiator ID of the execution of the operation . The management node discovers that operation 2 was previously performed by the same recovery plan . Consequently this is a re run of the recovery plan . Object 2 performs the remaining operations in the workflow template and completes the workflow template . The management node reports successful completion of the workflow template to the user .

Object 3 attempts to perform operation 2 and it fails to do so. Upon failing to perform operation 2 the management node evaluates whether operation 2 was previously performed and if so which recovery plan had previously performed the operation e.g. by evaluating the Initiator ID of the execution of the operation . The management node discovers that operation 2 was not previously performed. Consequently the execution of operation 2 failed for some reason other than having been already performed. The management node terminates the workflow template and reports an error to the user .

Upon receiving the reports from all the objects that the workflow templates are completed or terminated the management node ends the workflow . In some examples ending the workflow involves reporting results to another object or user which initiated the workflow .

Host computing device may include a user interface device for receiving data from a user and or for presenting data to user . User may interact indirectly with host computing device via another computing device such as a device running VMware s vCenter Server or other management device. User interface device may include for example a keyboard a pointing device a mouse a stylus a touch sensitive panel e.g. a touch pad or a touch screen a gyroscope an accelerometer a position detector and or an audio input device. In some examples user interface device operates to receive data from user while another device e.g. a presentation device operates to present data to user . In other examples user interface device has a single component such as a touch screen that functions to both output data to user and receive data from user . In such examples user interface device operates as a presentation device for presenting information to user . In such examples user interface device represents any component capable of conveying information to user . For example user interface device may include without limitation a display device e.g. a liquid crystal display LCD organic light emitting diode OLED display or electronic ink display and or an audio output device e.g. a speaker or headphones . In some examples user interface device includes an output adapter such as a video adapter and or an audio adapter. An output adapter is operatively coupled to processor and configured to be operatively coupled to an output device such as a display device or an audio output device.

Host computing device also includes a network communication interface which enables host computing device to communicate with a remote device e.g. another computing device via a communication medium such as a wired or wireless packet network. For example host computing device may transmit and or receive data via network communication interface . User interface device and or network communication interface may be referred to collectively as an input interface and may be configured to receive information from user .

Host computing device further includes a storage interface that enables host computing device to communicate with one or more storage devices which store virtual disk images software applications and or any other data suitable for use with the methods described herein. In examples storage interface couples host computing device to a storage area network SAN e.g. a Fibre Channel network and or to a network attached storage NAS system e.g. via a packet network . The storage interface may be integrated with network communication interface .

The virtualization software layer supports a virtual machine execution space within which multiple virtual machines VMs may be concurrently instantiated and executed. Hypervisor includes a device driver layer and maps physical resources of hardware platform e.g. processor memory network communication interface and or user interface device to virtual resources of each of VMs such that each of VMs has its own virtual hardware platform e.g. a corresponding one of virtual hardware platforms each virtual hardware platform having its own emulated hardware such as a processor a memory a network communication interface a user interface device and other emulated I O devices in VM . Hypervisor may manage e.g. monitor initiate and or terminate execution of VMs according to policies associated with hypervisor such as a policy specifying that VMs are to be automatically restarted upon unexpected termination and or upon initialization of hypervisor . In addition or alternatively hypervisor may manage execution VMs based on requests received from a device other than host computing device . For example hypervisor may receive an execution instruction specifying the initiation of execution of first VM from a management device via network communication interface and execute the execution instruction to initiate execution of first VM .

In some examples memory in first virtual hardware platform includes a virtual disk that is associated with or mapped to one or more virtual disk images stored on a disk e.g. a hard disk or solid state disk of host computing device . The virtual disk image represents a file system e.g. a hierarchy of directories and files used by first VM in a single file or in a plurality of files each of which includes a portion of the file system. In addition or alternatively virtual disk images may be stored on one or more remote computing devices such as in a storage area network SAN configuration. In such examples any quantity of virtual disk images may be stored by the remote computing devices.

Device driver layer includes for example a communication interface driver that interacts with network communication interface to receive and transmit data from for example a local area network LAN connected to host computing device . Communication interface driver also includes a virtual bridge that simulates the broadcasting of data packets in a physical network received from one communication interface e.g. network communication interface to other communication interfaces e.g. the virtual communication interfaces of VMs . Each virtual communication interface for each VM such as network communication interface for first VM may be assigned a unique virtual Media Access Control MAC address that enables virtual bridge to simulate the forwarding of incoming data packets from network communication interface . In an example network communication interface is an Ethernet adapter that is configured in promiscuous mode such that all Ethernet packets that it receives rather than just Ethernet packets addressed to its own physical MAC address are passed to virtual bridge which in turn is able to further forward the Ethernet packets to VMs . This configuration enables an Ethernet packet that has a virtual MAC address as its destination address to properly reach the VM in host computing device with a virtual communication interface that corresponds to such virtual MAC address.

Virtual hardware platform may function as an equivalent of a standard x86 hardware architecture such that any x86 compatible desktop operating system e.g. Microsoft WINDOWS brand operating system LINUX brand operating system SOLARIS brand operating system NETWARE or FREEBSD may be installed as guest operating system OS in order to execute applications for an instantiated VM such as first VM . Virtual hardware platforms may be considered to be part of virtual machine monitors VMM that implement virtual system support to coordinate operations between hypervisor and corresponding VMs . Those with ordinary skill in the art will recognize that the various terms layers and categorizations used to describe the virtualization components in may be referred to differently without departing from their functionality or the spirit or scope of the disclosure. For example virtual hardware platforms may also be considered to be separate from VMMs and VMMs may be considered to be separate from hypervisor . One example of hypervisor that may be used in an example of the disclosure is included as a component in VMware s ESX brand software which is commercially available from VMware Inc.

In some examples the recovery plan is associated with a collection of VMs replicated in the same direction. In another example the recovery plan is configured to reflect dependencies between VMs . Specific parameters are customized for each VM to be used during execution of a workflow assign scripts to each VM etc. in an additional example. As an example the recovery plan is configured to execute recovery workflows for the VMs . Specific workflow templates include in some examples planned failover forced failover test failover etc. During the execution of a workflow for a recovery plan the disclosed method performs all the necessary operations for each VM with as much parallelism as allowed by system constrains the constraints configured by a user e.g. VM power on dependencies and power on priority tiers unplanned events etc. Executing a workflow typically involves making multiple REST API calls to the intermediate nodes and the compute nodes .

In other examples an application crashes in the middle of the workflow and subsequent workflow re run is performed. Because the disclosed system uses other services it is possible for a crash to occur after external operations are initiated. In such cases the disclosed system does not know that the operation may have succeeded and the system tries to execute the operation again if the workflow is re run. Similar to a concurrent execution case the second attempt fails but the failure is ignored in some cases.

In some examples the Initiator ID includes an execution ID which makes them unique. If the user reruns the same or different workflow for the same recovery plan initiators will be different because they would include different execution IDs. In this example the decision whether to perform or skip steps is made based on the match of the recovery plans and not the full initiator IDs.

The operations described herein may be performed by a computer or computing device. The computing devices communicate with each other through an exchange of messages and or stored data. Communication may occur using any protocol or mechanism over any wired or wireless connection. A computing device may transmit a message as a broadcast message e.g. to an entire network and or data bus a multicast message e.g. addressed to a plurality of other computing devices and or as a plurality of unicast messages each of which is addressed to an individual computing device. Further in some examples messages are transmitted using a network protocol that does not guarantee delivery such as User Datagram Protocol UDP . Accordingly when transmitting a message a computing device may transmit multiple copies of the message enabling the computing device to reduce the risk of non delivery.

By way of example and not limitation computer readable media comprise computer storage media and communication media. Computer storage media include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media are tangible non transitory and are mutually exclusive to communication media. In some examples computer storage media are implemented in hardware. Exemplary computer storage media include hard disks flash memory drives digital versatile discs DVDs compact discs CDs floppy disks tape cassettes and other solid state memory. In contrast communication media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media.

Although described in connection with an exemplary computing system environment examples of the disclosure are operative with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with aspects of the disclosure include but are not limited to mobile computing devices personal computers server computers hand held or laptop devices multiprocessor systems gaming consoles microprocessor based systems set top boxes programmable consumer electronics mobile telephones network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

Examples of the disclosure may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. The computer executable instructions may be organized into one or more computer executable components or modules. Generally program modules include but are not limited to routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. Aspects of the disclosure may be implemented with any number and organization of such components or modules. For example aspects of the disclosure are not limited to the specific computer executable instructions or the specific components or modules illustrated in the figures and described herein. Other examples of the disclosure may include different computer executable instructions or components having more or less functionality than illustrated and described herein.

Aspects of the disclosure transform a general purpose computer into a special purpose computing device when programmed to execute the instructions described herein.

The examples illustrated and described herein as well as examples not specifically described herein but within the scope of aspects of the disclosure constitute exemplary means for concurrently executing distributed workflows sharing common operations. For example the elements illustrated in and or such as when encoded to perform the operations illustrated in and or B constitute exemplary means for receiving by one of a cluster of management nodes one of a plurality of workflow templates exemplary means for generating by a management node an Initiator ID associated with execution of the operations in the workflow and exemplary means for performing by each management node in the cluster the plurality of operations in the workflow template associated with the management node for the accessed at least one of the plurality of workflows.

At least a portion of the functionality of the various elements illustrated in the figures may be performed by other elements in the figures or an entity e.g. processor web service server application program computing device etc. not shown in the figures.

In some examples the operations illustrated in the figures may be implemented as software instructions encoded on a computer readable medium in hardware programmed or designed to perform the operations or both. For example aspects of the disclosure may be implemented as a system on a chip or other circuitry including a plurality of interconnected electrically conductive elements.

The order of execution or performance of the operations in examples of the disclosure illustrated and described herein is not essential unless otherwise specified. That is the operations may be performed in any order unless otherwise specified and examples of the disclosure may include additional or fewer operations than those disclosed herein. For example it is contemplated that executing or performing a particular operation before contemporaneously with or after another operation is within the scope of aspects of the disclosure.

When introducing elements of aspects of the disclosure or the examples thereof the articles a an the and said are intended to mean that there are one or more of the elements. The terms comprising including and having are intended to be inclusive and mean that there may be additional elements other than the listed elements. The term exemplary is intended to mean an example of. 

Having described aspects of the disclosure in detail it will be apparent that modifications and variations are possible without departing from the scope of aspects of the disclosure as defined in the appended claims. As various changes could be made in the above constructions products and methods without departing from the scope of aspects of the disclosure it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

