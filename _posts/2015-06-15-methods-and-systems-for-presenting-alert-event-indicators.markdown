---

title: Methods and systems for presenting alert event indicators
abstract: A method at an electronic device includes: displaying a video feed from a camera or a frame from the video feed; and concurrently with displaying the video feed or the frame, displaying a camera history timeline, including: displaying a representation of a camera event associated with one or more alert events in the camera history timeline as a bar overlaid on the event history timeline, the event bar having a length reflecting a duration of the camera event; and displaying, proximate to the event bar, one or more alert event indicators, each of the alert event indicators corresponding to a respective alert event of the alert events associated with the camera event, where each respective alert event indicator has a respective visually distinctive display characteristic associated with the corresponding respective alert event.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09380274&OS=09380274&RS=09380274
owner: GOOGLE INC.
number: 09380274
owner_city: Mountain View
owner_country: US
publication_date: 20150615
---
This application is a continuation of U.S. patent application Ser. No. 14 738 930 titled Methods and Systems for Presenting Multiple Live Video Feeds in a User Interface filed Jun. 14 2015 which is hereby incorporated by reference in its entirety.

This application is related to the following applications which are hereby incorporated by reference herein in their entirety 

U.S. patent application Ser. No. 14 738 928 titled Methods and Systems for Smart Home Automation Using a Multifunction Status and Entry Point Icon filed Jun. 14 2015 and

U.S. patent application Ser. No. 14 739 427 titled Methods and Systems for Presenting a Camera History filed Jun. 15 2015.

The disclosed implementations relate generally to video monitoring including but not limited to monitoring and reviewing video feeds and histories of videos saved from the video feeds.

The advancement of internet and mobile technologies has enabled the adoption of remote video surveillance by users. Users can now monitor an area under video surveillance using a website or a mobile application. Such websites or mobile apps typically allow a user to view live video and or saved video recordings but otherwise provide little or no additional information regarding the videos. Furthermore the user interfaces for viewing these live videos or saved video recordings occupy large amounts of display space and have a user control flow that is poor at maintaining context for the user. Thus more efficient informative and user friendly presentations of live and saved video surveillance are needed.

Accordingly there is a need for presentations of live and or saved video with a more efficient user control flow and more useful information. Such methods optionally complement or replace conventional methods for presenting live and or saved video from video streams.

In accordance with some implementations a method includes in an application executing at a client device having one or more processors and memory storing one or more programs for execution by the one or more processors receiving a plurality of video feeds each video feed of the plurality of video feeds corresponding to a respective remote camera of a plurality of remote cameras where the video feeds are received concurrently by the device from a server system communicatively coupled to the remote cameras displaying a first user interface the first user interface including a plurality of user interface objects each user interface object of the plurality of user interface objects being associated with a respective remote camera of the remote cameras and displaying in each user interface object of the plurality of user interface objects the video feed corresponding to the respective remote camera with which the user interface object is associated where at least one of the video feeds is displayed with cropping.

In accordance with some implementations a method includes at a server system having one or more processors and memory storing one or more programs for execution by the one or more processors receiving a video feed from a camera with an associated field of view receiving one or more alert events identifying as a camera event a portion of the video feed associated in time with the one or more alert events determining a start time and a duration of the camera event determining a chronological order of the alert events and saving in a history associated with the camera information associated with the camera event including a video clip and or a frame from the portion of the video feed and the chronological order of the alert events.

In accordance with some implementations a method includes at a client device having one or more processors and memory storing one or more programs for execution by the one or more processors displaying a video feed from a camera or a frame from the video feed and concurrently with displaying the video feed or the frame displaying a camera history timeline including displaying a representation of a camera event associated with one or more alert events in the camera history timeline as a bar overlaid on the event history timeline the event bar having a length reflecting a duration of the camera event and displaying proximate to the event bar one or more alert event indicators each of the alert event indicators corresponding to a respective alert event of the alert events associated with the camera event where each respective alert event indicator has a respective visually distinctive display characteristic associated with the corresponding respective alert event.

In accordance with some implementations a method includes at a client device having one or more processors and memory storing one or more programs for execution by the one or more processors displaying a camera history timeline including displaying a chronologically ordered sequence of event identifiers each event identifier corresponding to a respective camera event each respective camera event associated with one or more respective alert events and displaying for a respective event identifier one or more alert event indicators each of the alert event indicators corresponding to an alert event associated with the camera event corresponding to the respective event identifier each of the alert event indicators displayed with a visually distinctive display characteristic associated with a corresponding alert event.

In accordance with some implementations a method includes at a client device having one or more processors and memory storing one or more programs for execution by the one or more processors in an application executing on the client device displaying a camera event history provided by a remote server system where the camera event history is presented as a chronologically ordered set of event identifiers each event identifier corresponding to a respective event for which a remote camera has captured an associated video receiving a user selection of a displayed event identifier and in response to receiving the user selection of the displayed event identifier expanding the selected event identifier into a video player window the video player window consuming a portion of the displayed camera event history and playing in the video player window the captured video associated with the selected event identifier and in response to terminating playback of the captured video associated with the selected event identifier or user de selection of the displayed event identifier collapsing the video player window into the selected event identifier thereby stopping the playing of the captured video associated with the selected event identifier.

In accordance with some implementations a system includes a plurality of electronic devices wherein at least one of the plurality of electronic devices has one or more processors and memory storing one or more programs for execution by the processor the one or more programs including instructions for performing the operations of the method described above. In accordance with some implementations an electronic device has one or more processors and memory storing one or more programs for execution by the processor the one or more programs including instructions for performing the operations of the method described above. In accordance with some implementations a computer readable storage medium has stored therein one or more programs having instructions which when executed by an electronic device having one or more processors cause the electronic device to perform the operations of the method described above.

Thus computing systems are provided with more efficient methods for presenting live and or saved video and related information thereby increasing the effectiveness efficiency and user satisfaction with such systems. Such methods may complement or replace conventional methods for presenting live and or saved video.

Reference will now be made in detail to implementations examples of which are illustrated in the accompanying drawings. In the following detailed description numerous specific details are set forth in order to provide a thorough understanding of the various described implementations. However it will be apparent to one of ordinary skill in the art that the various described implementations may be practiced without these specific details. In other instances well known methods procedures components circuits and networks have not been described in detail so as not to unnecessarily obscure aspects of the implementations.

It will also be understood that although the terms first second etc. are in some instances used herein to describe various elements these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example a first user interface could be termed a second user interface and similarly a second user interface could be termed a first user interface without departing from the scope of the various described implementations. The first user interface and the second user interface are both types of user interfaces but they are not the same user interface.

The terminology used in the description of the various described implementations herein is for the purpose of describing particular implementations only and is not intended to be limiting. As used in the description of the various described implementations and the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms includes including comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

As used herein the term if is optionally construed to mean when or upon or in response to determining or in response to detecting or in accordance with a determination that depending on the context. Similarly the phrase if it is determined or if a stated condition or event is detected is optionally construed to mean upon determining or in response to determining or upon detecting the stated condition or event or in response to detecting the stated condition or event or in accordance with a determination that a stated condition or event is detected depending on the context.

It is to be appreciated that smart home environments may refer to smart environments for homes such as a single family house but the scope of the present teachings is not so limited. The present teachings are also applicable without limitation to duplexes townhomes multi unit apartment buildings hotels retail stores office buildings industrial buildings and more generally any living space or work space.

It is also to be appreciated that while the terms user customer installer homeowner occupant guest tenant landlord repair person and the like may be used to refer to the person or persons acting in the context of some particularly situations described herein these references do not limit the scope of the present teachings with respect to the person or persons who are performing such actions. Thus for example the terms user customer purchaser installer subscriber and homeowner may often refer to the same person in the case of a single family residential dwelling because the head of the household is often the person who makes the purchasing decision buys the unit and installs and configures the unit and is also one of the users of the unit. However in other scenarios such as a landlord tenant environment the customer may be the landlord with respect to purchasing the unit the installer may be a local apartment supervisor a first user may be the tenant and a second user may again be the landlord with respect to remote control functionality. Importantly while the identity of the person performing the action may be germane to a particular advantage provided by one or more of the implementations such identity should not be construed in the descriptions that follow as necessarily limiting the scope of the present teachings to those particular individuals having those particular identities.

The depicted structure includes a plurality of rooms separated at least partly from each other via walls . The walls may include interior walls or exterior walls. Each room may further include a floor and a ceiling . Devices may be mounted on integrated with and or supported by a wall floor or ceiling .

In some implementations the integrated devices of the smart home environment include intelligent multi sensing network connected devices that integrate seamlessly with each other in a smart home network e.g. and or with a central server or a cloud computing system to provide a variety of useful smart home functions. The smart home environment may include one or more intelligent multi sensing network connected thermostats hereinafter referred to as smart thermostats one or more intelligent network connected multi sensing hazard detection units hereinafter referred to as smart hazard detectors one or more intelligent multi sensing network connected entryway interface devices and hereinafter referred to as smart doorbells and smart door locks and one or more intelligent multi sensing network connected alarm systems hereinafter referred to as smart alarm systems .

In some implementations the one or more smart thermostats detect ambient climate characteristics e.g. temperature and or humidity and control a HVAC system accordingly. For example a respective smart thermostat includes an ambient temperature sensor.

The one or more smart hazard detectors may include thermal radiation sensors directed at respective heat sources e.g. a stove oven other appliances a fireplace etc. . For example a smart hazard detector in a kitchen includes a thermal radiation sensor directed at a stove oven . A thermal radiation sensor may determine the temperature of the respective heat source or a portion thereof at which it is directed and may provide corresponding blackbody radiation data as output.

The smart doorbell and or the smart door lock may detect a person s approach to or departure from a location e.g. an outer door control doorbell door locking functionality e.g. receive user inputs from a portable electronic device to actuate bolt of the smart door lock announce a person s approach or departure via audio or visual means and or control settings on a security system e.g. to activate or deactivate the security system when occupants go and come .

The smart alarm system may detect the presence of an individual within close proximity e.g. using built in IR sensors sound an alarm e.g. through a built in speaker or by sending commands to one or more external speakers and send notifications to entities or users within outside of the smart home network . In some implementations the smart alarm system also includes one or more input devices or sensors e.g. keypad biometric scanner NFC transceiver microphone for verifying the identity of a user and one or more output devices e.g. display speaker . In some implementations the smart alarm system may also be set to an armed mode such that detection of a trigger condition or event causes the alarm to be sounded unless a disarming action is performed.

In some implementations the smart home environment includes one or more intelligent multi sensing network connected wall switches hereinafter referred to as smart wall switches along with one or more intelligent multi sensing network connected wall plug interfaces hereinafter referred to as smart wall plugs . The smart wall switches may detect ambient lighting conditions detect room occupancy states and control a power and or dim state of one or more lights. In some instances smart wall switches may also control a power state or speed of a fan such as a ceiling fan. The smart wall plugs may detect occupancy of a room or enclosure and control supply of power to one or more wall plugs e.g. such that power is not supplied to the plug if nobody is at home .

In some implementations the smart home environment of includes a plurality of intelligent multi sensing network connected appliances hereinafter referred to as smart appliances such as refrigerators stoves ovens televisions washers dryers lights stereos intercom systems garage door openers floor fans ceiling fans wall air conditioners pool heaters irrigation systems security systems space heaters window AC units motorized duct vents and so forth. In some implementations when plugged in an appliance may announce itself to the smart home network such as by indicating what type of appliance it is and it may automatically integrate with the controls of the smart home. Such communication by the appliance to the smart home may be facilitated by either a wired or wireless communication protocol. The smart home may also include a variety of non communicating legacy appliances such as old conventional washer dryers refrigerators and the like which may be controlled by smart wall plugs . The smart home environment may further include a variety of partially communicating legacy appliances such as infrared IR controlled wall air conditioners or other IR controlled devices which may be controlled by IR signals provided by the smart hazard detectors or the smart wall switches .

In some implementations the smart home environment includes one or more network connected cameras that are configured to provide video monitoring and security in the smart home environment . In some implementations cameras also capture video when other conditions or hazards are detected in order to provide visual monitoring of the smart home environment when those conditions or hazards occur. The cameras may be used to determine occupancy of the structure and or particular rooms in the structure and thus may act as occupancy sensors. For example video captured by the cameras may be processed to identify the presence of an occupant in the structure e.g. in a particular room . Specific individuals may be identified based for example on their appearance e.g. height face and or movement e.g. their walk gait . For example cameras may additionally include one or more sensors e.g. IR sensors motion detectors input devices e.g. microphone for capturing audio and output devices e.g. speaker for outputting audio .

The smart home environment may additionally or alternatively include one or more other occupancy sensors e.g. the smart doorbell smart door locks touch screens IR sensors microphones ambient light sensors motion detectors smart nightlights etc. . In some implementations the smart home environment includes radio frequency identification RFID readers e.g. in each room or a portion thereof that determine occupancy based on RFID tags located on or embedded in occupants. For example RFID readers may be integrated into the smart hazard detectors .

The smart home environment may include one or more sound and or vibration sensors for detecting abnormal sounds and or vibrations. These sensors may be integrated with any of the devices described above. The sound sensors detect sound above a decibel threshold. The vibration sensors detect vibration above a threshold directed at a particular area e.g. vibration on a particular window when a force is applied to break the window .

Conditions detected by the devices described above e.g. motion sound vibrations hazards may be referred to collectively as alert events.

The smart home environment may also include communication with devices outside of the physical home but within a proximate geographical range of the home. For example the smart home environment may include a pool heater monitor that communicates a current pool temperature to other devices within the smart home environment and or receives commands for controlling the pool temperature. Similarly the smart home environment may include an irrigation monitor that communicates information regarding irrigation systems within the smart home environment and or receives control information for controlling such irrigation systems.

By virtue of network connectivity one or more of the smart home devices of may further allow a user to interact with the device even if the user is not proximate to the device. For example a user may communicate with a device using a computer e.g. a desktop computer laptop computer or tablet or other portable electronic device e.g. a mobile phone such as a smart phone . A webpage or application may be configured to receive communications from the user and control the device based on the communications and or to present information about the device s operation to the user. For example the user may view a current set point temperature for a device e.g. a stove and adjust it using a computer. The user may be in the structure during this remote communication or outside the structure.

As discussed above users may control smart devices in the smart home environment using a network connected computer or portable electronic device . In some examples some or all of the occupants e.g. individuals who live in the home may register their device with the smart home environment . Such registration may be made at a central server to authenticate the occupant and or the device as being associated with the home and to give permission to the occupant to use the device to control the smart devices in the home. An occupant may use their registered device to remotely control the smart devices of the home such as when the occupant is at work or on vacation. The occupant may also use their registered device to control the smart devices when the occupant is actually located inside the home such as when the occupant is sitting on a couch inside the home. It should be appreciated that instead of or in addition to registering devices the smart home environment may make inferences about which individuals live in the home and are therefore occupants and which devices are associated with those individuals. As such the smart home environment may learn who is an occupant and permit the devices associated with those individuals to control the smart devices of the home.

In some implementations in addition to containing processing and sensing capabilities devices and or collectively referred to as the smart devices are capable of data communications and information sharing with other smart devices a central server or cloud computing system and or other devices that are network connected. Data communications may be carried out using any of a variety of custom or standard wireless protocols e.g. IEEE 802.15.4 Wi Fi ZigBee 6LoWPAN Thread Z Wave Bluetooth Smart ISA100.11a WirelessHART MiWi etc. and or any of a variety of custom or standard wired protocols e.g. Ethernet HomePlug etc. or any other suitable communication protocol including communication protocols not yet developed as of the filing date of this document.

In some implementations the smart devices serve as wireless or wired repeaters. In some implementations a first one of the smart devices communicates with a second one of the smart devices via a wireless router. The smart devices may further communicate with each other via a connection e.g. network interface to a network such as the Internet . Through the Internet the smart devices may communicate with a smart home provider server system also called a central server system and or a cloud computing system herein . The smart home provider server system may be associated with a manufacturer support entity or service provider associated with the smart device s . In some implementations a user is able to contact customer support using a smart device itself rather than needing to use other communication means such as a telephone or Internet connected computer. In some implementations software updates are automatically sent from the smart home provider server system to smart devices e.g. when available when purchased or at routine intervals .

In some implementations the network interface includes a conventional network device e.g. a router and the smart home environment of includes a hub device that is communicatively coupled to the network s directly or via the network interface . The hub device is further communicatively coupled to one or more of the above intelligent multi sensing network connected devices e.g. smart devices of the smart home environment . Each of these smart devices optionally communicates with the hub device using one or more radio communication networks available at least in the smart home environment e.g. ZigBee Z Wave Insteon Bluetooth Wi Fi and other radio communication networks . In some implementations the hub device and devices coupled with to the hub device can be controlled and or interacted with via an application running on a smart phone household controller laptop tablet computer game console or similar electronic device. In some implementations a user of such controller application can view status of the hub device or coupled smart devices configure the hub device to interoperate with smart devices newly introduced to the home network commission new smart devices and adjust or view settings of connected smart devices etc. In some implementations the hub device extends capabilities of low capability smart device to match capabilities of the highly capable smart devices of the same type integrates functionality of multiple different device types even across different communication protocols and is configured to streamline adding of new devices and commissioning of the hub device.

In some implementations some low power nodes are incapable of bidirectional communication. These low power nodes send messages but they are unable to listen . Thus other devices in the smart home environment such as the spokesman nodes cannot send information to these low power nodes.

In some implementations some low power nodes are capable of only a limited bidirectional communication. For example other devices are able to communicate with the low power nodes only during a certain time period.

As described in some implementations the smart devices serve as low power and spokesman nodes to create a mesh network in the smart home environment . In some implementations individual low power nodes in the smart home environment regularly send out messages regarding what they are sensing and the other low powered nodes in the smart home environment in addition to sending out their own messages forward the messages thereby causing the messages to travel from node to node i.e. device to device throughout the smart home network . In some implementations the spokesman nodes in the smart home network which are able to communicate using a relatively high power communication protocol such as IEEE 802.11 are able to switch to a relatively low power communication protocol such as IEEE 802.15.4 to receive these messages translate the messages to other communication protocols and send the translated messages to other spokesman nodes and or the smart home provider server system using e.g. the relatively high power communication protocol . Thus the low powered nodes using low power communication protocols are able to send and or receive messages across the entire smart home network as well as over the Internet to the smart home provider server system . In some implementations the mesh network enables the smart home provider server system to regularly receive data from most or all of the smart devices in the home make inferences based on the data facilitate state synchronization across devices within and outside of the smart home network and send commands to one or more of the smart devices to perform tasks in the smart home environment.

As described the spokesman nodes and some of the low powered nodes are capable of listening. Accordingly users other devices and or the smart home provider server system may communicate control commands to the low powered nodes. For example a user may use the electronic device e.g. a smart phone to send commands over the Internet to the smart home provider server system which then relays the commands to one or more spokesman nodes in the smart home network . The spokesman nodes may use a low power protocol to communicate the commands to the low power nodes throughout the smart home network as well as to other spokesman nodes that did not receive the commands directly from the smart home provider server system .

In some implementations a smart nightlight which is an example of a smart device is a low power node. In addition to housing a light source the smart nightlight houses an occupancy sensor such as an ultrasonic or passive IR sensor and an ambient light sensor such as a photo resistor or a single pixel sensor that measures light in the room. In some implementations the smart nightlight is configured to activate the light source when its ambient light sensor detects that the room is dark and when its occupancy sensor detects that someone is in the room. In other implementations the smart nightlight is simply configured to activate the light source when its ambient light sensor detects that the room is dark. Further in some implementations the smart nightlight includes a low power wireless communication chip e.g. a ZigBee chip that regularly sends out messages regarding the occupancy of the room and the amount of light in the room including instantaneous messages coincident with the occupancy sensor detecting the presence of a person in the room. As mentioned above these messages may be sent wirelessly e.g. using the mesh network from node to node i.e. smart device to smart device within the smart home network as well as over the Internet to the smart home provider server system .

Other examples of low power nodes include battery operated versions of the smart hazard detectors . These smart hazard detectors are often located in an area without access to constant and reliable power and may include any number and type of sensors such as smoke fire heat sensors e.g. thermal radiation sensors carbon monoxide dioxide sensors occupancy motion sensors ambient light sensors ambient temperature sensors humidity sensors and the like. Furthermore smart hazard detectors may send messages that correspond to each of the respective sensors to the other devices and or the smart home provider server system such as by using the mesh network as described above.

Examples of spokesman nodes include smart doorbells smart thermostats smart wall switches and smart wall plugs . These devices are often located near and connected to a reliable power source and therefore may include more power consuming components such as one or more communication chips capable of bidirectional communication in a variety of protocols.

In some implementations the smart home environment includes service robots that are configured to carry out in an autonomous manner any of a variety of household tasks.

As explained above with reference to in some implementations the smart home environment of includes a hub device that is communicatively coupled to the network s directly or via the network interface . The hub device is further communicatively coupled to one or more of the smart devices using a radio communication network that is available at least in the smart home environment . Communication protocols used by the radio communication network include but are not limited to ZigBee Z Wave Insteon EuOcean Thread OSIAN Bluetooth Low Energy and the like. In some implementations the hub device not only converts the data received from each smart device to meet the data format requirements of the network interface or the network s but also converts information received from the network interface or the network s to meet the data format requirements of the respective communication protocol associated with a targeted smart device. In some implementations in addition to data format conversion the hub device further processes the data received from the smart devices or information received from the network interface or the network s preliminary. For example the hub device can integrate inputs from multiple sensors connected devices including sensors devices of the same and or different types perform higher level processing on those inputs e.g. to assess the overall environment and coordinate operation among the different sensors devices and or provide instructions to the different devices based on the collection of inputs and programmed processing. It is also noted that in some implementations the network interface and the hub device are integrated to one network device. Functionality described herein is representative of particular implementations of smart devices control application s running on representative electronic device s such as a smart phone hub device s and server s coupled to hub device s via the Internet or other Wide Area Network. All or a portion of this functionality and associated operations can be performed by any elements of the described system for example all or a portion of the functionality described herein as being performed by an implementation of the hub device can be performed in different system implementations in whole or in part on the server one or more connected smart devices and or the control application or different combinations thereof.

In some implementations the devices and services platform communicates with and collects data from the smart devices of the smart home environment . In addition in some implementations the devices and services platform communicates with and collects data from a plurality of smart home environments across the world. For example the smart home provider server system collects home data from the devices of one or more smart home environments where the devices may routinely transmit home data or may transmit home data in specific instances e.g. when a device queries the home data . Example collected home data includes without limitation power consumption data blackbody radiation data occupancy data HVAC settings and usage data carbon monoxide levels data carbon dioxide levels data volatile organic compounds levels data sleeping schedule data cooking schedule data inside and outside temperature humidity data television viewership data inside and outside noise level data pressure data video data etc.

In some implementations the smart home provider server system provides one or more services to smart homes and or third parties. Example services include without limitation software updates customer support sensor data collection logging remote access remote or distributed control and or use suggestions e.g. based on collected home data to improve performance reduce utility cost increase safety etc. In some implementations data associated with the services is stored at the smart home provider server system and the smart home provider server system retrieves and transmits the data at appropriate times e.g. at regular intervals upon receiving a request from a user etc. .

In some implementations the extensible devices and services platform includes a processing engine which may be concentrated at a single server or distributed among several different computing entities without limitation. In some implementations the processing engine includes engines configured to receive data from the devices of smart home environments e.g. via the Internet and or a network interface to index the data to analyze the data and or to generate statistics based on the analysis or as part of the analysis. In some implementations the analyzed data is stored as derived home data .

Results of the analysis or statistics may thereafter be transmitted back to the device that provided home data used to derive the results to other devices to a server providing a webpage to a user of the device or to other non smart device entities. In some implementations usage statistics usage statistics relative to use of other devices usage patterns and or statistics summarizing sensor readings are generated by the processing engine and transmitted. The results or statistics may be provided via the Internet . In this manner the processing engine may be configured and programmed to derive a variety of useful information from the home data . A single server may include one or more processing engines.

The derived home data may be used at different granularities for a variety of useful purposes ranging from explicit programmed control of the devices on a per home per neighborhood or per region basis for example demand response programs for electrical utilities to the generation of inferential abstractions that may assist on a per home basis for example an inference may be drawn that the homeowner has left for vacation and so security detection equipment may be put on heightened sensitivity to the generation of statistics and associated inferential abstractions that may be used for government or charitable purposes. For example processing engine may generate statistics about device usage across a population of devices and send the statistics to device users service providers or other entities e.g. entities that have requested the statistics and or entities that have provided monetary compensation for the statistics .

In some implementations to encourage innovation and research and to increase products and services available to users the devices and services platform exposes a range of application programming interfaces APIs to third parties such as charities governmental entities e.g. the Food and Drug Administration or the Environmental Protection Agency academic institutions e.g. university researchers businesses e.g. providing device warranties or service to related equipment targeting advertisements based on home data utility companies and other third parties. The APIs are coupled to and permit third party systems to communicate with the smart home provider server system including the services the processing engine the home data and the derived home data . In some implementations the APIs allow applications executed by the third parties to initiate specific data processing tasks that are executed by the smart home provider server system as well as to receive dynamic updates to the home data and the derived home data .

For example third parties may develop programs and or applications e.g. web applications or mobile applications that integrate with the smart home provider server system to provide services and information to users. Such programs and applications may be for example designed to help users reduce energy consumption to preemptively service faulty equipment to prepare for high service demands to track past service performance etc. and or to perform other beneficial functions or tasks.

In some implementations processing engine includes a challenges rules compliance rewards paradigm that informs a user of challenges competitions rules compliance regulations and or rewards and or that uses operation data to determine whether a challenge has been met a rule or regulation has been complied with and or a reward has been earned. The challenges rules and or regulations may relate to efforts to conserve energy to live safely e.g. reducing the occurrence of heat source alerts e.g. reducing exposure to toxins or carcinogens to conserve money and or equipment life to improve health etc. For example one challenge may involve participants turning down their thermostat by one degree for one week. Those participants that successfully complete the challenge are rewarded such as with coupons virtual currency status etc. Regarding compliance an example involves a rental property owner making a rule that no renters are permitted to access certain owner s rooms. The devices in the room having occupancy sensors may send updates to the owner when the room is accessed.

In some implementations processing engine integrates or otherwise uses extrinsic information from extrinsic sources to improve the functioning of one or more processing paradigms. Extrinsic information may be used to interpret data received from a device to determine a characteristic of the environment near the device e.g. outside a structure that the device is enclosed in to determine services or products available to the user to identify a social network or social network information to determine contact information of entities e.g. public service entities such as an emergency response team the police or a hospital near the device to identify statistical or environmental conditions trends or other information associated with a home or neighborhood and so forth.

In some implementations the smart home provider server system or a component thereof serves as the hub device server system the hub device server system is a part or component of the smart home provider server system . In some implementations the hub device server system is a dedicated video processing server that provides video processing services to video sources and client devices independent of other services provided by the hub device server system . An example of a video processing server is described below with reference to .

In some implementations each of the video sources includes one or more video cameras that capture video and send the captured video to the hub device server system substantially in real time. In some implementations each of the video sources optionally includes a controller device not shown that serves as an intermediary between the one or more cameras and the hub device server system . The controller device receives the video data from the one or more cameras optionally performs some preliminary processing on the video data and sends the video data to the hub device server system on behalf of the one or more cameras substantially in real time. In some implementations each camera has its own on board processing capabilities to perform some preliminary processing on the captured video data before sending the processed video data along with metadata obtained through the preliminary processing to the controller device and or the hub device server system .

In some implementations a camera of a video source captures video at a first resolution e.g. 720P and or 1080P and or a first frame rate 24 frames per second and sends the captured video to the hub device server system at both the first resolution e.g. the original capture resolution s the high quality resolution s such as 1080P and or 720P and the first frame rate and at a second different resolution e.g. 180P and or a second frame rate e.g. 5 frames per second or 10 frames per second . For example the camera captures a video at 720P and or 1080P resolution the camera may capture a video at 1080P and create a downscaled 720P version or capture at both 720P and 1080P . The video source creates a second or third rescaled and optionally at a different frame rate than the version version of the captured video at 180P resolution and transmits both the original captured version i.e. 1080P and or 720P and the rescaled version i.e. the 180P version to the hub device server system for storage. In some implementations the rescaled version has a lower resolution and optionally a lower frame rate than the original captured video. The hub device server system transmits the original captured version or the rescaled version to a client depending on the context. For example the hub device server system transmits the rescaled version when transmitting multiple videos to the same client device for concurrent monitoring by the user and transmits the original captured version in other contexts. In some implementations the hub device server system downscales the original captured version to a lower resolution and transmits the downscaled version.

In some other implementations a camera of a video source captures video at a first resolution e.g. 720P and or 1080P and or a first frame rate and sends the captured video to the hub device server system at the first resolution e.g. the original capture resolution s the high quality resolution s such as 1080P and or 720P and first frame rate for storage. When the hub device server system transmits the video to a client device the hub device server system may downscale the video to a second lower resolution e.g. 180P and or second lower frame rate for the transmission depending on the context. For example the hub device server system transmits the downscaled version when transmitting multiple videos to the same client device for concurrent monitoring by the user and transmits the original captured version in other contexts.

As shown in in accordance with some implementations each of the client devices includes a client side module . The client side module communicates with a server side module executed on the hub device server system through the one or more networks . The client side module provides client side functionalities for the event monitoring and review processing and communications with the server side module . The server side module provides server side functionalities for event monitoring and review processing for any number of client side modules each residing on a respective client device . The server side module also provides server side functionalities for video processing and camera control for any number of the video sources including any number of control devices and the cameras .

In some implementations the server side module includes one or more processors a video storage database device and account databases an I O interface to one or more client devices and an I O interface to one or more video sources . The I O interface to one or more clients facilitates the client facing input and output processing for the server side module . In some implementations the I O interface to clients or a transcoding proxy computer not shown rescales e.g. downscales and or changes the frame rate of video for transmission to a client . The databases store a plurality of profiles for reviewer accounts registered with the video processing server where a respective user profile includes account credentials for a respective reviewer account and one or more video sources linked to the respective reviewer account. The I O interface to one or more video sources facilitates communications with one or more video sources e.g. groups of one or more cameras and associated controller devices . The video storage database stores raw video data received from the video sources as well as various types of metadata such as motion events event categories event category models event filters and event masks for use in data processing for event monitoring and review for each reviewer account.

In some implementations the server side module receives information regarding alert events detected by other smart devices e.g. hazards sound vibration motion . In accordance with the alert event information the server side module instructs one or more video sources in the smart home environment where the alert event is detected to capture video and or associate with the alert event video received from the video sources in the same smart home environment that is contemporaneous or proximate in time with the alert event.

Examples of a representative client device include but are not limited to a handheld computer a wearable computing device a personal digital assistant PDA a tablet computer a laptop computer a desktop computer a cellular telephone a smart phone an enhanced general packet radio service EGPRS mobile phone a media player a navigation device a game console a television a remote control a point of sale POS terminal vehicle mounted computer an ebook reader or a combination of any two or more of these data processing devices or other data processing devices. For example client devices and are a smart phone a tablet computer and a laptop computer respectively.

Examples of the one or more networks include local area networks LAN and wide area networks WAN such as the Internet. The one or more networks are optionally implemented using any known network protocol including various wired or wireless protocols such as Ethernet Universal Serial Bus USB FIREWIRE Long Term Evolution LTE Global System for Mobile Communications GSM Enhanced Data GSM Environment EDGE code division multiple access CDMA time division multiple access TDMA Bluetooth Wi Fi voice over Internet Protocol VoIP Wi MAX or any other suitable communication protocol.

In some implementations the hub device server system is implemented on one or more standalone data processing apparatuses or a distributed network of computers. In some implementations the hub device server system also employs various virtual devices and or services of third party service providers e.g. third party cloud service providers to provide the underlying computing resources and or infrastructure resources of the hub device server system . In some implementations the hub device server system includes but is not limited to a handheld computer a tablet computer a laptop computer a desktop computer or a combination of any two or more of these data processing devices or other data processing devices.

The server client environment shown in includes both a client side portion e.g. the client side module and a server side portion e.g. the server side module . The division of functionalities between the client and server portions of operating environment can vary in different implementations. Similarly the division of functionalities between the video source and the hub device server system can vary in different implementations. For example in some implementations client side module is a thin client that provides only user facing input and output processing functions and delegates all other data processing functionalities to a backend server e.g. the hub device server system . Similarly in some implementations a respective one of the video sources is a simple video capturing device that continuously captures and streams video data to the hub device server system with no or limited local preliminary processing on the video data. Although many aspects of the present technology are described from the perspective of the hub device server system the corresponding actions performed by the client device and or the video sources would be apparent to ones skilled in the art without any creative efforts. Similarly some aspects of the present technology may be described from the perspective of the client device or the video source and the corresponding actions performed by the video server would be apparent to ones skilled in the art without any creative efforts. Furthermore some aspects of the present technology may be performed by the hub device server system the client device and the video sources cooperatively.

It should be understood that operating environment that involves the hub device server system the video sources and the video cameras is merely an example. Many aspects of operating environment are generally applicable in other operating environments in which a server system provides data processing for monitoring and facilitating review of data captured by other types of electronic devices e.g. smart thermostats smart hazard detectors smart doorbells smart wall plugs appliances and the like .

The electronic devices the client devices or the server system communicate with each other using the one or more communication networks . In an example smart home environment two or more devices e.g. the network interface device the hub device and the client devices are located in close proximity to each other such that they could be communicatively coupled in the same sub network A via wired connections a WLAN or a Bluetooth Personal Area Network PAN . The Bluetooth PAN is optionally established based on classical Bluetooth technology or Bluetooth Low Energy BLE technology. This smart home environment further includes one or more other radio communication networks B through which at least some of the electronic devices of the video sources exchange data with the hub device . Alternatively in some situations some of the electronic devices of the video sources communicate with the network interface device directly via the same sub network A that couples devices and . In some implementations e.g. in the network C both the client device and the electronic devices of the video sources communicate directly via the network s without passing the network interface device or the hub device .

In some implementations during normal operation the network interface device and the hub device communicate with each other to form a network gateway through which data are exchanged with the electronic device of the video sources . As explained above the network interface device and the hub device optionally communicate with each other via a sub network A.

In some implementations the hub device is omitted and the functionality of the hub device is performed by the hub device server system video server system or smart home provider server system .

In some implementations the hub device server system is or includes a dedicated video processing server. illustrates a representative operating environment in which a video server system serves as a dedicated video processing server and provides data processing for monitoring and facilitating review of alert events e.g. motion events in video streams captured by video cameras . As shown in the video server system receives video data from video sources including cameras located at various physical locations e.g. inside homes restaurants stores streets parking lots and or the smart home environments of . Each video source may be bound to one or more user e.g. reviewer accounts and the video server system provides video monitoring data for the video source to client devices associated with the reviewer accounts. For example the portable electronic device is an example of the client device .

In some implementations the smart home provider server system or a component thereof serves as the video server system the video server system is a part or component of the smart home provider server system . In some implementations the video server system is separate from the smart home provider server system and provides video processing services to video sources and client devices independent of other services provided by the smart home provider server system . In some implementations the smart home provider server system and the video server system are separate but communicate information with each other to provide functionality to users. For example a detection of a hazard may be communicated by the smart home provider server system to the video server system and the video server system in accordance with the communication regarding the detection of the hazard records processes and or provides video associated with the detected hazard.

In some implementations each of the video sources includes one or more video cameras that capture video and send the captured video to the video server system substantially in real time. In some implementations each of the video sources optionally includes a controller device not shown that serves as an intermediary between the one or more cameras and the video server system . The controller device receives the video data from the one or more cameras optionally performs some preliminary processing on the video data and sends the video data to the video server system on behalf of the one or more cameras substantially in real time. In some implementations each camera has its own on board processing capabilities to perform some preliminary processing on the captured video data before sending the processed video data along with metadata obtained through the preliminary processing to the controller device and or the video server system .

In some implementations a camera of a video source captures video at a first resolution e.g. 720P and or 1080P and or a first frame rate 24 frames per second and sends the captured video to the video server system at both the first resolution e.g. the original capture resolution s the high quality resolution s and the first frame rate and a second different resolution e.g. 180P and or a second frame rate e.g. 5 frames per second or 10 frames per second . For example the camera captures a video at 720P and or 1080P resolution the camera may capture a video at 1080P and create a downscaled 720P version or capture at both 720P and 1080P . The video source creates a second or third rescaled and optionally at a different frame rate than the version version of the captured video at 180P resolution and transmits both the original captured version i.e. 1080P and or 720P and the rescaled version i.e. the 180P version to the video server system for storage. In some implementations the rescaled version has a lower resolution and optionally a lower frame rate than the original captured video. The video server system transmits the original captured version or the rescaled version to a client depending on the context. For example the video server system transmits the rescaled version when transmitting multiple videos to the same client device for concurrent monitoring by the user and transmits the original captured version in other contexts. In some implementations the video server system downscales the original captured version to a lower resolution and transmits the downscaled version.

In some other implementations a camera of a video source captures video at a first resolution e.g. 720P and or 1080P and or a first frame rate and sends the captured video to the video server system at the first resolution e.g. the original capture resolution s the high quality resolution s such as 1080P and or 720P and the first fame rate for storage. When the video server system transmits the video to a client device the video server system may downscale the video to a second lower resolution e.g. 180P and or second lower frame rate for the transmission depending on the context. For example the video server system transmits the downscaled version when transmitting multiple videos to the same client device for concurrent monitoring by the user and transmits the original captured version in other contexts.

As shown in in accordance with some implementations each of the client devices includes a client side module . The client side module communicates with the video server system through the one or more networks . In some implementations the video server system includes a video server a client interface server and a camera interface server . In some implementations the video server includes the server side module and its components and modules or one or more respective components and or modules of the server side module . The client side module provides client side functionalities for the event monitoring and review processing and communications with the video server system . The video server system provides server side functionalities for event monitoring and review processing for any number of client side modules each residing on a respective client device . The video server system also provides server side functionalities for video processing and camera control for any number of the video sources including any number of control devices and the cameras .

In some implementations the video server includes one or more processors a video storage database and device and account databases . In some implementations the video server system also includes a client interface server and a camera interface server . The client interface server provides an I O interface to one or more client devices and the camera interface server provides an I O interface to one or more video sources . The client interface server facilitates the client facing input and output processing for the video server system . For example the client interface server generates web pages for reviewing and monitoring video captured by the video sources in a web browser application at a client . In some implementations the client interface server or a transcoding proxy computer rescales e.g. downscales and or changes the frame rate of video for transmission to a client . In some implementations the client interface server also serves as the transcoding proxy. The databases store a plurality of profiles for reviewer accounts registered with the video processing server where a respective user profile includes account credentials for a respective reviewer account and one or more video sources linked to the respective reviewer account. The camera interface server facilitates communications with one or more video sources e.g. groups of one or more cameras and associated controller devices . The video storage database stores raw video data received from the video sources as well as various types of metadata such as motion events event categories event category models event filters event masks alert events and camera histories for use in data processing for event monitoring and review for each reviewer account.

In some implementations the video server system receives information regarding alert events detected by other smart devices e.g. hazards sound vibration motion. In accordance with the alert event information the video server system instructs one or more video sources in the smart home environment where the alert event is detected to capture video and or associate with the alert event video received from the video sources in the same smart home environment that is contemporaneous or proximate in time with the alert event.

Examples of a representative client device include but are not limited to a handheld computer a wearable computing device a personal digital assistant PDA a tablet computer a laptop computer a desktop computer a cellular telephone a smart phone an enhanced general packet radio service EGPRS mobile phone a media player a navigation device a game console a television a remote control a point of sale POS terminal vehicle mounted computer an ebook reader or a combination of any two or more of these data processing devices or other data processing devices. For example client devices and are a smart phone a tablet computer and a laptop computer respectively.

Examples of the one or more networks include local area networks LAN and wide area networks WAN such as the Internet. The one or more networks are optionally implemented using any known network protocol including various wired or wireless protocols such as Ethernet Universal Serial Bus USB FIREWIRE Long Term Evolution LTE Global System for Mobile Communications GSM Enhanced Data GSM Environment EDGE code division multiple access CDMA time division multiple access TDMA Bluetooth Wi Fi voice over Internet Protocol VoIP Wi MAX or any other suitable communication protocol.

In some implementations the video server system is implemented on one or more standalone data processing apparatuses or a distributed network of computers. In some implementations the video server the client interface server and the camera interface server are each respectively implemented on one or more standalone data processing apparatuses or a distributed network of computers. In some implementations the video server system also employs various virtual devices and or services of third party service providers e.g. third party cloud service providers to provide the underlying computing resources and or infrastructure resources of the video server system . In some implementations the video server system includes but is not limited to a handheld computer a tablet computer a laptop computer a desktop computer or a combination of any two or more of these data processing devices or other data processing devices.

The server client environment shown in includes both a client side portion e.g. the client side module and a server side portion e.g. the components and modules in the video server system . The division of functionalities between the client and server portions of operating environment can vary in different implementations. Similarly the division of functionalities between the video source and the video server system can vary in different implementations. For example in some implementations client side module is a thin client that provides only user facing input and output processing functions and delegates all other data processing functionalities to a backend server e.g. the video server system . Similarly in some implementations a respective one of the video sources is a simple video capturing device that continuously captures and streams video data to the video server system with no or limited local preliminary processing on the video data. Although many aspects of the present technology are described from the perspective of the video server system the corresponding actions performed by the client device and or the video sources would be apparent to ones skilled in the art without any creative efforts. Similarly some aspects of the present technology may be described from the perspective of the client device or the video source and the corresponding actions performed by the video server would be apparent to ones skilled in the art without any creative efforts. Furthermore some aspects of the present technology may be performed by the video server system the client device and the video sources cooperatively.

It should be understood that operating environment that involves the video server system the video sources and the video cameras is merely an example. Many aspects of operating environment are generally applicable in other operating environments in which a server system provides data processing for monitoring and facilitating review of data captured by other types of electronic devices e.g. smart thermostats smart hazard detectors smart doorbells smart wall plugs appliances and the like .

The electronic devices the client devices or the server system communicate with each other using the one or more communication networks . In an example smart home environment two or more devices e.g. the network interface device the hub device and the client devices are located in close proximity to each other such that they could be communicatively coupled in the same sub network A via wired connections a WLAN or a Bluetooth Personal Area Network PAN . The Bluetooth PAN is optionally established based on classical Bluetooth technology or Bluetooth Low Energy BLE technology. This smart home environment further includes one or more other radio communication networks B through which at least some of the electronic devices of the video sources exchange data with the hub device . Alternatively in some situations some of the electronic devices of the video sources communicate with the network interface device directly via the same sub network A that couples devices and . In some implementations e.g. in the network C both the client device and the electronic devices of the video sources communicate directly via the network s without passing the network interface device or the hub device .

In some implementations during normal operation the network interface device and the hub device communicate with each other to form a network gateway through which data are exchanged with the electronic device of the video sources . As explained above the network interface device and the hub device optionally communicate with each other via a sub network A.

In some implementations a video source may be private e.g. its captured videos and history are accessible only to the associated user account public e.g. its captured videos and history are accessible by anyone or shared e.g. its captured videos and history are accessible only to the associated user account and other specific users accounts with whom the associated user has authorized access e.g. by sharing with the other specific users . Whether a video source is private public or shared is configurable by the associated user.

In some implementations the camera also performs preliminary motion detection on video captured by the camera . For example the camera analyzes the captured video for significant changes in pixels. When motion is detected by the preliminary motion detection the camera transmits information to the hub device server system or video server system informing the server system of the preliminary detected motion. The hub device server system or video server system in accordance with the information of the detected motion may activate sending of a motion detection notification to a client device log the preliminary detected motion as an alert event and or perform additional analysis of the captured video to confirm and or classify the preliminary detected motion.

The hub device optionally includes one or more built in sensors not shown including for example one or more thermal radiation sensors ambient temperature sensors humidity sensors IR sensors occupancy sensors e.g. using RFID sensors ambient light sensors motion detectors accelerometers and or gyroscopes.

The radios enables one or more radio communication networks in the smart home environments and allows a hub device to communicate with smart devices. In some implementations the radios are capable of data communications using any of a variety of custom or standard wireless protocols e.g. IEEE 802.15.4 Wi Fi ZigBee 6LoWPAN Thread Z Wave Bluetooth Smart ISA100.11a WirelessHART MiWi etc. custom or standard wired protocols e.g. Ethernet HomePlug etc. and or any other suitable communication protocol including communication protocols not yet developed as of the filing date of this document.

Communication interfaces include for example hardware capable of data communications using any of a variety of custom or standard wireless protocols e.g. IEEE 802.15.4 Wi Fi ZigBee 6LoWPAN Thread Z Wave Bluetooth Smart ISA100.11a WirelessHART MiWi etc. and or any of a variety of custom or standard wired protocols e.g. Ethernet HomePlug etc. or any other suitable communication protocol including communication protocols not yet developed as of the filing date of this document.

Memory includes high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and optionally includes non volatile memory such as one or more magnetic disk storage devices one or more optical disk storage devices one or more flash memory devices or one or more other non volatile solid state storage devices. Memory or alternatively the non volatile memory within memory includes a non transitory computer readable storage medium. In some implementations memory or the non transitory computer readable storage medium of memory stores the following programs modules and data structures or a subset or superset thereof 

Each of the above identified elements e.g. modules stored in memory of hub device may be stored in one or more of the previously mentioned memory devices e.g. the memory of any of the smart devices in smart home environment and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

Video data stored in the video storage database includes high quality versions and low quality versions of videos associated with each of the video sources . High quality video includes video in relatively high resolutions e.g. 720P and or 1080P and relatively high frame rates e.g. 24 frames per second . Low quality video includes video in relatively low resolutions e.g. 180P and relatively low frame rates e.g. 5 frames per second 10 frames per second .

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

In some implementations at least some of the functions of the video server client interface server and camera interface server are performed by the hub device server system and the corresponding modules and sub modules of these functions may be included in the hub device server system . In some implementations at least some of the functions of the hub device server system are performed by the video server client interface server and or camera interface server and the corresponding modules and sub modules of these functions may be included in the video server client interface server and or camera interface server .

Memory includes high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and optionally includes non volatile memory such as one or more magnetic disk storage devices one or more optical disk storage devices one or more flash memory devices or one or more other non volatile solid state storage devices. Memory optionally includes one or more storage devices remotely located from one or more processing units . Memory or alternatively the non volatile memory within memory includes a non transitory computer readable storage medium. In some implementations memory or the non transitory computer readable storage medium of memory stores the following programs modules and data structures or a subset or superset thereof 

Video data cache includes cached video image data for respective cameras associated with a user of the client device . For example as shown in the video data cache includes cached video image data for a first camera cached video image data for a second camera up to cached video image data for a p th camera. At a given moment video data cache may not have cached video image data for a given camera e.g. due to the camera being newly associated with the user due to the cache being cleared due to the cached video image data being expired and removed from the cache .

Blurred image data includes sets of progressively blurred images for respective cameras. For example as shown in the blurred image data includes blurred image data e.g. a set of progressively blurred images for the first camera blurred image data for the second camera up to blurred image data for the p th camera.

In some implementations the client device caches camera history as well as video data . For example whenever the client device receives camera events history data from the video server the most recent camera events history e.g. history from the past two hours the most recent 20 events is cached at the client device e.g. in client data . This cached history data may be accessed for quick display of camera history information e.g. in user interface .

In some implementations the client side module and user interface module are parts modules or components of a particular application e.g. a smart home management application .

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures modules or data structures and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

In some implementations at least some of the functions of the hub device server system or the video server system are performed by the client device and the corresponding sub modules of these functions may be located within the client device rather than the hub device server system or video server system . In some implementations at least some of the functions of the client device are performed by the hub device server system or video server system and the corresponding sub modules of these functions may be located within the hub device server system or video server system rather than the client device . The client device and the hub device server system or video server system shown in respectively are merely illustrative and different configurations of the modules for implementing the functions described herein are possible in various implementations.

The built in sensors include for example one or more thermal radiation sensors ambient temperature sensors humidity sensors IR sensors occupancy sensors e.g. using RFID sensors ambient light sensors motion detectors accelerometers and or gyroscopes.

The radios enable one or more radio communication networks in the smart home environments and allow a smart device to communicate with other devices. In some implementations the radios are capable of data communications using any of a variety of custom or standard wireless protocols e.g. IEEE 802.15.4 Wi Fi ZigBee 6LoWPAN Thread Z Wave Bluetooth Smart ISA100.11a WirelessHART MiWi etc. custom or standard wired protocols e.g. Ethernet HomePlug etc. and or any other suitable communication protocol including communication protocols not yet developed as of the filing date of this document.

Communication interfaces include for example hardware capable of data communications using any of a variety of custom or standard wireless protocols e.g. IEEE 802.15.4 Wi Fi ZigBee 6LoWPAN Thread Z Wave Bluetooth Smart ISA100.11a WirelessHART MiWi etc. and or any of a variety of custom or standard wired protocols e.g. Ethernet HomePlug etc. or any other suitable communication protocol including communication protocols not yet developed as of the filing date of this document.

Memory includes high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and optionally includes non volatile memory such as one or more magnetic disk storage devices one or more optical disk storage devices one or more flash memory devices or one or more other non volatile solid state storage devices. Memory or alternatively the non volatile memory within memory includes a non transitory computer readable storage medium. In some implementations memory or the non transitory computer readable storage medium of memory stores the following programs modules and data structures or a subset or superset thereof 

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

Communication interfaces include for example hardware capable of data communications using any of a variety of custom or standard wireless protocols e.g. IEEE 802.15.4 Wi Fi ZigBee 6LoWPAN Thread Z Wave Bluetooth Smart ISA100.11a WirelessHART MiWi etc. and or any of a variety of custom or standard wired protocols e.g. Ethernet HomePlug etc. or any other suitable communication protocol including communication protocols not yet developed as of the filing date of this document.

Memory includes high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and optionally includes non volatile memory such as one or more magnetic disk storage devices one or more optical disk storage devices one or more flash memory devices or one or more other non volatile solid state storage devices. Memory or alternatively the non volatile memory within memory includes a non transitory computer readable storage medium. In some implementations memory or the non transitory computer readable storage medium of memory stores the following programs modules and data structures or a subset or superset thereof 

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above. Additionally camera being an example of a smart device optionally includes components and modules included in smart device as shown in that are not shown in .

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various implementations. In some implementations memory optionally stores a subset of the modules and data structures identified above. Furthermore memory optionally stores additional modules and data structures not described above.

Furthermore in some implementations the functions of any of the devices and systems described herein e.g. hub device hub device server system video server system client device smart device camera smart home provider server system are interchangeable with one another and may be performed by any other devices or systems where the corresponding sub modules of these functions may additionally and or alternatively be located within and executed by any of the devices and systems. As one example generating of user interfaces may be performed by the user interface module which may be located at the client interface server or at the video server or by the user interface module depending on whether the user is accessing the video feeds and corresponding histories through a web browser or an application e.g. a dedicated smart home management application at the client device . The devices and systems shown in and described with respect to are merely illustrative and different configurations of the modules for implementing the functions described herein are possible in various implementations.

The menu and settings objects and when activated by the user provides access to an options menu or interface and a settings menu or interface for the smart home application respectively. In some implementations the menu object is displayed as a line hamburger menu icon e.g. as shown in and the settings icon is displayed as a gear icon e.g. as shown in . The mode icon and label indicates the operating mode of the smart home environment with which the information displayed in user interface is associated e.g. the user s home . For example the smart home environment may operate in a Home mode and an Away mode. In the Home mode the user is presumed to be in the smart home environment e.g. within the structure . In the Away mode the user is presumed to be remote from the smart home environment . Smart devices may operate differently in the Home mode than in the Away mode and certain notifications of events may be elided when in Home mode. The user may access a menu to change the mode by activating the mode icon e.g. by tapping on the mode icon with a single tap gesture .

Thermostat objects A and B correspond to respective smart thermostats in the smart home environment and display the current detected temperatures and or the set temperatures at the corresponding smart thermostats . Protect object provides access to a history of alert events e.g. detected hazards detected sounds detected vibrations operation of smart door lock etc. associated with the smart home environment . The user accesses the Protect history by activating the Protect object e.g. by tapping on the Protect object on the touch screen .

The camera objects A B and C correspond to respective video sources or more particularly respective cameras within the smart home environment . The labels A B and C indicate the respective video sources to which the respective camera objects correspond. For example the camera object A corresponds to a camera labeled Outside A.

Within a respective camera object a view of a video feed or stream from the corresponding camera is displayed. For example a view of the video feed from the Outside A camera is displayed in camera object A a view of the video feed from the Front door B camera is displayed in camera object B and a view of the video feed from the Dining room C camera is displayed in camera object C. In some implementations the view of a video feed is displayed in a camera object as a real time or near real time live video stream from the corresponding camera or as periodically refreshed e.g. at a rate less than typical frame rates for video still images. In some implementations the view is displayed at a resolution different from the original resolution and or frame rate in which the video was captured. For example the video views displayed in the camera objects are displayed at an 180P 180 horizontal lines progressive scan resolution and at a frame rate of 5 or 10 frames per second which is different from the original capture resolution e.g. 720P or 1080P and the original frame rate.

In some implementations the view displayed in a camera object is cropped from the original video to fit the size and shape of the camera object and the cropping is positioned to focus on a particular portion of the video for display. For example view is cropped to view to fit into circle shaped object A view is cropped to view to fit into circle shaped object B and view is cropped to view to fit into circle shaped object C. The cropping and focus is further illustrated in . illustrates the camera objects and the frame widths and heights scaled relative to the camera objects of the videos from the corresponding to cameras corresponding to the camera objects . As shown frame A corresponding to the video from the Outside A camera is relatively larger than camera object A frame B corresponding to the video from the Front door B camera is relatively larger than camera object B and frame C corresponding to the video from the Dining room C camera is relatively larger than camera object C. Frame A is cropped to view for display in A. Frame B is cropped to view for display in B. Frame C is cropped to view for display in C. The cropping may be adjusted by the smart home application e.g. the camera view module or by the hub device server system or video server system to a different position along the span of the video frame which puts a different portion of the frame into display. In some implementations the video frame is zoomed so that the height of the zoomed frame matches the height of the object and the object is centered relative to the frame i.e. the cropping is centered to display the center portion of the frame .

It should be appreciated that while the camera objects are shown as circular in the drawings the camera objects may be in other shapes e.g. square rectangle etc. or each camera object may have a distinct shape e.g. one camera object has a circular shape another camera object has a square shape and so on .

Additionally as shown in views and do not include the entirety of the respective frames but respective portions thereof. For example view includes an upper portion of the frame A. As will be described below the user may change the view to include a different portion of the corresponding frame. In some other implementations a video feed is displayed without cropping in a camera object the camera object is shaped and sized accordingly.

Continuing in a user may adjust the views and displayed in the camera objects by performing a user input such as a gesture on the touch screen e.g. a swipe gesture with contact e.g. finger contact across the touch screen just below the camera objects or a change in the orientation of the client device e.g. rotating the client device about a vertical axis . In response to the gesture with contact or to the orientation change the views and in camera objects change to views and respectively as shown in . In some implementations changing the view includes shifting the cropped area s displayed in the camera object s to different portion s of the corresponding frame s the view change pans the cropped area to another portion of the frame. For example view showed three cacti from the frame A. With the view change the view has shifted left relative to frame A to become view which shows just two cacti. It should be appreciated that while show a horizontal view change in response to a horizontal gesture or horizontal orientation change vertical and or diagonal view changes in response to other particular gestures or orientation changes are possible.

In some implementations after the view change in response to the gesture or orientation change the views stay in their post change states i.e. as and respectively even when the user input triggering the change is terminated e.g. the contact is no longer detected on the touch screen the orientation of client device stops changing e.g. as shown in . In some implementations the user may perform an input e.g. shaking the client device detected by the accelerometer a predefined gesture e.g. double tap or a tap and hold in an area in user interface near and away from the camera objects a voice command to force the smart home application to reset the changed views back to views and . In some implementations the user may perform an input e.g. single tap gesture a voice command on a camera object to selectively reset the view displayed in that camera object back to the pre change state while the remainder of the views remain in their post change states.

In some other implementations after the view change in response to the gesture or orientation change the views and automatically return or reset to their pre change states i.e. return to and respectively even when the user input triggering the change is terminated e.g. the contact is no longer detected on the touch screen the orientation of client device stops changing . For example the views and automatically return to views and respectively after a delay e.g. 3 seconds after the contact is lifted off the touch screen to complete the gesture . In some implementations to prevent the automatic return to the pre change views the user may perform another user input during the delay period e.g. a single tap gesture with contact near where contact was last detected . In accordance with the another user input the views and are maintained. In some implementations the user may perform an input e.g. single tap gesture a voice command on a camera object to selectively maintain the view displayed in that camera object in the post change state while the remainder of the views return or reset to their pre change states.

Continuing in a user may change the data usage of the views and displayed in the camera objects . In a gesture on the touch screen e.g. a swipe gesture with contact across the touch screen over the camera objects . In response to the gesture the smart home application displays user interface objects e.g. button icon etc. and in the user interface . User interface object is an affordance corresponding to a live stream mode and user interface object is an affordance corresponding to a data save mode. In live stream mode the views and are displayed in the camera objects as live video streams. In data save mode the views and are displayed in the camera objects as still images e.g. frames extracted from the corresponding videos refreshing periodically at rates less than the refresh rate or frame rate for the live stream video e.g. if refresh frame rate in live stream mode is 5 or 10 frames per second the refresh frame rate in data save mode may be for example 1 frame per second 1 frame per two seconds or 1 frame per 30 seconds . As shown in the live stream mode is active as indicated by the user interface object with Live stream in bold. In some implementations a user interface object corresponding to a toggle affordance for toggling between live stream mode and data save mode is displayed in lieu of displaying a user interface object for each mode.

While the user interface objects and are displayed the user may select the user interface object corresponding to the non active mode to change the views and to that mode. For example in a gesture e.g. a tap gesture with contact is detected on user interface object corresponding to the data save mode. In response to the selection of the user interface object the views and are displayed in camera objects in data save mode i.e. as periodically refreshed images. In some implementations when a view is displayed in data save mode a timer icon is displayed near the corresponding label to indicate that the view is in data save mode and also indicate a time until the next image refresh. For example a timer icon is displayed next to the Outside A label corresponding to view as shown in indicating that the view is displayed as periodically refreshed images. Similarly as shown in a timer icon is displayed next to the Front door B label corresponding to view and next to the Dining room C label corresponding to view indicating that views and are displayed in data save mode.

The user may want to adjust the view s displayed in particular camera objects rather than adjusting all of the views. In some implementations the smart home application facilitates selection of particular camera objects by the user. illustrates a selection input e.g. a single tap gesture with contact detected on object A. In response to the selection gesture camera object A is selected as illustrated in by the thick border around camera object A.

Multiple camera objects may be selected as shown in . After camera object A is selected a selection input e.g. a single tap gesture with contact is detected on camera object B. In response to that selection gesture camera object B is selected and camera object A remains selected as illustrated in .

After one or more camera objects are selected a user input to adjust the views in the selected camera objects may be performed. For example a swipe gesture e.g. swipe gesture just below the camera objects with contact or a change in the orientation of the client device may be detected. In response to the user input the views and in camera objects A and B respectively are changed to views and respectively. View displayed in the not selected camera object C is maintained.

In some implementations after the view change in response to the gesture or orientation change the selected camera objects are deselected and the changed views stay in their post change states i.e. as and respectively even when the user input triggering the change is terminated e.g. the contact is no longer detected on the touch screen the orientation of client device stops changing . In some implementations the user may perform an input e.g. shaking the client device detected by the accelerometer a predefined gesture e.g. double tap or a tap and hold in an area in user interface near and away from the camera objects a voice command to force the smart home application to reset the changed views back to views and . In some implementations the user may perform an input e.g. single tap gesture a voice command on a camera object to selectively reset the changed view displayed in that camera object back to the pre change state while the remainder of the changed views remain in their post change states.

In some other implementations after the view change in response to the gesture or orientation change the selected camera objects are deselected and the views and automatically return or reset to their pre change states i.e. return to and respectively even when the user input triggering the change is terminated e.g. the contact is no longer detected on the touch screen the orientation of client device stops changing . For example the views and automatically return to views and respectively after a delay e.g. 3 seconds after the contact is lifted off the touch screen to complete the gesture . In some implementations to prevent the automatic return to the pre change views for particular ones of the selected camera objects the user may perform one or more user inputs during the delay period on the camera object s with the views the user wishes to maintain in the post change state. For example in a single tap gesture is detected on camera object B with contact . In accordance with one or more user inputs during the delay period the selected camera objects are deselected the view is maintained and the view resets to view as shown in .

While one or more of the camera objects are selected the display mode of the views in the selected camera objects may be changed. shows camera objects A and B selected. A gesture e.g. a swipe gesture over the camera objects with contact is detected on the touch screen . In response to the gesture the smart home application displays user interface objects e.g. button icon etc. and in the user interface . As shown in live stream mode is active as indicated by the user interface object with Live stream in bold the views displayed in selected camera objects A and B are displayed in live stream mode.

While the user interface objects and are displayed the user may select the user interface object corresponding to the non active mode to change the views displayed in the selected camera objects A and B to that mode. For example in a gesture e.g. a tap gesture with contact is detected on user interface object corresponding to the data save mode. In response to the selection of the user interface object the views and are displayed in camera objects A and B respectively in data save mode i.e. as periodically refreshed images. A timer icon is displayed next to the Outside A label corresponding to view and next to the Front door label B corresponding to view as shown in indicating that the views and are displayed as periodically refreshed images.

In some implementations a user may freeze a video feed displayed in a camera object . illustrates a freeze gesture e.g. a touch and hold gesture with contact detected on camera object B. In response to the freeze gesture the view is frozen or locked to a frozen view corresponding to a frame of the corresponding video view stops streaming or refreshing. A lock icon is displayed next to the Front door B label corresponding to the camera object B indicating that the view is frozen as shown in . To end the view freezing the user can perform a tap and hold gesture on the camera object B i.e. repeating the gesture shown in .

Each of the camera objects are associated with a respective camera and its corresponding video feed. A user may perform a gesture on one of the camera objects to access a user interface that shows the video corresponding to that camera object uncropped. For example in a single tap gesture is detected on the camera object C with the contact . In response to the gesture user interface is displayed on the touch screen replacing user interface as shown in . User interface includes a camera name e.g. indicating the camera for which a video feed is shown in the interface backtracking affordance to backtrack to a previous user interface e.g. back to user interface settings affordance a live video indicator camera on off switch and video region . Within the video region the video feed from the Dining room C camera corresponding to the tapped upon camera object C is displayed without cropping and at a higher resolution and or higher frame rate than the resolution the video is displayed at within the camera object C e.g. at the original capture resolution of 720P or 1080P at 24 frames per second rather than the 180P at 5 or 10 frames per second used for display in the camera object C .

The user interface also includes various controls such as previous clip affordance A for skipping to a chronologically preceding video clip captured by the Dining room C camera rewind affordance B for rewinding back in the video shown in the video region fast forward affordance C for fast forwarding in the video shown in the video region next clip affordance D for skipping to a chronologically succeeding video clip captured by the Dining room C camera and live affordance E for jumping directly to the live video stream from the captured by the Dining room C camera. The user interface also includes a talk affordance for initiating voice functionality that includes voice input from the user that will be output by the Dining room C camera an enhance affordance for initiating enhancement of the video displayed in the video region a quality indicator affordance for switching or toggling the video quality e.g. resolution and or frame rate of the video displayed in the video region and history affordance for accessing a history of camera events for the Dining room C camera. In response to a gesture on the history affordance e.g. a single tap gesture on history affordance with contact a camera history user interface is displayed. Further details regarding the camera history user interface are described below in relation to .

In some implementations the views in the camera objects are displayed with blurring transitions. For example when the smart home application is opened the views in the camera objects are shown with a blurring transition as shown in . The views and gradually de blur or un blur. For example blurred views become progressively less blurry the blurred views that are shown in being one stage in the progressive de blurring toward the views shown in . Additionally when a camera corresponding to a camera object goes offline e.g. that camera lost its Wi Fi connection the view for that camera may progressively blur to a blurry image and then black out.

In some implementations the blurring de blurring includes displaying a set of progressively blurred images e.g. 16 or 24 blurred images from cached video image data for the pertinent camera and displaying these images sequentially. For example when the smart home application is opened and a camera is connected a de blurring transition from a blurred view to an unblurred view involves starting from a blurred cached image from the camera and then displaying in sequence progressively less blurry images in the set of blurred images. Meanwhile as the blurred images are displayed in sequence the client device receives the video feed from the camera and inserts images from the video feed under e.g. in a lower z layer the blurred image. After the last blurred image is displayed the blurred image is removed revealing the updated images from the video feed. The set of progressively blurred images are generated from cached video image data for the camera in some implementations the smart home application caches the most recent e.g. in the last hour video image data from each associated camera at the client device as cached video image data using well known blurring techniques e.g. Gaussian blur bokeh . For example as shown in a set of n e.g. 16 24 blurred images is generated using Gaussian blur techniques. Each blurred image is blurred at a respective blur radius r. As n increases rdecreases. To de blur images thru are displayed in sequence in order of decreasing blur radius and then image is removed revealing updated image from the video feed. To blur the set of blurred images are displayed in order of increasing blur radius. If cached video image data is not available a default image e.g. an image of a shutter is displayed in the camera object and then removed to reveal updated image from the video feed.

In some implementations one or more of the views displayed within the camera objects may be zoomed in or out. The zooming in or out includes zooming in or out the frames of the video feed from which the view is derived and cropping the zoomed in out frames if needed thus deriving the zoomed in out view. A view may be zoomed out to fit the video frames entirely within the camera object without cropping or zoomed in to further focus on a particular portion of the frames.

In some implementations the sizes of the camera objects are static the camera objects have a fixed size. In some other implementations the camera objects have sizes that may change. For example the hub device server system or video server system analyzes the video feeds from the cameras to detect motion activity occurring in the video feeds. The camera object corresponding to the video feed with the most detected activity e.g. currently detected activity historically detected activity over predefined period of time e.g. within the last hour within the last 12 hours within the last 24 hours etc. is displayed at a larger size than the other camera objects . In some implementations the sizes of camera objects are individually configurable by the user e.g. in a settings interface accessible from the settings affordance . For example the user can configure the camera objects corresponding to cameras whose video feeds he wishes to give more attention to be larger. In some implementations the size of a camera object is automatically configured by the smart home application based on for example how many times the user has accessed the video feed corresponding to the camera object e.g. by tapping on the camera object and opening the interface for the corresponding video feed as in or if a new alert event has been detected for the corresponding camera . For example a camera object may be enlarged when an alert event associated with the corresponding camera has been detected and the camera object remains enlarged until the user views the video associated with the alert event or after a predefined amount of time elapsing after the alert event detection.

In some implementations the user interfaces illustrated in or variations thereof may be displayed on other devices e.g. on a smart home application running on a tablet computer and applications e.g. in a web browser running on a desktop or laptop computer .

Outside of the video region the user interface includes additional user interface objects and information. For example the user interface includes a timeline for displaying camera events and their corresponding times and durations a calendar affordance to jump to a particular calendar day in the timeline a time scale selector for selecting a level of detail in the timeline arrows and for scrolling backward and forward in the timeline respectively an activity zone affordance for accessing a list of alert event types and user defined zones of interest for filtering the timeline by alert event type and or zone of interest as well as options to create or edit a zone of interest in the area monitored by the associated camera and video clip affordance for accessing a user interface for creating user custom video clips from video captured by the associated camera. The defining of zones of interest is described in the following U.S. patent applications filed on Oct. 8 2014 which are incorporated by reference herein in their entirety Ser. Nos. 14 509 999 14 510 050 14 510 015 14 510 029 14 510 007 14 510 040 14 510 030 14 510 042 and 14 510 059.

The timeline shows a chronology of camera events associated with the associated camera. A camera event includes a corresponding video one or more alert events that are associated with the video and the chronology of the associated alert events. The alert events may be any event detected in the smart home environment by a smart device e.g. detected hazard detected sound detected vibration operation of a smart door lock detected motion etc. the smart home environment is configured to log and or alert the user of detection of such events. In some implementations alert events include motion events detected non visually e.g. detected by motion detectors as well as motion events detected through video captured by a camera e.g. motion through or in a user defined zone of interest . Detection of motion activity in a zone of interest is described in the following U.S. patent applications filed on Oct. 8 2014 which were incorporated by reference above Ser. Nos. 14 509 999 14 510 050 14 510 015 14 510 029 14 510 007 14 510 040 14 510 030 14 510 042 and 14 510 059. In some implementations each zone of interest is its own type of alert event motion detected in one zone of interest and motion detected in another zone of interest are considered different types of alert events.

In some implementations when an alert event is detected one or more cameras proximate to the detected event or proximate to the smart devices that detected the event are instructed to capture video so as to capture a visual state of one or more areas proximate in location to and contemporaneous or otherwise proximate in time with the detected alert event. The alert event is associated with the captured video.

The smart devices may detect concurrent overlapping or sequenced alert events. Any two alert events that are in sequence with a less than a threshold amount of time e.g. 2 seconds between them concurrent or overlapping are associated with the same camera event. Thus a camera event and its corresponding video may be associated with multiple alert events e.g. detected sound and vibration at same time motion detected across multiple zones of interest in sequence .

A time marker is displayed on the timeline . The time marker indicates the time in focus on the timeline and in the video region . In the time marker is displayed at the rightmost end of the timeline at a position between the solid portion and the dotted portion. The dotted portion indicates future time and the solid portion includes past time up to the present. In some implementations past time on the timeline in which the camera was off may also be represented as a dotted portion on the timeline . Thus the time marker positioned as shown in is marking the current time.

Camera events are represented on the timeline by bars displayed over e.g. overlaid on the timeline . Each bar has a length reflecting the duration of the camera event. For example the camera event A is longer than the camera event B. In some implementations the duration of a camera event is from the start of the earliest alert event in the camera event to the end of the last alert event in the camera event.

It should be appreciated that camera events may or may not be displayed as bars depending on the fineness of the time scale of the timeline . For example camera events that are too short in duration to be displayed as bars for a particular time scale e.g. a 5 second camera event at the hours scale may be displayed as a shaped dot on the timeline .

In some implementations one or more icons corresponding to types of alert events are displayed in or near respective camera event bars to indicate the alert events associated with the respective alert events. For example icons A B and C are displayed in camera event bar A and icons A B and C and D are displayed in camera event bar B. Each icon corresponds to a respective type of alert event and visually distinct from each other. In some implementations the visual distinction is based on shape. For example as shown in icon A is a circle icon B is a hexagon icon C is a triangle and icon D is a square. In some other implementations the visual distinction is based on color. For example the icons may be circles of different colors.

In some implementations the shape or color definitions for the icons may be automatically defined and or user defined. For example shapes or colors for icons corresponding to predefined alert event types e.g. hazard sound vibration non visual motion are defined according to a default scheme and shapes or colors for icons corresponding to user defined zones of interest are defined according to the default scheme or user definition.

In some implementations if multiple instances of a particular type of alert event were detected during a camera event the icon corresponding to that particular type is displayed just once within the camera event bar .

In some implementations the icons displayed within a camera event bar are ordered within the camera event bar . In some implementations the ordering is based on the chronological order of the alert events in the camera event. For example in the icons within a camera event bar are ordered from left to right with the further right icons within the camera event bar corresponding to alert events more recent in time. In camera event A there may be one or more multiple instances of the alert type of the type corresponding to icon C detected but the most recent instance of that type is also the most recent alert event detected within the corresponding camera event and thus the icon C is displayed in the right most position. In some other implementations the icons displayed within a camera event are randomly ordered.

In some implementations the icons within a camera event bar are ordered based on the chronological order of the most recent instances of each detected type of alert event as just one icon is displayed for each type of alert event detected. Within the camera event bar A an instance of the alert event of the type corresponding to icon C is the most recent alert event for the corresponding camera event and is more recent than the most recent instance of the alert event type corresponding to icon B detected for the corresponding camera event. As another example within the camera event bar B the most recent instance of the alert event type corresponding to icon D is more recent than the most recent instance of the alert event type corresponding to icon A and thus icon D is displayed to the right of icon A.

In some implementations if alert event type icons are distinguished based on color and a camera event includes just one alert event type then the corresponding camera event bar may be displayed with the color corresponding to the alert event type.

A user may click on e.g. with a mouse or tap on e.g. with a contact on a touch screen or hover over e.g. with a mouse pointer a camera event to view additional information about the camera event. For example in a mouse pointer is hovered over camera event bar A. In response to the hovering mouse pointer an information pop up for the camera event A is displayed. The information pop up includes a thumbnail of the video associated with the camera event A date and time information for the camera event and icons ordered chronologically in same manner as the icons within camera event bar A corresponding to alert event types detected for the camera event A. In some implementations the thumbnail is the video corresponding to the camera event A played back at the thumbnail size. In some other implementations the thumbnail is a still image e.g. a frame from the video corresponding to the camera event A.

In some implementations the time marker may be moved e.g. dragged along the timeline by the user to scrub the timeline and manually jump to a desired time in the timeline . illustrates an example of scrubbing the timeline where the time marker has been moved to a position over the camera event bar A. In response to the time marker being positioned over the camera bar A a preview bar is displayed. The preview bar includes a chronological sequence of thumbnails of still frames of the video associated with the camera event A. The thumbnail A is the thumbnail of the still frame closest in time to the time corresponding to the timeline position where time marker is positioned. Thumbnails B and C are the next thumbnails after thumbnail A in the chronological sequence. Thumbnails D and E are the previous thumbnails before thumbnail A in the chronological sequence. With each thumbnail the time of the frame corresponding to the thumbnail and icons are displayed. In some implementations the icons displayed for each thumbnail include just the icons corresponding to alert event types for which instances are detected at the same time as the time of the frame corresponding to the thumbnail.

In some implementations the user defined zones of interest may be displayed in the video region over the video feed . For example when affordance is activated a filtering list of user defined zones of interest and alert event types is displayed as well as options to edit and create respectively a zone of interest not shown . The user may select one or more of the zones and alert event types for filtering of the timeline and the camera events therein by the selected zones and alert event types. The user may also select the option to edit a zone. In response to the user selecting the option to edit a zone the defined zones are displayed in the video region while the video feed continues to be played in video region along with a prompt for the user to select a zone for editing. For example shows zones A and B displayed in the video region over the video feed while the video feed continues to play and a prompt for a user to select one of the displayed zones for editing. If zones are associated with respective colors the zones A and B are displayed in their respective associated colors. Zones of interest are described in the following U.S. patent applications filed on Oct. 8 2014 which were incorporated by reference above Ser. Nos. 14 509 999 14 510 050 14 510 015 14 510 029 14 510 007 14 510 040 14 510 030 14 510 042 and 14 510 059.

In some other implementations instead of being an affordance for opening a user interface for filtering zones of interest and alert event types the affordance is an affordance for toggling between showing and not showing zones of interest in the video region aside from any filtering of the timeline or any option edit or create a zone of interest.

As described above icons may be displayed within or near a camera event bar . illustrates an example of icons displayed near e.g. above camera event bars . For example icons A B and C are displayed above camera event bar A and icons B A D and C are displayed above camera event bar B. The icons for a respective camera event bar are ordered in accordance with the same criteria as those described above in relation to . Thus for example for camera event A icon C corresponding to the alert event type with the most recently detected instance.

Each camera event entry which corresponds to a respective camera event associated with the respective camera includes a thumbnail e.g. a still frame from the video associated with the camera event an activity type identifier a timestamp and a duration indicator . For example the camera event entry A has the thumbnail A. The corresponding camera event includes motion activity as indicated by activity type identifier A started at 12 49 PM as indicated by timestamp A and lasted 12 seconds as indicated by duration indicator A . As another example the camera event entry B has the thumbnail B. The corresponding camera event includes motion activity as indicated by activity type identifier B started at 12 10 PM as indicated by timestamp B and lasted 29 seconds as indicated by duration indicator B .

The activity type identifier for a camera event identifies the type of the only or primary alert event detected for the camera event. For example in each of the camera events corresponding to entries A thru I have detected motion activity e.g. detection of motion in a zone of interest or detection of motion through non visual sensors as the only or primary alert event. In some implementations the primary alert event among alert events associated with a respective camera event is the most recent among the alert events. In some other implementations the primary alert event among alert events associated with a respective camera event is the one among the alert events with the longest duration.

In the separator bar the hour indicator identifies the hour of the day identified by date indicator in which the camera events corresponding to the camera event entries displayed highest i.e. closet to the separator bar in the scrollable list at the moment are detected. Hour separator bars each of which includes an hour indicator are displayed to separate camera event entries by hour. For example camera event entries A and B correspond to camera events that occur in the 12 PM hour and camera event entries C D E and F correspond to camera events that occur in the 11 AM hour.

As described above the list of camera event entries is scrollable. For example in upward swipe gesture with contact is detected on the touch screen . In response to the swipe gesture the list of camera event entries is scrolled upward the result of which is illustrated in . Entries D E and F have moved upward entries A B and C have scrolled out of view and entries G H and I have scrolled into view. The hour indicator in separator bar is updated to reflect the hour of day when the camera events corresponding to scrolled up entries D E and F occurred. Hour separator bar A is scrolled out of view and hour separator bar B is scrolled into view. Hour separator bar B identifies the hour day when the camera events corresponding to entries G H and I occurred.

An individual camera event entry may be expanded to display further information about the corresponding camera event. For example illustrates a gesture e.g. a single tap gesture with contact detected on camera event entry E. In response to detecting the gesture on camera event entry E camera event entry E is expanded inline into a video player interface as shown in . The video player interface includes an activity type identifier a timestamp and a duration indicator . The activity type identifier timestamp and duration indicator repeats the information shown in the activity type identifier E timestamp E and duration indicator E for camera event entry E respectively.

The video player interface also displays information on the types of alert events associated with the corresponding camera event i.e. types of alert events including zones of interest detected and associated with the camera event. The alert event type information includes alert event type identifiers and corresponding icons . For example in the video player interface alert event types including zones of interest Table Window and Sound are associated with the camera event corresponding to camera event entry E alert events of the types Table Window and Sound were detected and associated with the corresponding video and the corresponding camera event. As with icons icons may be distinguished by shape or color associated with respective alert event types and zones. If the icons are distinguished based on color the corresponding labels may also be displayed in the corresponding associated colors as well.

The video player interface includes the video associated with the corresponding camera event. In the scene in the video includes a person a window and a table . A playback progress bar represents the full length of the video with the shaded portion representing the playback progress of the video so far.

In some implementations playback of the video is automatically started when the camera event entry E is expanded into the video player interface . In some other implementations playback is manually started playback affordance is displayed over the video as shown in and playback is started by the user performing a gesture e.g. a single tap on the playback affordance . Before the video finishes playback the user may toggle between playing the video and pausing the video by performing a gesture e.g. a single tap on the video . While the video is playing or paused the user may collapse the video player interface back to the camera event entry E by performing a gesture e.g. a single tap on an area in the video player interface outside of the video e.g. single tap gesture with contact outside of video as shown in .

When playback of the video is complete as indicated by the playback progress bar being completely filled in some other implementations the playback progress bar is omitted from display when the playback is complete replay affordance and continue affordance are displayed over the video . The user may perform a gesture e.g. a single tap on the replay affordance to have the video replayed from the start.

In some implementations the user may perform a gesture e.g. a single tap on the continue affordance to replace the user interface with user interface for the associated camera and play the next video from the associated camera in the user interface from where the video left off.

The user may collapse the video player interface back to the camera event entry E by performing a gesture e.g. a single tap on an area in the video player interface outside of the video e.g. single tap gesture with contact outside of video as shown in .

In some implementations the list of camera event entries may be filtered to highlight particular alert event types e.g. particular zones of interest . illustrates a gesture being detected on the filtering affordance . In response to detecting the filtering affordance a filtering menu is displayed over an obscured user interface as shown in . The filtering menu includes one or more filtering criteria a cancel affordance for cancelling filtering and Done affordance for confirming selected filtering criteria and proceeding with filtering.

Each filtering criterion includes an identifier of the corresponding alert event type and icon . In the filtering criteria presented are zones of interest Filter by Zone including the Table A criterion A accompanied by icon A Window B criterion B accompanied by icon B and Motion outside zones D criterion C accompanied by icon C. Motion outside zones corresponds to motion detected outside any of the user defined zones of interest such as Table A and Window B.

In some implementations a user selects a filtering criterion by performing a gesture e.g. a single tap over the desired criterion. For example a single tap gesture with contact is detected over criterion A. In response to detecting the gesture criterion A is checked indicating selection as shown in . The same gesture may be repeated on criterion A to deselect the criterion. The gesture may be performed on other criteria in the menu to select or deselect them as desired. When the user has completed selecting the filtering criteria the user performs a gesture on the Done affordance . For example a single tap gesture with contact is detected on the Done affordance .

In response to detecting the gesture on the Done affordance the selected filtering criteria are applied. For example in the Table A criterion is applied to the camera event entries . When filtering is active the criterion being applied e.g. as identified by alert event type identifier s and or icon s is displayed under the interface title . When filtering is applied camera event entries corresponding to camera events that have the alert event type for which filtering is applied includes indicators of the alert even types being applied. For example entries D E and F include the Table identifier A and corresponding icon A indicating that the camera events corresponding to these entries have motion detected in the Table A zone of interest.

Multiple criteria may be selected for filtering. For example illustrates multiple filtering criteria being applied. These multiple criteria are identified by icons A corresponding to Table A and B corresponding to Window B . Camera event entries corresponding to camera events that have the alert event type for which filtering is applied includes indicators of the alert even types being applied. For example entries D E and F include the Table identifier A and or corresponding icon A indicating that the camera events corresponding to these entries have motion detected in the Table A zone of interest. Entries E F G H and I include the Window identifier B and or corresponding icon B indicating that the camera events corresponding to these entries have motion detected in the Window B zone of interest.

Entries E and F include both icons A and B indicating that the camera events corresponding to these entries have motion detected in the Table A zone of interest and in the Window B zone of interest. In some implementations when multiple filtering criteria are applied and an entry meets more than one of the filtering criteria the icons corresponding to the met criteria are displayed in an order. The order is a chronological order similar to that used for icons chronological order of the most recent instances of each alert event type in question. For example for entry E the most recent instance of motion in the Window B zone corresponding to icon B displayed more to the right is more recent than the most recent instance of motion in the Table A zone corresponding to icon A . For entry F the most recent instance of motion in the Table A zone corresponding to icon A displayed more the right is more recent than the most recent instance of motion in the Window B zone corresponding to icon B .

As described above the filtering criteria are alert even types where respective zones of interest are considered as distinct alert event types. illustrates a filtering menu that includes filtering criteria that includes zones of interest and other types of alert events. In addition to criteria A B and C described above in relation to the filtering menu also includes criteria D E F and G. Criterion D corresponds to the Sound C alert event type. Criterion E corresponds to the Vibration E alert event type. Criterion F corresponds to the Hazard F alert event type. Criterion G corresponds to the Motion G or more particularly motion detected by sensors other than cameras e.g. motion sensors alert event type. The criteria may be selected and applied as described above in relation to .

In some other implementations alert event type and or zone labels and icons are displayed for each entry by default even before any filtering. In other words by default each entry is displayed with its associated alert event types and zones information displayed as well. When filtering entries that satisfy the filtering criteria are displayed with all of their associated alert event types and zones i.e. none are omitted and none are specifically highlighted and entries that do not meet the filtering criteria i.e. not associated with at least one alert event type or zone selected for the filtering criteria are not displayed. Thus for example in entries G H and I would not be displayed when filtering by the Table zone.

In some implementations one or more of the functionalities described above in relation to e.g. changing view displayed in camera object accessing camera history user interface etc. may be activated by respective predefined voice inputs or commands.

In an application executing at the electronic device e.g. client device the electronic device receives a plurality of video feeds each video feed of the plurality of video feeds corresponding to a respective remote camera of a plurality of remote cameras wherein the video feeds are received concurrently by the device from a server system communicatively coupled to the remote cameras . The client device for example receives respective video feeds from multiple cameras through the hub device server system or video server system . Each of the received video feeds corresponds to a respective camera .

The electronic device displays a first user interface the first user interface including a plurality of user interface objects each user interface object of the plurality of user interface objects being associated with a respective remote camera of the remote cameras . The client device displays a user interface that includes one or more camera objects . Each camera object is associated with a respective camera . For example camera object A is associated with the Outside A camera camera object B is associated with the Front door B camera and camera object C is associated with the Dining room C camera.

The electronic device displays in each user interface object of the plurality of user interface objects the video feed corresponding to the respective remote camera with which the user interface object is associated wherein at least one of the video feeds is displayed with cropping . The client device displays in each camera object the video feed from the respective associated camera . For example the video feed from the Outside A camera is displayed in the camera object A as view the video feed from the Front door B camera is displayed in the camera object B as view and the video feed from the Dining room C camera is displayed in the camera object C as view . Each of the video feeds is displayed with cropping e.g. as described above in relation to as opposed to for example zooming out i.e. de magnifying the video feed to fit the frames of the feed into the camera object .

In some implementations each respective remote camera of the plurality of remote cameras has a respective field of view and a user interface object associated with a respective remote camera has a virtual field of view relatively smaller than the respective field of view of the associated respective remote camera . As shown in each respective camera has a field of view represented by the full frame size . The camera object is relatively smaller than the full frame size .

In some implementations displaying in each user interface object of the plurality of user interface objects the video feed corresponding to the respective remote camera with which the user interface object is associated comprises displaying in a respective user interface object periodically refreshed still images corresponding to frames from the corresponding video feed . One or more of the video feeds may be displayed in their respective camera objects as periodically refreshed e.g. at 1 image per second 1 image per two seconds etc. images e.g. still frames from the video feed .

In some implementations the electronic device receives a first user input to adjust a cropping of one or more of the video feeds displayed in the user interface objects . In response to receiving the first user input the electronic device adjusts the cropping of the one or more video feeds displayed in the user interface objects . The client device for example while displaying the video feeds in the camera objects receives an input to adjust the views and of the video feeds e.g. gesture orientation change a predefined voice input not shown . In response to the input the views and of the video feeds are adjusted as shown in a respective view is panned so that a different area of the video feed is cropped.

In some implementations the views displayed in the camera objects are selectively adjustable. For example illustrate camera objects A and B selected and then a gesture to activate view adjustment is detected. In response to the gesture the croppings of the views and corresponding to the selected camera objects A and B are adjusted with the results being views and . View is not adjusted as camera object was not selected prior to the gesture .

In some implementations the mobile device comprises an accelerometer and the first user input comprises a change by a user of an orientation of the mobile device. The electronic device receives a first user input to adjust a cropping of one or more of the video feeds displayed in the user interface objects by detecting the change of the orientation of the mobile device using the accelerometer . The electronic device adjusts the cropping in response to receiving the first user input by adjusting the cropping of the one or more video feeds in accordance with the orientation change . For example as shown in the client device may be rotated to change its orientation. The orientation change is detected by the accelerometer . In response to the orientation change the views and in the camera objects are changed in accordance with the orientation change. For example a clockwise rotation of the client device causes the views and to respectively shift rightward relative to the associated camera objects .

In some implementations the mobile device comprises a touch sensitive display and the first user input comprises a gesture performed on the touch sensitive display. The electronic device receives a first user input to adjust a cropping of one or more of the video feeds displayed in the user interface objects by detecting the gesture on the touch sensitive display . The electronic device adjusts the cropping in response to receiving the first user input by adjusting the cropping in accordance with the gesture . For example as shown in gesture may be performed by a user on the touch screen . In response to detecting the gesture the views and in the camera objects are changed in accordance with the gesture. For example a rightward gesture on the touch screen causes the views and to respectively shift rightward relative to the associated camera objects .

In some implementations the mobile device comprises an audio input device and the first user input comprises a voice command. The electronic device receives a first user input to adjust a cropping of one or more of the video feeds displayed in the user interface objects by detecting the voice command using the audio input device . The electronic device adjusts the cropping in response to receiving the first user input by adjusting the cropping in accordance with the voice command . For example the user may issue a predefined voice command instructing the smart home application running on the client device to adjust the views displayed in the camera objects . In response to detecting the voice command the views and in the camera objects are changed in accordance with the voice command.

In some implementations prior to receiving the first input the electronic device receive a second user input selecting one or more user interface objects of the plurality of the user interface objects . Adjusting the cropping in response to receiving the first user input comprises adjusting the cropping of only the video feeds displayed in the selected user interface objects in accordance with the first user input . For example the client device may receive one or more inputs e.g. gestures with contacts and . In response to these inputs one or more camera objects e.g. camera objects A and B are selected as shown in . With camera objects A and B selected in response to detecting user input to change the views displayed in the camera objects e.g. gesture orientation change predefined voice input just the views and in camera objects A and B are adjusted.

In some implementations the electronic device receives a third user input to lock the cropping of the one or more of the video feeds . In response to receiving the third user input the electronic device maintains the adjustment of the cropping of the one or more of the video feeds . In for example after the views and are adjusted in response to detecting the gesture the user may perform a gesture e.g. a single tap with contact before the views reset to their previous states. In response to that gesture the views and and are maintained instead of resetting to views and and .

In some implementations the electronic device detects a termination of the first user input and in response to detecting the termination of the first user input maintains the adjustment of the cropping . In for example after the views and are adjusted to views and and in response to detecting the gesture the user may terminate the gesture by lifting the contact off the touch screen . In some implementations in response to the termination of the gesture the views and and are maintained.

In some implementations the electronic device detects a termination of the first user input and in response to detecting the termination of the first user input ceases the adjustment of the cropping . In for example after the views and are adjusted to views and and in response to detecting the gesture the user may terminate the gesture by lifting the contact off the touch screen . In some implementations in response to the termination of the gesture the views and and are automatically reset to views and .

In some implementations while the application is in a foreground the video feeds are received and displayed in the user interface objects as video streams and while the application is in a background the video feeds are received as periodically refreshed still images corresponding to frames from the video feeds . The smart home application may be in the foreground and its user interface displayed or in the background and not displayed at any given moment. When the smart home application is in the foreground the smart home application may receive the video feeds for the camera objects as video streams. When the smart home application is in the background the smart home application may receive the video feeds for the camera objects in the background as periodically refreshed images instead of video streams.

In some implementations each respective video feed of the plurality of video feeds comprises a first version at a first resolution and a second version at a second resolution higher than first resolution and both the first version and the second version are received from the server system . In some implementations the client device receives each video feed from the hub device server system or the video server system in both an original capture resolution e.g. 720P or 1080P version and a lower resolution version e.g. 180P .

In some implementations the video feeds displayed in the user interface objects are the first versions . The lower resolution version e.g. the 180P version is displayed in the camera objects .

In some implementations the first version of a respective video feed is displayed in an associated user interface object . The electronic device receives user selection of the associated user interface object and in response to receiving the user selection ceases display of the first user interface and displays a second user interface the second user interface including the second version of the respective video feed wherein within the second user interface the respective video feed is uncropped . As described above the lower resolution version of a video feed is displayed in the corresponding camera object . For example the 180P version of the Dining room C camera video feed is displayed in the camera object C. As shown in while the Dining room C camera video feed is displayed in a camera object C a single tap gesture with contact is detected on the camera object C. In response to detecting the gesture user interface is displayed on the touch screen replacing user interface . The Dining room C camera video feed is displayed in video region without cropping and at a higher resolution than that for the camera object C e.g. at the original capture resolution .

In some implementations the cropping of the at least one of the video feeds is performed at the mobile device in accordance with instructions from the server system wherein the cropping instructions are generated by the server system based on an analysis of the at least one of the video feeds by the server system to determine a portion of the at least one of the video feed of potential interest to a user . The cropping may be performed by the hub device server system or the video server system . The hub device server system or the video server system crops a video feed before transmitting the video feed to a client device . In some implementations the cropping by the server system or is based on an analysis by the server system or of a video feed to be cropped to determine which portion of the video feed e.g. which portion of the frame is of potential interest to the user. For example the servers system or may based on an analysis of the video feed determine that there is motion occurring in the video and crops the video feed to focus on that motion e.g. crop the video feed to focus on the area where the motion occurred .

In some implementations a plurality of the video feeds is displayed with cropping and each of the cropped video feeds is cropped in accordance with a same cropping mask . For example in the video feeds displayed in the camera objects A B and C are cropped. As the camera objects camera objects A B and C all have the same circular shape and size the video feeds are cropped with the same cropping mask that fits the video feeds into the same circular shape and size. In some implementations all of the video feeds displayed in the camera objects are cropped. In some other implementations some of the video feeds displayed in the camera objects are cropped one or more of the feeds are instead zoomed out within the camera object to fit the entire frame into the camera object. In some implementations each video feed is cropped to same mask or mask shape the mask or mask shape is based on the shape and size of the camera objects . In some other implementations each camera object has a distinct shape and the corresponding masks mask shapes differ in accordance with the distinct shapes of the camera objects .

The server system receives a video feed from a camera with an associated field of view . The hub device server system or video server system e.g. the video data receiving module receives video feeds from one or more cameras . Each camera has a respective field of view.

The server system receives one or more alert events . The hub device server system or video server system e.g. the alert events module receives detected alert events from smart devices . For example whenever a smart device detects an alert event e.g. a hazard sound etc. the hub device server system or video server system receives information corresponding to the detected alert event e.g. start time and end time of the alert event alert event type .

The server system identifies as a camera event a portion of the video feed associated in time with the one or more alert events . For example the hub device server system or video server system e.g. the camera events module correlates alert events or sequences of alert events with portions of the video feed based on when the alert events start and end and from the correlation identify camera events which include a portion of the video feed e.g. captured video from the video feed from a start date time to an end date time and an associated set of one or more alert events the alert events are proximate in time e.g. contemporaneous with the associated video. In some implementations two alert events are associated with the same camera event if the alert events overlap or occur in succession with a time gap i.e. time elapsed between one alert event ending and the next alert event starting between the two consecutive alert events being less than a threshold amount.

The server system determines a start time and duration of the camera event . The hub device server system or video server system e.g. the camera events module determines the start time and duration of the camera event based on the times of the associated alert events. For example the start time of the earliest alert event of the associated alert events is determined to be the start time of the camera event and the end time of the alert event of the associated alert events that ends latest is determined to be the end time of the camera event. With these start and end times the duration o the camera event may be determined. Also the video that is associated with the camera event has these start and end times and optionally plus some slack time in either direction e.g. 1 5 seconds before the start time and or 1 5 seconds after the end time .

The server system determines a chronological order of the alert events . The hub device server system or video server system e.g. the alert events module determines the chronological sequence of the alert events associated with the camera event. In some implementations the chronological order is determined based on the start times of the alert events i.e. when the respective alert events are first detected . In some implementations the chronological order is determined based on the end times of the alert events i.e. when the respective alert events are last detected .

The server system saves in a history associated with the camera information associated with the camera event including a video clip and or a frame from the portion of the video feed and the chronological order of the alert events . The hub device server system or video server system saves in the server database e.g. in camera events history and video storage database the information associated with the camera event. The camera events information e.g. the associated alert events and corresponding chronology camera event times and durations etc. is stored in the camera events history and the camera events information references video stored in the video storage database .

In some implementations the alert events include one or more of a hazard alert event an audio alert event a vibration alert event and a motion alert event . The alert events may be detected hazards e.g. detected by smart hazard detectors detected sound above a minimum decibel threshold e.g. detected by any smart device with audio input detected vibrations above a minimum threshold e.g. detected by any smart device with vibration sensors and detected motion e.g. detected in video captured by a camera or detected by non camera sensors such as motion sensors .

In some implementations the motion alert event corresponds to motion detected in a defined spatial zone associated with the field of view . The motion alert event may correspond to motion detected in video by the camera where the motion is occurring in a defined zone of interest in the scene or area monitored by the camera . The zone of interest is a zone designating a space in the scene or area monitored by the camera for which detected motion is treated as a distinct alert event type in addition to being treated as detected motion generally.

In some implementations the spatial zone is defined by a user . The spatial zone may be defined by a user. The user may enter into a user interface which shows video captured by the camera and mark off a portion in the video as the zone of interest. For example if the camera is monitoring a scene that includes a door the user may mark off the door as the zone of interest e.g. by marking a zone boundary around the door motion detected in the door zone is treated as a distinct alert event type. In some implementations the user is associated with the camera the camera is tied to the user s account e.g. in account database and device information database .

In some implementations the server system associates each of the alert events with a respective visually distinctive display characteristic . The hub device server system or video server system associates each alert event type with a visually distinctive display characteristic so that when indicators e.g. icons of alert events are displayed the user can identify and differentiate between alert event types based on the distinctive display characteristics which the indicators adopt. In some implementations these associations are made per user. Multiple users may have the same mappings of alert event types to display characteristics but it is sufficient that for any one respective user each alert event type is mapped to a distinct display characteristic. For example illustrate icons indicating alert event types and respectively with distinct shapes.

In some implementations the display characteristic is visually distinctive based on color . In some implementations the display characteristic is visually distinctive based on shape . The alert event types may be distinct based on the color or shape of their corresponding indicators. For example for a respective user the hazard alert event type is assigned red the sound alert event type is assigned blue the vibration alert event type is assigned orange the general motion event type is assigned brown and motion in a particular user defined zone of interest is assigned green. Then for that user icons indicating hazard alert events are red icons indicating sound alert events are blue icons indicating vibration alert events are orange etc. Similarly icons indicating different alert event types may have different shapes. For example illustrate icons indicating alert event types and respectively with distinct shapes.

In some implementations the server system responsive to a request from a client device transmits contents of the history to the client device for display in a desktop browser application at the client device where the contents of the history is formatted for display in the desktop browser application as a camera history timeline . For example when interface or is accessed a request is made to the hub device server system or video server system for the history of the camera . In response to the request the hub device server system or video server system transmits camera history information to the client device for display. The camera history information may be displayed differently depending on the interface e.g. displayed differently in interface than in . For example in interface the camera history information is displayed in a linear timeline .

In some implementations the camera history timeline comprises an event bar corresponding to the camera event where a length of the event bar reflects the duration of the camera event and one or more alert event indicators proximate to the event bar each of the alert event indicators corresponding to a respective alert event associated with the camera event wherein each respective alert event indicator has a respective visually distinctive display characteristic associated with the corresponding respective alert event . The timeline may include an event bar for each camera event. The length of the event bar which is to scale relative to timeline indicates a duration of the camera event. One or more alert event icons are displayed in proximity to the event bar e.g. within the event bar near the event bar . The icons have distinctive shapes and or colors mapped to alert event types the user can identify the alert event types indicated by the icons based on the shapes and or colors.

In some implementations the alert event indicators are ordered in accordance with the chronological order of the alert events . The icons are ordered while displayed based on the chronological order of the alert events indicated. In some implementations the icons are ordered based on the chronological order of the most recent instance of each alert event type for that camera event.

In some implementations responsive to a request from a client device the serve system transmits contents of the history to the client device for display in a mobile application at the device where the contents of the history is formatted for display in the mobile application as a scrollable camera history list the scrollable camera history list including one or more chronologically ordered event identifiers each event identifier corresponding to a respective camera event . For example when interface or is accessed a request is made to the hub device server system or video server system for the history of the camera . In response to the request the hub device server system or video server system transmits camera history information to the client device for display. The camera history information may be displayed differently depending on the interface e.g. displayed differently in interface than in . For example in interface the camera history information is displayed as a scrollable list of event identifiers which may still be viewed as a timeline because the event identifiers are chronologically ordered and separated by hour .

In some implementations the scrollable camera history list comprises an event identifier corresponding to the camera event and one or more alert event indicators each of the alert event indicators corresponding to a respective alert event associated with the camera event where each respective alert event indicator has a respective visually distinctive display characteristic associated with the corresponding respective alert event . The scrollable list of event identifiers displayed in interface includes event identifiers corresponding to respective camera events. Alert event icons may be displayed in the event identifiers when a filter is applied e.g. as in or by default. The icons have distinctive shapes and or colors mapped to alert event types the user can identify the alert event types indicated by the icons based on the shapes and or colors.

In some implementations the alert event indicators are ordered in accordance with the chronological order of the alert events . The icons are ordered while displayed based on the chronological order of the alert events indicated. In some implementations the icons are ordered based on the chronological order of the most recent instance of each alert event type for that camera event.

The client device displays a video feed from a camera or a frame from the video feed . For example in interface the client device displays a video feed or a frame from the video feed e.g. if the video is paused in the video region .

The client device concurrently with displaying the video feed or the frame displays a camera history timeline including displaying a representation of a camera event associated with one or more alert events in the camera history timeline as a bar overlaid on the event history timeline the event bar having a length reflecting a duration of the camera event and displaying proximate to the event bar one or more alert event indicators each of the alert event indicators corresponding to a respective alert event of the alert events associated with the camera event wherein each respective alert event indicator has a respective visually distinctive display characteristic associated with the corresponding respective alert event . In interface the client device displays concurrently with the video a timeline with camera event bars corresponding to respective camera events and icons indicating alert events associated with the camera events . The length of a camera event bar indicates a duration of the corresponding camera event. The icons are displayed in the camera event bars as in or near the camera event bars as in .

In some implementations the display characteristic is visually distinctive based on color . In some implementations the display characteristic is visually distinctive based on shape . The icons have distinctive shapes and or colors mapped to alert event types the user can identify the alert event types indicated by the icons based on the shapes and or colors.

In some implementations the activity alert indicators are ordered in accordance with a chronological order of the corresponding alert events . The icons are ordered while displayed based on the chronological order of the alert events indicated. In some implementations the icons are ordered based on the chronological order of the most recent instance of each alert event type for that camera event.

The client device displays a camera history timeline including displaying a chronologically ordered sequence of event identifiers each event identifier corresponding to a respective camera event each respective camera event associated with one or more respective alert events and displaying for a respective event identifier one or more alert event indicators each of the alert event indicators corresponding to an alert event associated with the camera event corresponding to the respective event identifier each of the alert event indicators displayed with a visually distinctive display characteristic associated with a corresponding alert event . For example in interface camera history information is displayed as a scrollable list of event identifiers which may still be viewed as a timeline because the event identifiers are chronologically ordered and separated by hour . The scrollable list of event identifiers displayed in interface includes event identifiers corresponding to respective camera events. Alert event icons may be displayed in the event identifiers when a filter is applied e.g. as in or by default. The icons have distinctive shapes and or colors mapped to alert event types the user can identify the alert event types indicated by the icons based on the shapes and or colors.

In some implementations the camera history timeline is displayed as a scrollable list and the event identifiers are items in the scrollable list . The event identifiers are displayed in a scrollable list .

In some implementations for the respective event identifier the alert event indicators are ordered in accordance with a chronological order of the corresponding alert events . The icons are ordered while displayed based on the chronological order of the alert events indicated. In some implementations the icons are ordered based on the chronological order of the most recent instance of each alert event type for that camera event.

The client device in an application executing on the client device displays a camera event history provided by a remote server system where the camera event history is presented as a chronologically ordered set of event identifiers each event identifier corresponding to a respective event for which a remote camera has captured an associated video . For example the smart home application displays in interface camera history information as a scrollable list of chronologically ordered event identifiers which may still be viewed as a timeline because the event identifiers are chronologically ordered and separated by hour . The scrollable list of event identifiers displayed in interface includes event identifiers corresponding to respective camera events with associated video from a camera .

The client device receives a user selection of a displayed event identifier . As shown in for example the user may perform a gesture e.g. a single tap gesture with contact on an event identifier E.

The client device in response to receiving the user selection of the displayed event identifier expands the selected event identifier into a video player window the video player window consuming a portion of the displayed camera event history and plays in the video player window the captured video associated with the selected event identifier . In response to detecting the gesture on the event identifier E the smart home application expands the event identifier inline into a video player interface . The video player interface consumes space in the scrollable list. The video player interface includes and plays the video associated with the corresponding camera event.

In some implementations the video player window consumes only a portion of the displayed camera event history. The video player interface for example consumes a portion of the scrollable list other event identifiers e.g. identifiers D and F are still displayed.

In some implementations the playback of video is automatically initiated when the event identifier expands into the video player interface . In some implementations the playback of video needs to be manually initiated after the event identifier expands into the video player interface . For example the user manually activates playback by performing a gesture on the playback affordance displayed after the event identifier expands into the video player interface .

In response to terminating playback of the captured video associated with the selected event identifier or user de selection of the displayed event identifier the client device collapses the video player window into the selected event identifier thereby stopping the playing of the captured video associated with the selected event identifier . The user may de select the event identifier E by performing a gesture in the video player interface away from the video e.g. gesture with contact in an empty area in video player interface . In response to the gesture the video player interface collapses back into event identifier E and stops playback of the video if playback was in progress. In some implementations additionally when playback of the video is completed the smart home application automatically collapses the video player interface back into event identifier E.

In some implementations the set of event identifiers are displayed as a scrollable list of the event identifiers . The event identifiers are displayed in a scrollable list .

In some implementations a respective event corresponding to the displayed event identifier is a non camera event and a video associated with the respective event is captured by the remote camera during the non camera event . An event identifier may corresponding to a camera event where motion is not detected in the video itself but one or more non camera alert event s e.g. hazard sound vibration etc. are detected and video associated with the camera event is captured during the non camera alert event s .

In some implementations each event identifier has a thumbnail image associated with the corresponding respective event . Each event identifier includes a thumbnail which may be a frame of the associated video or the associated video in thumbnail size.

In some implementations a video associated with a respective event and captured by the remote camera is contemporaneous with the associated respective event . For example when the alert event s for a camera event are non camera alert event s the associated video is video captured proximate in time with the alert event s . Video captured proximate in time with the alert event s may be captured while the alert event s are detected or immediately after the alert event s is last detected e.g. for instantaneous alert events such as sounds .

In some implementations a video associated with a respective event is saved at the remote server system . Video associated with camera events are saved and stored at the hub device server system or video server system e.g. in video storage database .

In some implementations for the displayed event identifier the client device displays one or more alert event icons each alert event icon corresponding to a respective alert event triggered in response to a respective event corresponding to the displayed event identifier . For example when a filter is applied icons may be displayed in event identifiers . The icons indicate alert events that have been detected for the camera event corresponding to the respective event identifiers .

In some implementations within the displayed event identifier the alert event icons are visually distinctive from each other based on icon color . In some implementations within the displayed event identifier the alert event icons are visually distinctive from each other based on icon shape . The icons have distinctive shapes and or colors mapped to alert event types the user can identify the alert event types indicated by the icons based on the shapes and or colors.

In some implementations within the displayed event identifier the alert event icons are ordered in accordance with a chronological order in which the triggered alert events were triggered in response to the respective event corresponding to the displayed event identifier . Within an event identifier the icons are ordered by the chronological order of the instances of alert events to which the displayed icons correspond. For example in in event identifier F the alert event corresponding to icon A is more recent than the alert event corresponding to icon B.

In some implementations the client device displays in the displayed event identifier information regarding a most recently triggered alert event of the triggered alert events . The activity type identifier of an event identifier identifies in some implementations the alert event type of the most recent alert event associated with the corresponding camera event. For example in for event identifier Q the alert event type of the most recent alert event is Motion Activity Q whereas for event identifier R the alert event type of the most recent alert event is Sound Activity R.

The scrollable list may be filtered to show just camera events that satisfy particular filtering criteria e.g. camera events that have particular alert event types . shows a menu analogous to filtering menu to select particular alert event types and zones of interest for filtering. For example in Zone Motion not in your activity zones and Sound are selected. shows the result of the filtering in which includes any camera event that includes at least one of Zone Motion not in your activity zones and Sound. 

In some implementations additional controls and affordances are displayed when a mouse pointer is hovered over the video. For example shows the additional controls and affordances displayed. The additional controls and affordances include for example zooming controls a pause button buttons to jump forward or backward by a predefined amount of time a button to jump to the current live video feed a volume control and a button to expand the video to full screen. In some implementations the controls and affordances include in addition to or in lieu of the buttons to jump forward or backward by a predefined amount of time a button to jump forward to a video corresponding to the next camera event chronologically e.g. Next Video Next Event and or a button jump backward to a video corresponding to the previous camera event chronologically e.g. Previous Video Previous Event .

For situations in which the systems discussed above collect information about users the users may be provided with an opportunity to opt in out of programs or features that may collect personal information e.g. information about a user s preferences or usage of a smart device . In addition in some implementations certain data may be anonymized in one or more ways before it is stored or used so that personally identifiable information is removed. For example a user s identity may be anonymized so that the personally identifiable information cannot be determined for or associated with the user and so that user preferences or user interactions are generalized for example generalized based on user demographics rather than associated with a particular user.

Although some of various drawings illustrate a number of logical stages in a particular order stages that are not order dependent may be reordered and other stages may be combined or broken out. While some reordering or other groupings are specifically mentioned others will be obvious to those of ordinary skill in the art so the ordering and groupings presented herein are not an exhaustive list of alternatives. Moreover it should be recognized that the stages could be implemented in hardware firmware software or any combination thereof.

The foregoing description for purpose of explanation has been described with reference to specific implementations. However the illustrative discussions above are not intended to be exhaustive or to limit the scope of the claims to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The implementations were chosen in order to best explain the principles underlying the claims and their practical applications to thereby enable others skilled in the art to best use the implementations with various modifications as are suited to the particular uses contemplated.

