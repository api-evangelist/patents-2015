---

title: History preserving data pipeline
abstract: A history preserving data pipeline computer system and method. In one aspect, the history preserving data pipeline system provides immutable and versioned datasets. Because datasets are immutable and versioned, the system makes it possible to determine the data in a dataset at a point in time in the past, even if that data is no longer in the current version of the dataset.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09483506&OS=09483506&RS=09483506
owner: Palantir Technologies, Inc.
number: 09483506
owner_city: Palo Alto
owner_country: US
publication_date: 20151009
---
This application is a Continuation of U.S. patent application Ser. No. 14 533 433 filed Nov. 5 2014 the entire contents of which is hereby incorporated by reference for all purposes as if fully set forth herein. The applicant s hereby rescind any disclaimer of claim scope in the parent application s or the prosecution history thereof and advise the USPTO that the claims in this application may be broader than any claim in the parent application s .

The disclosed technologies relate generally to data pipeline computer systems and more particularly to a data pipeline computer system with methodology for preserving history of datasets.

Computers are very powerful tools for processing data. A computerized data pipeline is a useful mechanism for processing large amounts of data. A typical data pipeline is an ad hoc collection of computer software scripts and programs for processing data extracted from data sources and for providing the processed data to data sinks As an example a data pipeline for a large insurance company that has recently acquired a number of smaller insurance companies may extract policy and claim data from the individual database systems of the smaller insurance companies transform and validate the insurance data in some way and provide validated and transformed data to various analytical platforms for assessing risk management compliance with regulations fraud etc.

Between the data sources and the data sinks a data pipeline system is typically provided as a software platform to automate the movement and transformation of data from the data sources to the data sinks. In essence the data pipeline system shields the data sinks from having to interface with the data sources or even being configured to process data in the particular formats provided by the data sources. Typically data from the data sources received by the data sinks is processed by the data pipeline system in some way. For example a data sink may receive data from the data pipeline system that is a combination e.g. a join of data of from multiple data sources all without the data sink being configured to process the individual constituent data formats.

One purpose of a data pipeline system is to execute data transformation steps on data obtained from data sources to provide the data in format expected by the data sinks A data transformation step may be defined as a set of computer commands or instructions which when executed by the data pipeline system transforms one or more input datasets to produce one or more output or target datasets. Data that passes through the data pipeline system may undergo multiple data transformation steps. Such a step can have dependencies on the step or steps that precede it. One example of a computer system for carrying out data transformation steps in a data pipeline is the well known MapReduce system. See e.g. Dean Jeffrey et al. MapReduce Simplified Data Processing on Large Clusters Google Inc. 2004.

Often data pipeline systems are maintained by hand . That is a software engineer or system administrator is responsible for configuring the system so that data transformation steps are executed in the proper order and on the correct datasets. If a data transformation step needs to be added removed or changed the engineer or administrator typically must reconfigure the system by manually editing control scripts or other software programs. Similar editing tasks may be needed before the pipeline can process new datasets. Overall current approaches for maintaining existing data pipeline systems may require significant human resources.

Another problem with existing data pipeline systems is the lack of dataset versioning. In these systems when a dataset needs to be updated with new data the data transformation step typically overwrites the old version of the dataset with the new version. This can be problematic if it is suspected or discovered thereafter that the old version of the dataset contained incorrect data that the new version does not contain. For example the old version of the dataset may have been imported into an analytical software program which generated anomalous results based on the incorrect data. In this case since the old version is lost when the new version is generated it can be difficult to track down the source of the incorrect data.

Given the increasing amount of data collected by businesses and other organizations processing data of all sorts through data pipeline systems can only be expected to increase. This trend is coupled with a need for a more automated way to maintain such systems and for the ability to trace and track data including old versions of the data as it moves through the data pipeline from data sources to data sinks.

The approaches described in this section are approaches that could be pursued but not necessarily approaches that have been previously conceived or pursued. Therefore unless otherwise indicated it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.

A history preserving data pipeline computer system and associated method are described. In a first embodiment for example a method for preserving history of a derived dataset comprises the steps of storing a first version of a derived dataset wherein the first version of the derived dataset is derived from at least a first version of another dataset by executing a first version of derivation program associated with the derived dataset storing a first build catalog entry the first build catalog entry associated with the derived dataset and comprising an identifier of the first version of the other dataset and comprising an identifier of the first version of the derivation program updating the other dataset to produce a second version of the other dataset storing a second version of the derived dataset wherein the second version of the derived dataset is derived from at least the second version of the other dataset by executing the first version of the derivation program associated with the derived dataset and storing a second build entry the second build entry associated with the derived dataset and comprising an identifier of the second version of the other dataset and comprising an identifier of the first version of the derivation program.

In one aspect of the first embodiment embodiment the method further comprises the step of storing the first version of the derived dataset and the second version of the derived dataset in a data lake. In one embodiment the data lake comprises a distributed file system.

In one aspect of the first embodiment the first build catalog entry comprises a name of the derived dataset and an identifier of the first version of the derived dataset. In one embodiment the identifier of the first version of the derived dataset is an identifier assigned to a commit of a transaction that stored the first version of the derived dataset.

In one aspect of the first embodiment the second build catalog entry comprises a name of the derived dataset and an identifier of the second version of the derived dataset. In one embodiment the identifier of the second version of the derived dataset is an identifier assigned to a commit of a transaction that stored the second version of the derived dataset.

In one aspect of the first embodiment the first version of the derived dataset is stored in a first set of one or more data containers and the second version of the derived dataset is stored in a second set of one or more data containers. In one embodiment the second set of one or more data containers comprises delta encodings reflecting deltas between the first version of the derived dataset and the second version of the derived dataset.

In one aspect of the first embodiment the first version of the derivation program when executed to produce the first version of the derived dataset transforms data of the first version of the other dataset to produce data of the first version of the derived dataset.

In one aspect of the first embodiment the first version of the derivation program when executed to produce the second version of the derived dataset transforms data of the second version of the other dataset to produce data of the second version of the derived dataset.

In one aspect of the first embodiment the method steps of storing the first version of the derived dataset and storing the second version of the derived dataset are performed by a data lake.

In one aspect of the first embodiment the steps of storing the first build catalog entry and storing the second build catalog entry are performed by a build service.

In one aspect of the first embodiment the step updating the other dataset to produce the second version of the other dataset is performed by a transaction service.

In one aspect of the first embodiment the first build catalog entry and the second build catalog entry are stored in a database.

In one aspect of the first embodiment the method further comprises the step of storing a transaction entry in a database comprising a transaction commit identifier of the first version of the derived dataset. In one embodiment the first build catalog entry comprises the transaction commit identifier.

In one aspect of the first embodiment the method further comprises the step of storing a transaction entry in a database comprising a transaction commit identifier of the second version of the derived dataset. In one embodiment the second build catalog entry comprises the transaction commit identifier.

In one aspect of the first embodiment the method further comprises the step of storing a transaction entry in a database comprising a transaction commit identifier of the first version of the other dataset. In one embodiment the identifier of the first version of the other dataset in the first build catalog entry is the transaction commit identifier.

In one aspect of the first embodiment the method further comprises storing a transaction entry in a database comprising a transaction commit identifier of the second version of the other dataset. In an embodiment the identifier of the second version of the other dataset in the second build catalog entry is the transaction commit identifier.

In other aspects of the first embodiment the invention encompasses a computer system and a computer readable medium configured to carry out the foregoing operations of the various aspects of the first embodiment.

In a second embodiment for example a method for preserving history of a derived dataset is performed at one or more computing devices comprising one or more processors and storage media storing one or more computer programs executed by the one or more processors to perform the method. Performance of the method includes performing operations comprising storing a first version of a derived dataset storing a first build catalog entry updating another dataset to produce a second version of the other dataset storing a second version of the derived dataset and storing a second build catalog entry. The first build catalog entry is associated with the derived dataset and comprises an identifier of the first version of the other dataset and an identifier of the first version of the derivation program. The first version of the derived dataset is derived from at least a first version of the other dataset by executing a first version of derivation program associated with the derived dataset. The first build catalog entry comprises a name of the derived dataset and an identifier of the first version of the derived dataset. The second version of the derived dataset is derived from at least the second version of the other dataset by executing the first version of the derivation program associated with the derived dataset. The second build catalog entry is associated with the derived dataset and comprises an identifier of the second version of the other dataset and an identifier of the first version of the derivation program. The second build catalog entry comprises a name of the derived dataset and an identifier of the second version of the derived dataset.

In one aspect of the second embodiment the operations further comprising the operation of storing the first version of the derived dataset and the second version of the derived dataset in a data lake.

In one aspect of the second embodiment the identifier of the first version of the derived dataset is an identifier assigned to a commit of a transaction that stored the first version of the derived dataset.

In one aspect of the second embodiment the identifier of the second version of the derived dataset is an identifier assigned to a commit of a transaction that stored the second version of the derived dataset.

In one aspect of the second embodiment the first version of the derived dataset is stored in a first set of one or more data containers and the second version of the derived dataset is stored in a second set of one or more data containers.

In one aspect of the second embodiment the second set of one or more data containers comprises delta encodings reflecting deltas between the first version of the derived dataset and the second version of the derived dataset.

In one aspect of the second embodiment the first version of the derivation program when executed to produce the first version of the derived dataset transforms data of the first version of the other dataset to produce data of the first version of the derived dataset.

In one aspect of the second embodiment the first version of the derivation program when executed to produce the second version of the derived dataset transforms data of the second version of the other dataset to produce data of the second version of the derived dataset.

In one aspect of the second embodiment the operations of storing the first version of the derived dataset and storing the second version of the derived dataset are performed by a data lake.

In one aspect of the second embodiment the operations of storing the first build catalog entry and storing the second build catalog entry are performed by a build service.

In one aspect of the second embodiment the operation of updating the other dataset to produce the second version of the other dataset is performed by a transaction service.

In one aspect of the second embodiment the first build catalog entry and the second build catalog entry are stored in a database.

In one aspect of the second embodiment the operations further comprise the operation of storing a transaction entry in a database comprising a transaction commit identifier of the first version of the derived dataset. And the first build catalog entry comprises the transaction commit identifier.

In one aspect of the second embodiment the operations further comprise the operation of storing a transaction entry in a database comprising a transaction commit identifier of the second version of the derived dataset. And the second build catalog entry comprises the transaction commit identifier.

In one aspect of the second embodiment the operations further comprise the operation of storing a transaction entry in a database comprising a transaction commit identifier of the first version of the other dataset. And the identifier of the first version of the other dataset in the first build catalog entry is the transaction commit identifier.

In one aspect of the second embodiment the operations further comprise the operation of storing a transaction entry in a database comprising a transaction commit identifier of the second version of the other dataset. And the identifier of the second version of the other dataset in the second build catalog entry is the transaction commit identifier.

In other aspects of the second embodiment the invention encompasses a computer system and a computer readable medium configured to carry out the foregoing operations of the various aspects of the second embodiment.

In a third embodiment for example a method is performed at one or more computing devices comprising one or more processors and one or more storage media storing one or more computer programs executed by the one or more processors to perform the method. The method includes performing operations comprising maintaining a build catalog comprising a plurality of build catalog entries where each build catalog entry of the plurality of build catalog entries comprises an identifier of a version of a derived dataset corresponding to the build catalog entry one or more dataset build dependencies of the version of the derived dataset corresponding to the build catalog entry each of the one or more dataset build dependencies comprising an identifier of a version of a child dataset from which the version of the derived dataset corresponding to the build catalog entry is derived and a derivation program build dependency of the version of the derived dataset corresponding to the build catalog entry the derivation program build dependency comprising an identifier of a version of a derivation program executed to generate the version of the derived dataset corresponding to the build catalog entry. The method further includes performing operations comprising creating a new version of a particular derived dataset in context of a successful transaction and adding a new build catalog entry to the build catalog the new build catalog entry comprising an identifier of the new version of the particular derived dataset the identifier of the new version of the particular derived dataset being a transaction commit identifier assigned to the successful transaction.

In one aspect of the third embodiment the operation of creating the new version of the particular derived dataset comprises the operation of executing a particular version of a particular derivation program and wherein the new build catalog entry comprises an identifier of the particular version of the particular derivation program.

In one aspect of the third embodiment a particular build catalog entry of the plurality of build catalog entries comprises a plurality of dataset build dependencies of the version of the derived dataset corresponding to the particular build catalog entry each of the plurality of dataset build dependencies comprising an identifier of a version of a child dataset from which the version of the derived dataset corresponding to the particular build catalog entry is derived.

In one aspect of the third embodiment the build catalog comprises a particular build catalog entry comprising an identifier of a version of a derived dataset corresponding to the build catalog entry but does not include any of a dataset build dependency or a derivation program build dependency.

In other aspects of the third embodiment the invention encompasses a computer system and a computer readable medium configured to carry out the foregoing operations of the various aspects of the third embodiment

In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the disclosed technologies. It will be apparent however that the disclosed technologies can be practiced without these specific details. In other instances well known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the disclosed technologies. As to the flowcharts each block within the flowcharts represents both a method step and an apparatus element for performing the method step. Depending upon the requirements of the particular implementation at hand the corresponding apparatus element may be configured in hardware software firmware or combinations thereof.

Given the deficiencies of current manual and ad hoc approaches for implementing and managing a data pipeline system a more automated and integrated approach would clearly be preferable. In accordance with an embodiment of the disclosed technologies a history preserving data pipeline system is provided.

In one aspect the history preserving data pipeline system improves on existing data pipeline technologies to provide immutable and versioned datasets. A dataset may be defined as a named collection of data. The datasets are immutable in the sense that it is not necessary to overwrite existing dataset data in order modify the dataset. The datasets are versioned in the sense that modifications to a dataset including historical modifications are separately identifiable.

Because datasets are immutable and versioned the system makes it possible to determine the data in a dataset at a point in time in the past even if that data is no longer in the current version of the dataset. More generally the history preserving data pipeline system improves on existing data pipeline systems by providing the ability to trace dataset data to the data source data from which the dataset data was derived or obtained even if the dataset data is no longer in the current version of the dataset and even if the data source data is no longer available from the data source.

In another aspect the history preserving data pipeline system improves on existing data pipeline technologies to provide immutable and versioned derived datasets. A derived dataset may be defined as a dataset that is generated built by executing a derivation program potentially providing one or more other datasets as input to the derivation program. When executed the derivation program may perform one or more operations on the input dataset s . For example the derivation program may transform the data in the input dataset s in some way to produce the derived dataset. For example a derivation program may produce a derived dataset by filtering records in an input dataset to those comprising a particular value or set of values or by joining together two related input datasets or by replacing references in an input dataset to values in another input dataset with actual data referenced. Because derived datasets like datasets generally are immutable and versioned in the system it is possible to trace dataset data to the data source data from which the dataset data was derived or obtained even if the dataset data is no longer in the current version of the derived dataset and even if the data source data is no longer available from the data source.

In yet another aspect the history preserving data pipeline system improves on existing data pipeline systems by versioning derivation programs. By doing so not only does the system provide the ability to trace dataset data to the data source data the dataset data is based on but also if the dataset is a derived dataset to the version of the derivation program used to build the derived dataset. This is useful for tracking down errors in dataset data caused by errors or bugs i.e. programming errors in the version of the derivation program that was executed to build the dataset.

In yet another aspect the history preserving data pipeline system improves on existing data pipeline systems by maintaining build dependency data . The build dependency data represents one or more directed acyclic graphs of build dependencies. From the build dependency data the system can determine for a given dataset the order in which to build other datasets before the given dataset can be built. By doing so human engineers are alleviated from some manual tasks required by existing data pipeline systems related to maintaining and determining dataset build dependencies.

These and other aspects of the history preserving data pipeline system are described in greater detail elsewhere in this document. First however an example of the basic underlying computer components that may be employed for implementing the disclosed technologies are described.

The disclosed technologies may be implemented on one or more computing devices. Such a computing device may be implemented in various forms including but not limited to a client a server a network device a mobile device a cell phone a smart phone a laptop computer a desktop computer a workstation computer a personal digital assistant a blade server a mainframe computer and other types of computers. The computing device described below and its components including their connections relationships and functions is meant to be exemplary only and not meant to limit implementations of the disclosed technologies described in this specification. Other computing devices suitable for implementing the disclosed technologies may have different components including components with different connections relationships and functions.

Main memory such as a random access memory RAM or other dynamic storage device is coupled to bus for storing information and instructions to be executed by processor s . Main memory also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor s . Such instructions when stored in non transitory storage media accessible to processor s render computing device into a special purpose computing device that is customized to perform the operations specified in the instructions.

Computing device further includes read only memory ROM or other static storage device coupled to bus for storing static information and instructions for processor s .

One or more mass storage devices are coupled to bus for persistently storing information and instructions on fixed or removable media such as magnetic optical solid state magnetic optical flash memory or any other available mass storage technology. The mass storage may be shared on a network or it may be dedicated mass storage. Typically at least one of the mass storage devices e.g. the main hard disk for the device stores a body of program and data for directing operation of the computing device including an operating system user application programs driver and other support files as well as other data files of all sorts.

Computing device may be coupled via bus to display such as a liquid crystal display LCD or other electronic visual display for displaying information to a computer user. Display may also be a touch sensitive display for communicating touch gesture e.g. finger or stylus input to processor s .

An input device including alphanumeric and other keys is coupled to bus for communicating information and command selections to processor .

Another type of user input device is cursor control such as a mouse a trackball or cursor direction keys for communicating direction information and command selections to processor and for controlling cursor movement on display . This input device typically has two degrees of freedom in two axes a first axis e.g. x and a second axis e.g. y that allows the device to specify positions in a plane.

Computing device may implement the methods described herein using customized hard wired logic one or more application specific integrated circuits ASICs one or more field programmable gate arrays FPGAs firmware or program logic which in combination with the computing device causes or programs computing device to be a special purpose machine.

Methods disclosed herein may also be performed by computing device in response to processor s executing one or more sequences of one or more instructions contained in main memory . Such instructions may be read into main memory from another storage medium such as storage device s . Execution of the sequences of instructions contained in main memory causes processor s to perform the process steps described herein. In alternative embodiments hard wired circuitry may be used in place of or in combination with software instructions.

The term storage media as used herein refers to any non transitory media that store data and or instructions that cause a computing device to operate in a specific fashion. Such storage media may comprise non volatile media and or volatile media. Non volatile media includes for example optical disks magnetic disks or solid state drives such as storage device . Volatile media includes dynamic memory such as main memory . Common forms of storage media include for example a floppy disk a flexible disk hard disk solid state drive magnetic tape or any other magnetic data storage medium a CD ROM any other optical data storage medium any physical medium with patterns of holes a RAM a PROM and EPROM a FLASH EPROM NVRAM any other memory chip or cartridge.

Storage media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between storage media. For example transmission media includes coaxial cables copper wire and fiber optics including the wires that comprise bus . Transmission media can also take the form of acoustic or light waves such as those generated during radio wave and infra red data communications.

Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor s for execution. For example the instructions may initially be carried on a magnetic disk or solid state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computing device can receive the data on the telephone line and use an infra red transmitter to convert the data to an infra red signal. An infra red detector can receive the data carried in the infra red signal and appropriate circuitry can place the data on bus . Bus carries the data to main memory from which processor s retrieves and executes the instructions. The instructions received by main memory may optionally be stored on storage device s either before or after execution by processor s .

Computing device also includes one or more communication interface s coupled to bus . A communication interface provides a two way data communication coupling to a wired or wireless network link that is connected to a local network e.g. Ethernet network Wireless Local Area Network cellular phone network Bluetooth wireless network or the like . Communication interface sends and receives electrical electromagnetic or optical signals that carry digital data streams representing various types of information. For example communication interface may be a wired network interface card a wireless network interface card with an integrated radio antenna or a modem e.g. ISDN DSL or cable modem .

Network link s typically provide data communication through one or more networks to other data devices. For example a network link may provide a connection through a local network to a host computer or to data equipment operated by an Internet Service Provider ISP . ISP in turn provides data communication services through the world wide packet data communication network now commonly referred to as the Internet . Local network s and Internet use electrical electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link s and through communication interface s which carry the digital data to and from computing device are example forms of transmission media.

Computing device can send messages and receive data including program code through the network s network link s and communication interface s . In the Internet example a server might transmit a requested code for an application program through Internet ISP local network s and communication interface s .

The received code may be executed by processor as it is received and or stored in storage device or other non volatile storage for later execution.

Software system may include a graphical user interface GUI for receiving user commands and data in a graphical e.g. point and click or touch gesture fashion. These inputs in turn may be acted upon by the system in accordance with instructions from operating system and or client application module s . The GUI also serves to display the results of operation from the OS and application s whereupon the user may supply additional inputs or terminate the session e.g. log off .

The OS can execute directly on the bare hardware e.g. processor s of device . Alternatively a hypervisor or virtual machine monitor VMM may be interposed between the bare hardware and the OS . In this configuration VMM acts as a software cushion or virtualization layer between the OS and the bare hardware of the device .

VMM instantiates and runs virtual machine instances guest machines . Each guest machine comprises a guest operating system such as OS and one or more applications such as applications designed to execute on the guest operating system. The VMM presents the guest operating systems with a virtual operating platform and manages the execution of the guest operating systems. In some instances the VMM may allow a guest operating system to run as through it is running on the bare hardware of the device directly. In these instances the same version of the guest operating system configured to execute on the bare hardware directly may also be able to execute on VMM without modification or reconfiguration. In other words VMM may provide full hardware and CPU virtualization to a guest operating system in some instances. In other instances a guest operating system may be specially designed or configured to execute on VMM for efficiency. In these instances the guest operating system is aware that it executes on a virtual machine monitor. In other words VMM may provide para virtualization to a guest operating system in some instances.

The above described computer hardware and software are presented for purpose of illustrating basic underlying computer components that may be employed for implementing the disclosed technologies. The disclosed technologies however are not limited to any particular computing environment or computing device configuration. Instead the disclosed technologies may be implemented in any type of system architecture or processing environment capable of supporting the disclosed technologies presented in detail below.

While the disclosed technologies may operate within a single standalone computing device e.g. device of the disclosed technologies may be implemented in a distributed computing environment. is a block diagram of a distributed computing environment in which the disclosed technologies may be implemented.

As shown environment comprises a history preserving data pipeline system that implements one or more embodiments of the disclosed technologies one or more data sources e.g. A B C . . . N that provide data to the pipeline system and one or more data sinks e.g. A B C . . . N that consume data from the pipeline system .

In general the data sources provide data to the pipeline system and the data sinks consume data from the pipeline system . The pipeline system stores data it obtains from the data sources and data it provides to data sinks in datasets which are named collections of data. As described in greater detail elsewhere in this document datasets are immutable and versioned to facilitate tracing of dataset data through the data pipeline including historical i.e. not current versions of dataset data. In an embodiment the current version of a dataset is the latest most recent version of the dataset.

The pipeline system also manages aspects of building derived datasets which are datasets that are generated by executing the current version of an associated derivation program.

In an embodiment the current version of a derivation program is the latest most recent version of the derivation program. The derivation program may generate the data in a derived dataset it creates based on data in one or more other datasets. Alternatively the derivation program may generate derived dataset set independent of any input datasets. For example a derivation program may obtain data from one or more data sources directly and use the obtained data to generate data of a derived dataset. It is also possible for a derivation program to generate derived dataset data in this way where the derivation program also accepts one or more other datasets as input used for generating the derived dataset.

In many cases data provided by a data source to the pipeline system that is consumed by a data sink from the pipeline system is not consumed by the data sink in the same data format as which it was provided. In other words the data pipeline may transform data provided by a data source in one or more data transformation steps before it is provided to a data sink . More specifically derivation programs may transform data in datasets when generating building derived datasets in one or more data transformation steps before the derived datasets are provided to data sinks .

A data transformation step generally involves converting data in a source data format to data in a target data format. Such a data transformation step may involve mapping data elements of the data in the source data format to data elements in the target data format. Such mapping can be one to one one to many many to one or many to many. In an embodiment a data transformation step on dataset data is carried out at least in part with a data analytics cluster computing instance such as for example APACHE SPARK instance an APACHE HIVE instance or the like. For example a derivation program may contain one or more SPARK SQL HIVEQL or GROOVY commands which when executed by the data pipeline system carry out one or more data transformation steps on dataset data.

A data source e.g. B is any source of data provided to the data pipeline system for storing in one or more datasets. A dataset may be defined as a named collection of data. From the perspective of a data source e.g. C data provided by the data source to the pipeline system can be structured semi structured or unstructured data.

Structured data includes but is not limited to data that conforms to a well known data model. Examples of structured data include but are not limited to data stored in a relational database and spreadsheet data.

Semi structured data includes but is not limited to data that may not necessarily conform to a well defined data model but nonetheless includes self describing structure. Such self describing structure may be in the form of tags markup elements or other syntactic elements that separate semantic elements from each other within the data and enforce hierarchical relationships between semantic elements. Non limiting examples of semi structured data include but are not limited to eXtensible Markup Language XML data and JavaScript Object Notation JSON data.

Unstructured data includes but is not limited to data that does not conform to a data model and does not contain self describing structure. Examples of unstructured data include but are not limited to HyperText Markup Language HTML data e.g. web pages and other text data.

A data source e.g. A typically comprises one or more non volatile data storage devices e.g. one or more hard disks solid state drives or the like on which the provided data is physically stored. Typically the data is physically stored in one or more data containers such as for example in one or more file system files or in one or more other suitable data containers e.g. a disk block . The one or more data storage devices and hence the data source may be embodied in a single computing device or distributed across multiple computing devices.

A data source e.g. A typically also comprises a data access mechanism that a data requesting mechanism can use to obtain data from the data source. Typically the data access mechanism of a data source comprises one or more executing software programs e.g. application program A for reading data from one or more data containers of one or more data storage devices of the data source in response to a request for the data from a data requesting mechanism and for providing the requested data to the data requesting mechanism in response to the request.

Typically the data requesting mechanism also comprises one or more executing software programs e.g. application program B . The data requesting mechanism may be a component of or a component separate from a data source from which it requests data. Non limiting examples of a data access mechanism include a database management system server a network file server a web server or other server. Examples of a data requesting mechanism include but are not limited to a client application or other application for requesting data from a server.

The request for data from a data requesting mechanism to the data access mechanism of a data source e.g. N may be made according to a well known inter process communication protocol such as for example a well known networking protocol such as for example the HyperText Transfer Protocol HTTP the Structured Query Language SQL or other database query language networking protocol a Remote Procedure Call RPC protocol e.g. the Simple Object Access Protocol SOAP a Network File System NFS protocol and so forth. The network request may also be cryptographically secured according to a cryptographic protocol e.g. Transport Layer Security Secure Sockets Layer TLS SSL .

In some instances a data requesting mechanism may not use an inter process communication mechanism such as a networking protocol to request data from a data access mechanism of a data source e.g. B . For example if the data source e.g. B is one or more file system files then a data requesting mechanism may use an operating system application programming interface API to read data from the file s . In this example the operating system is considered to be the data access mechanism.

The distributed computing environment may have tens hundreds or even thousands or more data sources . Each of the data sources may provide different data possibly even in different data formats. As just one simple example one data source e.g. A may be a relational database server that provides rows of data another data source e.g. B may be a log file that stores log entries as lines of character data and another data source e.g. C may be a web service that provides data in one or more Simple Object Access Protocol SOAP messages. Overall the data pipeline system may be provided with heterogeneous data from multiple heterogeneous data sources .

A data requesting mechanism that provides data obtained from a data source e.g. B to the history preserving data pipeline system is referred to herein as a data provider . The environment may comprise multiple data providers. For example there could be a separate data provider for each data source that is to provide data to the data pipeline system . As described in greater detail elsewhere in this document a data provider can use a transaction service to provide data to the data pipeline system .

A data sink e.g. B is any consumer of dataset data from the data pipeline system . For the perspective of a data sink e.g. C the consumed data can be structured semi structured or unstructured data.

A data sink e.g. A typically comprises a data analysis mechanism for processing data obtained from the data pipeline system in some particular way. Typically the data analysis mechanism comprises one or more executing software programs e.g. application program A for analyzing organizing or otherwise processing data and presenting the results of data processing to a user. Examples of a data analysis mechanism include but are not limited to a graphical analysis software application or other software application for generating graphical charts reports or other graphical analysis of data in a graphical user interface. Another example of a data analysis mechanism is a text based search engine that parses and indexes text data to provide a full text searching service to users of the search engine.

The distributed computing environment may have tens hundreds or even thousands or more data sinks . Each of the data sinks may consume different data possibly even in different data formats. Further a data sink e.g. B may consume data provided by one or more data sources . In other words a data sink may consume data obtained by the data pipeline system from one data source e.g. A or more than one data source e.g. A and B . Accordingly a function of the data pipeline system may be to combine data from multiple data sources into a format that is consumable by a data sink . This is just one example of a possible function performed by the data pipeline system .

Overall the environment may comprise N data sources and M data sinks where N is equal to or different from M. Further data the pipeline system obtains from a data source e.g. B may be provided by the pipeline system to one or more data sinks e.g. one or more of A B C . . . N . Similarly the pipeline system may combine data obtained from multiple data sources e.g. A and B and provide the combined data to one or more data sinks e.g. one or more of A B C . . . N . As data moves through the pipeline system from the data sources to the data sinks a number of data transformation steps may be performed on the data to prepare the data obtained from the data sources for consumption by the data sinks .

Environment may include one or more data consuming mechanisms data consumers for consuming obtaining dataset data from the data pipeline system and providing the obtained data to one or more data sinks . Typically a data consumer comprises one or more executing software programs e.g. application program C . The data consumer may be a component of or a component separate from a data sink to which it provides data. A data consumer may provide data obtained from the data pipeline system in any manner that is suitable to a data sink to which it is providing the data. For example the data consumer may store the obtained data in a database or in a file system file or send the obtained data to a data sink over a network e.g. in one or more Internet Protocol IP packets . As described in greater detail elsewhere in this document a data consumer can use the transaction service of the history preserving data pipeline system to consume obtained dataset data from the pipeline system .

The storage plane may be implemented with one or more non volatile data storage devices which may be distributed across one or more computing devices e.g. device on one or more data networks. The storage plane comprises data lake build database and transaction database .

The data lake is where datasets are stored. In an exemplary embodiment the data lake comprises a distributed file system implemented with commodity computing devices. For example the data lake may comprise the APACHE HADOOP DISTRIBUTED FILE SYSTEM HDFS or other distributed file system built on commodity computing hardware. The data lake may also comprise archive storage for storing older dataset versions and or to serve as a backup for a primary storage system of the data lake e.g. a distributed file system . In one exemplary embodiment the AMAZON GLACIER archive storage service is used for archiving older versions of datasets.

The build database and the transaction database store metadata supporting functionality provided by the logic plane of the history preserving data pipeline system including metadata for supporting immutable and versioned datasets and for determining dataset build dependencies. The metadata stored and maintained in the build database and the transaction database by the logic plane is described in greater detail elsewhere in this document with respect to .

The build database and the transaction database may be implemented with one or more conventional database systems that store data in one or more tables. The build database and the transaction database may be managed by the same database system or different database systems. At a minimum the implementing database system should support atomic row updates. However support for multi row transactions is not required. In an exemplary embodiment the APACHE HBASE database system is used to implement the build database and the transaction database . In another exemplary embodiment the APACHE CASSANDRA database system is used to implement the build database and the transaction database . Another possible database system that may be used to implement the build database and the transaction database is the POSTGRES also known as POSTGRESQL open source database system.

Logic plane may be implemented as one or more software programs e.g. one or more application programs that are configured to execute on one or more computing devices e.g. device . Logic plane comprises to two services a build service and a transaction service .

The transaction service provides support for atomically creating and updating immutable and versioned datasets in the context of transactions. Data providers may use the transaction service to create and update datasets in the data lake with data obtained from data sources in the context of transactions. Data consumers may use the transaction service to read data from datasets in the data lake in the context of transactions that is then provided to the data sinks . In some embodiments the transaction service ensures that the data that can be read from a dataset is only data that has already been committed to the dataset by a previously successful transaction.

The build service leverages the transaction service to provide immutable and versioned derived datasets. A derived dataset may be defined as a dataset that is generated built by applying a derivation program or one or more sets of computer executable instructions to one or more other datasets. Thus it can be said that a derived dataset has a dependency on at least one other base dataset. A base dataset may accordingly be defined as a dataset on which at least one derived dataset has a dependency.

According to some embodiments a derivation program may be defined as a set of instructions associated with a derived dataset and which when executed uses the transaction service to read data from the base dataset s in the context of a transaction transforms and or validates the data in some way and uses the transaction service to write the transformed and or validated data to the derived dataset in the context of a transaction. Each transaction that modifies a dataset is assigned a transaction identifier by the transaction service that is unique to at least that dataset. The transaction service records the transaction identifier in the transaction database . By doing so each transaction that modifies a dataset is separately identifiable by its assigned transaction identifier. In addition the transaction service orders transactions on a dataset by the time that they are committed with corresponding transaction commit identifiers.

In order to increase automation of the pipeline the build service may maintain build dependency data that represents one or more directed acyclic graphs of dataset build dependencies. From the build dependency data the build service can determine for a given derived dataset the order in which to build other derived datasets before the given derived dataset can be built. As result it is no longer necessary for a human engineer to determine the order in which datasets need to be built.

When a new version of a derived dataset is built the build service may create a build catalog entry e.g. a row or record in the build database . The build catalog entry identifies the version s of the base dataset s from which the new version of the derived dataset was built. By doing so it can be determined for any given version of a derived dataset including historical versions the version s of the base dataset s from which the version of the derived dataset was built. Further because datasets including derived datasets are immutable data of a historical version of a derived dataset can be traced to the data from which it was derived even if that data is also historical.

The build service may also version derivation programs for tracing and tracking purposes. In this case the build catalog entry may also contain the version of the derivation program that was executed by the build service to build the new version of the derived dataset.

The functionality of the build service and the transaction service are described in greater detail elsewhere in this document.

The build service and the transaction service may each provide an interface by which users and or other software programs can invoke the services thereof by issuing one or more commands thereto and or requests thereof. For example the interface may be a graphical user interface a command line interface a networking interface or an application programming interface API .

The dataset builder periodically invokes the build service to build derived datasets. For example the dataset builder may send a network request to or otherwise invoke the build service to build one or more specifically identified datasets or to build all datasets.

In an embodiment the dataset builder issues a build all command to the build service on a periodic basis e.g. once a day . The build service interprets the build all command as a command to build all known derived datasets that are out of date . Known datasets are those specified in the build dependency data . Generally a derived dataset is out of date if no version of the derived dataset exists in the data lake or the current version of the derived dataset in the data lake is out of date.

The build dependency data represents one or more directed acyclic graphs also referred to herein as a build dependency graph . There may be multiple such graphs if for example none of the datasets represented by a graph has a build dependency on a dataset represented by another graph. Each graph comprises nodes and one or more directed edges connecting the nodes. A leaf node of a graph corresponds to a dataset that does not have any build dependencies on another dataset. A non leaf node of a graph corresponds to a dataset that has a build dependency on at least one other dataset. A root node of a graph is a non leaf node but where there are no build dependencies on the dataset represented by the root node. A graph may have only one root node or may have multiple root nodes. A directed edge connecting two nodes in a graph represents a build dependency between two datasets. A graph may be represented in a computer memory as an N ary tree data structure or other suitable data structure.

To illustrate a build dependency graph by a simple example consider graph of . Each circle of graph represents a node of the build dependency graph and each arrow connecting two circles of graph represents a directed edge of the build dependency graph. The letter in each circle of graph represents the name of the dataset represented by the corresponding node. As shown datasets F and A are represented by root nodes of the build dependency graph datasets C D and E are represented by leaf nodes of the build dependency graph and dataset B is represented by a non leaf node of the build dependency graph. Also shown dataset F has a build dependency on dataset C dataset B has build dependencies on datasets C and D and dataset A has build dependencies on datasets B C D and E. Dataset A s build dependency on dataset C is transitive by way of dataset B. Datasets F and B may be considered the parent datasets of dataset C and dataset C the child of datasets F and B datasets B and A the parent datasets of dataset D and dataset D the child of datasets B and A and dataset A the parent dataset of datasets B D and E and datasets B D and E the children of dataset A . However dataset A is not considered a parent of dataset C and dataset C is not considered a child of dataset A.

Referring once again to the dataset builder may be implemented as one or more computer programs or computer controls scripts i.e. one or more sets of computer executable instructions . The dataset builder may execute as part of the build service and or the transaction service i.e. in the same process space . Alternatively the dataset builder may execute as a separate process from the process es of the build service and the transaction service .

In an embodiment the dataset builder implements a message queue between the transaction service and the build service . When a new version of a dataset in the data lake is created or updated in the context of a committed transaction the transaction service adds a message to the tail of the message queue specifying the name of the created or updated dataset and a version identifier for the new version of the dataset. In an embodiment the version identifier for the new version of the dataset is a transaction identifier e.g. of the transaction that successfully committed the new version of the dataset.

The build service removes messages from the head of the message queue. For each such message removed from the message queue the build service determines from build dependency data any datasets that directly depend on the dataset named in the message. The datasets that directly depend on the named dataset can be identified in the build dependency data from any parent node s of the node corresponding to the named dataset in a build dependency graph assuming each node in the build dependency graph is associated in the build dependency data with the name or identifier of the dataset the node represents.

In some embodiments the build service then builds new version s of the dataset s that directly depend on the named dataset with the aid of the transaction service . Assuming the new version s of the dataset s are successfully committed to the data lake this causes the transaction service to add message s to the message queue for the new version s of the derived dataset s that directly depend on the named dataset. The build service continuously removes messages from the head of the message queue and builds new versions of datasets in this way until the message queue becomes empty e.g. after a dataset that has no dependencies on it is built .

In some embodiments the build service only builds a new version of a given dataset that depends on i.e. is a parent of a dataset named in a message obtained from the message queue if the current version of the given dataset is out of date with respect to the named dataset. As explained in greater detail elsewhere in this document to determine whether the current version of a dataset is out of date with respect to a child dataset the build service consults build catalog entries stored in the build database .

The build service can receive a command from the dataset builder to build a specifically named derived dataset. Alternatively the build service can receive a command from the dataset builder to build all derived datasets. In the latter case the build service may treat the command to build all derived datasets as one or more commands to build each derived dataset corresponding to a root node in the build dependency data . In both cases the build service may rebuild a given derived dataset only if the dataset is out of date with respect to its build dependencies.

To determine whether a given derived data is out of date with respect to its build dependencies the build service traverses the build dependency graph starting at the node corresponding to the given derived dataset and visits at least every non leaf node in the graph sub tree that is rooted at the node corresponding to the given derived dataset. During the traversal nodes are visited in post order according to a depth first traversal algorithm. For example referring briefly to if the given dataset is A then a post order depth first recursive traversal of graph would visit the node for dataset C and the node for dataset D before visiting the node for dataset B and would visit the node for dataset D and the node for dataset E before visiting the node for dataset A.

For each non leaf node visited during the traversal a determination is made whether the current version of the derived dataset corresponding to the visited non leaf node is out of date with respect to any of its child datasets. As described in greater detail elsewhere in this document to determine whether the current version of a dataset is out of date with respect to a child dataset the build service consults build catalog entries stored in the build database . If the current version of the derived dataset is out of date with respect to any of its child datasets then the build service executes the current version of the derivation program for the derived dataset to generate a new version of the derived dataset. After executing the new version of the derived dataset has been generated the build service adds a new build catalog entry e.g. A to the build database reflecting the new version of the derived dataset. In an embodiment datasets are recursively rebuilt if dependencies of the dataset to be rebuilt are also out of date.

In an embodiment as exemplified in a build catalog entry e.g. A corresponding to a non leaf node in the build dependency data may comprise a dataset name a dataset version and build dependency information . Build service adds a new build catalog entry e.g. A to build database each time a new version of a derived dataset is built and committed to the data lake in the context of a transaction facilitated by the transaction service . Thus build database may store a build catalog entry for each version of a derived dataset including the current version of the derived dataset and any historical prior versions of the derived dataset.

The dataset name is a unique identifier of a derived dataset. The dataset name may be used to identify the derived dataset across all versions of the derived dataset. In other words the dataset name may be the same in all build catalog entries for all versions of the derived dataset.

The dataset version is a unique identifier of a version of the derived dataset. Typically the dataset version is an ordinal or other information that can be used to determine whether the version of the derived dataset represented by the dataset version happened before or happened after other versions of the derived dataset represented by other build catalog entries in the build database with the same dataset name . In an embodiment the dataset version is an identifier e.g. a transaction commit identifier assigned by the transaction service to a commit of a transaction that stored the version of the derived dataset to the data lake .

The build dependencies may comprises a list of one or more dataset build dependencies and a derivation program build dependency . The list of dataset build dependencies correspond to any child datasets input to the version of the derivation program used to build the version of the derived dataset. If no such datasets were input then the list of dataset build dependencies may be an empty list.

In an embodiment each dataset build dependency e.g. A specifies the name and the version of a dataset that the version of the derived dataset was built generated from. For example the name and the version of a dataset build dependency e.g. B may correspond to a dataset name and a dataset version of a build catalog entry e.g. A for a version of a dataset that the version of the derived dataset was generated built from.

In an embodiment the derivation program build dependency specifies the name and the version of a derivation program that the build service executed to generate build the version of the derived dataset. For example the name and the version of the derivation program dependency may correspond to a derivation program entry e.g. A for the version of the derivation program that was executed by the build service to generate build the version of the derived dataset.

In an embodiment the build service identifies the current version of a derived dataset by querying build catalog entries for the build catalog entry e.g. A comprising the latest most recent dataset version and having a dataset name matching a name for the derived dataset specified as a query parameter.

In an embodiment the build service determines whether the current version of a given dataset is out of date based on the build catalog entry e.g. A for the current version of the given dataset. The current version of the given dataset may be considered out of date for any one of a number of reasons including because 1 there is a version of the derivation program that is newer than the version used to build the current version of the given dataset 2 there is a version of a child dataset that is newer the version of the child dataset from which the current version of the given dataset was built or 3 a dependency of the given dataset on another dataset was added or removed.

As shown in a derivation program entry e.g. A in the build database may comprise a derivation program name or other identifier a derivation program version a list of dataset dependencies and the executable code of the version of the derivation program itself.

The derivation program name is a unique identifier of a derivation program. The derivation program name may be used to identify the derivation program across all versions of the derivation program. In other words the derivation program name may be the same in all derivation program entries e.g. A for all versions of the derivation program.

The derivation program version is a unique identifier of a version of the derivation program. Typically the derivation program version is an ordinal or other information that can be used to determine whether the version of the derivation program represented by the derivation program version happened before or happened after other versions of the derivation program represented by other build catalog entries in the build database with the same derivation program name . For example if there are three versions of a derivation program then three derivation program entries may be stored in build database all with the same program name and with different derivation program versions . For example the derivation program version in the three derivation program entries could be 1 2 and 3 respectively.

The derivation program entry A may also comprises a list of one or more dataset dependencies . The list of dataset dependencies correspond to any datasets that the version of the derivation program depends on. If the version of the derivation program does not depend on any other datasets then the list of dataset build dependencies may be an empty list.

In an embodiment each dataset dependency e.g. A specifies the name of a dataset that the version of the derivation program depends on. For example the name of a dataset dependency e.g. B may correspond to a dataset name of one or more build catalog entries in the build database .

The derivation program code comprises the actual computer executable instructions of the version of the derivation program. Alternatively the derivation program code comprises a pointer or address to a storage location of the actual computer executable instructions.

In an embodiment a dataset in build dependency data is associated with a derivation program the dataset depends on. Such association can be made in the data between the name e.g. of the dataset and the name e.g. of the derivation program.

In an embodiment when a new derivation program entry e.g. A is added to the build database for a new version of the derivation program the direct dependencies in the build dependency data for any datasets that depend on the derivation program are updated based on the list of dataset dependencies in the new derivation program entry.

For example consider the following sequence of events 1 build dependency data indicates that dataset A has direct dependencies on datasets B and C and on derivation program P and 2 a new derivation program entry is added to the build database for a new version of the derivation program P the new derivation program entry has a list of dataset dependencies of indicating datasets B C and D. In response to the new derivation program entry for derivation program entry P being added to build database the build dependency data may be updated to indicate that dataset A now has direct dependencies on datasets B C and D.

In an embodiment the build service identifies the current version of a derivation program by querying derivation program entries for the derivation program entry e.g. A comprising the latest most recent dataset version and having a dataset name matching a name for the derivation program specified as a query parameter.

As mentioned data providers provide data to the data pipeline system obtained from data sources and data consumers obtain data from the data pipeline system and provide it to data sinks . To do so the data providers and the data consumers may invoke the services of the transaction service .

The transaction service facilitates writing immutable and versioned datasets in the context of transactions. To do so the transaction service implements a transaction protocol that the data providers and data consumers can invoke to carry out a transaction on a dataset.

As shown in the transaction protocol for conducting write transaction on a dataset comprises a start transaction command one or more write dataset commands and a commit command .

The transaction commands are issued by a client of the transaction service . The client may issue the commands to the transaction service via an interface offered to the client by the transaction service . The interface may be for example an application programming interface accessible invoke able over a network or from within a process. In an embodiment the client is one of the build service a data provider or a data consumer. At any given time the transaction service may be facilitating transactions on multiple datasets on behalf of multiple clients. For example one client may write to a dataset in the context of a transaction while another client is reading from the dataset in the context of a transaction.

A transaction on a dataset is initiated by a client issuing a start transaction command providing the name of the dataset. In response to receiving the start transaction command the transaction service assigns a transaction identifier to the transaction. The transaction identifier uniquely identifies the transaction at least for the dataset. After assigning a transaction identifier to the transaction on the dataset the transaction identifier is returned to the client.

Once a transaction has been started the client can perform a number of write operations on the dataset.

For a write command the client provides the name of the dataset the transaction identifier and the data to write to the dataset. In response the transaction service writes the data to a container in the data lake . The container may be a file in a distributed file system for example. To support immutable datasets the transaction service does not overwrite or otherwise delete or remove existing data from the dataset. In some embodiments this is accomplished by storing differences between dataset data. For example the data of a first version of a dataset may be stored in a first container in the data lake and the differences or deltas between the first version of the dataset and a second version of the dataset may be stored in a second container in the data lake . This delta encoding approach can be more space efficient in terms of space consumed in the data lake when compared to an approach where all data of each version of a dataset is stored in a separate container . If the write to the data lake is successful the transaction service returns an acknowledgement of the success to the client. Otherwise the acknowledgement may indicate that the write failed in which case the client may abort the transaction.

Once the client has finished writing to the dataset the client may commit any writes to the dataset by issuing a commit command providing the dataset name and the transaction identifier. To commit the transaction the transaction service assigns a commit identifier to the transaction and automatically updates a transaction entry e.g. A for the transaction in the transaction database . If the transaction is successfully committed the transaction service returns an acknowledgement to the client indicating so. Otherwise the acknowledgement indicates that the commit operation was not successful.

While the transaction service may be used to write data to a dataset in the context of a transaction the transaction service may also facilitate reading committed data from a dataset version. To do so a client may issue a read command to the transaction service . In the read command the client may specify the name and the version of the dataset version to read data from. In response to receiving the read command the transaction service may consult read the transaction entry in the transaction database for the dataset name and version specified in the read command if one exists. To identify this transaction entry the transaction service may query the transaction database for a transaction entry having a dataset name e.g. equal to the dataset name specified in the read command and having a transaction commit identifier e.g. equal to the dataset version specified in the read command. The query may also exclude any transaction entries that do not have a value for the transaction committed flag e.g. that indicates that the corresponding transaction was successfully committed. Alternatively the query may include only transaction entries that have a value for the transaction committed flag e.g. that indicate that the corresponding transaction was successfully committed.

If a transaction entry exists for a transaction that successfully committed the dataset name and version specified in the read command then the transaction service may provide data from the dataset version to the client or otherwise provide access to the client to data from the dataset version. If the transaction was not successfully committed then the transaction service may not provide data from the dataset version to the client. In this case the transaction service may also return an error or other indication that the dataset version was not successfully committed or that the read command failed.

In an embodiment a read command from a client specifies a dataset name but does not specify any particular dataset version. The transaction service may interpret this read command as a command to read data from the latest more recent successfully committed version of the dataset identified by the dataset name specified in the read command. The transaction service can identify the latest version of the dataset by identifying the transaction entry in the transaction database having a dataset name e.g. equal to the dataset name specified in the read command that has a value for the transaction committed flag e.g. that indicates the transaction represented by the transaction entry was successfully committed and that has the highest transaction commit identifier e.g. among all transactions successfully committed for the dataset.

In an embodiment as shown in a transaction entry e.g. A comprises a dataset name a transaction identifier a transaction start timestamp a transaction committed flag a transaction commit identifier and a list of data lake container identifiers . In other embodiments a transaction entry comprises more or less information that is shown in . For example a transaction entry may also have a transaction commit timestamp in addition to the transaction start timestamp .

A transaction entry e.g. A for a transaction on a dataset may be created at a first time and updated at a second time. The first time corresponds to when the transaction is started and the second time corresponds to when the transaction is committed. For example a transaction entry e.g. A may be created in response to a start transaction command and then subsequently updated in response to a commit transaction command .

When a transaction entry e.g. A is created in the transaction database in response to a start transaction command the dataset name the transaction identifier and the transaction start time stamp may be populated in the entry. The transaction start time stamp may be a system clock time corresponding to when the transaction was started. For example the transaction service may obtain a system clock time in response to receiving a start transaction command to use to populate the transaction start time stamp in the created entry. The transaction committed flag may also be set when the entry is created to indicate that the transaction has not yet committed. To indicate this the flag can be a predefined value e.g. N or 0 or left blank NULL . The flag may be checked to determine whether the transaction was committed. For example if the flag is present in an entry e.g. A for a transaction and has a certain predefined value e.g. Y TRUE or 1 that indicates that the transaction was successfully committed then the transaction is considered to have been successfully committed.

When a transaction entry e.g. A is updated in the transaction database in response to a commit transaction command the transaction committed flag the transaction commit identifier and the list of data lake container identifiers may be updated in the entry. The update to the entry to indicate that the transaction has been committed is preferably performed atomically to avoid putting the transaction database in an incomplete or inconsistent state. For example the transaction service may attempt to update a transaction entry in the transaction database with a put if absent operation.

As mentioned the flag may be updated to a predefined value that indicates that the transaction was committed.

The transaction commit identifier provides a total ordering of all committed transactions on the dataset identified by the dataset name of the entry. The transaction service may assign a transaction commit identifier to a transaction in response to a command e.g. to commit the transaction. For example the transaction commit identifier may be an integer or any other type of value e.g. a timestamp that can used for total ordering of transactions on a dataset.

The list of data lake container identifiers identify one or more data containers in the data lake in which any data written by the transaction is stored. The one or more data containers may contain just the data written by the transaction for example in the form of differences or deltas to prior version s of the dataset. Alternatively the one or more data containers may contain all data of the version of the dataset resulting from the transaction.

The following description presents method steps that may be implemented using computer executable instructions for directing operation of a device under processor control. The computer executable instructions may be stored on a computer readable storage medium such as CD DVD hard disk flash memory or the like. The computer executable instructions may also be stored as a set of downloadable computer executable instructions for example for downloading and installation from an Internet location e.g. a Web server .

Turning now to it illustrates an example process performed by history preserving data pipeline system for preserving history of a derived dataset.

The example process illustrates immutable and versioned derived datasets. Because the derived datasets like datasets generally are immutable and versioned in the system it is possible to trace dataset data to the data source data from which the dataset data was derived or obtained even if the dataset data is no longer in the current version of the derived dataset and even if the data source data is no longer available from the data source

The example process also illustrates how the history preserving data pipeline system improves on existing data pipeline systems by providing the ability to trace dataset data to the data source data from which the dataset data was derived or obtained even if the dataset data is no longer in the current version of the dataset and even if the data source data is no longer available from the data source.

The example process also illustrates how the system provides the ability to trace dataset data to the data source data the dataset data is based on but also if the dataset is a derived dataset to the version of the derivation program used to build the derived dataset which can be useful for tracking down errors in dataset data caused by errors or bugs i.e. programming errors in the version of the derivation program that was executed to build the dataset.

The example process also illustrates how the system alleviates human engineers from some manual tasks required by existing data pipeline systems related to maintaining and determining dataset build dependencies.

At step the data lake stores a first version of a derived dataset in one or more containers . At the same time a first transaction entry for a first transaction that committed the first version of the derived dataset to the data lake is stored in the transaction database . The first transaction entry comprises the name of the derived dataset the identifier of the first transaction a timestamp indicating when the first transaction was started a flag indicating that the first transaction was successfully committed a transaction commit identifier indicating when the first transaction was committed and a list of of one or more data lake container identifiers identifying one or more containers in the data lake containing data of the first version of the derived dataset.

At step in response to the first version of the derived dataset being successfully committed to the data lake the build service stores a first build catalog entry in the build database . The first build catalog entry comprises the name of the derived dataset a version identifier for the first version of the derived dataset which can be for example the transaction commit identifier stored in the first transaction entry for the first version of the derived dataset and build dependencies reflecting any dataset dependencies the first version of the derived dataset has on other datasets. For example the first version of the derived dataset may depend on i.e. may have been built based on at least a first version of another dataset and this dependency may be reflected in the build dependencies of the first build catalog entry. The build dependencies of the first build catalog entry may also reflect through the derivation program build dependency a first version of a derivation program used to build the first version of the derived dataset.

At step the transaction service updates the other dataset i.e. a dataset the first version of the derived dataset depends on to produce a second version of the other dataset resulting in the data lake storing the second version of the other dataset.

At step the data lake stores a second version of the derived dataset in one or more containers . At the same time a second transaction entry for a second transaction that committed the second version of the derived dataset to the data lake is stored in the transaction database . The second transaction entry comprises the name of the derived dataset the identifier of the second transaction a timestamp indicating when the second transaction was started a flag indicating that the second transaction was successfully committed a transaction commit identifier indicating when the second transaction was committed and a list of of one or more data lake container identifiers identifying one or more containers in the data lake containing data for the second version of the derived dataset.

At step in response to the second version of the derived dataset being successfully committed to the data lake the build service stores a second build catalog entry in the build database . The second build catalog entry comprises the name of the derived dataset a version identifier for the second version of the derived dataset which can be for example the transaction commit identifier stored in the second transaction entry for the second version of the derived dataset and build dependencies reflecting any dataset dependencies the second version of the derived dataset has on other datasets. For example the second version of the derived dataset may depend on i.e. may have been built based on at least the second version of the other dataset and this dependency may be reflected in the build dependencies of the second build catalog entry. The build dependencies of the second build catalog entry may also reflect through the derivation program build dependency the first version of the derivation program used to build the second version of the derived dataset.

While the invention is described in some detail with specific reference to a single preferred embodiment and certain alternatives there is no intent to limit the invention to that particular embodiment or those specific alternatives. Therefore those skilled in the art will appreciate that modifications may be made to the preferred embodiment without departing from the teachings of the present invention.

