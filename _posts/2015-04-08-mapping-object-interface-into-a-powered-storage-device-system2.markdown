---

title: Mapping object interface into a powered storage device system
abstract: Apparatus and associated method contemplating: operating a storage server having access to a plurality of storage devices, at least a first storage device of the plurality of storage devices defining a first bank and at least a second storage device of the plurality of storage devices defining a second bank; during a first time interval, energizing the first bank to enable data transfer, and de-energizing the second bank to disable data transfer; during the first time interval, mapping data received from the storage server to the first bank; during a second time interval that begins before a storage capacity of the first bank is full, de-energizing the first bank to disable data transfer, and energizing the second bank to enable data transfer; and during the second time interval, mapping the data received from the storage server to the second bank.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09569125&OS=09569125&RS=09569125
owner: Spectra Logic, Corp.
number: 09569125
owner_city: Boulder
owner_country: US
publication_date: 20150408
---
The present embodiments relate generally to interfacing object storage into a storage system that selectively manages a power distribution to extend the longevity of the storage system.

Information and management computer applications are used extensively to track and manage data relevant to an enterprise such as marketing and sales data manufacturing data inventory data and the like. Typically the application data resides in a centralized database within a distributed storage system. Local applications integrate remote clients and network servers to use and manage the application data and to make the application data available to remote applications such as via remote procedure calls RPCs .

The centralized location of the application data can be problematic in that it places on the enterprise owner the onus of maintaining complex computer systems in order to support the applications. For example it has traditionally been necessary for the enterprise owner to acquire the knowledge necessary to purchase and maintain the physical storage devices that store the data. To avoid running out of storage space typically more space is acquired than is actually needed. To avoid losing important data typically more data is retained than is actually needed. These pitfalls can spiral out of control resulting in a lack of storage space becoming an emergency or important data being lost or corrupted.

The enterprise owner also disadvantageously has the responsibility for the many administrative details required for efficient and effective data management such as managing individual utilizations of the physical data storage devices in view of the expected longevities of the data storage devices.

This has caused a proliferation in cloud based service providers offering remote storage and maintenance of the application data shifting the need for expertise from the enterprise owner to information technology experts. That advantageously enables the enterprise owner to pay for only the amount of storage space actually used because the charges can be provisioned as the amount of storage changes. Further shifting the administrative tasks to a contracted service provider frees up the enterprise owner to focus on its core business concerns.

With the shift toward cloud based storage of enterprise data improvements are needed that interface remote object storage activities into storage device systems that manage power distributions in order to significantly increase the expected longevity of the backend storage system. It is to a solution for that need that the embodiments of the present technology are directed.

Some embodiments of the claimed technology contemplate a method that includes operating a storage controller having data storage access to each of a plurality of storage devices grouped into a first bank of storage devices and a second bank of different storage devices and the storage controller caching application data from a host pending transfer to at least one of the banks of storage devices during a first time interval energizing the first bank of storage devices to enable data transfer and de energizing the second bank of storage devices to disable data transfer during the first time interval mapping the cached application data to the first bank of storage devices during a second time interval that begins before the storage capacity of the first bank of storage devices is full de energizing the first bank of storage devices to disable data transfer and energizing the second bank of storage devices to enable data transfer and during the second time interval mapping the cached data to the second bank of storage devices.

Some embodiments of the claimed technology contemplate an apparatus having a plurality of storage devices at least a first storage device of the plurality of storage devices defining a first bank and at least a second storage device of the plurality of storage devices defining a second bank. A data cache operably contains cache data received from a host pending transfer to the banks. A power manager module includes computer instructions stored in a computer memory and configured to be executed by a computer processor to selectively energize the banks to enable data transfer and to de energize the banks to disable data transfer in accordance with a longevity power profile the longevity power profile during a first time interval causing the first bank to be energized and the second bank to be de energized and during a subsequent second time interval causing the second bank to be energized and the first bank to be de energized the second interval beginning before a storage capacity of the first bank is full. A data transfer controller module includes computer instructions stored in computer memory and configured to be executed by computer processor to map first cached data to the first bank during the first time interval and to map second cached data to the second bank during the second time interval.

Some embodiments of the claimed technology contemplate a method including operating a storage server having access to a plurality of storage devices at least a first storage device of the plurality of storage devices defining a first bank and at least a second storage device of the plurality of storage devices defining a second bank during a first time interval energizing the first bank to enable data transfer and de energizing the second bank to disable data transfer during the first time interval mapping data received from the storage server to the first bank during a second time interval that begins before a storage capacity of the first bank is full de energizing the first bank to disable data transfer and energizing the second bank to enable data transfer and during the second time interval mapping the data received from the storage server to the second bank.

Initially it is to be appreciated that this disclosure is by way of example only not by limitation. The power management concepts herein are not limited to use or application with any specific system or method. Thus although the instrumentalities described herein are for the convenience of explanation shown and described with respect to exemplary embodiments it will be appreciated that the principles herein may be applied equally in other types of systems and methods involving interfacing object storage with a selectively powered storage device system.

To illustrate an exemplary environment in which preferred embodiments of the present technology can be advantageously practiced is a simplified depiction of a distributed storage system that includes a first server executing a first application APP1 and a second server executing a second application APP2 . The servers are sometimes referred to herein a clients in equivalent embodiments. The circuitries represented by the block depiction in and otherwise throughout this description generally can be embodied in a single integrated circuit or can be distributed among a number of discrete circuits as desired.

A detailed description of the computer applications APP1 APP2 is unnecessary for the skilled artisan to understand the scope of the claimed technology. Generally APP1 and APP2 can be any type of computer application such as but not limited to a point of sale application an inventory application a supply chain application a manufacturing application and the like. The servers can communicate with each other via a communications link such as a local area network LAN . Although not depicted in alternative embodiments the servers can be remotely located from each other and communicate individually via a wide area network such as the Internet. In any event the servers transfer application data to and retrieve application data from the network in the normal course of executing the respective applications APP1 APP2.

An archive storage controller ASC stores backup copies backups of the application data to an internal storage array . In this technology the ASC includes longevity management logic LML designed to extend the longevity of the storage devices collectively forming the internal storage array . In various embodiments not discussed in detail the ASC can subsequently provision the backups to a backend deep storage system such as to a tape library in these illustrative embodiments.

Although not depicted in it will be understood that backup logic resides within the storage system to provide top level control of what backups of the application data are obtained and how long the backups are retained. To that end the backup logic enforces policy rules that are established by an administrator a skilled information technology person managing the storage system . The administrator can control parametric inputs that define the backup policy rules. Although in the illustrative embodiments of this description the backup policy rules are the same for both the APP1 data and the APP2 data in alternative embodiments the APP1 data and the APP2 data can have different individual backup policy rules. The backup logic can include snapshot rules an entire copy of stored data incremental update rules changes in the stored data between successive snapshots thinning rules retention of archived data and the like. A detailed discussion of employing any particular backup logic is not necessary for the skilled artisan to understand the scope of the claimed subject matter.

The ASC can provide a cloud computing interface for migrating the backups from the servers to the ASC . For example a client application in each server or in some other network device can send data via the network by implementation of representational state transfer REST calls from the client application. That is the client application can send and receive data via connection with the ASC configured as a native hypertext transfer protocol HTTP device. The client connection with the ASC is built into the client application so that both sending data to and receiving data from the ASC is self contained and automatically established by the client application when necessary. Generally the client application maps requests responses to REST request response streams to carry out predetermined transfers of data.

Turning to the client applications logically organize the backups in a cache memory or cache residing in the ASC . A virtualization module creates a logical storage hierarchy in terms referred to herein as creating a logical container or more generally creating a container for the backups in the cache . Each container and its contents can be identified by strategic use of addressable HTTP uniform resource identifiers URIs . For this example it is presumed that first snapshot SS copies are obtained for the applications APP1 APP2 in accordance with the backup logic. The backup logic migrates the APP1 first snapshot SS1 copy to a newly created container by the PUT request https ASC.APP1.SS1. depicts the container contains that first snapshot labeled in terms of its sequential snapshot number and application number SS1 meaning the first snapshot copy for application APP1 for this example. depicts a subsequent point in time when the contents of the container include the first snapshot SS1in the root directory of the container and a number of incremental updates INC INC. . . INCnin respective subdirectories of the container .

Returning to the backup logic migrates the first APP2 snapshot to a newly created container by the PUT request https ASC.APP2.SS1 and so labeled SS1 meaning the first snapshot copy for application APP2 for this example. For purposes of this description the term migrate means that each backup is transferred to the ASC without leaving a copy remaining at the source. Alternatively the backups can be copied to the ASC while leaving a redundant copy residing at the source.

The cache in these illustrative embodiments is only a temporary repository for the backups. Eventually the ASC flushes the cached backups to prevent saturation of the cache . Preferably the backup logic only controls so far as migrating the backups to the containers . That is the backup logic is unaware of and has no control of what happens to the backups after they are migrated to the containers . Post container control is preferably performed by the ASC with no communication between the server or the backup logic and the ASC via the network . In alternative embodiments the backups can be transferred directly to the long term data storage without the caching.

To make the storage devices operational for transferring data in these illustrative embodiments the banks can be individually energized via an electrical link . Energizing a bank for purposes of this disclosure means making the storage devices within the bank capable of transferring data with the ASC via the network link . Each of the banks is selectively energized by a power manager which in these illustrative embodiments resides in the ASC .

Although in the illustrative embodiments that follow an entire bank is either energized or de energized the contemplated embodiments are not so limited. In alternative embodiments the skilled artisan will understand that subgroups of the storage devices within a bank can be selectively energized in the same way or even each of the storage devices within a bank can be individually energized within the contemplated meaning of the claimed invention.

It will also be noted that depicts each of the banks has an undefined number of storage devices . The number of storage devices only relates to the total amount of storage capacity in each bank such that the skilled artisan needs no explanation of any particular number of storage devices forming each bank or the shape of the array forming each bank . In successful reductions to practice a total of ninety six 96 one terabyte disk drives were employed. Four 4 of those disk drives were allocated as spares for use in any of the banks leaving the remaining disk drives grouped into four banks of twenty three 23 disk drives each. Although in the illustrative embodiments that follow each of the banks has the same number of disk drives the contemplated claimed invention is not so limited.

With reference to in the following description the power manager selectively energizes one or more of the banks according to predetermined criteria . In these illustrative embodiments the LML includes both the criteria module and the power manager module . The storage controller maps data residing in the cache to the one or more banks that are energized at the time and transfers the cached data accordingly. A detailed discussion of how the data is stored in each bank is not necessary for the skilled artisan to understand the subject matter and scope of the claimed invention. That is for example the skilled artisan knows the data can be striped across multiple disk drives to increase the write throughput capability by simultaneously writing buffered data to multiple disk drives. Preferably the data can be stored redundantly a RAID format to provide the opportunity for recovery of corrupted data.

The utilizations can be very generally approximated in terms of time energized. For example in simple embodiments the LML can energize each of the banks for a predetermined interval of time such as for one hour each. However it has been determined that a more meaningful utilization is tied to actual usage of the storage devices because an interval of time in and of itself does not account for variations in the network load of I O activity. Thus the LML can be programmed to switch energization from one bank to another after satisfying a set threshold number of REST calls or a number of seeks by a disk drive and the like. The threshold can be a predetermined constant or it can be varied based on network load feedback from preventive maintenance routines and the like.

Returning to in block the controller such as transfers the data to satisfy the GET request in accordance with the power manager s such as provision of energizing the banks such as . That is the GET request is satisfied by transferring the requested data from all the pertinent banks such as either simultaneously sequentially or a blend of both simultaneously and sequentially. Control then passes back to block to process the next REST call.

If on the other hand the determination of block is no then in block the controller such as ascertains the present status of energizing the banks such as . This informs the controller such as as to which bank s any available chunk can presently be mapped to the storage system such as . In block the controller such as determines whether a chunk is available in cache such as for flushing. If the determination of block is no then control returns to block for processing the next REST call.

If on the other hand the determination of block is yes then in block the controller such as determines whether the energization is scheduled to change before the chunk identified in block can be flushed from the cache such as . This is depicted in as the occurrence of a timeout prior to completion of the flushing. If the determination of block is yes then in block the power manager such as initializes the next bank such as and in block the controller such as maps the chunk identified in block to both the current and next banks such as . If on the other hand the determination of block is no then in block the controller such as maps the chunk identified in block to the current bank such as . In either case control then passes to block where the controller such as transfers the chunk to its mapped destination in the storage array . If block determines that the last REST call has been processed then the method ends else control returns to block to process the next REST call.

Embodiments of the present invention can be commercially practiced in a Black Pearl archive storage system connected to a Spectra Logic T Finity tape cartridge library on the backend manufactured by Spectra Logic of Boulder Colo. shows a commercial embodiment of a Black Pearl archive storage system communicatively linked with the T Finity unit see via a cable . The T Finity unit is depicted without an enclosure. The T Finity unit as depicted is a single cabinet but in alternative embodiments multiple cabinets can be combined as necessary to make an expanded tape library or to expand an existing tape library. The Black Pearl archive storage system has an ASC as described herein not depicted a plurality of data storage drives not shown and software that facilitates receiving data from a server not shown caching that data in at least one of the plurality of data storage drives and storing that data to tape cartridges in the T Finity library . The Black Pearl archive storage system is capable of handling all tape related storage commands without the server s involvement. The T Finity unit has a first and second shelf system that are adapted to support a plurality of the removable storage devices such as tape cartridge magazines in these illustrative embodiments. The second shelf system has at least one tape drive not depicted adapted to read and write data to and from a tape cartridge. Functionally interposed between the first and second shelf system is a magazine transport space . The magazine transport space provides adequate space for a tape cartridge magazine not depicted to be moved via a magazine transport not depicted from a position in the shelf system to a tape drive not depicted . Tape cartridge magazines can be transferred into and out from the T Finity library via an entry exit port . The T Finity tape library includes a means for cooling as shown by the fans located at the base.

It is to be understood that even though numerous characteristics and advantages of various embodiments of the present technology have been set forth in the foregoing description together with the details of the structure and function of various embodiments of the invention this disclosure is illustrative only and changes may be made in detail especially in matters of structure and arrangement of parts within the principles of the present technology to the full extent indicated by the broad general meaning of the terms in which the appended claims are expressed. For example in alternative equivalent embodiments the REST calls can be associated with some purpose other than archiving backups as in the disclosed illustrative embodiments while still maintaining substantially the same functionality without departing from the scope and spirit of the claimed invention. Another example can include using these techniques across multiple library partitions while still maintaining substantially the same functionality without departing from the scope and spirit of the claimed invention. Further though communication is described herein as between a server and a tape library communication can be received directly by a tape drive for example without departing from the scope and spirit of the claimed invention. Further for purposes of illustration the tape drive and tape cartridges are used herein to simplify the description for a plurality of tape drives and tape cartridges. Finally although the preferred embodiments described herein are directed to tape drive systems and related technology it will be appreciated by those skilled in the art that the claimed technology can be applied to other systems without departing from the spirit and scope of the present technology.

It will be clear that the claimed technology is well adapted to attain the ends and advantages mentioned as well as those inherent therein. While presently preferred embodiments have been described for purposes of this disclosure numerous changes may be made which readily suggest themselves to those skilled in the art and which are encompassed in the spirit of the claimed technology disclosed and as defined in the appended claims.

It is to be understood that even though numerous characteristics and advantages of various aspects have been set forth in the foregoing description together with details of the structure and function this disclosure is illustrative only and changes may be made in detail especially in matters of structure and arrangement to the full extent indicated by the broad general meaning of the terms in which the appended claims are expressed.

