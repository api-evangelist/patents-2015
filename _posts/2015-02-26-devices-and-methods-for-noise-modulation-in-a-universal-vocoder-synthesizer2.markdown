---

title: Devices and methods for noise modulation in a universal vocoder synthesizer
abstract: A device may receive an input indicative of acoustic feature parameters associated with speech. The device may determine a modulated noise representation for noise pertaining to one or more of an aspirate or a fricative in the speech based on the acoustic feature parameters. The aspirate may be associated with a characteristic of an exhalation of at least a threshold amount of breath. The fricative may be associated with a characteristic of airflow between two or more vocal tract articulators. The device may also provide an audio signal indicative of a synthetic audio pronunciation of the speech based on the modulated noise representation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09607610&OS=09607610&RS=09607610
owner: Google Inc.
number: 09607610
owner_city: Mountain View
owner_country: US
publication_date: 20150226
---
This application claims priority to U.S. Provisional Patent Application Ser. No. 62 020 754 filed on Jul. 3 2014 the entirety of which is herein incorporated by reference.

Unless otherwise indicated herein the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

A vocoder may include an analysis and synthesis system for reproducing human speech. As an example of vocoder analysis the vocoder may generate a parametric representation of a speech signal. The parametric representation may be amenable to modification encoding quantization and or statistical processing. As an example of vocoder synthesis the vocoder may utilize the parametric representation to generate a synthetic audio pronunciation of the speech.

In one example a method is provided that includes a device receiving an input indicative of acoustic feature parameters associated with speech. The device may include one or more processors. The method also includes determining a modulated noise representation for noise pertaining to one or more of an aspirate or a fricative in the speech based on the acoustic feature parameters. The aspirate may be associated with a characteristic of an exhalation of at least a threshold amount of breath. The fricative may be associated with a characteristic of airflow between two or more vocal tract articulators. The method also includes the device providing an audio signal indicative of a synthetic audio pronunciation of the speech based on the modulated noise representation.

In another example a computer readable medium is provided. The computer readable medium may have instructions stored therein that when executed by a computing device cause the computing device to perform functions. The functions comprise receiving an input indicative of acoustic feature parameters associated with speech. The functions further comprise determining a modulated noise representation for noise pertaining to one or more of an aspirate or a fricative in the speech based on the acoustic feature parameters. The aspirate may be associated with a characteristic of an exhalation of at least a threshold amount of breath. The fricative may be associated with a characteristic of airflow between two or more vocal tract articulators. The functions further comprise providing an audio signal indicative of a synthetic audio pronunciation of the speech based on the modulated noise representation.

In yet another example a device is provided that comprises one or more processors and data storage configured to store instructions executable by the one or more processors. The instructions may cause the device to receive an input indicative of acoustic feature parameters associated with speech. The instructions may also cause the device to determine a modulated noise representation for noise pertaining to one or more of an aspirate or a fricative in the speech based on the acoustic feature parameters. The aspirate may be associated with a characteristic of an exhalation of at least a threshold amount of breath. The fricative may be associated with a characteristic of airflow between two or more vocal tract articulators. The instructions may also cause the device to provide an audio signal indicative of a synthetic audio pronunciation of the speech based on the modulated noise representation.

In still another example a system is provided that comprises a means for a device receiving an input indicative of acoustic feature parameters associated with speech. The device may include one or more processors. The system further comprises a means for determining a modulated noise representation for noise pertaining to one or more of an aspirate or a fricative in the speech based on the acoustic feature parameters. The aspirate may be associated with a characteristic of an exhalation of at least a threshold amount of breath. The fricative may be associated with a characteristic of airflow between two or more vocal tract articulators. The system further comprises a means for the device providing an audio signal indicative of a synthetic audio pronunciation of the speech based on the modulated noise representation.

These as well as other aspects advantages and alternatives will become apparent to those of ordinary skill in the art by reading the following detailed description with reference where appropriate to the accompanying figures.

The following detailed description describes various features and functions of the disclosed systems and methods with reference to the accompanying figures. In the figures similar symbols identify similar components unless context dictates otherwise. The illustrative system device and method embodiments described herein are not meant to be limiting. It may be readily understood by those skilled in the art that certain aspects of the disclosed systems devices and methods can be arranged and combined in a wide variety of different configurations all of which are contemplated herein.

Vocoder systems may be utilized in various applications of speech processing. For example speech processing systems such as text to speech TTS systems may utilize a vocoder system to synthesize speech for various devices that include a speech based user interface. Such devices may be utilized in residences businesses vehicles or any other environment. For concatenative TTS synthesis utilizing the vocoder system may allow such devices to reduce a size of a speech corpus by encoding speech signals in the corpus. For statistical parametric TTS synthesis utilizing the vocoder system may allow statistical parametrization of speech signals that is amenable to statistical modeling and parameter generation. For example a statistical TTS device may adjust voice characteristics of a speech signal e.g. pitch etc. using data from a vocoder analyzer and utilize a vocoder synthesizer to generate a synthetic audio pronunciation of the adjusted speech signal. Additionally the vocoder system may allow fusing a concatenative TTS system with a statistical parameteric TTS system.

A vocoder may include an analysis unit for generating a parametric representation of a speech signal and a synthesis unit for reconstructing a speech waveform using the parametric representation.

Within examples a vocoder synthesis device is provided that is configured to process data from vocoder analysis systems having various types of parameterizations. Decoupling speech processing of the vocoder analysis systems from the parameter processing of the vocoder synthesis device in accordance with the present disclosure is advantageous for many reasons.

In one example the vocoder synthesis device may be configured to utilize asynchronous phase information that is incompatible with the speech processing of the vocoder analysis systems to enhance speech quality of synthetic audio output of the vocoder synthesis device. In another example the vocoder synthesis device may determine a modulated noise representation for noise pertaining to aspirates and or fricatives in an input speech signal. The modulated noise representation for example may be determined in a frequency domain and associated with harmonic frequencies of the speech signal. In turn for example the vocoder synthesis device may determine a representation for the speech signal that includes the modulated noise representation e.g. aspiration frication speech model and other acoustic feature parameters of the speech signal e.g. in the same frequency domain space . Such representation for example may allow manipulation e.g. modulation at run time etc. of the noise to further enhance synthesized speech quality.

Accordingly the vocoder synthesis device may be configured to provide an output audio signal indicative of a synthetic audio pronunciation of an input speech signal based on a modulation of the noise associated with the aspirates and or fricatives in the input speech signal. Example methods and systems herein may therefore allow high resolution fast e.g. low computational complexity and flexible e.g. universal vocoder speech synthesis.

Referring now to the figures illustrates a vocoder system according to an example embodiment. The system includes a speech signal a vocoder analysis module acoustic feature parameters a vocoder synthesis module and a synthetic audio signal .

In some examples functional blocks of the system such as the vocoder analysis module and or the vocoder synthesis module may be implemented as program instructions executable by one or more processors of a computing device to perform the functions described herein. Additionally in some examples the various functions of the system may be performed by more than one computing device. Therefore for example the illustration of the system in may represent a conceptual block diagram of the vocoder system that can be implemented according to various computing architectures that include one or more computing devices.

The speech signal may be associated with speech content such as recorded audio speech from a particular speaker. For example a microphone may output electronic signals that indicate various aspects of the speech content and or other sounds in an environment of the microphone and the speech signal may be indicative of the electronic signals from the microphone.

The vocoder analysis module may include various implementations to generate the acoustic feature parameters . Example implementations may include channel vocoders e.g. STRAIGHT TANDEM STRAIGHT etc. AHOcoder Sinusoidal Transform Codec Multi band Excitation Vocoder LF vocoder Harmonic plus Noise model a combination of these or any other type of vocoder analysis implementation.

Depending on the implementation s utilized by the vocoder analysis module the acoustic feature parameters may include one or more of spectral parameters e.g. spectral envelopes aperiodicity parameters e.g. aperiodicity envelopes or phase parameters e.g. phase envelopes . Spectral parameters for example may associate frequencies of the speech signal with a timbre of the speech signal . Aperiodicity parameters for example may indicate distribution e.g. noisiness aperiodicity etc. of spectral content around a given frequency of the speech signal e.g. harmonic to noise power ratio etc. . Further the acoustic feature parameters may have various types or formats according to the implementation utilized by the vocoder analysis module to generate the acoustic feature parameters .

In some examples the vocoder analysis module may be configured to provide the acoustic feature parameters as a sequence of speech frames. A given speech frame may include an acoustic feature representation of the speech signal at a given time within a duration of the speech signal . In some examples the sequence of speech frames may be provided at a fixed rate. For example adjacent speech frames may be separated by a given time period e.g. 5 ms etc. .

The vocoder synthesis module may be configured to receive any combination of the acoustic feature parameters from the vocoder analysis module to generate the synthetic audio signal . Therefore methods and systems herein allow for processing the various types of the acoustic feature parameters to provide fast and high resolution speech synthesis of the synthetic audio signal . Accordingly for example the vocoder synthesis module may correspond to a universal vocoder synthesizer.

In some examples the vocoder synthesis module may be configured to modify the acoustic feature parameters to enhance speech quality of the synthetic audio signal and or to modify voice characteristics of the synthetic audio signal . For example the vocoder synthesis module may be configured to determine an aspiration and or frication speech model for the speech signal and may allow modulation of such speech models at run time of the system . To facilitate such mode of synthesis in some examples the vocoder synthesis module may perform pitch synchronous synthesis to process a first pitch period of speech followed by a second pitch period of speech. Exemplary operation modes of the vocoder synthesis module are described in greater detail in other embodiments of the present disclosure.

In some examples the synthetic audio signal may be structured as a sequence of synthetic speech sounds provided at a fixed rate. For example where processing by the vocoder synthesis module is in a pitch synchronous mode the vocoder synthesis module may include output buffering to facilitate generating the fixed rate sequence of synthetic speech sounds.

It is noted that the functional blocks in are described in connection with functional modules for convenience in description. For example the functional block in shown as the vocoder analysis module does not necessarily need to be implemented as being physically present in the same device as the vocoder synthesis module but can be present in another memory included in another device not shown in . For example the vocoder analysis module may be physically located in a remote server accessible to the vocoder synthesis module via a network. Alternatively for example output of the vocoder analysis module e.g. the acoustic feature parameters may be stored in a memory accessible by the vocoder synthesis module and the vocoder synthesis module may generate the synthetic audio signal without any communication with the vocoder analysis module . Further in some examples embodiments of the system may be arranged with one or more of the functional modules subsystems implemented in a single chip integrated circuit and or physical component.

In some examples functional blocks of the system illustrated in may be implemented as program instructions executable by one or more processors of a computing device to perform the functions described herein. Additionally in some examples the various functions of the system may be performed by more than one computing device. Therefore for example the illustration of the system in may represent a conceptual block diagram of the vocoder synthesis system that can be implemented according to various computing architectures that include one or more computing devices.

The input may include acoustic feature parameters such as spectral parameters aperiodicity parameters and or phase parameters similarly to the acoustic feature parameters of the system . The acoustic feature parameters in the input may be structured as a sequence of speech frames provided at a fixed rate. A given speech frame may include the acoustic feature parameters that describe a speech signal at an analysis time instant of the speech signal e.g. within the duration of the speech signal .

The input buffering unit may be configured to receive the input including the fixed rate parameters and generate pitch synchronous parameters . The pitch synchronous parameters may correspond to a given sequence of speech frames from within the sequence of speech frames where adjacent speech frames of the given sequence are separated by a given pitch period. Thus for example the system may process one pitch period at a time using the pitch synchronous parameters .

By way of example a first speech frame in the given sequence of the parameters may be associated with a first time. The input buffering unit may determine a pitch period of the first speech frame and may provide a subsequent speech frame of the given sequence that is at a second time greater than the first time by the pitch period. Various methods for determining the pitch period are described in greater detail in other embodiments of the present disclosure.

The spectral sampling unit may be configured to receive the pitch synchronous parameters and generate spectral samples at harmonic frequencies of the speech signal indicated by the pitch synchronous parameters . In some examples the spectral samples may include spectral parameters aperiodicity parameters and or phase parameters mapped to the harmonic frequencies of the speech signal indicated by the pitch synchronous parameters .

The spectral samples may be received by the spectral processing unit for modification of corresponding acoustic feature parameters to enhance speech quality and to generate the processed spectral samples . In one example the aperiodicity parameters may be reduced or increased according to characteristics of the speech signal in a particular speech frame. In another example a dispersion factor may be applied by the spectral processing unit to the phase parameters for certain speech frames. Other examples are possible as well and are described in greater detail in other embodiments of the present disclosure.

The processed spectral samples may be received by the wave synthesis unit . In turn the wave synthesis unit may utilize the processed spectral samples to generate pitch synchronous audio signals . A given pitch synchronous audio signal may have a duration that corresponds to the pitch period between adjacent samples of the processed spectral samples and may correspond to a portion of the speech signal indicated by the input that is associated with the duration. The given pitch synchronous audio signal may be indicative of a synthetic speech waveform e.g. sinusoidal speech model etc. for the duration. By processing the pitch synchronous audio signals in the pitch synchronous mode for example the wave synthesis unit may provide a speech model for noise e.g. aspiration noise frication noise etc. and may therefore improve synthetic speech quality of the output .

The output buffering unit may receive the pitch synchronous audio signals and may generate the output that is structured as a sequence of synthetic audio sounds provided at the fixed rate. For example a given synthetic audio sound in the sequence may have a duration of 5 ms similarly to the time period between adjacent speech frames of the input .

It is noted that functional blocks of the system are illustrated in as separate blocks for convenience. In some embodiments the various functions described for the functional blocks of the system may be implemented by one computing device. Additionally in some examples the various functions may be combined or separated in an alternative arrangement to the arrangement of . For example a computing device may be configured to combine the functions of the spectral sampling unit and the spectral processing unit . Accordingly various implementations of the system are described in greater detail within exemplary device system and method embodiments of the present disclosure.

In addition for the method and other processes and methods disclosed herein the flowchart shows functionality and operation of one possible implementation of present embodiments. In this regard each block may represent a module a segment a portion of a manufacturing or operation process or a portion of program code which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process. The program code may be stored on any type of computer readable medium for example such as a storage device including a disk or hard drive. The computer readable medium may include non transitory computer readable medium for example such as computer readable media that stores data for short periods of time like register memory processor cache and Random Access Memory RAM . The computer readable medium may also include non transitory media such as secondary or persistent long term storage like read only memory ROM optical or magnetic disks compact disc read only memory CD ROM for example. The computer readable media may also be any other volatile or non volatile storage systems. The computer readable medium may be considered a computer readable storage medium for example or a tangible storage device.

In addition for the method and other processes and methods disclosed herein each block in may represent circuitry that is wired to perform the specific logical functions in the process.

In some examples functions of the method may be implemented by one or more components of the system such as the input buffering unit and or the output buffering unit .

At block the method includes receiving a sequence of speech frames indicative of speech. A first speech frame may include a first acoustic feature representation of the speech at a first time within a duration of the speech. The sequence may be associated with a given time period between adjacent speech frames of the sequence.

The sequence of speech frames may be similar to speech frames of the acoustic feature parameters of the system or speech frames of the input of the system . For example the first acoustic feature representation may be indicative of acoustic feature parameters such as spectral parameters aperiodicity parameters and or phase parameters provided by a vocoder analysis system similar to the vocoder analysis module of the system . Additionally for example the sequence of speech frames may be received and or structured at a fixed rate indicated by the given time period. For example the sequence of speech frames may be received by the method at speech frames second one every 5 ms .

At block the method includes determining a pitch period of the first speech frame based on a pitch frequency indicated by the first acoustic feature representation. The determination may be based also on the first speech frame being a voiced speech frame.

Voicing is a term used in phonetics and phonology to characterize speech sounds. A voiced speech sound may be articulated by vibration of vocal cords of a speaker. For example a pronunciation of the letter z in the word zebra corresponds to the voiced phone z and the articulation thereof may cause the vocal cords to vibrate at a particular pitch frequency e.g. fundamental frequency etc. . Further for example a pronunciation of the letter s in the word sing corresponds to the voiceless unvoiced phone s and the articulation thereof may not cause the vocal cords to vibrate similarly.

The method and other methods and systems herein may be configured to process input speech parameters e.g. the sequence of speech frames in a pitch synchronous mode of operation that corresponds to processing one pitch period at a time for example. In such pitch synchronous mode the method may allow modeling and or modification of speech characteristics that are associated with the pitch period such as aspiration and or frication speech characteristics.

Accordingly in some examples a device of the method may determine that the first speech frame is a voiced speech frame based on the first acoustic feature representation of the first speech frame. In turn the device may determine the pitch period of the first speech frame based on the pitch frequency of a speech sound associated with the first speech frame. For example if the pitch frequency is 10 Hz the pitch period may be determined as 1 10 100 milliseconds ms .

In some examples the method may also include identifying the first speech frame based on the first time corresponding to a voiced glottal closure time instant of the speech. The voiced glottal closure time instant may pertain to a characteristic of a closure of at least a portion of a glottis of a speaker for articulation of at least a portion of the speech. Thus for the voiced speech frame the voiced glottal closure time instant may be selected as the first time for which the pitch period length speech sound may be processed by the method for example. However in some examples other reference time instants of a glottal cycle of the speech may be utilized for determination of the first time.

At block the method includes providing a given pitch period as the pitch period of the first speech frame based on the first speech frame being an unvoiced speech frame. For example where the first acoustic feature representation indicates that the first speech frame is unvoiced e.g. phone s etc. the first speech frame may not have a pitch frequency. In turn for example the method may provide the given pitch period as the pitch period to allow for the pitch synchronous operation mode. In one example the given pitch period may be a fixed amount such as 10 ms that is assigned when an unvoiced speech frame is detected. In other examples the given pitch period may correspond to any other time period.

At block the method includes identifying a second speech frame from within the sequence that is associated with a second time within the duration of the speech. The second time may be based on a sum of the first time and the pitch period of the first speech frame. For example if the pitch period is 15 ms and the given time period between adjacent speech frames is 5 ms the second speech frame may be at a distance of three speech frames to the first speech frame.

In some examples the method may also include identifying the first speech frame based on the first time corresponding to an unvoiced time instant of the speech. For example unvoiced speech sounds such as the phone s within the speech may be associated with the unvoiced time instant and in turn the method at block may provide the given pitch period as the pitch period.

At block the method includes providing a synthetic audio sound based on the first acoustic feature representation and a second acoustic feature representation of the second speech frame. The synthetic audio sound may be associated with a portion of the speech between the first time and the second time. The synthetic audio sound may have a given duration that corresponds to the given time period between the adjacent speech frames in the sequence.

For example a system performing the method such as the system and or may process the first speech frame and the second speech frame e.g. via blocks and or of the system to generate the synthetic audio sound indicative of a pronunciation of the portion of the speech between the first time and the second time. Various methods may be utilized to generate the synthetic audio sound and are described in greater detail in embodiments of the present disclosure.

In some examples the method may also include determining a plurality of synthetic audio sounds associated with portions of the speech. For example a second pitch period associated with the second acoustic feature representation may be similarly determined. In turn a third speech frame that is at a distance of the second pitch period from the second speech frame may then be identified. Further a second synthetic audio sound of the plurality of synthetic audio sounds may be provided based on the second acoustic feature representation of the second speech frame and a third acoustic feature representation of the third speech frame. Thus for example a system performing the method such as the system may perform the functions of the wave synthesis unit and the output buffering unit .

The buffer may include any data structure such as a circular buffer. As illustrated in the buffer includes speech frames f f that may be similar to the acoustic feature parameters of the system and or the input of the system . For example the speech frames f f may include a sequence of speech frames received from a vocoder analysis device e.g. the vocoder analysis module similarly to the sequence of speech frames at block of the method . Although shows that the buffer includes ten speech frames f f in some examples the buffer may include less or more speech frames. To that end in some examples the buffer may be configured to include at least enough speech frames for a maximum expected pitch period of input speech. Other configurations of the buffer are possible as well.

The speech waveform is illustrated in along a space that includes a speech signal axis e.g. vertical axis and a time axis e.g. horizontal axis . In some examples functionality of systems and methods of the present disclosure may be performed in accordance with the system as follows.

The system may receive the speech frame f and store it in the buffer . The system may then determine the pitch period T of the speech frame f based on acoustic feature parameters associated with the speech frame f. For example the acoustic feature parameters may indicate that the speech frame f is a voiced speech frame having a pitch period of 15 ms. Further in some examples as illustrated by the speech waveform the speech frame f may include the acoustic feature parameters of the input speech at time t 0 ms. In turn for example if the speech frames f f are separated by a time period of 5 ms the speech frame f may be selected as the subsequent speech frame for processing e.g. the second speech frame of the method and the speech frames f and f may be provided to a spectral sampling unit e.g. the spectral sampling unit for vocoder speech synthesis. For example the speech frame f may correspond to a time of t T.

Further in the system the speech frame f may be associated with an unvoiced speech frame. Accordingly a given pitch period T may be provided e.g. 10 ms etc. such that the next speech frame provided may correspond to the speech frame f. For example the speech frame f may correspond to time t T T within the speech wave form . At this point the speech frame f may be associated with a voiced speech frame having a pitch period T of 20 ms e.g. pitch frequency of 50 hz and therefore the speech frame f may be provided as the subsequent speech frame for processing by an example vocoder synthesis system. For example the speech frame f may correspond to time t T T T within the speech waveform .

Therefore in the system the speech frames f f f and f may be provided for pitch synchronous vocoder speech synthesis.

In some examples functions of the method may be implemented by one or more components of the system such as the spectral sampling unit .

At block the method includes receiving an input indicative of acoustic feature parameters associated with speech. The input for example may be similar to the acoustic feature parameters of the system or the input of the system .

At block the method includes determining the acoustic feature parameters including spectral parameters associated with the speech aperiodicity parameters associated with the speech and phase parameters associated with the speech.

Devices and systems of the present disclosure allow for receiving the acoustic feature parameters from various types of vocoder analysis systems e.g. vocoder analysis module of the system . Accordingly in some examples the method at block may be configured to determine a representation that includes the various acoustic feature parameters sampled at harmonic frequencies of the speech. Therefore the method allows for universality of an example vocoder synthesizer to receive the various types of vocoder analysis data and provide a representation for the data.

Example spectral parameter types may include Cepstrum Mel Cepstrum Generalized Mel Cepstrum Discrete Mel Cepstrum Log Spectral Envelope Auto Regressive Filter Line Spectrum Pairs LSP Line Spectrum Frequencies LSF Mel LSP Reflection Coefficients Log Area Ratio Coefficients a combination of these or any other type of spectral parameter. Example aperiodicity parameter types may include Mel Cepstrum log aperiodicity envelope filterbank based quantization maximum voiced frequency a combination of these or any other type of aperiodicity parameter. Example phase parameter types may include minimum phase maximum phase sum of cosines pulse sum of sines pulse constant random pulse a combination of these or any other type of phase parameter. Other types of the acoustic feature parameters are possible as well such as deltas or deta deltas of the types described herein.

Accordingly in some examples the method may also include receiving a selection indicative of selected types of the acoustic feature parameters from one or more of Cepstrum Mel Cepstrum Generalized Mel Cepstrum Discrete Mel Cepstrum Log Spectral Auto Regressive Line Spectrum Pairs Line Spectrum Frequencies Mel Line Spectrum Pairs Reflection Coefficients Log Area Ratio Coefficients minimum phase maximum phase sum of cosines pulse sum of sines pulse constant random pulse log aperiodicity filterbank based quantization or maximum voiced frequency. In these examples determining the acoustic feature parameters may be based on the selection.

In examples that include such selection the method may determine the acoustic feature parameters including the spectral parameters the aperiodicity parameters and the phase parameters while associating the various acoustic feature parameters with the same harmonic frequencies.

By sampling the acoustic feature parameters at the harmonic frequencies an order of the speech parameterization may be unconstrained or may be marginally constrained thereby allowing high resolution speech processing. For example the acoustic feature parameters may be sampled exactly at glottal closure time instants e.g. pitch synchronous mode similarly to the method . In this example the method may determine the phase parameters at the harmonic frequencies as well as the spectral parameters and the aperiodicity parameters.

Accordingly in some examples the determined phase parameters may be based on measured phase values indicated in the input and associated with one or more particular times within a duration of the speech. The one or more particular times for example may correspond to the glottal closure time instants.

In some examples where the input includes a sequence of speech frames similarly to the method the pitch period may be quantized to an integer value according to a sampling rate e.g. fixed rate etc. of the input sequence of speech sounds according to equation 1 below.

In the equation 1 circumflex over may be the quantized pitch period may be the pitch period and Fmay be the sampling rate. In the example of the system Fmay be based on the given tip e period e.g. at block between adjacent speech frames in the input. Such quantization may simplify processing of the acoustic feature parameters during wave synthesis e.g. wave synthesis unit of the system .

Additionally in some examples sampled harmonic amplitudes of the spectral parameters may be power normalized according to equation 2 below.

In the equation 2 may correspond to the power normalized amplitude Amay correspond to the sampled harmonic amplitude of the spectral parameters.

At block the method includes providing an audio signal indicative of a synthetic audio pronunciation of the speech based on the acoustic feature parameters. Various methods may be employed for providing the audio signal such as by a unit of the system e.g. units and or . It is noted that providing the audio signal is in some examples based on a representation that includes all the acoustic feature parameters e.g. spectral aperiodicity and phase based on the sampling at harmonic frequencies at block . Thus various advantages may be realized in accordance with the method such as high resolution processing and specialized speech models e.g. for aspiration and or frication speech .

In some examples functions of the method may be implemented by one or more components of the system such as the spectral processing unit .

At block the method includes receiving an input indicative of acoustic feature parameters associated with speech. The input for example may be similar to the acoustic feature parameters of the system or the input of the system .

At block the method includes identifying a given speech frame that includes a given acoustic feature representation of the speech at a given time within a duration of the speech. The given speech frame may correspond for example to one of the speech frames f f in the buffer of the system . Therefore for example the given time may correspond to a voiced glottal closure time instant or an unvoiced time instant similarly to blocks of the method .

At block the method includes determining the acoustic feature parameters based on samples of the given acoustic feature representation at harmonic frequencies associated with the given speech frame. Similarly to block of the method for example the acoustic feature parameters may include spectral parameters aperiodicity parameters and or phase parameters.

At block the method includes modifying the acoustic feature parameters to enhance quality of the speech. For example the acoustic feature parameters such as aperiodicity parameters may be modified to reduce noisiness of the given speech frame. In turn for example phase parameters may be modified to include random dispersion according to the modified aperiodicity parameters.

In one example the given speech frame may correspond to an unvoiced speech frame. In this example the method may include modifying the acoustic feature parameters for the given speech frame that are associated with given harmonic frequencies less than a threshold. For example for the given harmonic frequencies less than 500 Hz the method may apply a suppression function to harmonic amplitudes to mitigate vocoder analysis errors. Further in this example the method may also include modifying phase parameters of the given speech frame to correspond to random values e.g. in the range .

In another example the given speech frame may correspond to a voiced speech frame. In this example the method may include modifying aperiodicity parameters of the given speech frame to correspond to a first value for first harmonic frequencies greater than a threshold a second value for second harmonic frequencies less than a second threshold and one or more values between the first value and the second value for given harmonic frequencies less than the first threshold and greater than the second threshold. For example the aperiodicity parameters having the first harmonic frequencies greater than 4.4 kHz e.g. the first threshold may be set to a value of 1 the aperiodicity parameters having the second harmonic frequencies less than 1 kHz e.g. the second threshold may be set to a value of 0 and the aperiodicity parameters for the given harmonic frequencies e.g. between 1 kHz and 4.4 kHz may be assigned values between 0 and 1. Thus in this example the noisiness corresponding to the aperiodicity parameters may be reduced at least for the first harmonic frequencies and the second harmonic frequencies. In some examples such process may be employed when the given speech frame is deeply within a voice region of the speech. For example the modification of the aperiodicity parameters may be performed if the given speech frame is at a threshold e.g. 20 ms etc. time from the last unvoiced speech frame processed by the method .

In some examples modifying the aperiodicity parameters may also include monotonically increasing the one or more values associated with the given harmonic frequencies to further reduce noisiness associated with the given harmonic frequencies e.g. between 1 kHz and 4.4 kHz etc. . Equation 3 below illustrates an example for the monotonic increase. circumflex over 3 

In the equation 3 circumflex over may correspond to the monotonically increased aperiodicity may correspond to the modified periodicity e.g. having valued between 0 and 1 prior to the monotonic increase and may correspond to the monotonically increasing function. Equation 4 below illustrates the operation of the monotonically increasing function . circumflex over 4 

Additionally in some examples the method may include determining a dispersion factor for phase parameters of the given speech frame based on the modified aperiodicities and modifying the phase parameters based on the dispersion factor. Equation 5 below illustrates example modification of the phase parametres. circumflex over circumflex over 5 

In the equation 5 may correspond to the phase circumflex over may correspond to the modified phase parameters circumflex over U may correspond to the dispersion factor and U may correspond to a uniform random value e.g. in the range 1 1 .

At block the method includes providing an audio signal indicative on a synthetic audio pronunciation of the speech based on the modified acoustic feature parameters. Various methods of the present disclosure may be employed for providing the audio signal similarly to the wave synthesis unit of the system .

In some examples functions of the method may be implemented by one or more components of the system such as the wave synthesis unit .

At block the method includes receiving an input indicative of acoustic feature parameters associated with speech. The input for example may be similar to the acoustic feature parameters of the system or the input of the system .

At block the method includes determining a modulated noise representation for the speech based on the acoustic feature parameters. The modulated noise representation may for example allow modulating noise pertaining to one or more of an aspirate or a fricative in the speech. The aspirate may be associated with a characteristic of an exhalation of at least a threshold amount of breath. The fricative may be associated with a characteristic of airflow between two or more vocal tract articulators.

In some examples the speech may include articulation of various speech sounds that involve exhalation of breath. Such articulation may be described as aspiration and or frication and may cause noise in the input speech signal. An example aspirate may correspond to the pronunciation of the letter p in the word pie. During articulation of such aspirate the at least threshold amount of breath may be exhaled by a speaker pronouncing the word pie. In turn an audio recording of the pronunciation of the speaker may include breathing noise due to the exhalation. Accordingly in some examples the method and other systems and methods herein may include determining the modulated noise representation for such speech e.g. the aspirate .

Further in some examples the speech may include the fricative that is associated with airflow between two or more vocal tract articulators. A non exhaustive list of example vocal tract articulators may include a tongue lips teeth gums palate etc. Noise due to such fricative speech may also be characterized by the method to enhance quality of synthesized speech. For example breathing noise due to airflow between a lip and teeth may be different from breathing noise due to airflow between a tongue and teeth.

Further for example the fricative speech may be included in voiced speech and or unvoiced speech. Voicing is a term used in phonetics and phonology to characterize speech sounds. A voiced speech sound may be articulated by vibration of vocal cords of a speaker. For example a pronunciation of the letter z in the word zebra corresponds to the voiced phone z and the articulation thereof may cause the vocal cords to vibrate at a particular pitch frequency e.g. fundamental frequency etc. . Further for example a pronunciation of the letter s in the word sing corresponds to the voiceless unvoiced phone s and the articulation thereof may not cause the vocal cords to vibrate similarly.

Accordingly the modulated noise representation determined at block may modulate the speech to account for such differences e.g. voiced unvoiced frication aspiration etc. and allow modulation of corresponding noise accordingly to enhance quality of synthesized speech.

Table 1 below illustrates example fricatives in the English language. In the example of the first row in Table 1 a pronunciation of the letter f in the word fan e.g. corresponding to the phone f may be associated with airflow between the lower lip and the teeth and may be unvoiced e.g. no vibration of the vocal cords . Further in the example a pronunciation of the letter v in the word van e.g. corresponding to the phone v may also be associated with the airflow between the lower lip and the teeth but may be voiced e.g. vibration of the vocal chords at a pitch frequency . Other vocal tract articulators than the vocal tract articulators illustrated in Table 1 are possible and positions of the vocal tract articulators may also be different than those illustrated in Table 1. For example other languages such as French may include additional and or alternative voiced fricatives.

In some examples the speech indicated by the input may be processed in a pitch synchronous mode e.g. method and the acoustic feature parameters may be determined and processed at harmonic frequencies e.g. methods . In turn the method at block may provide a representation e.g. speech model of the speech indicated by the input based on the acoustic feature parameters. Such representation may be compatible with any type of speech e.g. periodic aperiodic semi periodic etc. in high resolution. Equation 6 below illustrates such representation. cos cos cos 6 

In equation 6 s n may correspond to the representation of the speech K may correspond to the number of the harmonics n may correspond to a time index e.g. 2 . . . T where T is the synthesis period or the pitch period of the method A n may correspond to the instantaneous amplitude of a k th harmonic n may correspond to the instantaneous phase of the k th harmonic n may correspond to the instantaneous aperiodicity of the k th harmonic e.g. in the range 0 1 may correspond to a modulation bias e.g. 1.2 and may correspond to a modulation factor e.g. 0.5 .

As illustrated in equation 6 the representation of the speech s n includes the modulated noise representation associated with the aspirate and or the fricative. Equation 7 below identifies the modulated noise representation g n that is included in equation 6 . cos 7 

Accordingly in some examples the method may include determining a representation e.g. equation 6 of the speech that includes the acoustic feature parameters mapped to harmonic frequencies of the speech e.g. k 2 . . . K . In these examples the representation may also include the modulated noise representation e.g. g n of equation 7 mapped also to the harmonic frequencies. Further in these examples the representation may include one or more modulation factors e.g. and or in the modulated noise representation. For example such representation may correspond to a sinusoidal speech model for the speech indicated by the input that is augmented to include a noise modulation model e.g. the modulated noise representation g n .

At block the method includes providing an audio signal indicative of a synthetic audio pronunciation of the speech based on the modulated noise representation.

The modulated noise representation g n of equation 7 for example may correspond to an explicit aspiration and or frication model. Accordingly the method may allow incorporating aspiration noise into the speech signal representation equation 6 to enhance quality of the audio signal at block . Further the modulated noise representation may also allow modeling and or modulating associated noise of fricatives and or other breathy lax speech characteristics. By incorporating the modulated noise representation in the speech model e.g. the representation of the speech in some examples the audio signal may simulate aspiration frication noise patterns of actual phonation sounds.

In some examples the method may receive the input at block as a sequence of speech sounds similarly to the method . Similarly to the method in these examples the method may process two speech frames that correspond to a left speech frame and a right speech frame bordering a pitch period. Further in some examples the equations 6 7 may be modified by the method to process the two speech frames according to types e.g. voiced unvoiced of the two speech frames. Table 2 below illustrates four different possibilities for the speech frame types.

By way of example the method may match harmonics e.g. sinusoids of the left speech frame and the right speech frame based on satisfying particular criteria. For example the particular criteria may include the left speech frame and the right speech frame being a same type e.g. voiced voiced or unvoiced unvoiced that correspond to the first and last rows of the Table 2. Further the particular criteria may include voiced harmonics e.g. last row of Table 2 being matched based on a difference between the harmonic frequencies of the left speech frame and the right speech frame being less than a threshold e.g. 30 . If the particular criteria are not met in some examples other speech processing techniques may be utilized such as fade in fade out windows.

As an example for matching harmonics a single matched harmonic s n may be represented by equation 8 below. cos cos 8 

In equation 8 the instantaneous amplitude A n and the instantaneous aperiodicity n may be determined based on a linear interpolation between the corresponding left speech frame and right speech frame acoustic feature parameters. Alternatively other types of interpolation may be utilized such as splines etc. For the instantaneous phase n other types of interpolation e.g. cubic phase interpolation etc. may be utilized by the method that are suitable for modulo n circular nature of phase parameters.

The device may include a computing device such as a smart phone digital assistant digital electronic device body mounted computing device personal computer or any other computing device configured to execute program instructions included in the data storage to operate the device . The device may include additional components not shown in such as a camera an antenna or any other physical component configured based on the program instructions executable by the processor to operate the device . The processor included in the device may comprise one or more processors configured to execute the program instructions to operate the device .

The input interface may include an input device such as a microphone or any other component configured to provide an input signal comprising audio content associated with speech to the processor . The output interface may include an audio output device such as a speaker headphone or any other component configured to receive an output audio signal from the processor and output sounds that may indicate synthetic speech content based on the output audio signal.

Additionally or alternatively the input interface and or the output interface may include network interface components configured to respectively receive and or transmit the input signal and or the output signal described above. For example an external computing device e.g. server etc. may provide the input signal e.g. speech content acoustic feature parameters sequence of speech frames etc. to the input interface via a communication medium such as Wifi WiMAX Ethernet Universal Serial Bus USB or any other wired or wireless medium. Similarly for example the external computing device may receive the output signal from the output interface via the communication medium described above.

The data storage may include one or more memories e.g. flash memory Random Access Memory RAM solid state drive disk drive etc. that include software components configured to provide the program instructions executable by the processor to operate the device . Although illustrated in that the data storage is physically included in the device in some examples the data storage or some components included thereon may be physically stored on a remote computing device. For example some of the software components in the data storage may be stored on a remote server accessible by the device . The data storage may include the program instructions and a vocoder analysis dataset . In some examples the data storage may optionally include a linguistic feature dataset .

The program instructions include a vocoder synthesis module to provide instructions executable by the processor to cause the device to perform functions of the present disclosure. For example the functions may include generating a synthetic speech audio signal via the output interface in accordance with the systems and or the methods . For example the vocoder synthesis module may be similar to the vocoder synthesis module of the system and or the vocoder synthesis system . The vocoder synthesis module may comprise for example a software component such as an application programming interface API dynamically linked library DLL or any other software component configured to provide the program instructions to the processor .

The vocoder analysis dataset may include data from a vocoder analysis module such as the vocoder analysis module of the system . For example the vocoder analysis dataset may include a sequence of speech frames indicative of acoustic feature parameters pertaining to the speech indicated by the input interface . Such sequence for example may be received by the vocoder synthesis module to provide a synthetic audio signal output via the output interface e.g. in accordance with the methods and or .

To facilitate the operation of the vocoder synthesis module in some examples the linguistic feature dataset may be included in the data storage and may be utilized to determine the sequence of speech frames from the vocoder analysis dataset . For example the linguistic feature dataset may include one or more phonemes that correspond to text for which the device is configured to provide the output synthetic audio signal. Accordingly for example the vocoder synthesis module may obtain vocoder analysis data from the vocoder analysis dataset that corresponds to the one or more phonemes indicated in the linguistic feature dataset and provide the synthetic audio signal that corresponds to such data.

Although shows three programmable devices distributed application architectures may serve tens hundreds thousands or any other number of programmable devices. Moreover the programmable devices and or any additional programmable devices may be any sort of computing device such as an ordinary laptop computer desktop computer network terminal wireless communication device e.g. a tablet a cell phone or smart phone a wearable computing device etc. and so on. In some examples the programmable devices and may be dedicated to the design and use of software applications. In other examples the programmable devices and may be general purpose computers that are configured to perform a number of tasks and may not be dedicated to software development tools. For example the programmable devices may be configured to provide speech processing functionality similar to that discussed in . For example the programmable devices may include a device such as the device or may include a system such as the systems or .

The server devices and can be configured to perform one or more services as requested by programmable devices and or . For example server device and or can provide content to the programmable devices . The content may include but is not limited to web pages hypertext scripts binary data such as compiled software images audio and or video. The content can include compressed and or uncompressed content. The content can be encrypted and or unencrypted. Other types of content are possible as well.

As another example the server device and or can provide the programmable devices with access to software for database search computation e.g. vocoder speech synthesis graphical audio e.g. speech content video World Wide Web Internet utilization and or other functions. Many other examples of server devices are possible as well. In some examples the server devices and or may perform at least some of the functions described in .

The server devices and or can be cloud based devices that store program logic and or data of cloud based applications and or services. In some examples the server devices and or can be a single computing device residing in a single computing center. In other examples the server devices and or can include multiple computing devices in a single computing center or multiple computing devices located in multiple computing centers in diverse geographic locations. For example depicts each of the server devices and residing in different physical locations.

In some examples data and services at the server devices and or can be encoded as computer readable information stored in non transitory tangible computer readable media or computer readable storage media and accessible by programmable devices and and or other computing devices. In some examples data at the server device and or can be stored on a single disk drive or other tangible storage media or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.

As noted above in some embodiments the disclosed techniques e.g. methods and can be implemented by computer program instructions encoded on a computer readable storage media in a machine readable format or on other media or articles of manufacture e.g. the program instructions of the device or the instructions that operate the server devices and or the programmable devices in . is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device arranged according to at least some embodiments disclosed herein.

In one embodiment the example computer program product is provided using a signal bearing medium . The signal bearing medium may include one or more programming instructions that when executed by one or more processors may provide functionality or portions of the functionality described above with respect to . In some examples the signal bearing medium can be a computer readable medium such as but not limited to a hard disk drive a Compact Disc CD a Digital Video Disk DVD a digital tape memory etc. In some implementations the signal bearing medium can be a computer recordable medium such as but not limited to memory read write R W CDs R W DVDs etc. In some implementations the signal bearing medium can be a communication medium e.g. a fiber optic cable a waveguide a wired communications link etc. . Thus for example the signal bearing medium can be conveyed by a wireless form of the communications medium .

The one or more programming instructions can be for example computer executable and or logic implemented instructions. In some examples a computing device such as the processor equipped device of and or programmable devices of may be configured to provide various operations functions or actions in response to the programming instructions conveyed to the computing device by one or more of the computer readable medium the computer recordable medium and or the communications medium . In other examples the computing device can be an external device such as server devices of in communication with a device such as the device and or the programmable devices 

The computer readable medium can also be distributed among multiple data storage elements which could be remotely located from each other. The computing device that executes some or all of the stored instructions could be an external computer or a mobile computing platform such as a smartphone tablet device personal computer wearable device etc. Alternatively the computing device that executes some or all of the stored instructions could be remotely located computer system such as a server. For example the computer program product can implement the functionalities discussed in the description of .

It should be understood that arrangements described herein are for purposes of example only. As such those skilled in the art will appreciate that other arrangements and other elements e.g. machines interfaces functions orders and groupings of functions etc. can be used instead and some elements may be omitted altogether according to the desired results. Further many of the elements that are described are functional entities that may be implemented as discrete or distributed components or in conjunction with other components in any suitable combination and location or other structural elements described as independent structures may be combined.

While various aspects and embodiments have been disclosed herein other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting with the true scope being indicated by the following claims along with the full scope of equivalents to which such claims are entitled. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting.

