---

title: Scaling a cloud infrastructure
abstract: A method for scaling a cloud infrastructure, comprises receiving at least one of resource-level metrics and application-level metrics, estimating parameters of at least one application based on the received metrics, automatically and dynamically determining directives for scaling application deployment based on the estimated parameters, and providing the directives to a cloud service provider to execute the scaling.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09300553&OS=09300553&RS=09300553
owner: International business Machines Corporation
number: 09300553
owner_city: Armonk
owner_country: US
publication_date: 20150707
---
This application is a Continuation of U.S. patent application Ser. No. 14 554 331 filed on Nov. 26 2014 which claims the benefit of U.S. Provisional Patent Application Ser. No. 61 916 348 filed on Dec. 16 2013 the contents of which are each herein incorporated by reference in their entireties.

The field generally relates to systems and methods for scaling a cloud infrastructure and in particular systems and methods for automatically and dynamically scaling a cloud infrastructure based on parameter estimates.

With the advent of cloud computing many application owners have started moving their deployments into the cloud. Cloud computing offers many benefits over traditional physical deployments including lower infrastructure costs and elastic resource allocation. These benefits can be especially advantageous for applications with a dynamic workload demand. Such applications can be deployed in the cloud based on the current demand and the deployment can be scaled dynamically in response to changing workload demand. This is in contrast to the expensive and wasteful traditional physical deployments where the application is always provisioned with sufficient resources to handle worst case workload demand.

While cloud computing is a promising option for application owners it may not be easy to take full advantage of the benefits of the cloud. Specifically while cloud computing offers flexible resource allocation it is up to the customer application owner to leverage the flexible infrastructure. That is the user must decide when and how to scale the application deployment to meet the changing workload demand. Since traditional deployments were often over provisioned to handle worst case demands users never had to worry about dynamically resizing their deployment. However to fully leverage the advantages of cloud computing users will now have to worry about dynamically resizing their deployments.

Applications with a dynamic workload demand need access to a flexible infrastructure to meet performance guarantees and minimize resource costs. While cloud computing provides the elasticity to scale the infrastructure on demand cloud service providers lack control and visibility of user space applications making it difficult to accurately scale the underlying infrastructure. Thus the burden of scaling falls on the user. That is the user must determine when to trigger scaling and how much to scale. Scaling becomes even more challenging when applications exhibit changes in their behavior during run time.

In general exemplary embodiments of the invention include systems and methods for scaling a cloud infrastructure and in particular systems and methods for automatically and dynamically scaling a cloud infrastructure based on parameter estimates.

Embodiments of the present invention provide a new cloud service for example Dependable Compute Cloud DC2 which automatically scales an infrastructure to meet user specified performance requirements even when multiple user requests execute concurrently. As used herein Dependable Compute Cloud DC2 refers to an implementation of an embodiment of the present invention. While portions of this disclosure are discussed in the context of DC2 it is to be understood that the embodiments of the present invention are not necessarily limited to the specific DC2 implementation and may cover other implementations incorporating the features and aspects of the disclosed embodiments.

The embodiments of the present invention employ Kalman filtering to automatically learn the possibly changing system parameters for each application allowing for proactively scaling the infrastructure to meet performance guarantees. The embodiments of the present invention are designed for the cloud are application agnostic and do not require any offline application profiling or benchmarking training data or expert knowledge about the application. An implementation of the embodiments of the present invention DC2 was evaluated on OpenStack open source cloud computing software using a multi tier application under a range of workloads and arrival traces. The experimental results demonstrated a robustness and superiority of DC2 over existing rule based approaches with respect to avoiding service level agreement SLA violations and minimizing resource consumption. It is to be understood that the embodiments of the present invention are not necessarily limited to DC2 and OpenStack and may include other implementations.

According to an exemplary embodiment of the present invention a method for scaling a cloud infrastructure comprises receiving at least one of resource level metrics and application level metrics estimating parameters of at least one application based on the received metrics and automatically and dynamically determining directives for scaling application deployment based on the estimated parameters.

According to an exemplary embodiment of the present invention a computer program product for scaling a cloud infrastructure comprises a non transitory computer readable storage medium having program instructions embodied therewith the program instructions executable by a processor to cause the processor to perform the above method.

According to an exemplary embodiment of the present invention a system for scaling a cloud infrastructure comprises a memory and a memory and at least one processor coupled to the memory and a modeling and optimization component executed via the at least one processor wherein the modeling and optimization component receives at least one of resource level metrics from a monitoring component and application level metrics from at least one application estimates parameters of the at least one application based on the received metrics and automatically and dynamically determines directives for scaling application deployment based on the estimated parameters.

These and other exemplary embodiments of the invention will be described or become apparent from the following detailed description of exemplary embodiments which is to be read in connection with the accompanying drawings.

Exemplary embodiments of the invention will now be discussed in further detail with regard to systems and methods for scaling a cloud infrastructure and in particular systems and methods for automatically and dynamically scaling a cloud infrastructure based on parameter estimates. This invention may however be embodied in many different forms and should not be construed as limited to the embodiments set forth herein.

Dynamically sizing a deployment can be challenging for many reasons. From the perspective of a user who is also an application owner some of the specific hurdles that complicate the dynamic sizing of an application are i requires expert knowledge about the dynamics of the application including the service requirements of the application at each tier and ii requires sophisticated modeling expertise to determine when and how to resize the deployment. These hurdles may not be problematic for big businesses that have enough resources to employ a team of experts for dealing with these issues. However for small and medium businesses SMB which may be a targeted customer base for many cloud service providers CSPs these hurdles are non trivial to overcome. In general SMB users would be more likely to contract a cloud service that manages their dynamic sizing than invest in employing a team of experts. Embodiments of the present invention provide a service to manage dynamic sizing by providing an application agnostic cloud offering that will automatically and dynamically resize user applications to meet performance requirements in a cost effective manner.

Some existing CSPs offer some sort of assistance to users for elastically sizing their deployments. For example providing a monitoring service for tracking resource usage is one such solution that is offered not necessarily for free by many CSPs. Some such examples include Amazon CloudWatch SoftLayer Nimsoft Monitoring and Rackspace Cloud Monitoring. While such monitoring services provide valuable information to the users the burden of scaling still lies with the user. That is the user still requires expert knowledge about the application and the performance modeling expertise to convert the monitored information into scaling actions.

Some CSPs also offer rule based triggers to help users scale their applications. These rule based triggers allow the users to specify some conditions on the monitored metrics which when met will trigger a pre defined scaling action. Some of the companies that offer such rule based triggers include Amazon Technologies Inc. CiRBA Inc. and RightScale Inc. Even with the help of rule based triggers however the burden of determining the threshold conditions for the metrics still rests with the user. For example in order to use a CPU utilization based trigger for scaling the user must determine the CPU threshold at which to trigger scale up and scale down and the number of instances to scale up and scale down. Further the offered monitoring services only provide resource level statistics which are not sufficient for determining the optimal scaling actions required to meet application level performance guarantees. This is especially true when the user has multiple applications sharing the same infrastructure in which case the monitored resource level statistics only provide aggregate information.

CSPs cannot gather all the necessary application level statistics without intruding into the user space application. Given the lack of control and visibility into the application CSPs cannot leverage most of the existing work on dynamic scaling of applications since these works typically require access to the application for measurement and profiling purposes. Further most of the existing work is not application agnostic which is a requirement for a practical cloud service.

The embodiments of the present invention provide systems and methods for a completely automated cloud service e.g. DC2 that proactively and dynamically scales the application deployment based on user specified performance requirements. The embodiments of the present invention leverage resource level and application level statistics to infer the underlying system parameters of the application s and determine the required scaling actions to meet the performance goals in a cost effective manner. These scaling directives can then be passed on to cloud computing software for example OpenStack open source cloud computing software to execute the scaling. The scaling directives can also be passed on to a policy based execution engine to ensure colocation and high availability constraints if needed.

As used herein scaling can refer to the allocation of resources to handle increased or decreased usage demands and can include for example directives indicating the addition or removal of containers virtual machines VMs or physical machines PMs migration of VMs across PMs and or a change in the resources allocated to VMs or PMs. Scaling can also refer to resizing user applications to meet changing workload demand.

As used herein parameters can refer to for example per tier service requirements for each request class per tier central processing unit CPU memory network and input output I O requirements for each request class per tier background utilization across all classes per class network latency across all tiers and or per tier interference or congestion due to consolidation for each request class.

As used herein deployment can refer to a number of resources in each tier of a multi tier topology for example a number of VMs in each tier a number of PMs tier a number of containers tier a number of cores tier memory capacity tier network and or I O bandwidth tier.

As used herein infrastructure can refer to application deployment. More particularly infrastructure can refer to an entire end to end application deployed in the cloud consisting of multiple tiers such as a load balancer tier a web server tier an application tier and a database tier. In each tier the number of VMs PMs containers resources are scaled.

As used herein workload can refer to the level of functionality for which a computing environment e.g. cloud computing environment is being utilized. For example the workload can refer to the relative ratio of different request classes. For example in the Rice University Bidding System RUBiS case discussed below there are multiple workloads such as Base MoreWeb and MoreDB. These workloads differ in the relative ratio of for example buy browse and store request classes. Another example is the workload in a shopping site such as Amazon.com which could consist of customers browsing catalogs and customers buying and selling merchandise. In this example workload 1 could be 100 browsing customers workload 2 could be 50 browsing and 50 buying etc.

As used herein workload demand can refer to a demand for different classes. For example workload demand for workload 1 could be 100 requests second add workload demand for workload 2 could be 300 requests second. Workload demand can also be referred to as arrival rate.

Referring to a system architecture in accordance with an embodiment of the present invention includes a modeling and optimization engine that internalizes monitored statistics and infers the necessary system parameters for each application . While this engine can employ any grey box or black box modeling approach an embodiment of the present invention uses Kalman filtering to infer the system parameters.

As shown in by lines and or arrows the components of the system architecture are operatively coupled to each other via for example physical connections such as wired and or direct electrical contact connections and wireless connections such as for example WiFi BLUETOOTH IEEE 802.11 and or networks including but not limited to a local area network LAN wide area network WAN cellular network satellite network or the Internet.

As used herein Kalman filtering can refer to a robust feedback control algorithm that combines monitoring information with a user specified system model to create accurate estimations of the system state. In accordance with an embodiment of the present invention Kalman filtering is employed by specifying a generic queueing theoretic model details discussed herein below . Since Kalman filtering leverages monitored statistics to come up with estimations the underlying system model need not be accurate as can be the case when using queueing theory or other mathematical modeling techniques to model complex systems.

In an experimental example an embodiment of the present invention DC2 is evaluated via implementation on OpenStack deployed on top of physical machines such as for example SoftLayer physical machines. It is to be understood that the embodiments of the present invention are not limited to Softlayer physical machines. A three tier bidding benchmark for example Rice University Bidding System RUBiS is employed as the user application and experiments are performed with various workload traces. It is to be understood that the embodiments of the present invention are not limited to RUBiS and other applications can be used. The results demonstrate that DC2 successfully scales the application in response to changing workload demand without any user input and without any offline profiling. When compared with existing rule based triggers the experimental results show that DC2 is superior to such approaches especially when the workload behavior changes during run time and when multiple classes of requests execute concurrently. A detailed evaluation of the DC2 implementation is presented herein also with other use cases including hypervisor scaling and cost optimization which demonstrate the versatility of the Kalman filtering approach.

Embodiments of the present invention provide a design and concept of a generic cloud service that dynamically and automatically resizes user applications in the cloud to meet user specified performance requirements in a cost effective manner by leveraging resource level and application level metrics.

Embodiments of the present invention also propose a Kalman filtering based modeling approach to automatically infer system parameters for each application which are typically hidden to the cloud service provider. Using the inferred model the scaling actions necessary to meet performance guarantees in response to varying workload demand can be determined.

An implementation of an embodiment of the present invention DC2 on OpenStack is presented. Experimental examples demonstrate the superiority of the embodiments of the present invention over existing rule based approaches by managing the sizing of a popular multi tier benchmark with multiple workload classes.

The application deployer customizes an image for deployment and ties up the endpoints for an application during installation and configuration. The application deployer installs the required applications on the VMs during boot time based on the specific tier to which the VM belongs. In accordance with an embodiment of the present invention the application deployer can include a script that leverages Chef recipes provided by Opscode Inc. or customized VM images and that automates the installation of software on VMs during booting.

In accordance with an embodiment of the present invention OpenStack open source cloud computing software can be used as the underlying scalable cloud operating system. It is to be understood that the embodiments of the present invention are not limited to OpenStack open source cloud computing software and other cloud computing software can be used. A server for example a Chef server provides the automation for driving rapid creation of the OpenStack environment with the acquisition of initial hardware and the setup of a high availability infrastructure as a service IaaS layer. It is to be understood that the embodiments of the present invention are not limited to the Chef server and other servers can be used. The VMs for the application s are created on an OpenStack managed private cloud deployment on physical machines for example SoftLayer physical machines. It is to be understood that the embodiments of the present invention are not limited to SoftLayer physical machines and other physical machines can be used. As the need for additional VMs increases a new dedicated physical machine PM can be ordered and added to the private cloud e.g. OpenStack private cloud via the server e.g. Chef server . As the need for additional VMs decreases PMs can be deprovisioned.

A monitoring agent is responsible for retrieving the resource level metrics from a hypervisor and application level metrics from the application s . The modeling and optimization engine described in further detail below takes as input the monitored metrics and outputs a list of directives indicating the addition or removal of VMs or PMs migration of VMs across PMs or a change in the resources allocated to VMs or PMs. These directives are passed on to a policy based execution engine that issues commands to Softlayer application programming interface API via physical machines and or to OpenStack API via server that in turn perform the scaling operations. The policy based execution engine also determines the placement of VMs on the actual PMs based on availability security or colocation constraints.

As described above an open source multi tier application for example RUBiS was used in the experimental example but embodiments of the present invention are not limited thereto. RUBiS is an auction site prototype modeled after eBay.com supporting 26 different classes of web requests such as bid browse buy etc. In the experimental example the implementation of RUBiS employs Apache as a frontend web server Tomcat as the Java servlets container and MySQL as the backend database but embodiments of the invention are not limited thereto. The experiments focused on scaling the Tomcat application tier.

RUBiS s benchmarking tool is employed to generate load for the experiments. The benchmarking tool emulates user behavior by defining sessions consisting of a sequence of requests. The think time between requests is exponentially distributed with a mean of 1 second. The number of clients for each experiment was fixed and the load was varied by dynamically changing the composition of the workload mix.

In the experimental example multiple hypervisors with 8 CPU cores and 8 GB of memory each were employed. The Apache and MySQL tiers were each hosted on a 4 CPU VM. The Tomcat application tier was hosted on multiple 2 CPU VMs. The provisioning time for a new Tomcat VM was about 30 40 seconds. The boot time was optimized by creating a customized VM image for the Tomcat tier which is pre installed with the required application tier software. Once the new VM was online automated scripts configure the Java database connectivity JDBC with the Internet protocol IP address of the MySQL database and update the load balancer on the Apache web server to include the new Tomcat VM. This allows the application tier to be scaled dynamically without any manual intervention.

In accordance with an embodiment of the present invention the monitoring agent can include a Libvirt virtualization API to collect VM CPU utilization statistics from each hypervisor periodically. For the application level metrics there can be periodic communication with the frontend e.g. Apache frontend to fetch the web logs. These log files can them be parsed to compute a request rate and response time per request class. An alternative monitoring service can also be implemented that analyzes the request URLs directed at the application e.g. RUBiS application to compute the request rate and response time. This alternative service does not need to communicate with the application s . A user can choose to provide the application level metrics directly for example using a representational state transfer REST call . The monitoring interval is set to for example 10 seconds but the embodiments of the present invention are not limited thereto. The collected statistics are then provided as input to the modeling and optimization engine .

In accordance with an embodiment of the present invention the execution engine is primarily responsible for issuing commands for VM and PM scaling based on the scaling directives received from the modeling engine . According to an embodiment the execution engine can issue the VM and PM scaling commands to OpenStack and SoftLayer respectively or to other software programs only after two successive scaling directives from the modeling engine .

The execution engine is also responsible for placing the new VMs on specific hypervisors. This mapping can be enabled by a scheduler e.g. OpenStack Nova scheduler via a host aggregate and availability zone directives. Host aggregates which can be defined as logical cloud partitions can be used to place one set of VMs e.g. Apache and MySQL VMs on one hypervisor and another set of VMs e.g. Tomcat VMs on a different set of hypervisors.

In accordance with an embodiment of the present invention the modeling and optimization engine uses a queueing network model to approximate a multi tier cloud application. However since the user application s are not accessed to derive the parameters of the model a Kalman filtering technique is used to infer these unobservable parameters. Further by leveraging the current monitoring information via the monitoring agent the model is refined to dynamically adapt to any changes in the system. By employing the Kalman filter to leverage the actual monitored values dependence on the approximate queueing model of the system is minimized.

The queueing model and Kalman filtering technique is described below followed by an analysis of the modeling and optimization engine and an explanation of how the modeling and optimization engine determines the required scaling actions for SLA compliance.

The system is driven by a workload including i distinct request classes each class being characterized by its arrival rate l and end to end response time R. Let nbe the number of servers at tier j. With homogeneous servers and perfect load balancing the arrival rate of requests at any server in tier j is l l n. Since servers at a tier are identical for ease of analysis each tier is modeled as a single representative server. The representative server at tier j is referred to as tier j. Let u 0 1 be the utilization of tier j. The background utilization of tier j is denoted by u and models the resource utilization due to other jobs not related to the workload running on that tier. The end to end network latency for a class i request is denoted by d. Let S 0 denote the average service time of a class i request at tier j. Assuming we have Poisson arrivals and a processor sharing policy at each server the stationary distribution of the queueing network is known to have a product form for any general distribution of service time at servers. Under the product form assumption the following analytical results from queueing theory are 

While u Rand l i j can be monitored relatively easily and are thus observable the parameters S u and dare non trivial to measure and are thus deemed unobservable. While existing work on auto scaling typically obtains these values by directly accessing or modifying application software for example by parsing the log files at each tier the proposed application agnostic cloud service in accordance with an embodiment of the present invention does not encroach the user s application space. Instead a parameter estimation technique is employed e.g. Kalman filtering to derive estimates for the unobservable parameters of an application. Further since the system parameters can dynamically change during runtime the Kalman filter or other estimation technique is employed as an on line parameter estimator to continually adapt the parameter estimates.

While the product form is shown to be a reasonable assumption for tiered web services according to an embodiment it is only used as an approximation for our complex system. By employing the Kalman filter or other estimation technique to leverage the actual monitored values the dependence on the approximation is minimized.

In connection with Kalman filtering for a three class three tier system i.e. i j 3 let z u u u R R R h x and x u u u d d d S S S S S S S S S . Note that z is a 6 dimensional vector whereas x is a 15 dimensional vector. The problem is to determine the unobservable parameters x from measured values of z and l l l l .

According to an embodiment Kalman filtering is used to estimate the unobservable parameters. The dynamic evolution of system parameters can be described through the following Kalman filtering equations System State 1 Measurement Model where F t is the state transition model and H t is the observation model mapping the true state space into the observed state space. In our case F t t is the identity matrix. The variables w t N 0 Q t and v t N 0 R t are process noise and measurement noise which are assumed to be zero mean multi variate Normal distributions with covariance matrices Q t and R t respectively. The matrices Q t and R t are not directly measurable but can be tuned via best practices.

Since the measurement model z is a non linear function of the system state x see Eqns. 1 and 2 the Extended Kalman filer is used with

The Kalman filter has two phases Predict and Update. In the predict phase a priori estimates of state and error matrix are calculated. In the update phase these estimates are refined using the current observation to get a posteriori estimates of state and error matrix. The filter model for the predict and update phases for our 3 class 3 tier model is given by 

The above filter model is employed by seeding our initial estimate of circumflex over X t t 1 and P t t 1 with random values then applying the Update equations by monitoring z t to get circumflex over x t t and P t t and finally using the Predict values to arrive at the estimated circumflex over x t t 1 and P t t 1 . This process is continued iteratively at each 10 second monitoring interval to derive new estimates of the system state.

The Kalman filtering technique described above provides estimates of the unobservable system parameters S u and d. These estimates are used along with Eqns. 1 and 2 to predict the future values of uand R. is a graph illustrating accuracy and convergence of the Kalman filtering technique when employed in experimental examples according to an embodiment of the present invention. The solid line with crosses shows the monitored values of response time for a specific class of requests in the three tier application described herein. Here the monitoring interval is 10 seconds. The dashed line with circles shows estimated values for the predicted response time based on the Kalman filtering technique in accordance with an embodiment of the present invention. It initially takes about a minute for the estimates to converge. After convergence the estimated values are in very good agreement with the monitored values thus validating the techniques of the embodiments of the present invention and highlighting their accuracy. Since the current monitored values of z and l are leveraged the estimated system parameters can adapt to changes in the application. In order to demonstrate this ability a change in the workload is triggered at about the 10 minute mark shown in which causes the response time to increase. The change in the workload causes a change in the service time of the requests. The Kalman filter detects this change based on the monitored values and quickly adapts in about 2 minutes its estimates to converge to the new system state as shown in .

In sum initially the Kalman filter takes about a minute to accurately capture the system parameters. After a dynamic change the workload mix at about the 10 minute mark the Kalman filter takes about 2 minutes to capture the new system parameters and adapts its estimation accordingly.

The estimated values of the system state are used to compute the required scaling actions for the cloud service e.g. DC2 in accordance with an embodiment of the present invention. Specifically given a response time SLA Eqns. 1 and 2 are used to determine the minimum nrequired to ensure SLA compliance. Note that l l nin Eqn. 1 . Auto scaling abilities of the Kalman filtering based approach according to an embodiment is discussed herein below.

A scaling policy of the cloud service according to an embodiment of the present invention is evaluated in various settings using the RUBiS application. Traces from the Waikato Internet Traffic Storage WITS traffic archive and the WorldCup98 dataset from the Internet Traffic Archive ITA were used to drive the load generator. The WITS archive contains a large collection of internet traces from Internet service providers ISPs and university networks. The WorldCup98 dataset contains 3 months worth of requests made to the 1998 World Cup website. The traces were scaled to fit the deployment. The normalized traces are shown in .

Several workloads were used to evaluate the service e.g. DC2 in accordance with an embodiment of the present invention. The workloads were generated by modifying the RUBiS client to create different mixes of web requests that stress different components and tiers of the system.

As used herein Base represents a default workload mix and stresses the application tier. Unless stated otherwise we Base is used as the workload.

As used herein MoreDB uses the same mix of requests as Base but contains a higher fraction of requests that stress the database.

As used herein MoreApp is derived from Base and includes additional classes of requests that stress the application tier.

As used herein MoreWeb is derived from Base and replaces some of the requests that stress the database with requests that stress the web tier.

In the experimental examples the response time of browse requests was focused on since customers often base their web experience based on how long it takes to browse through online catalogues. The target response time for the browse requests was less than 40 ms on average for every 10 second monitoring interval. This goal is more challenging than requiring the response time be less than 40 ms over an entire length of the experiment. The response time SLA for all other classes was set to 100 ms. A secondary goal was to minimize the number of application tier VMs employed during the experiment. The following two metrics were considered V the percentage of time that the response time SLA was violated and K the average number of application tier VMs used during the experiment. For each experiment we compare the service according to an embodiment of the present invention was compared with the following two policies 

 STATIC OPT refers to the best static provisioning policy that maintains a fixed number of application tier VMs throughout the experiment. STATIC OPT is determined by experimenting with different number of application VMs and choosing the one which results in the lowest value of K with V 0.

 THRES x y refers to a rule based provisioning policy that adds one application VM when the average application tier utilization exceeds y for successive intervals and removes one application VM when the average utilization falls below x for successive intervals. In practice it suffices to consider two successive intervals for the scaling decisions just as in the case of an embodiment of the present invention.

Using the THRES x y policy in practice can be complicated since it requires finding the right values for x and y. To find the optimal THRES policy according to an embodiment start with x 20 and y 70 and then iterate via trial and error until the optimal values are found. shows different settings for THRES with x 20 under the Bursty trace. As can be seen y 60 results in the lowest K with V 0. shows results of experimentation with different x values with y 60 . Based on the results it was concluded that THRES 30 60 or THRES 20 60 is the optimal THRES policy for the Bursty trace.

Table I summarizes the performance of the different policies for the Bursty trace using the Base workload.

For the STATIC OPT policy it was found that a minimum of 3 VMs were needed to ensure V 0. Based on these results the dynamic policies THRES 30 60 and DC2 result in lower resource consumption than STATIC OPT. While both DC2 and THRES result in zero SLA violations and low resource consumption under the Bursty trace THRES requires a relatively large amount of experimentation and calibration to achieve the desired performance.

The comparison under different traces namely the Hill trace and the Rampdown trace was also considered. show experimental results for an embodiment of the present invention e.g. DC2 under these traces. It is again observed that the monitored response time under DC2 is below the SLA throughout the experiment for both traces. The algorithm for DC2 is not changed between experiments. DC2 automatically adapts based on the Kalman filtering technique discussed above to the different traces and takes corrective actions to ensure that the SLA is not violated.

The THRES 30 60 policy is no longer optimal for the Hill or Rampdown traces. For the Hill trace it was found that THRES 30 50 is optimal because the Hill trace exhibits a steep rise in load requiring more aggressive scale up. For the Rampdown trace it was found that THRES 40 60 is optimal because the Rampdown trace exhibits a gradually lowering request rate allowing for more aggressive scale down. Not using the right THRES policy for each trace can result in expensive SLA violations or increased resource consumption as can be seen from the results in Table II. Table II shows a comparison of the different policies for each of the traces using the Base workload for each trace. The optimal policies values are displayed in bold.

The performance of all of the policies under different workload settings was compared and evaluated. Table III shows the experimental results for various policies for different workload mixes under the Bursty trace.

THRES 30 60 was chosen since it is the optimal THRES policy under the Bursty trace for the Base workload. As can be seen from Table III DC2 results in zero violations for all workloads without requiring any changes in configuration between experiments. This is because the Kalman filtering technique used by DC2 quickly adapts to changes in workload as shown in and discussed above . In contrast THRES can result in severe SLA violations and or increased resource consumption when the underlying workload changes. In particular THRES 30 60 results in SLA violations when using the MoreDB and MoreApp workloads. For the MoreDB workload since there is increased load in the database tier more aggressive scaling of the application tier is required for the same CPU utilization to meet the end to end response time SLA. Since THRES is ignorant of the dependencies between tiers it does not take the required corrective actions to ensure SLA compliance. According to an embodiment of the present invention DC2 on the other hand infers the system parameters from the monitored values and takes the necessary scaling actions that result in zero violations. Likewise for the MoreApp workload when the additional request classes create memory contention in the application tier DC2 detects a change in the service requirement and responds appropriately whereas THRES does not. For the MoreWeb workload DC2 detects the change in load at all the tiers and responds more conservatively when scaling up since there is less database contention whereas THRES responds only to the localized CPU utilization at the application tier VMs. In summary while THRES 30 60 can be optimal for the Base workload it results in SLA violations for the MoreDB and MoreApp workloads and increased resource consumption for the MoreWeb workload. This indicates that no fixed setting of x and y will be optimal for the four workloads considered. Thus DC2 in accordance with an embodiment of the present invention exhibits robustness to changes in workload whereas THRES does not.

The experimental results highlight the advantages of the Kalman filtering approach employed in accordance with embodiments of the present invention. The two important ingredients in the application of embodiments of the present invention e.g. DC2 that make it robust are i automated inference of underlying system parameters and ii access to end to end application level metrics. Rule based solutions such as THRES lack these properties making them vulnerable to changes in request patterns and or workloads. Further an automated approach such as in the embodiments of the present invention has the advantage that it does not require any profiling or trial and error calibrations. This is crucial in a cloud based system where the cloud service provider cannot control the user application. In fact because of the lack of visibility of the user application in a cloud system cloud service providers cannot leverage the extensive prior work on dynamic application sizing see below since such works typically require access to user application for profiling purposes.

A possible implementation of an algorithm in accordance with an embodiment of the present invention has been presented herein. It is to be understood that the embodiments of the present invention are not limited thereto and may be further modified and or improved by incorporating more feedback and monitoring information more sophisticated machine learning techniques as well as predictions about future request rate. Some of these refinements are discussed below.

It should be noted the only information an implementation of an embodiment of the present embodiment e.g. DC2 requires about the application is the number of tiers and SLA specifications. The DC2 and other possible implementations do not require any information about the specific software running on each tier or the service requirements of the requests at each tier. The fact that DC2 and other possible implementations of embodiments of the present invention can automatically adapt to the application is demonstrated by their ability to respond to changes in the workload. Likewise the embodiments of the present invention can automatically adapt to changes in the underlying application and do not require any information about the VM instance size. Thus the embodiments of the present invention are agnostic to the application and the hosting environment and result in a very robust and practical solution.

In the evaluation of THRES CPU utilization was used as the trigger for scaling actions. While THRES may be augmented to also monitor memory I O network bandwidth and other resource level metrics to improve its scaling accuracy THRES would still fail when faced with multi tier applications since it would lack information on how each tier affects the underlying application performance. In fact with more triggers for scaling THRES would require a relatively large amount of experimentation to optimize its performance. Likewise if THRES were augmented with application level metrics such as request rate and response time an inferencing mechanism such as Kalman filtering would be needed to convert that information into useful scaling actions. Without inferencing THRES would again require experimentation and profiling to determine when scaling should be triggered based on monitored resource and application level metrics. THRES may be augmented with predictions about a future request rate but THRES would still require a mechanism to convert that information into scaling actions.

From a cloud service provider s perspective the known algorithms for scaling applications are not applicable because of the lack of control and visibility into the user s application. In fact it is this lack of control that has led to the simple rule based services that are offered by most cloud service providers today. The embodiments of the present invention e.g. DC2 improve upon rule based services by automatically determining when and how the scaling actions should be triggered and executing them successfully in a robust manner.

The embodiments of the present invention may have applications beyond those discussed above. According to an embodiment for cases where a request rate follows a periodic trend it is possible to accurately predict future request rate based on the observed history. The embodiments of the present invention for example DC2 can be integrated with prediction tools to forecast the need for additional VMs in the future. The Hill trace obtained from the WorldCup98 dataset is a periodic trace and is thus a good candidate for prediction. In order to demonstrate prediction based scaling with DC2 the request rate of the Hill trace was scaled up and a week s worth of historical data from the WorldCup98 dataset was taken. Autoregressive integrated moving average ARIMA modeling was used to predict the request rate 100 seconds into the future. Referring to the monitored solid line and predicted solid line with dots request rates are shown. The scaled up Hill trace exhibits a relatively steep rise in request rate resulting in SLA violations when using the optimal THRES or native DC2 policies. However by leveraging the predicted request rate the need for VMs can be estimated 100 seconds in advance. This allows proactive scaling of capacity to avoid SLA violations. The performance of DC2 integrated with ARIMA modeling is shown in . The up and down triangles represent scaling actions. The stacked triangles represent multiple simultaneous scaling actions. Since DC2 predicts a required number of VMs based on Kalman filtering it is possible to scale up down multiple VMs simultaneously if needed to quickly provision the desired number of VMs.

Certain cloud service providers such as for example SoftLayer offer access to bare metal instances PMs in addition to VMs to allow customers to create their dedicated cloud. According to an embodiment in such cases DC2 or other possible implementation of an embodiment of the present invention can be employed to manage hypervisor scaling in addition to VM scaling. For example in the prediction based scaling use case above DC2 can be used to proactively estimate the need for new hypervisors. Since each hypervisor in the experimental setup can host application tier VMs the number of hypervisors is scaled if the estimated number of VMs exceeds the number of VMs that can be hosted on the existing hypervisors. The up and down triangles in represent hypervisor scale up and scale down actions respectively.

In general the addition of a PM can take for example anywhere from one hour to one day depending on the required PM configuration. In the experimental setup discussed herein a PM is ordered instance via the SoftLayer API and added it to an OpenStack deployment via a Chef server. However it is not added to the Tomcat host aggregate until DC2 issues a hypervisor scaling command as in .

For multi tier applications it is typically necessary to detect the bottleneck tier before making scaling decisions. Adding capacity to the wrong tier can have adverse effects on system performance and cost. According to an embodiment of the present invention a Kalman filtering based DC2 approach allows for relatively easy detection of a bottleneck tier since it infers the service requirements of the requests at each tier. In order to demonstrate this ability the MoreDB workload is taken and fraction of requests that stress the database is significantly increases. When using this new workload with the Bursty trace DC2 does not advocate any scaling actions for the application tier since it detects that the bottleneck is at the database tier. The Kalman filtering model can be extended to also suggest scaling actions for the database and web tiers.

According to an embodiment DC2 can be employed to analyze what if decisions proactively. For example DC2 can relatively easily estimate the change in response time if a certain number of VMs are added or removed. Likewise since DC2 knows the service requirements of the requests it can estimate the change in response time if vCPUs are added or removed from the VM instances. An SLA goal may also be changed to allow cost optimization. For example consider the case where there is access to a range of VM sizes differing in their vCPU and memory configurations and differing in their rental costs. In this case a Kalman filtering goal can be changed to ensure response time compliance using the minimum cost as opposed to minimum number of VMs . Based on this new optimization goal DC2 or other implementation of an embodiment of the present invention picks the most cost effective VM instance when scaling to minimize the VM rental cost.

With the exception of the number of tiers the systems and methods of the embodiments of the present invention do not require any historical application data or information and are capable of operating at the level of an application deployment comprising multiple VMs and PMs. In a cloud setting the cloud service providers typically do not have access to the user application and typically do not have prior knowledge about the service requirements of the user application. Thus it is not easy to convert the monitored system state to scaling actions. In order to overcome this limitation the embodiments of the present invention infer the system parameters based on Kalman filtering without requiring access to the user application.

The embodiments of the present invention work on a multi tier system and use an inference engine such as Kalman filtering to compute the unknown system parameters. Specifically in accordance with an embodiment Kalman filtering is leveraged to determine VM and PM scaling actions. Further the Kalman filtering can be generalized to a three class three tier system and supplemented with time series modeling for predicting application resource requirements. The embodiments can also leverage a queueing based model for making online scalability decisions.

Existing CSP offered auto scaling solutions are rule based and typically require the user to specify the threshold values on the resource usage e.g. CPU memory storage for triggering scaling actions. While rule based solutions can be suitable for the cloud environment where the user application cannot be accessed they ultimately place the burden of the auto scaling logic on the user. Further such rule based approaches have to be tuned to the specific demand pattern and workload for best results as demonstrated by the THRES policy discussed above. By contrast the systems and methods of the embodiments of the present invention do not require the user to specify scaling rules and automatically determine the required scaling actions and execute them in a timely manner to ensure SLA compliance.

The process further includes at block providing the directives to a cloud service provider to execute the scaling. The process may also include determining placement of a virtual machine on a physical machine based on the directives and at least one of a colocation constraint an availability constraint and a security constraint.

The estimating is performed using a Kalman filtering technique wherein using the Kalman filtering technique comprises specifying a generic queueing theoretic model. The determining is performed without user input about dynamically resizing deployment and the parameters are unobservable. The scaling is performed to meet performance goals which can be specified for example in an SLA.

Referring back to according to an embodiment of the present invention the steps in connection with blocks and can be performed by the modeling and optimization engine . An execution engine can determine placement of a virtual machine on a physical machine based on the directives and at least one of a colocation constraint an availability constraint and a security constraint.

The present invention may be a system a method and or a computer program product. The computer program product may include a computer readable storage medium or media having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.

The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be for example but is not limited to an electronic storage device a magnetic storage device an optical storage device an electromagnetic storage device a semiconductor storage device or any suitable combination of the foregoing. A non exhaustive list of more specific examples of the computer readable storage medium includes the following a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory a static random access memory SRAM a portable compact disc read only memory CD ROM a digital versatile disk DVD a memory stick a floppy disk a mechanically encoded device such as punch cards or raised structures in a groove having instructions recorded thereon and any suitable combination of the foregoing. A computer readable storage medium as used herein is not to be construed as being transitory signals per se such as radio waves or other freely propagating electromagnetic waves electromagnetic waves propagating through a waveguide or other transmission media e.g. light pulses passing through a fiber optic cable or electrical signals transmitted through a wire.

Computer readable program instructions described herein can be downloaded to respective computing processing devices from a computer readable storage medium or to an external computer or external storage device via a network for example the Internet a local area network a wide area network and or a wireless network. The network may comprise copper transmission cables optical transmission fibers wireless transmission routers firewalls switches gateway computers and or edge servers. A network adapter card or network interface in each computing processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing processing device.

Computer readable program instructions for carrying out operations of the present invention may be assembler instructions instruction set architecture ISA instructions machine instructions machine dependent instructions microcode firmware instructions state setting data or either source code or object code written in any combination of one or more programming languages including an object oriented programming language such as Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The computer readable program instructions may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider . In some embodiments electronic circuitry including for example programmable logic circuitry field programmable gate arrays FPGA or programmable logic arrays PLA may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry in order to perform aspects of the present invention.

Aspects of the present invention are described herein with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer readable program instructions.

These computer readable program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer a programmable data processing apparatus and or other devices to function in a particular manner such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function act specified in the flowchart and or block diagram block or blocks.

The computer readable program instructions may also be loaded onto a computer other programmable data processing apparatus or other device to cause a series of operational steps to be performed on the computer other programmable apparatus or other device to produce a computer implemented process such that the instructions which execute on the computer other programmable apparatus or other device implement the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in the Figures illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of instructions which comprises one or more executable instructions for implementing the specified logical function s . In some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.

It is understood in advance that although this disclosure includes a detailed description on cloud computing implementation of the teachings recited herein are not limited to a cloud computing environment. Rather embodiments of the present invention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.

Cloud computing is a model of service delivery for enabling convenient on demand network access to a shared pool of configurable computing resources e.g. networks network bandwidth servers processing memory storage applications virtual machines and services that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. This cloud model may include at least five characteristics at least three service models and at least four deployment models.

On demand self service a cloud consumer can unilaterally provision computing capabilities such as server time and network storage as needed automatically without requiring human interaction with the service s provider.

Broad network access capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms e.g. mobile phones laptops and PDAs .

Resource pooling the provider s computing resources are pooled to serve multiple consumers using a multi tenant model with different physical and virtual resources dynamically assigned and reassigned according to demand. There is a sense of location independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction e.g. country state or datacenter .

Rapid elasticity capabilities can be rapidly and elastically provisioned in some cases automatically to quickly scale out and rapidly released to quickly scale in. To the consumer the capabilities available for provisioning often appear to be unlimited and can be purchased in any quantity at any time.

Measured service cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service e.g. storage processing bandwidth and active user accounts . Resource usage can be monitored controlled and reported providing transparency for both the provider and consumer of the utilized service.

Software as a Service SaaS the capability provided to the consumer is to use the provider s applications running on a cloud infrastructure. The applications are accessible from various client devices through a thin client interface such as a web browser e.g. web based email . The consumer does not manage or control the underlying cloud infrastructure including network servers operating systems storage or even individual application capabilities with the possible exception of limited user specific application configuration settings.

Platform as a Service PaaS the capability provided to the consumer is to deploy onto the cloud infrastructure consumer created or acquired applications created using programming languages and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including networks servers operating systems or storage but has control over the deployed applications and possibly application hosting environment configurations.

Infrastructure as a Service IaaS the capability provided to the consumer is to provision processing storage networks and other fundamental computing resources where the consumer is able to deploy and run arbitrary software which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems storage deployed applications and possibly limited control of select networking components e.g. host firewalls .

Private cloud the cloud infrastructure is operated solely for an organization. It may be managed by the organization or a third party and may exist on premises or off premises.

Community cloud the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns e.g. mission security requirements policy and compliance considerations . It may be managed by the organizations or a third party and may exist on premises or off premises.

Public cloud the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.

Hybrid cloud the cloud infrastructure is a composition of two or more clouds private community or public that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability e.g. cloud bursting for loadbalancing between clouds .

A cloud computing environment is service oriented with a focus on statelessness low coupling modularity and semantic interoperability. At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.

Referring now to a schematic of an example of a cloud computing node is shown. Cloud computing node is only one example of a suitable cloud computing node and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the invention described herein. Regardless cloud computing node is capable of being implemented and or performing any of the functionality set forth hereinabove.

In a cloud computing node there is a computer system server which is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with computer system server include but are not limited to personal computer systems server computer systems thin clients thick clients handheld or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputer systems mainframe computer systems and distributed cloud computing environments that include any of the above systems or devices and the like.

Computer system server may be described in the general context of computer system executable instructions such as program modules being executed by a computer system. Generally program modules may include routines programs objects components logic data structures and so on that perform particular tasks or implement particular abstract data types. Computer system server may be practiced in distributed cloud computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed cloud computing environment program modules may be located in both local and remote computer system storage media including memory storage devices.

As shown in computer system server in computing node is shown in the form of a general purpose computing device. The components of computer system server may include but are not limited to one or more processors or processing units a system memory and a bus that couples various system components including system memory to processor .

The bus represents one or more of any of several types of bus structures including a memory bus or memory controller a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnects PCI bus.

The computer system server typically includes a variety of computer system readable media. Such media may be any available media that is accessible by computer system server and it includes both volatile and non volatile media removable and non removable media.

The system memory can include computer system readable media in the form of volatile memory such as random access memory RAM and or cache memory . The computer system server may further include other removable non removable volatile nonvolatile computer system storage media. By way of example only storage system can be provided for reading from and writing to a non removable non volatile magnetic media not shown and typically called a hard drive . Although not shown a magnetic disk drive for reading from and writing to a removable non volatile magnetic disk e.g. a floppy disk and an optical disk drive for reading from or writing to a removable non volatile optical disk such as a CD ROM DVD ROM or other optical media can be provided. In such instances each can be connected to the bus by one or more data media interfaces. As depicted and described herein the memory may include at least one program product having a set e.g. at least one of program modules that are configured to carry out the functions of embodiments of the invention.

A program utility having a set at least one of program modules may be stored in memory by way of example and not limitation as well as an operating system one or more application programs other program modules and program data. Each of the operating system one or more application programs other program modules and program data or some combination thereof may include an implementation of a networking environment. Program modules generally carry out the functions and or methodologies of embodiments of the invention as described herein.

Computer system server may also communicate with one or more external devices such as a keyboard a pointing device a display etc. one or more devices that enable a user to interact with computer system server and or any devices e.g. network card modem etc. that enable computer system server to communicate with one or more other computing devices. Such communication can occur via Input Output I O interfaces . Still yet computer system server can communicate with one or more networks such as a local area network LAN a general wide area network WAN and or a public network e.g. the Internet via network adapter . As depicted network adapter communicates with the other components of computer system server via bus . It should be understood that although not shown other hardware and or software components could be used in conjunction with computer system server . Examples include but are not limited to microcode device drivers redundant processing units external disk drive arrays RAID systems tape drives and data archival storage systems etc.

While the description in connection with references a cloud computing node it is to be understood that can also be representative of a computer system including a computing node which is not necessarily a cloud computing node in accordance with which one or more components steps of the techniques of the invention may be implemented. For example one or more embodiments can make use of software running on computing node .

Referring now to illustrative cloud computing environment is depicted. As shown cloud computing environment comprises one or more cloud computing nodes with which local computing devices used by cloud consumers such as for example personal digital assistant PDA or cellular telephone A desktop computer B laptop computer C and or automobile computer system N may communicate. Nodes may communicate with one another. They may be grouped not shown physically or virtually in one or more networks such as Private Community Public or Hybrid clouds as described hereinabove or a combination thereof. This allows cloud computing environment to offer infrastructure platforms and or software as services for which a cloud consumer does not need to maintain resources on a local computing device. It is understood that the types of computing devices A N shown in are intended to be illustrative only and that computing nodes and cloud computing environment can communicate with any type of computerized device over any type of network and or network addressable connection e.g. using a web browser .

Referring now to a set of functional abstraction layers provided by cloud computing environment is shown. It should be understood in advance that the components layers and functions shown in are intended to be illustrative only and embodiments of the invention are not limited thereto. As depicted the following layers and corresponding functions are provided 

Hardware and software layer includes hardware and software components. Examples of hardware components include mainframes in one example IBM zSeries systems RISC Reduced Instruction Set Computer architecture based servers in one example IBM pSeries systems IBM xSeries systems IBM BladeCenter systems storage devices networks and networking components. Examples of software components include network application server software in one example IBM WebSphere application server software and database software in one example IBM DB2 database software. IBM zSeries pSeries xSeries BladeCenter WebSphere and DB2 are trademarks of International Business Machines Corporation registered in many jurisdictions worldwide .

Virtualization layer provides an abstraction layer from which the following examples of virtual entities may be provided virtual servers virtual storage virtual networks including virtual private networks virtual applications and operating systems and virtual clients.

In one example management layer may provide the functions described below. Resource provisioning provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing environment. Metering and Pricing provide cost tracking as resources are utilized within the cloud computing environment and billing or invoicing for consumption of these resources. In one example these resources may comprise application software licenses. Security provides identity verification for cloud consumers and tasks as well as protection for data and other resources. User portal provides access to the cloud computing environment for consumers and system administrators. Service level management provides cloud computing resource allocation and management such that required service levels are met. Service Level Agreement SLA planning and fulfillment provide pre arrangement for and procurement of cloud computing resources for which a future requirement is anticipated in accordance with an SLA.

Referring back to components of system architecture may be part of or operatively coupled to one or more layers of the cloud computing environment . For example in a non limiting illustrative embodiment the CloudPool component monitoring agent component modeling and optimization engine and policy based execution engine may be part of or operatively coupled to at least the management layer . The application owner can access the cloud computing environment via the user portal in the management layer and the one or more applications application deployer server and physical machines may be part of or operatively coupled to at least one or more of the hardware and software layer and the virtualization layer .

Workloads layer provides examples of functionality for which the cloud computing environment may be utilized. Examples of workloads and functions which may be provided from this layer include mapping and navigation software development and lifecycle management virtual classroom education delivery data analytics processing transaction processing and mobile desktop.

The embodiments of the present invention provide design and implementation of a new cloud service for example DC2 that automatically scales user applications in a cost effective manner to provide performance guarantees. Since CSPs do not have complete control and visibility of a user s cloud deployment the embodiments of the present invention are application agnostic. In particular the embodiments do not require any offline profiling or benchmarking of the application nor does it require a deep understanding of the application dynamics. Instead as noted the embodiments employ a Kalman filtering technique in combination with a queueing theoretic model to proactively determine the right scaling actions for an application deployed in the cloud using relatively easily available metrics such as utilization and request rate.

Applications of embodiments of the present invention provide a CSP offered auto scaling service that is proposed to be superior to existing rule based offerings. Since the cloud is marketed as a platform designed for all levels of tenants by utilizing the embodiments of the present invention application owners who do not have expert knowledge in performance modeling and system optimization should be able to easily scale their applications.

Although illustrative embodiments of the present invention have been described herein with reference to the accompanying drawings it is to be understood that the invention is not limited to those precise embodiments and that various other changes and modifications may be made by one skilled in the art without departing from the scope or spirit of the invention.

