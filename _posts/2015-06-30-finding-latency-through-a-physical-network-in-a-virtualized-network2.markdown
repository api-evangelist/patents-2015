---

title: Finding latency through a physical network in a virtualized network
abstract: Techniques are described for determining latency in a physical network that includes a number of network devices over which packets travel. A virtual network controller receives a plurality of messages from a plurality of network devices in a network, each of the messages including a packet signature comprising a hash of an invariant portion of an original packet that uniquely identifies the original packet, an identifier of one of the plurality of network devices from which the respective message was received, and a timestamp indicating a time an original packet was processed by the network device from which the respective message was received. The virtual network controller determines a latency of a physical network path in the network based on analysis of contents of the identified messages having a common packet signature.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09596159&OS=09596159&RS=09596159
owner: Juniper Networks, Inc.
number: 09596159
owner_city: Sunnyvale
owner_country: US
publication_date: 20150630
---
This application is continuation of U.S. application Ser. No. 13 840 657 filed Mar. 15 2013 which claims the benefit of U.S. Provisional Application No. 61 722 696 filed Nov. 5 2012 U.S. Provisional Application No. 61 721 979 filed Nov. 2 2012 U.S. Provisional Application No. 61 721 994 filed Nov. 2 2012 U.S. Provisional Application No. 61 718 633 filed Oct. 25 2012 U.S. Provisional Application No. 61 656 468 filed Jun. 6 2012 U.S. Provisional Application No. 61 656 469 filed Jun. 6 2012 and U.S. Provisional Application No. 61 656 471 filed Jun. 6 2012 the entire content of each of which being incorporated herein by reference.

In a typical cloud data center environment there is a large collection of interconnected servers that provide computing and or storage capacity to run various applications. For example a data center may comprise a facility that hosts applications and services for subscribers i.e. customers of data center. The data center may for example hosts all of the infrastructure equipment such as networking and storage systems redundant power supplies and environmental controls. In a typical data center clusters of storage systems and application servers are interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. More sophisticated data centers provide infrastructure spread throughout the world with subscriber support equipment located in various physical hosting facilities.

In general the disclosure provides techniques for determining latency in a physical network that includes a number of network devices over which packets travel. In a virtual network architecture information regarding latency of any particular flow i.e. the time it takes for a packet to travel from one network device e.g. server to another network device via a particular path of switches and connectors may not be readily available to the virtual network.

When a packet matching a defined set of monitored packets travels through a network device e.g. a switch or router during the defined time period the network device can make a copy of the packet without affecting the flow of the packet and send information from the copied packet back to an analytics engine of a logically centralized virtual network controller along with the time stamp and the identity of the network device. In other words the analytics engine receives information on when and where the packet has travelled. By analyzing this information from a number of network devices analytics engines of the virtual network controller can determine the time taken by specific packets to traverse the physical network and can identify network devices and or connections in the physical network that slows the speed of the network. Additionally instead of sending back an entire copy of the monitored packet the network device can take a hash i.e. signature of an invariant portion of the copied packet that uniquely identifies the packet for instance the payload and send the signature back to the analytic engine along with a device identifier and timestamp information. Sending the signatures instead of the entire packet can provide a more scalable mechanism by compressing the amount of information that needs to be sent and stored in the network.

Using a collection of such latency information the virtual network controller can identify places in the physical network that are slow or where bottlenecks in traffic are occurring. Such a bottleneck may be indicative of a problem with the physical network such as for example a deteriorated cable. Identifying such problems in the physical network without having to run specific testing on each of the components of the network may save time and money and can help ensure that the network performs optimally and without interruption.

In one embodiment a method for determining latency of a physical network path in a network includes receiving by a virtual network controller a plurality of messages from a plurality of network devices in a network wherein each of the messages includes 1 a packet signature comprising a hash of an invariant portion of an original packet that uniquely identifies the original packet 2 an identifier of one of the plurality of network devices from which the respective message was received and 3 a timestamp indicating a time an original packet was processed by the network device from which the respective message was received. The method also includes identifying by the virtual network controller two or more of the plurality of messages having a common packet signature and determining by the virtual network controller a latency of a physical network path in the network based on analysis of contents of the identified messages having a common packet signature.

In another embodiment a method includes receiving from a virtual network controller by a network device information specifying packet characteristics of packets to be analyzed receiving a packet responsive to determining that the packet matches the specified characteristics and by a virtual network agent executing on the network device determining a hash of an invariant portion of the packet that uniquely identifies the packet to obtain a packet signature and forwarding to the virtual network controller a message that specifies 1 the packet signature 2 an identifier of the network device and 3 a timestamp indicating a time the packet was processed by the network device.

In another embodiment a computer readable storage medium includes instructions for causing a programmable processor to receive a plurality of messages from a plurality of network devices in a network wherein each of the messages includes 1 a packet signature comprising a hash of an invariant portion of an original packet that uniquely identifies the original packet 2 an identifier of one of the plurality of network devices from which the respective message was received and 3 a timestamp indicating a time an original packet was processed by the network device from which the respective message was received identify two or more of the plurality of messages having a common packet signature and determine a latency of a physical network path in the network based on analysis of contents of the identified messages having a common packet signature.

In a further embodiment a virtual network controller includes one or more processors and a plurality of virtual machines executed by the processors to receive a plurality of messages from a plurality of network devices in a network wherein each of the messages includes 1 a packet signature comprising a hash of an invariant portion of an original packet that uniquely identifies the original packet 2 an identifier of one of the plurality of network devices from which the respective message was received and 3 a timestamp indicating a time an original packet was processed by the network device from which the respective message was received. The virtual network controller also includes a plurality of analytics virtual machines wherein the plurality of virtual machines identify two or more of the plurality of messages having a common packet signature and determine a latency of a physical network path in the network based on analysis of contents of the identified messages having a common packet signature.

In another example a system includes a virtual network controller that includes one or more processors a plurality of virtual machines executed by the processors and a plurality of network devices comprising one or more processors wherein the plurality of network devices receive from the virtual network controller information specifying packet characteristics of packets to be analyzed receiving a packet wherein the plurality of virtual machines receive a plurality of messages from the plurality of network devices wherein each of the messages includes 1 a packet signature comprising a hash of an invariant portion of an original packet that uniquely identifies the original packet 2 an identifier of one of the plurality of network devices from which the respective message was received and 3 a timestamp indicating a time an original packet was processed by the network device from which the respective message was received wherein the virtual network controller further comprises a plurality of analytics virtual machines that identify two or more of the plurality of messages having a common packet signature and determine a latency of a physical network path in the network based on analysis of contents of the identified messages having a common packet signature and wherein the plurality of network devices comprise a virtual network agent executing on the processors that responsive to determining that the packet matches the specified characteristics determining a hash of an invariant portion of the packet that uniquely identifies the packet to obtain a packet signature and forward to the virtual network controller a message that specifies 1 the packet signature 2 an identifier of the network device and 3 a timestamp indicating a time the packet was processed by the network device.

The details of one or more aspects of this disclosure are set forth in the accompanying drawings and the description below. Other features objects and advantages will be apparent from the description and drawings and from the claims.

In some examples data center may represent one of many geographically distributed network data centers. As illustrated in the example of data center may be a facility that provides network services for customers . Customers may be collective entities such as enterprises and governments or individuals. For example a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage virtual private networks traffic engineering file service data mining scientific or super computing and so on. In some embodiments data center may be individual network servers network peers or otherwise.

In this example data center includes set of storage systems and application servers A X herein servers interconnected via high speed switch fabric provided by one or more tiers of physical network switches and routers. Switch fabric is provided by a set of interconnected top of rack TOR switches A BN TOR switches coupled to a distribution layer of chassis switches . Although not shown data center may also include for example one or more non edge switches routers hubs gateways security devices such as firewalls intrusion detection and or intrusion prevention devices servers computer terminals laptops printers databases wireless mobile devices such as cellular phones or personal digital assistants wireless access points bridges cable modems application accelerators or other network devices.

In this example TOR switches and chassis switches provide servers with redundant multi homed connectivity to IP fabric and service provider network . Chassis switches aggregates traffic flows and provides high speed connectivity between TOR switches . TOR switches A and B may be network devices that provide layer 2 MAC address and or layer 3 IP address routing and or switching functionality. TOR switches and chassis switches may each include one or more processors and a memory and that are capable of executing one or more software processes. Chassis switches are coupled to IP fabric which performs layer 3 routing to route network traffic between data center and customers using service provider network .

Virtual network controller VNC provides a logically centralized controller for facilitating operation of one or more virtual networks within data center in accordance with one or more embodiments of this disclosure. In some examples virtual network controller may operate in response to configuration input received from network administrator . As described in further detail below servers may include one or more virtual switches that create and manage one or more virtual networks that are used by applications.

Typically the traffic between any two network devices such as between network devices within IP fabric not shown or between servers and customers for example can traverse the physical network using many different paths. For example there may be several different paths of equal cost between two network devices. In some cases packets belonging to network traffic from one network device to the other may be distributed among the various possible paths using a routing strategy called multi path routing at each network switch node. For example the Internet Engineering Task Force IETF RFC 2992 Analysis of an Equal Cost Multi Path Algorithm describes a routing technique for routing packets along multiple paths of equal cost. The techniques of RFC 2992 analyzes one particular multipath routing strategy involving the assignment of flows to bins by hashing packet header fields that sends all packets from a particular network flow over a single deterministic path.

For example a flow can be defined by the five values used in a header to a packet or five tuple i.e. the protocol Source IP address Destination IP address Source port and Destination port that are used to route packets through the physical network. For example the protocol specifies the communications protocol such as TCP or UDP and Source port and Destination port refer to source and destination ports of the connection. A set of one or more packet data units PDUs that match a particular flow entry represent a flow. Flows may be broadly classified using any parameter of a PDU such as source and destination MAC and IP addresses a Virtual Local Area Network VLAN tag transport layer information a Multiprotocol Label Switching MPLS or Generalized MPLS GMPLS label and an ingress port of a network device receiving the flow. For example a flow may be all PDUs transmitted in a Transmission Control Protocol TCP connection all PDUs sourced by a particular MAC address or IP address all PDUs having the same VLAN tag or all PDUs received at the same switch port.

Each individual switch router in the network will perform its own independent hashing computation to determine the path that will be used by a particular flow. The ECMP paths between the first and second network devices may be viewed by the virtual network as one physical connection as their packet inner packet is encapsulated by the outer IP encapsulation.

In such a network information regarding network controller . To find the real latency a statistical technique may need to be employed. Typically the virtual network domain is controlled separately from the physical network domain and as a result the ability to ascertain the actual path of a packet for a given network flow within the virtual network domain is not straightforward and typically requires knowledge of the state of the physical network.

One technique that could be employed to determine the path taken by a network IP packet through a switch router network is to use an IP trace route function which is supported by most operating systems as well as network operating systems. However such a trace route function does not work well when a multi path latency of any particular flow i.e. the time it takes for a packet to travel from one network device e.g. server to another network device via a particular path of switches and connectors e.g. within IP fabric is not readily available to the virtual network and to virtual routing technique is employed as different network flows use different paths through the network and the trace route packet that is used to ascertain the route will not have the same header as the application packet. Because hashing functions in most network switch routers depends on the packet header this trace route packet may not follow the same path.

In accordance with the techniques of this disclosure one method for determining latency in a multi path routing environment is to collect statistics on every virtual switch node e.g. residing on servers that is every switch node collects data on which packets have travelled through the switch node and when they travelled through the switch node between servers . The switch then sends this data to an analytics engine executing on virtual network controller . The analytics engine can use the data to calculate latency. Collecting all such data from the virtual switch nodes on all servers however may result in massive amounts of data which may be difficult to use effectively and will eat away network bandwidth. So in some exemplary embodiments an administrator may choose to restrict the data that is gathered. For example the administrator may specify at virtual network controller that statistics are to be captured for a certain class of traffic and may also restrict the period of time over which statistic are collected. To capture a certain class of traffic the packet can be used like a match list called a packet classifier. Virtual network controller can send the packet classifiers down to be installed on the appropriate servers .

Each virtual switch may execute within a hypervisor a host operating system or other component of each of servers . In the example of virtual switch executes within hypervisor also often referred to as a virtual machine manager VMM which provides a virtualization platform that allows multiple operating systems to concurrently run on one of host servers . In the example of virtual switch A manages virtual networks each of which provides a network environment for execution of one or more virtual machines VMs on top of the virtualization platform provided by hypervisor . Each VM is associated with one of the virtual subnets VN VN managed by the hypervisor .

In general each VM may be any type of software application and may be assigned a virtual address for use within a corresponding virtual network where each of the virtual networks may be a different virtual subnet provided by virtual switch A. A VM may be assigned its own virtual layer three L3 IP address for example for sending and receiving communications but may be unaware of an IP address of the physical server A on which the virtual machine is executing. In this way a virtual address is an address for an application that differs from the logical address for the underlying physical computer system i.e. server A in the example of .

In one implementation each of servers includes a virtual network agent VN agent A X VN agents that controls the overlay of virtual networks and that coordinates the routing of data packets within server . In general each VN agent communicates with virtual network controller which generates commands to control routing of packets through data center . VN agents may operate as a proxy for control plane messages between virtual machines and virtual network controller . For example a VM may request to send a message using its virtual address via the VN agent A and VN agent A may in turn send the message and request that a response to the message be received for the virtual address of the VM that originated the first message. In some cases a VM may invoke a procedure or function call presented by an application programming interface of VN agent A and the VN agent A may handle encapsulation of the message as well including addressing.

In one example network packets e.g. layer three L3 IP packets or layer two L2 Ethernet packets generated or consumed by the instances of applications executed by virtual machines within the virtual network domain may be encapsulated in another packet e.g. another IP or Ethernet packet that is transported by the physical network. The packet transported in a virtual network may be referred to herein as an inner packet while the physical network packet may be referred to herein as an outer packet. Encapsulation and or de capsulation of virtual network packets within physical network packets may be performed within virtual switches e.g. within the hypervisor or the host operating system running on each of servers . As another example encapsulation and de capsulation functions may be performed at the edge of switch fabric at a first hop TOR switch that is one hop removed from the application instance that originated the packet. This functionality is referred to herein as tunneling and may be used within data center to create one or more overlay networks. Other example tunneling protocols may be used including IP over GRE VxLAN MPLS over GRE etc.

As noted above virtual network controller provides a logically centralized controller for facilitating operation of one or more virtual networks within data center . Virtual network controller may for example maintain a routing information base e.g. on or more routing tables that store routing information for the physical network as well as the overlay network of data center . Similarly switches and virtual switches maintain routing information such as one or more routing and or forwarding tables. In one example implementation virtual switch A of hypervisor implements a network forwarding table NFT for each virtual network . In general each NFT stores forwarding information for the corresponding virtual network and identifies where data packets are to be forwarded and whether the packets are to be encapsulated in a tunneling protocol such as with one or more outer IP addresses.

The routing information may for example map packet key information e.g. destination IP information and other select information from packet headers to one or more specific next hops within the networks provided by virtual switches and switch fabric . In some case the next hops may be chained next hop that specify a set of operations to be performed on each packet when forwarding the packet such as may be used for flooding next hops and multicasting replication. In some cases virtual network controller maintains the routing information in the form of a radix tree having leaf nodes that represent destinations within the network. U.S. Pat. No. 7 184 437 provides details on an exemplary embodiment of a router that utilizes a radix tree for route resolution the contents of U.S. Pat. No. 7 184 437 being incorporated herein by reference in its entirety.

As shown in each virtual network provides a communication framework for encapsulated packet communications for the overlay network established through switch fabric . In this way network packets associated with any of virtual machines may be transported as encapsulated packet communications via the overlay network. In addition in the example of each virtual switch includes a default network forwarding table NFTand provides a default route that allows packet to be forwarded to virtual subnet VN without encapsulation i.e. non encapsulated packet communications per the routing rules of the physical network of data center . In this way subnet VN and virtual default network forwarding table NFTprovide a mechanism for bypassing the overlay network and sending non encapsulated packet communications to switch fabric .

Moreover virtual network controller and virtual switches may communicate using virtual subnet VN in accordance with default network forwarding table NFTduring discovery and initialization of the overlay network and during conditions where a failed link has temporarily halted communication via the overlay network. In some aspects once connectivity with the virtual network controller is established the virtual network controller updates its local routing table to take into account new information about any failed links and directs virtual switches to update their local network forwarding tables . For example virtual network controller may output commands to virtual network agents to update one or more NFTs to direct virtual switches to change the tunneling encapsulation so as to re route communications within the overlay network for example to avoid a failed link.

When link failure is detected a virtual network agent local to the failed link e.g. VN Agent A may immediately change the encapsulation of network packet to redirect traffic within the overlay network and notifies virtual network controller of the routing change. In turn virtual network controller updates its routing information any may issues messages to other virtual network agents to update local routing information stored by the virtual network agents within network forwarding tables .

In accordance with the techniques of this disclosure administrator may configure packet classifiers to specify which packets are to be monitored for latency and on which time domains on virtual network controller via commands entered in web console . Virtual network controller notifies relevant VN agents of the packet monitoring definitions based on the packet classifiers. VN agents install packet capture logic on respective virtual switches . Virtual switches match packets using the packet capture logic and sends copies of the matching packets to VN agents . VN agents calculate a packet signature for each packet and send information to virtual network controller such as information specifying the packet signature a switch identifier of the virtual switch that matched the packets and a timestamp indicating the time of calculating the packet signature or a time of matching the packets for example . Distributed analytics engines of virtual network controller analyze the received information and compile results regarding packet latency as described in further detail below. Virtual network controller may send results such as a report to web console for display to administrator .

In this example chassis switch CH which may be any of chassis switches of is coupled to Top of Rack TOR switches A B TORs by chassis link A and chassis link B respectively chassis links . TORs may in some examples be any of TORs of . In the example of TORs are also coupled to servers A B servers by TOR links A D TOR links . Servers may be any of servers . Here servers communicate with both TORs and can physically reside in either associated rack. TORs each communicate with a number of network switches including chassis switch A.

Chassis switch A has a processor A in communication with an interface for communication with a network as shown as well as a bus that connects a memory not shown to processor A. The memory may store a number of software modules. These modules include software that controls network routing such as an Open Shortest Path First OSPF module not shown containing instructions for operating the chassis switch A in compliance with the OSPF protocol. Chassis switch A maintains routing table RT table A containing routing information for packets which describes a topology of a network. Routing table A may be for example a table of packet destination Internet protocol IP addresses and the corresponding next hop e.g. expressed as a link to a network component.

TORs each have a respective processor B C an interface in communication with chassis switch A and a memory not shown . Each memory contains software modules including an OSPF module and routing table B C as described above.

TORs and chassis switch A may exchange routing information specifying available routes such as by using a link state routing protocol such as OSPF or IS IS. TORs may be configured as owners of different routing subnets. For example TOR A is configured as the owner of Subnet 1 which is the subnet 10.10.10.0 24 in the example of and TOR A is configured as the owner of Subnet 2 which is the subnet 10.10.11.0 24 in the example of . As owners of their respective Subnets TORs locally store the individual routes for their subnets and need not broadcast all route advertisements up to chassis switch A. Instead in general TORs will only advertise their subnet addresses to chassis switch A.

Chassis switch A maintains a routing table RT table A which includes routes expressed as subnets reachable by TORs based on route advertisements received from TORs . In the example of RT table A stores routes indicating that traffic destined for addresses within the subnet 10.10.11.0 24 can be forwarded on link B to TOR B and traffic destined for addresses within the subnet 10.10.10.0 24 can be forwarded on link A to TOR A.

In typical operation chassis switch A receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on routing table A to determine the corresponding destination component and forwards the packets accordingly. For example if the destination IP address of a received packet is 10.10.0.0 i.e. the address of the subnet of TOR A the routing table of chassis switch A indicates that the packet is to be sent to TOR A via link A and chassis switch A transmits the packet accordingly ultimately for forwarding to a specific one of the servers .

Similarly each of TORs receives Internet Protocol IP packets through its network interface reads the packets destination IP address looks up these addresses on its routing table to determine the corresponding destination component and forwards the packets according to the result of the lookup.

Virtual network controller VNC of illustrates a distributed implementation of a VNC that includes multiple VNC nodes A N collectively VNC nodes to execute the functionality of a data center VNC including managing the operation of virtual switches for one or more virtual networks implemented within the data center. Each of VNC nodes may represent a different server of the data center e.g. any of servers of or alternatively on a server or controller coupled to the IP fabric by e.g. an edge router of a service provider network or a customer edge device of the data center network. In some instances some of VNC nodes may execute as separate virtual machines on the same server.

Each of VNC nodes may control a different non overlapping set of data center elements such as servers individual virtual switches executing within servers individual interfaces associated with virtual switches chassis switches TOR switches and or communication links. VNC nodes peer with one another using peering links to exchange information for distributed databases including distributed databases A K collectively distributed databases and routing information e.g. routes for routing information bases A N collectively RIBs . Peering links may represent peering links for a routing protocol such as a Border Gateway Protocol BGP implementation or another peering protocol by which VNC nodes may coordinate to share information according to a peering relationship.

VNC nodes of VNC include respective RIBs each having e.g. one or more routing tables that store routing information for the physical network and or one or more overlay networks of the data center controlled by VNC . In some instances one of RIBs e.g. RIB A may store the complete routing table for any of the virtual networks operating within the data center and controlled by the corresponding VNC node e.g. VNC node A .

In general distributed databases define the configuration or describe the operation of virtual networks by the data center controlled by distributed VNC . For instance distributes databases may include databases that describe a configuration of one or more virtual networks the hardware software configurations and capabilities of data center servers performance or diagnostic information for one or more virtual networks and or the underlying physical network the topology of the underlying physical network including server chassis switch TOR switch interfaces and interconnecting links and so on. Distributed databases may each be implemented using e.g. a distributed hash table DHT to provide a lookup service for key value pairs of the distributed database stored by different VNC nodes .

In accordance with the techniques of this disclosure when virtual network controller notifies VN agents of the servers of the packet classifier information and when VN agents send packet signatures back up to virtual network controller these communications may occur over peering links such as via a routing protocol like BGP or other peering protocol. Analytics engines of virtual network controller may analyze the signature data based on distributed databases as described in further detail below.

As illustrated in the example of distributed virtual network controller VNC includes one or more virtual network controller VNC nodes A N collectively VNC nodes . Each of VNC nodes may represent any of VNC nodes of virtual network controller of . VNC nodes that peer with one another according to a peering protocol operating over network . Network may represent an example instance of switch fabric and or IP fabric of . In the illustrated example VNC nodes peer with one another using a Border Gateway Protocol BGP implementation an example of a peering protocol. VNC nodes provide to one another using the peering protocol information related to respective elements of the virtual network managed at least in part by the VNC nodes . For example VNC node A may manage a first set of one or more servers operating as virtual network switches for the virtual network. VNC node A may send information relating to the management or operation of the first set of servers to VNC node N by BGP A. For example referring to when virtual network controller notifies VN agents of the servers of the packet classifier information and when VN agents send packet signatures back up to virtual network controller these communications may occur as interactions between VNC nodes by BGP A for example.

Other elements managed by VNC nodes may include network controllers and or appliances network infrastructure devices e.g. L2 or L3 switches communication links firewalls and VNC nodes for example. Because VNC nodes have a peer relationship rather than a master slave relationship information may be sufficiently easily shared between the VNC nodes . In addition hardware and or software of VNC nodes may be sufficiently easily replaced providing satisfactory resource fungibility.

Each of VNC nodes may include substantially similar components for performing substantially similar functionality said functionality being described hereinafter primarily with respect to VNC node A. VNC node A may include an analytics database A for storing diagnostic information related to a first set of elements managed by VNC node A. VNC node A may share at least some diagnostic information related to one or more of the first set of elements managed by VNC node A and stored in analytics database as well as to receive at least some diagnostic information related to any of the elements managed by others of VNC nodes . Analytics database A may represent a distributed hash table DHT for instance or any suitable data structure for storing diagnostic information for network elements in a distributed manner in cooperation with others of VNC nodes . Analytics databases A N collectively analytics databases may represent at least in part one of distributed databases of distributed virtual network controller of .

VNC node A may include a configuration database A for storing configuration information related to a first set of elements managed by VNC node A. Control plane components of VNC node A may store configuration information to configuration database A using interface A which may represent an Interface for Metadata Access Points IF MAP protocol implementation. VNC node A may share at least some configuration information related to one or more of the first set of elements managed by VNC node A and stored in configuration database A as well as to receive at least some configuration information related to any of the elements managed by others of VNC nodes . Configuration database A may represent a distributed hash table DHT for instance or any suitable data structure for storing configuration information for network elements in a distributed manner in cooperation with others of VNC nodes . Configuration databases A N collectively configuration databases may represent at least in part one of distributed databases of distributed virtual network controller of .

Virtual network controller may perform any one or more of the illustrated virtual network controller operations represented by modules which may include orchestration user interface VNC global load balancing and one or more applications . VNC executes orchestration module to facilitate the operation of one or more virtual networks in response to a dynamic demand environment by e.g. spawning removing virtual machines in data center servers adjusting computing capabilities allocating network storage resources and modifying a virtual topology connecting virtual switches of a virtual network. VNC global load balancing executed by VNC supports load balancing of analytics configuration communication tasks e.g. among VNC nodes . Applications may represent one or more network applications executed by VNC nodes to e.g. change topology of physical and or virtual networks add services or affect packet forwarding.

User interface includes an interface usable to an administrator or software agent to control the operation of VNC nodes . For instance user interface may include methods by which an administrator may modify e.g. configuration database A of VNC node A. Administration of the one or more virtual networks operated by VNC may proceed by uniform user interface that provides a single point of administration which may reduce an administration cost of the one or more virtual networks.

VNC node A may include a control plane virtual machine VM A that executes control plane protocols to facilitate the distributed VNC techniques described herein. Control plane VM A may in some instances represent a native process. In the illustrated example control VM A executes BGP A to provide information related to the first set of elements managed by VNC node A to e.g. control plane virtual machine N of VNC node N. Control plane VM A may use an open standards based protocol e.g. BGP based L3VPN to distribute information about its virtual network s with other control plane instances and or other third party networking equipment s . Given the peering based model according to one or more aspects described herein different control plane instances e.g. different instances of control plane VMs A N may execute different software versions. In one or more aspects e.g. control plane VM A may include a type of software of a particular version and the control plane VM N may include a different version of the same type of software. The peering configuration of the control node devices may enable use of different software versions for the control plane VMs A N. The execution of multiple control plane VMs by respective VNC nodes may prevent the emergence of a single point of failure.

Control plane VM A communicates with virtual network switches e.g. illustrated VM switch executed by server using a communication protocol operating over network . Virtual network switches facilitate overlay networks in the one or more virtual networks. In the illustrated example control plane VM A uses Extensible Messaging and Presence Protocol XMPP A to communicate with at least virtual network switch by XMPP interface A. Virtual network route data statistics collection logs and configuration information may in accordance with XMPP A be sent as XML documents for communication between control plane VM A and the virtual network switches. Control plane VM A may in turn route data to other XMPP servers such as an analytics collector or may retrieve configuration information on behalf of one or more virtual network switches. Control plane VM A may further execute a communication interface A for communicating with configuration virtual machine VM A associated with configuration database A. Communication interface A may represent an IF MAP interface.

VNC node A may further include configuration VM A to store configuration information for the first set of element to and manage configuration database A. Configuration VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A. Configuration VM A and control plane VM A may communicate using IF MAP by communication interface A and using XMPP by communication interface A. In some aspects configuration VM A may include a horizontally scalable multi tenant IF MAP server and a distributed hash table DHT based IF MAP database that represents configuration database A. In some aspects configuration VM A may include a configuration translator which may translate a user friendly higher level virtual network configuration to a standards based protocol configuration e.g. a BGP L3VPN configuration which may be stored using configuration database A. Communication interface may include an IF MAP interface for communicating with other network elements. The use of the IF MAP may make the storage and management of virtual network configurations very flexible and extensible given that the IF MAP schema can be dynamically updated. Advantageously aspects of virtual network controller may be flexible for new applications .

VNC node A may further include an analytics virtual machine VM A to store diagnostic information and or visibility information related to at least the first set of elements managed by VNC node A. Control plane VM and analytics VM may communicate using an XMPP implementation by communication interface A. Analytics VM A although described as a virtual machine may in some aspects represent a native process executing on an operating system of VNC node A.

Analytics VM A may include analytics database A which may represent an instance of a distributed database that stores visibility data for virtual networks such as one of distributed database of distributed virtual network controller of . Visibility information may describe visibility of both distributed VNC itself and of customer networks. The distributed database may include an XMPP interface on a first side and a REST JASON XMPP interface on a second side.

Virtual network switch may implement the layer 3 forwarding and policy enforcement point for one or more end points and or one or more hosts. The one or more end points or one and or one or more hosts may be classified into a virtual network due to configuration from control plane VM A. Control plane VM A may also distribute virtual to physical mapping for each end point to all other end points as routes. These routes may give the next hop mapping virtual IP to physical IP and encapsulation technique used e.g. one of IPinIP NVGRE VXLAN etc. . Virtual network switch may be agnostic to actual tunneling encapsulation used. Virtual network switch may also trap interesting layer 2 L2 packets broadcast packets and or implement proxy for the packets e.g. using one of Address Resolution Protocol ARP Dynamic Host Configuration Protocol DHCP Domain Name Service DNS etc.

In some cases different VNC nodes may be provided by different suppliers. However the peering configuration of VNC nodes may enable use of different hardware and or software provided by different suppliers for implementing the VNC nodes of distributed VNC . A system operating according to the techniques described above may provide logical view of network topology to end host irrespective of physical network topology access type and or location. Distributed VNC provides programmatic ways for network operators and or applications to change topology to affect packet forwarding and or to add services as well as horizontal scaling of network services e.g. firewall without changing the end host view of the network.

In accordance with the techniques of this disclosure analytics VM which may also be referred to herein as analytics engines analyze the status of the physical network indicated by network which may include IP fabric . Network may include for example switches routers and connectors. Analytics VM includes for example analytics databases and memory not shown that may be linearly scaled via same virtual network forming the network . Analytics VM are connected to network via connectors A N. The system of includes various servers including server which may be servers such as described in as servers . Server includes a virtual switch also sometimes referred to as a virtual network router VN router which encapsulates and forwards the application packets over the physical network and a virtual network agent VN switch agent which provides the intelligence to virtual switch by talking to virtual network controller and provides statistics to analytics VM . The virtual switch hides the physical network from the physical switches and routers as found in the IP fabric of network . Thus it can appear that for example server A is directly connected to server N . Servers also include a series of guest virtual machines VM . In some examples analytics VM are actually some instances of VMs .

One method for determining latency in a multi path routing environment is to collect statistics on every virtual switch that is every virtual switch collects data on which packets have travelled through the virtual switch and when they travelled through the virtual switch between servers . The virtual switch then sends this data to one of the analytics VM . The analytics VMs can use the data to calculate latency. For example the administrator may specify that statistics are to be captured for a certain class of traffic and may also restrict the period of time over which statistic are collected. To capture a certain class of traffic the packet can be used like a match list called a packet classifier.

This will capture web traffic to load balancer and traffic from the load balancer that are sent to firewall starting on 8 am Dec. 5 2012 for one minute. This classifier can be set by web console to analytics VM which will inform all relevant VN switch agents .

The Analytics VM in turn receives the message and notifies and delivers the packet classifier and time period securely to the appropriate VN switch agent in the network . Analytics VM can identify which VN switch agents need to be notified based on the packet classifier such as based on a comparison of the IP addresses in the packet classifier relative to which subnets are owned by the VN switch agent . Each of the notified VN switch agents can install this packet classifier on their respective virtual switch to capture the appropriate packets e.g. at their egress interface. Each virtual switch thus can enable the received packet classifier at the specified start time. If the end time is in the past virtual switch can ignore this packet classifier. If start time is in the past virtual switch can enable the packet classifier immediately. The virtual switch will disable the packet classifier at the end time.

When a packet traveling in the network matches a packet classifier on the virtual switch the virtual switch sends a copy of the packet to a slow path for processing at the VN switch agent without affecting delivery of the original received packet. In a networking data path of the switches and router when a packet comes to be forwarded there may exist two paths fast path and slow path. Fast path is like cached memory and determines what to do with the packet such as where to send it to without delay. If the information is not at hand for example like cache miss the packet is queued for further processing where some other program looks up database to what to do with this packet and if necessary update the fast path cache.

Usually a network device performs this flow based forwarding by caching or otherwise storing flow state for the packet flows of a given communication session between two devices. Generally upon recognizing a first packet of a new flow a network device initializes data to record the state data for the session. The VN switch agent may inspect packet flows for the sessions. In some cases the VN switch agent may comprise two forwarding paths a first path for processing a first packet of a newly established flow and a second path for inspecting and forwarding subsequent packets associated with a pre existing flow. The first path through the VN switch agent may be referred to as the first path slow path or session management path. At this time after processing the first packet of the newly established flow the VN switch agent may update flow tables to record the session and otherwise initialize session data. The second path through VN switch agent may be referred to as the fast path because the second path normally does not take as long to traverse as the first path due to the lack of detailed inspection needed for subsequent packets in an already established flow. Further details relating to network devices having a fast path and slow path can be found in U.S. Pat. No. 8 339 959 filed Jul. 30 2008 entitled Streamlined Packet Forwarding using Dynamic Filters for Routing and Security in a Shared Forwarding Plane the entire content of which is incorporated by reference herein. Virtual switch sends additional information such as a timestamp ingress port and egress port etc. to the slow path along with the copy of the packet.

As will be described in more detail below with respect to the VN switch agent calculates the packet signature and sends that with statistics to analytics VM analytics VM calculates by incoming signature and may distribute the calculation across other Analytics VM in virtual network controller and analytics VMs compiles the result and may optionally send the compiled result to the web console for display and or further use . The web console may display the result .

On the expiry of the end time plus some jitter analytics VMs will start processing each packet by the hash keys. Such initial processing may include gathering data per key and forming a list of values of the key and assigning a job per key packet hash to one of the analytics VMs across virtual network controller . That is each original packet yielded a unique hash which can be used as the key to identify each of the packets and their information. Analytics VM may obtain for each hash a list of switch ids timestamps ingress ports and egress ports etc.

For each hash analytics VM will then sort the associated list by timestamp and construct the topology map that the packet has traversed virtual network topology map based on the list of switch ids and timestamps and match the topology map up with the known physical topology of the network . The virtual network topology map includes a topology map of virtual switches based on the virtual switch ids. As analytics VMs are linearly scaled each gets a part of the job to process and determine the results. Near consistency of the timestamp is assumed to allow the clock drifts.

Next the analytic engine identifies the source and destination of this packet represented by the hash and this hash can be broken down as n distinct flows . Then on each of the flows analytics VM generates the path list which consists of switch 1 switch 2 . . . switch r which are the specific physical switches that the packet traversed. Analytics VM generates a hash on this switch list or path map which is used as the key for the subsequent calculation. For each path map hash the near consistent time that the packet took from its source to its destination can be determined. The expected error is also calculated which will be used to calculate the jitter or latency per path.

With the path map hash all the flows can be combined detected for a path map and from there analytics VM can compute the statistical measure of the latency . By combining across the packet classifier analytics VMs can determine the real latency by evaluating minimum maximum mean and standard deviation per path in this network.

Using a collection of such latency information virtual network controller can identify places in the physical network that are slow or where bottlenecks in traffic are occurring. Such a bottleneck may be indicative of a problem with the physical network such as for example a deteriorated cable. Identifying such problems in the physical network without having to run specific testing on each of the components of the network saves time and money and can help ensure that the network performs optimally and without interruption.

Additionally the method can be used with any sets of physical switches provided that for each physical switch in the set there is an associated VN agent capable of receiving the PC identifying and hashing the identified packets and forwarding it to an analytics engine for further processing as described above.

Various embodiments are described herein including methods and techniques. Techniques of this disclosure may also be used in an article of manufacture that includes a non transitory computer readable medium on which computer readable instructions for carrying out embodiments of the inventive technique are stored. The computer readable medium may include for example semiconductor magnetic opto magnetic optical or other forms of computer readable medium for storing computer readable code. Further the invention may also cover apparatuses for practicing embodiments of the invention. Such apparatus may include circuits dedicated and or programmable to carry out operations pertaining to embodiments of the invention. Examples of such apparatus include a general purpose computer and or a dedicated computing device when appropriately programmed and may include a combination of a computer computing device and dedicated programmable hardware circuits such as electrical mechanical and or optical circuits adapted for the various operations pertaining to embodiments of the invention.

As shown in the specific example of computing device includes one or more processors one or more communication units one or more input devices one or more output devices and one or more storage devices . Computing device in the specific example of further includes operating system virtualization module and one or more applications A N collectively applications . Each of components and may be interconnected physically communicatively and or operatively for inter component communications. As one example in components and may be coupled by one or more communication channels . In some examples communication channels may include a system bus network connection interprocess communication data structure or any other channel for communicating data. Virtualization module and applications as well as operating system may also communicate information with one another as well as with other components in computing device . Virtualization may allow the functions of these components to be distributed over multiple machines or multiple virtual machines while a hypervisor gives the appearance of single component.

Processors in one example are configured to implement functionality and or process instructions for execution within computing device . For example processors may be capable of processing instructions stored in storage devices . Examples of processors may include any one or more of a microprocessor a controller a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or equivalent discrete or integrated logic circuitry.

One or more storage devices may be configured to store information within computing device during operation. Storage devices in some examples are described as a computer readable storage medium. In some examples storage devices are a temporary memory meaning that a primary purpose of storage devices is not long term storage. Storage devices in some examples are described as a volatile memory meaning that storage devices do not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories RAM dynamic random access memories DRAM static random access memories SRAM and other forms of volatile memories known in the art. In some examples storage devices are used to store program instructions for execution by processors . Storage devices in one example are used by software or applications running on computing device e.g. operating system virtualization module and the like to temporarily store information during program execution.

Storage devices in some examples also include one or more computer readable storage media. Storage devices may be configured to store larger amounts of information than volatile memory. Storage devices may further be configured for long term storage of information. In some examples storage devices include non volatile storage elements. Examples of such non volatile storage elements include magnetic hard discs tape cartridges or cassettes optical discs floppy discs flash memories or forms of electrically programmable memories EPROM or electrically erasable and programmable memories EEPROM .

Computing device in some examples also includes one or more communication units . Computing device in one example utilizes communication units to communicate with external devices. Communication units may communicate in some examples by sending data packets over one or more networks such as one or more wireless networks via inbound and outbound links. Communication units may include one or more network interface cards IFCs such as an Ethernet card an optical transceiver a radio frequency transceiver or any other type of device that can send and receive information. Other examples of such network interfaces may include Bluetooth 3G and WiFi radio components. In some examples computing device utilizes communication units to communicate with other network devices such as to send or receive packet signatures as described herein.

Computing device in one example also includes one or more input devices . Input devices in some examples are configured to receive input from a user through tactile audio or video feedback. Examples of input devices include a presence sensitive display a mouse a keyboard a voice responsive system video camera microphone or any other type of device for detecting a command from a user. In some examples a presence sensitive display includes a touch sensitive screen.

One or more output devices may also be included in computing device . Output devices in some examples are configured to provide output to a user using tactile audio or video stimuli. Output devices in one example include a presence sensitive display a sound card a video graphics adapter card or any other type of device for converting a signal into an appropriate form understandable to humans or machines. Additional examples of output devices include a speaker a cathode ray tube CRT monitor a liquid crystal display LCD or any other type of device that can generate intelligible output to a user.

Computing device may include operating system . Operating system in some examples controls the operation of components of computing device . For example operating system in one example facilitates the communication of modules applications with processors communication units input devices output devices and storage devices . Applications may each include program instructions and or data that are executable by computing device . As one example application A may include instructions that cause computing device to perform one or more of the operations and actions described in the present disclosure.

In accordance with techniques of the present disclosure computing device may operate in accordance with the example processes described in .

The techniques described in this disclosure may be implemented at least in part in hardware software firmware or any combination thereof. For example various aspects of the described techniques may be implemented within one or more processors including one or more microprocessors digital signal processors DSPs application specific integrated circuits ASICs field programmable gate arrays FPGAs or any other equivalent integrated or discrete logic circuitry as well as any combinations of such components. The term processor or processing circuitry may generally refer to any of the foregoing logic circuitry alone or in combination with other logic circuitry or any other equivalent circuitry. A control unit including hardware may also perform one or more of the techniques of this disclosure.

Such hardware software and firmware may be implemented within the same device or within separate devices to support the various techniques described in this disclosure. In addition any of the described units modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware firmware or software components. Rather functionality associated with one or more modules or units may be performed by separate hardware firmware or software components or integrated within common or separate hardware firmware or software components.

The techniques described in this disclosure may also be embodied or encoded in an article of manufacture including a computer readable storage medium encoded with instructions. Instructions embedded or encoded in an article of manufacture including a computer readable storage medium encoded may cause one or more programmable processors or other processors to implement one or more of the techniques described herein such as when instructions included or encoded in the computer readable storage medium are executed by the one or more processors. Computer readable storage media may include random access memory RAM read only memory ROM programmable read only memory PROM erasable programmable read only memory EPROM electronically erasable programmable read only memory EEPROM flash memory a hard disk a compact disc ROM CD ROM a floppy disk a cassette magnetic media optical media or other computer readable storage media. In some examples an article of manufacture may include one or more computer readable storage media.

A computer readable storage medium comprises a non transitory medium. The term non transitory indicates that the storage medium is not embodied in a carrier wave or a propagated signal. In certain examples a non transitory storage medium may store data that can over time change e.g. in RAM or cache .

Various examples have been described. These and other examples are within the scope of the following claims.

